import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional


def train_model(X_train, y_train, X_test, y_test, epochs=50):
    # Tokenizer 생성 및 텍스트 시퀀스로 변환
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(X_train)

    X_train_sequences = tokenizer.texts_to_sequences(X_train)
    X_test_sequences = tokenizer.texts_to_sequences(X_test)

    max_sequence_len = max(len(seq) for seq in X_train_sequences)
    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_len, padding='post', truncating='post')
    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_len, padding='post', truncating='post')

    vocab_size = len(tokenizer.word_index) + 1

    # 모델 생성
    model = Sequential([
        Embedding(vocab_size, 64, input_length=max_sequence_len),
        Bidirectional(LSTM(64)),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')  # Binary classification, so only 1 output neuron
    ])

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 데이터의 형식을 변경하여 모델에 입력
    X_train_padded = np.array(X_train_padded)  # 넘파이 배열로 변환
    X_test_padded = np.array(X_test_padded)  # 넘파이 배열로 변환

    # 모델 학습
    model.fit(X_train_padded, np.array(y_train), epochs=epochs, validation_data=(X_test_padded, np.array(y_test)))

    return model, tokenizer, max_sequence_len
