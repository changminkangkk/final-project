__label__0 raise typeerror ( f'object { obj } is not json-serializable . you may implement ' ' a ` get_config ( ) ` method on the class ' ' ( returning a json-serializable dictionary ) to make it ' 'serializable . ' )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def __enter__ ( self ) : self._lock.acquire ( self._group_id )
__label__0 def testduplicatedarg2 ( self ) : with self.assertraisesregex ( typeerror , `` got multiple values for argument 'transpose_a ' '' ) : self._matmul_func.canonicalize ( 2 , 3 , false , transpose_a=true )
__label__0 args : sv : a ` supervisor ` . sess : a ` session ` . `` '' '' super ( svtimercheckpointthread , self ) .__init__ ( sv.coord , sv.save_model_secs ) self._sv = sv self._sess = sess
__label__0 def __init__ ( self , fence_label='python ' ) : super ( ) .__init__ ( )
__label__0 def getmro ( cls ) : `` '' '' tfdecorator-aware replacement for inspect.getmro . '' '' '' return _inspect.getmro ( cls )
__label__0 you number checkpoint filenames by passing a value to the optional ` global_step ` argument to ` save ( ) ` :
__label__0 with self.assertraisesregex ( valueerror , 'keyword-only arguments ' ) : tf_inspect.getargspec ( partial_func )
__label__0 a few composite indexes are created ( upload_test_benchmarks_index.yaml ) for fast retrieval of benchmark data and reduced i/o to the client without adding a lot of indexing and storage burden :
__label__0 class foohasnokwargs ( object ) :
__label__0 `` `` '' import argparse from builtins import bytes # pylint : disable=redefined-builtin import json import os import shutil import subprocess
__label__0 def testpreparesessionwithcyclicinitializer ( self ) : # regression test . previously variable._build_initializer_expr would enter # into an infinite recursion when the variable 's initial_value involved # cyclic dependencies . with ops.graph ( ) .as_default ( ) : i = while_loop.while_loop ( lambda i : i < 1 , lambda i : i + 1 , [ 0 ] ) v = variable_v1.variablev1 ( array_ops.identity ( i ) , name= '' v '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) sess = sm.prepare_session ( `` '' , init_op=v.initializer ) self.assertequal ( 1 , sess.run ( v ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) )
__label__0 table_header = textwrap.dedent ( `` '' ''
__label__0 the ` tf.train.sequenceexample ` proto can be thought of as a proto implementation of the following python type :
__label__0 > > > from google.protobuf import text_format > > > example = text_format.parse ( `` ' ... features { ... feature { key : `` my_feature '' ... value { float_list { value : [ 1. , 2. , 3. , 4 . ] } } } ... } '' ' , ... tf.train.example ( ) ) > > > > > > example.features.feature [ 'my_feature ' ] .float_list.value [ 1.0 , 2.0 , 3.0 , 4.0 ]
__label__0 @ property def global_step ( self ) : `` '' '' return the global_step tensor used by the supervisor .
__label__0 # pruning first n_prune_args arguments . args = args [ n_prune_args : ]
__label__0 args : structure : the nested structure to flatten . separator : string to separate levels of hierarchy in the results , defaults to '/ ' . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 returns : a string : path prefix used for the checkpoint files . if the saver is sharded , this string ends with : '- ? ? ? ? ? -of-nnnnn ' where 'nnnnn ' is the number of shards created . if the saver is empty , returns none .
__label__0 def __init__ ( self , func ) : self._func = func self._cache = { }
__label__0 def __init ( self ) : pass
__label__0 def testimportintonamescope ( self ) : # test that we can import a meta graph into a namescope . test_dir = self._get_test_dir ( `` import_into_namescope '' ) filename = os.path.join ( test_dir , `` ckpt '' ) # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) : image = array_ops.placeholder ( dtypes.float32 , [ none , 784 ] , name= '' image '' ) label = array_ops.placeholder ( dtypes.float32 , [ none , 10 ] , name= '' label '' ) with session.session ( ) as sess : weights = variable_v1.variablev1 ( random_ops.random_uniform ( [ 784 , 10 ] ) , name= '' weights '' ) bias = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' bias '' ) logit = nn_ops.relu ( math_ops.matmul ( image , weights ) + bias , name= '' logits '' ) nn_ops.softmax ( logit , name= '' prediction '' ) cost = nn_ops.softmax_cross_entropy_with_logits ( labels=label , logits=logit , name= '' cost '' ) adam.adamoptimizer ( ) .minimize ( cost , name= '' optimize '' ) saver = saver_module.saver ( ) self.evaluate ( variables.global_variables_initializer ( ) ) saver.save ( sess , filename )
__label__0 # # # checkpoint management in tf2
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _object ( ) ._fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 def testmanagedendofinputtwoqueues ( self ) : # tests that the supervisor finishes without an error when using # a fixed number of epochs , reading from two queues , the second # one producing a batch from the first one . logdir = self._test_dir ( `` managed_end_of_input_two_queues '' ) os.makedirs ( logdir ) data_path = self._csv_data ( logdir ) with ops.graph ( ) .as_default ( ) : # create an input pipeline that reads the file 3 times . filename_queue = input_lib.string_input_producer ( [ data_path ] , num_epochs=3 ) reader = io_ops.textlinereader ( ) _ , csv = reader.read ( filename_queue ) rec = parsing_ops.decode_csv ( csv , record_defaults= [ [ 1 ] , [ 1 ] , [ 1 ] ] ) shuff_rec = input_lib.shuffle_batch ( rec , 1 , 6 , 4 ) sv = supervisor.supervisor ( logdir=logdir ) with sv.managed_session ( `` '' ) as sess : while not sv.should_stop ( ) : sess.run ( shuff_rec )
__label__0 # assert calling new fn with default deprecated value issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 # fetch params to validate initial values self.assertallclose ( [ 1.0 , 2.0 ] , self.evaluate ( var0 ) ) self.assertallclose ( [ 3.0 , 4.0 ] , self.evaluate ( var1 ) )
__label__0 @ test_util.run_all_in_graph_and_eager_modes class dispatchtest ( test_util.tensorflowtestcase ) :
__label__0 def testwaitforsessionlocalinit ( self ) : server = server_lib.server.create_local_server ( ) with ops.graph ( ) .as_default ( ) as graph : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) sm = session_manager.sessionmanager ( graph=graph , ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( variables.global_variables ( ) ) , local_init_op=w.initializer )
__label__0 import collections import collections.abc import dataclasses import time from typing import namedtuple
__label__0 def discard ( self , key ) : self._storage.discard ( self._wrap_key ( key ) )
__label__0 # the next one has the graph . ev = next ( rr ) self.asserttrue ( ev.graph_def )
__label__0 def is_nested_or_composite ( seq ) : `` '' '' returns true if its input is a nested structure or a composite .
__label__0 if parent_path [ -1 ] not in [ `` tf '' , `` v1 '' , `` v2 '' ] : return new_children
__label__0 # restore the saved values in the parameter nodes . save.restore ( sess , save_path2 ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 args : structure : an atom or a nested structure . note , numpy arrays are considered atoms and are not flattened . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 import_header = ( `` import tensorflow.compat.v1 as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) text = import_header + old_symbol expected_header = ( `` import tensorflow as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) expected_text = expected_header + new_symbol _ , _ , _ , new_text = self._upgrade ( text , import_rename=false , upgrade_compat_v1_import=true ) self.assertequal ( new_text , expected_text )
__label__0 > > > tf.nest.map_structure ( lambda x : x * 2 , mt ) maskedtensor ( mask=true , value= < tf.tensor : ... numpy=array ( [ 2 ] , dtype=int32 ) > )
__label__0 # the next one should also have the values from the summary . ev = next ( rr ) self.assertprotoequals ( `` '' '' value { tag : 'c1 ' simple_value : 1.0 } value { tag : 'c2 ' simple_value : 2.0 } value { tag : 'c3 ' simple_value : 3.0 } `` '' '' , ev.summary )
__label__0 is_namedtuple = nest_util.is_namedtuple _is_namedtuple = nest_util.is_namedtuple _is_attrs = _pywrap_utils.isattrs _is_mapping = _pywrap_utils.ismapping same_namedtuples = nest_util.same_namedtuples
__label__0 the method returns ` true ` if both the text comparison and the numeric comparison are successful .
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' run doctests for tensorflow . '' '' ''
__label__0 def __init__ ( self , proto , proto_size , size_check=_greedy_split , * * kwargs ) : `` '' '' initializer . '' '' ''
__label__0 def _testnonreshape ( self , variable_op ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` non_reshape '' )
__label__0 self.assertequal ( { ' a ' : 5 } , tf_inspect.getcallargs ( func , 5 ) )
__label__0 def test_tfn_argmax ( self ) : @ def_function.function def wrapped_fn ( x ) : return math_ops.argmax ( x , axis=2 )
__label__0 class tfnamescore ( namedtuple ) : canonical_score : int name_score : doc_generator_visitor.docgeneratorvisitor.namescore
__label__0 nvcc_name = `` nvcc.exe '' if _is_windows ( ) else `` nvcc '' nvcc_path , nvcc_version = _find_versioned_file ( base_paths , [ `` '' , `` bin '' , `` local/cuda/bin '' , ] , nvcc_name , cuda_version , get_nvcc_version )
__label__0 mt3 = maskedtensor2 ( mask=true , value=constant_op.constant ( [ 1 ] ) ) # these assertions are expected to pass : two dataclasses with the same # component size are considered to have the same shallow structure . nest.assert_shallow_structure ( shallow_tree=mt , input_tree=mt3 , check_types=false ) nest.assert_shallow_structure ( shallow_tree=mt3 , input_tree=mt , check_types=false )
__label__0 args : node : current node `` '' '' for import_alias in node.names : # detect based on full import name and alias if ( import_alias.name == `` tensorflow.compat.v1 '' and import_alias.asname == `` tf '' ) : import_alias.name = `` tensorflow '' self.generic_visit ( node )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_one_line_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 # note : the file locking code uses flock ( ) instead of lockf ( ) because benchmark # files are only opened for reading ( not writing ) and we still want exclusive # locks on them . this imposes the limitation that the data directory must be # local , not nfs-mounted . def lock ( fd ) : fcntl.flock ( fd , fcntl.lock_ex )
__label__0 for symbol , replacement in all_renames_v2.addons_symbol_mappings.items ( ) : warning = ( ast_edits.warning , ( `` ( manual edit required ) ` { } ` has been migrated to ` { } ` in `` `` tensorflow addons . the api spec may have changed during the `` `` migration . please see https : //github.com/tensorflow/addons `` `` for more info . `` ) .format ( symbol , replacement ) ) self.function_warnings [ symbol ] = warning
__label__0 def testinitwithnonelocalinitoperror ( self ) : # creating a sessionmanager with a none local_init_op but # non-none ready_for_local_init_op raises valueerror with self.assertraisesregex ( valueerror , `` if you pass a ready_for_local_init_op `` `` you must also pass a local_init_op `` ) : session_manager.sessionmanager ( ready_for_local_init_op=variables.report_uninitialized_variables ( variables.global_variables ( ) ) , local_init_op=none )
__label__0 class raggedtensorspec ( object ) : `` '' '' interface for internal isinstance checks to ops/ragged/ragged_tensor.py .
__label__0 def __get__ ( self , instance , owner ) : return self
__label__0 def update_xla_tsl_imports ( srcs_dir : str ) - > none : `` '' '' workaround for tsl and xla vendoring . '' '' '' replace_inplace ( srcs_dir , `` from tsl '' , `` from tensorflow.tsl '' ) replace_inplace ( srcs_dir , `` from local_xla.xla '' , `` from tensorflow.compiler.xla '' , ) replace_inplace ( srcs_dir , `` from xla '' , `` from tensorflow.compiler.xla '' )
__label__0 def run_loop ( self ) : logging.info ( `` saving checkpoint to path % s '' , self._sv.save_path ) self._sv.saver.save ( self._sess , self._sv.save_path , global_step=self._sv.global_step ) if self._sv.summary_writer and self._sv.global_step is not none : current_step = training_util.global_step ( self._sess , self._sv.global_step ) self._sv.summary_writer.add_session_log ( sessionlog ( status=sessionlog.checkpoint , checkpoint_path=self._sv.save_path ) , current_step )
__label__0 class basesaverbuilder : `` '' '' base class for savers .
__label__0 if modified : node.keywords = new_keywords return modified
__label__0 example results : showing runtimes in microseconds . ` ? ` means not available . model , batch , vanilla , onednn , speedup bert-large , 1 , x , y , x/y bert-large , 16 , ... , ... , ... inception , 1 , ... , ... , ... inception , 16 , ... , ... , ... ⋮ ssd-resnet34 , 1 , ? , ... , ? ssd-resnet34 , 16 , ? , ... , ?
__label__0 saver = _create_saver_from_imported_meta_graph ( meta_graph_def , import_scope , imported_vars ) return saver , imported_return_elements
__label__0 with self.cached_session ( ) as sess : # build a graph with 1 node , and save and restore for them . v = variable_v1.variablev1 ( np.int64 ( 15 ) , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } , restore_sequentially=true ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 return { `` cudnn_version '' : cudnn_version , `` cudnn_include_dir '' : os.path.dirname ( header_path ) , `` cudnn_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 1. create a keras optimizer , which generates an ` iterations ` variable . this variable is automatically incremented when calling ` apply_gradients ` . 2. manually create and increment a ` tf.variable ` .
__label__0 with ` expand_composites=true ` , we return the component tensors that make up the raggedtensor representation ( the values and row_splits tensors )
__label__0 super ( hiddentfapiattribute , self ) .__init__ ( raise_error )
__label__0 uid = uuid.uuid4 ( ) .hex
__label__0 # save the initialized values in the file at `` save_path '' val = save.save ( sess , save_path ) self.assertisinstance ( val , str ) self.assertequal ( save_path , val )
__label__0 @ staticmethod def parse_from_string ( string , version_type ) : `` '' '' returns version object from semver string .
__label__0 parser.add_argument ( `` -- gen_root_path '' , type=str , help= '' root path to place generated git files ( created by -- configure ) . '' )
__label__0 returns : true if symbol has deprecation decorator. `` '' '' decorators , symbol = tf_decorator.unwrap ( symbol ) if contains_deprecation_decorator ( decorators ) : return true if tf_inspect.isfunction ( symbol ) : return false if not tf_inspect.isclass ( symbol ) : return false if not hasattr ( symbol , '__init__ ' ) : return false init_decorators , _ = tf_decorator.unwrap ( symbol.__init__ ) return contains_deprecation_decorator ( init_decorators )
__label__0 args : op : a callable with whose signature the returned function is compatible . func : a callable which is called by the returned function .
__label__0 self.asserttrue ( function_utils.has_kwargs ( double_wrapped_fn ) ) some_kwargs = dict ( x=1 , y=2 , z=3 ) self.assertequal ( double_wrapped_fn ( * * some_kwargs ) , some_kwargs )
__label__0 inner_decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , argspec ) outer_decorator = tf_decorator.tfdecorator ( `` , inner_decorator ) self.assertequal ( argspec , tf_inspect.getfullargspec ( outer_decorator ) )
__label__0 # lets worker 0 execute first . # it will wait as we need 2 workers to finish this step and the global step # should be still 1. thread_0.start ( ) self.assertallequal ( 1 , sessions [ 1 ] .run ( global_step ) )
__label__0 self.assertequal ( 3 , func_without_decorator ( 1 , 2 ) ) self.assertequal ( 3 , func_without_decorator ( a=1 , b=2 ) ) self.assertequal ( 3 , func_with_decorator ( a=1 , b=2 ) )
__label__0 def testunboundfuncwithtwoparamsdefaulttwokeywords ( self ) :
__label__0 # pass name w/ keyword arg a = math_ops.add ( x , y , name= '' myadd '' ) if not context.executing_eagerly ( ) : # names not defined in eager mode . self.assertregex ( a.values.name , r '' ^myadd/add . * '' ) self.assertregex ( a.mask.name , r '' ^myadd/and . * '' )
__label__0 # validate that the number of lines is the same assert len ( original_raw_lines ) == len ( updated_code_lines ) , \ ( `` the lengths of input and converted files are not the same : `` `` { } vs { } '' .format ( len ( original_raw_lines ) , len ( updated_code_lines ) ) )
__label__0 def fn ( ) : wrapped_fn ( 0.5 )
__label__0 def remove_undocumented ( module_name , allowed_exception_list=none , doc_string_modules=none ) : `` '' '' removes symbols in a module that are not referenced by a docstring .
__label__0 # otherwise delete the files . try : checkpoint_management.remove_checkpoint ( self._checkpointfilename ( p ) , self.saver_def.version , meta_graph_suffix ) except exception as e : # pylint : disable=broad-except logging.warning ( `` ignoring : % s '' , str ( e ) )
__label__0 class _floatextractor ( object ) : `` '' '' class for extracting floats from a string .
__label__0 goodbye `` '' '' ) , ( 'skip-wrong-label ' , [ ] , `` '' '' hello
__label__0 if not self.saver_def or context.executing_eagerly ( ) : if self._builder is none : self._builder = bulksaverbuilder ( self._write_version )
__label__0 the script searches for cuda library and header files on the system , inspects them to determine their version and prints the configuration to stdout . the paths to inspect and the required versions are specified through environment variables . if no valid configuration is found , the script prints to stderr and returns an error code .
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` apple '' , `` banana '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_output_layer = variable_scope.get_variable ( `` fruit_output_layer '' , initializer= [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] ) ws_util._warm_start_var_with_vocab ( fruit_output_layer , new_vocab_path , current_vocab_size=3 , prev_ckpt=self.get_temp_dir ( ) , prev_vocab_path=prev_vocab_path , axis=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( [ [ 0.3 , 0.5 , 0 . ] , [ 0.8 , 1.0 , 0 . ] , [ 1.2 , 1.5 , 0 . ] , [ 2.3 , 2. , 0 . ] ] , fruit_output_layer.eval ( sess ) )
__label__0 # tf.train.featurelists featurelists = dict [ str , featurelist ]
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 __slots__ = ( `` fn '' , `` proto_size '' )
__label__0 redirects_path = output_dir / `` tf/_redirects.yaml '' with edit_yaml_file ( redirects_path ) as redirects : redirects [ `` redirects '' ] .append ( { `` from '' : str ( site_path / `` tf_overview '' ) , `` to '' : str ( site_path / `` tf '' ) , } )
__label__0 applicable_lines = [ idx for idx , code_line in enumerate ( original_raw_lines ) if code_line.cell_number == code_cell_idx ]
__label__0 def get_field_tag ( proto : message.message , fields : fieldtypes ) - > sequence [ chunk_pb2.fieldindex ] : `` '' '' generates fieldindex proto for a nested field within a proto .
__label__0 args : obj : python object. `` '' '' return ( hasattr ( obj , '_tf_decorator ' ) and isinstance ( getattr ( obj , '_tf_decorator ' ) , tfdecorator ) )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( arg0 , arg1 , deprecated=true ) : `` '' '' fn doc .
__label__0 # generates metagraphdef . meta_graph_def = save.export_meta_graph ( ) ops = [ o.name for o in meta_graph_def.meta_info_def.stripped_op_list.op ] if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( ops , [ `` addv2 '' , `` assign '' , `` const '' , `` identity '' , `` noop '' , `` placeholderwithdefault '' , `` restorev2 '' , `` saveslices '' , `` sub '' , `` variablev2 '' ] ) else : self.assertequal ( ops , [ `` addv2 '' , `` assign '' , `` const '' , `` identity '' , `` noop '' , `` placeholderwithdefault '' , `` restorev2 '' , `` savev2 '' , `` sub '' , `` variablev2 '' ] )
__label__0 # stored metagraphdef ev = next ( rr ) ev_meta_graph = meta_graph_pb2.metagraphdef ( ) ev_meta_graph.parsefromstring ( ev.meta_graph_def ) self.assertprotoequals ( meta_graph_def , ev_meta_graph ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_meta_graph.graph_def )
__label__0 compat_v1_import = versionedtfimport ( `` compat.v1 '' ) compat_v2_import = versionedtfimport ( `` compat.v2 '' )
__label__0 _fn ( arg0=0 ) self.assertequal ( 1 , mock_warning.call_count ) _fn ( arg0=0 ) self.assertequal ( 1 , mock_warning.call_count ) _fn ( arg1=0 ) self.assertequal ( 2 , mock_warning.call_count ) _fn ( arg0=0 ) self.assertequal ( 2 , mock_warning.call_count ) _fn ( arg1=0 ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 hlo_str = f.experimental_get_compiler_ir ( tf.tensorspec ( shape= ( 10 , 20 ) ) ) ( stage='hlo ' ) `` `
__label__0 # nested dicts , ordereddicts and namedtuples . input_tree = collections.ordereddict ( [ ( `` a '' , ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) ) , ( `` c '' , { `` d '' : 3 , `` e '' : collections.ordereddict ( [ ( `` f '' , 4 ) ] ) } ) ] ) shallow_tree = input_tree input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ 0 , 1 , 2 , 3 , 4 ] ) shallow_tree = collections.ordereddict ( [ ( `` a '' , 0 ) , ( `` c '' , { `` d '' : 3 , `` e '' : 1 } ) ] ) input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) , 3 , collections.ordereddict ( [ ( `` f '' , 4 ) ] ) ] ) shallow_tree = collections.ordereddict ( [ ( `` a '' , 0 ) , ( `` c '' , 0 ) ] ) input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) , { `` d '' : 3 , `` e '' : collections.ordereddict ( [ ( `` f '' , 4 ) ] ) } ] )
__label__0 def testnndilation2d ( self ) : text = `` tf.nn.dilation2d ( v , k , s , r , p ) '' expected_text = `` tf.nn.dilation2d ( v , k , s , r , p , data_format='nhwc ' ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def testcallingatfdecoratorcallsthetarget ( self ) : self.assertequal ( 124 , tf_decorator.tfdecorator ( `` , test_function ) ( 123 ) )
__label__0 field_values = [ 1 , 2 ] sample_attr = nesttest.sampleattr ( * field_values ) self.assertfalse ( nest._is_attrs ( field_values ) ) self.asserttrue ( nest._is_attrs ( sample_attr ) ) flat = nest.flatten ( sample_attr ) self.assertequal ( field_values , flat ) restructured_from_flat = nest.pack_sequence_as ( sample_attr , flat ) self.assertisinstance ( restructured_from_flat , nesttest.sampleattr ) self.assertequal ( restructured_from_flat , sample_attr )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( arg0 , arg1 , deprecated=true ) : `` '' '' fn doc . '' '' '' return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 3. for a nested dictionary of dictionaries :
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.immutableconst . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 @ atheris.instrument_func def testoneinput ( input_bytes ) : `` '' '' test randomized integer fuzzing input for tf.raw_ops.dataformatvecpermute . '' '' '' fh = fuzzinghelper ( input_bytes )
__label__0 > > > @ dispatch_for_api ( tf.concat , { 'values ' : typing.list [ maskedtensor ] } ) ... def masked_concat ( values , axis ) : ... return maskedtensor ( tf.concat ( [ v.values for v in values ] , axis ) , ... tf.concat ( [ v.mask for v in values ] , axis ) )
__label__0 args : filename_tensor : tensor for the path of the file to load . per_device : a list of ( device , saveableobject ) pairs , as returned by _groupbydevices ( ) . restore_sequentially : true if we want to restore variables sequentially within a shard . reshape : true if we want to reshape loaded tensors to the shape of the corresponding variable .
__label__0 def getcallargs ( * func_and_positional , * * named ) : `` '' '' tfdecorator-aware replacement for inspect.getcallargs .
__label__0 _ , undecorated_func = tf_decorator.unwrap ( func ) self.set_attr ( undecorated_func , api_names_attr , self._names ) self.set_attr ( undecorated_func , api_names_attr_v1 , self._names_v1 )
__label__0 4. numpy array ( considered a scalar ) :
__label__0 def __tf_flatten__ ( self ) : metadata = ( self.mask , ) components = ( self.value , ) return metadata , components
__label__0 if ( not context.executing_eagerly ( ) and not isinstance ( sess , session.sessioninterface ) ) : raise typeerror ( `` 'sess ' must be a session ; % s '' % sess )
__label__0 def __iter__ ( self ) : return iter ( self._wrapped )
__label__0 args : module_name : the name of the module ( usually ` __name__ ` ) . doc_string_modules : a list of modules from which to take docstring . if none , then a list containing only the module named ` module_name ` is used .
__label__0 from typing import dict
__label__0 class saverestoreshardedtestv2 ( saverestoreshardedtest ) : _write_version = saver_pb2.saverdef.v2
__label__1 class solution : def maxcoins ( self , nums : list [ int ] ) - > int : nums = [ 1 ] + nums + [ 1 ] n = len ( nums ) dp = [ [ 0 ] * n for _ in range ( n ) ] for length in range ( 1 , n ) : for i in range ( n - length ) : j = i + length - 1 for k in range ( i , j + 1 ) : coins = nums [ i ] * nums [ k ] * nums [ j + 1 ] # coins from bursting balloon at position k coins += dp [ i ] [ k - 1 ] + dp [ k + 1 ] [ j ] # coins from left and right subproblems dp [ i ] [ j ] = max ( dp [ i ] [ j ] , coins ) return dp [ 0 ] [ n - 1 ]
__label__0 def map_structure_with_tuple_paths_up_to ( shallow_tree , func , * inputs , * * kwargs ) : `` '' '' applies a function or op to a number of partially flattened inputs .
__label__0 with ops.name_scope ( none , self._name ) : for grad , var in grads_and_vars : var_list.append ( var ) with ops.device ( var.device ) : # dense gradients . if grad is none : aggregated_grad.append ( none ) # pass-through . continue elif isinstance ( grad , tensor.tensor ) : grad_accum = data_flow_ops.conditionalaccumulator ( grad.dtype , shape=var.get_shape ( ) , shared_name=var.name + `` /grad_accum '' ) train_ops.append ( grad_accum.apply_grad ( grad , local_step=self._local_step ) ) aggregated_grad.append ( grad_accum.take_grad ( self._replicas_to_aggregate ) ) else : if not isinstance ( grad , indexed_slices.indexedslices ) : raise valueerror ( `` unknown grad type ! '' ) grad_accum = data_flow_ops.sparseconditionalaccumulator ( grad.dtype , shape= ( ) , shared_name=var.name + `` /grad_accum '' ) train_ops.append ( grad_accum.apply_indexed_slices_grad ( grad , local_step=self._local_step ) ) aggregated_grad.append ( grad_accum.take_indexed_slices_grad ( self._replicas_to_aggregate ) )
__label__0 sv0 , sess0 , v0 , _ , w0 = get_session ( true ) sv1 , sess1 , _ , vadd1 , w1 = get_session ( false )
__label__0 with self.assertraisesregex ( valueerror , 'keyword-only arguments ' ) : tf_inspect.getargspec ( partial_func )
__label__0 def test_no_identity_renames ( self ) : identity_renames = [ old_name for old_name , new_name in all_renames_v2.symbol_renames.items ( ) if old_name == new_name ] self.assertempty ( identity_renames )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 yields : the iterable 's ( key , value ) pairs , in order of sorted keys. `` '' '' # ordered to check common structure types ( list , tuple , dict ) first . if isinstance ( iterable , list ) : for item in enumerate ( iterable ) : yield item # namedtuples handled separately to avoid expensive namedtuple check . elif type ( iterable ) == tuple : # pylint : disable=unidiomatic-typecheck for item in enumerate ( iterable ) : yield item elif isinstance ( iterable , ( dict , _collections_abc.mapping ) ) : # iterate through dictionaries in a deterministic order by sorting the # keys . notice this means that we ignore the original order of ` ordereddict ` # instances . this is intentional , to avoid potential bugs caused by mixing # ordered and plain dicts ( e.g. , flattening a dict but using a # corresponding ` ordereddict ` to pack it back ) . for key in _tf_core_sorted ( iterable ) : yield key , iterable [ key ] elif _is_attrs ( iterable ) : for item in _get_attrs_items ( iterable ) : yield item elif is_namedtuple ( iterable ) : for field in iterable._fields : yield field , getattr ( iterable , field ) elif _is_composite_tensor ( iterable ) : type_spec = iterable._type_spec # pylint : disable=protected-access yield type_spec.value_type.__name__ , type_spec._to_components ( iterable ) # pylint : disable=protected-access elif _is_type_spec ( iterable ) : # note : to allow compositetensors and their typespecs to have matching # structures , we need to use the same key string here . yield iterable.value_type.__name__ , iterable._component_specs # pylint : disable=protected-access elif isinstance ( iterable , customnestprotocol ) : flat_component = iterable.__tf_flatten__ ( ) [ 1 ] assert isinstance ( flat_component , tuple ) yield from enumerate ( flat_component ) else : for item in enumerate ( iterable ) : yield item
__label__0 for modality.core refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a nested structure .
__label__0 def list_files_by_mtime ( dirpath ) : `` '' '' return a list of files in the directory , sorted in increasing `` mtime '' .
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' common constants used by proto splitter modules . '' '' ''
__label__0 def __getattr__ ( self , item ) : if item in ( `` _tfll_mode '' , `` _tfll_initialized '' , `` _tfll_name '' ) : return super ( types.moduletype , self ) .__getattribute__ ( item ) if not self._tfll_initialized : self._initialize ( ) if self._tfll_keras_version == `` keras_3 '' : if ( self._tfll_mode == `` v1 '' and not self._tfll_submodule and item.startswith ( `` compat.v1 . '' ) ) : raise attributeerror ( `` ` tf.compat.v1.keras ` is not available with keras 3. keras 3 has `` `` no support for tf 1 apis . you can install the ` tf_keras ` package `` `` as an alternative , and set the environment variable `` `` ` tf_use_legacy_keras=true ` to configure tensorflow to route `` `` ` tf.compat.v1.keras ` to ` tf_keras ` . '' ) elif ( self._tfll_mode == `` v2 '' and not self._tfll_submodule and item.startswith ( `` compat.v2 . '' ) ) : raise attributeerror ( `` ` tf.compat.v2.keras ` is not available with keras 3. just use `` `` ` import keras ` instead . '' ) elif self._tfll_submodule and self._tfll_submodule.startswith ( `` __internal__.legacy . '' ) : raise attributeerror ( f '' ` { item } ` is not available with keras 3 . '' ) module = self._load ( ) return getattr ( module , item )
__label__0 def __getattr__ ( self , item ) : module = self._load ( ) return getattr ( module , item )
__label__0 def get_qualified_name ( function ) : # python 3 if hasattr ( function , '__qualname__ ' ) : return function.__qualname__
__label__0 return compatible_func
__label__0 # this test is based on the fact that the standard services start # right away and get to run once before sv.stop ( ) returns . # we still sleep a bit to make the test robust . @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def teststandardserviceswithoutglobalstep ( self ) : logdir = self._test_dir ( `` standard_services_without_global_step '' ) # create a checkpoint . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 ] , name= '' foo '' ) summary.scalar ( `` v '' , v [ 0 ] ) sv = supervisor.supervisor ( logdir=logdir ) meta_graph_def = meta_graph.create_meta_graph_def ( saver_def=sv.saver.saver_def ) sess = sv.prepare_or_wait_for_session ( `` '' ) save_path = sv.save_path self._wait_for_glob ( save_path , 3.0 ) self._wait_for_glob ( os.path.join ( logdir , `` * events * '' ) , 3.0 , for_checkpoint=false ) # wait to make sure everything is written to file before stopping . time.sleep ( 1 ) sv.stop ( ) # there should be an event file with a version number . rr = _summary_iterator ( logdir ) ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version ) ev = next ( rr ) ev_graph = graph_pb2.graphdef ( ) ev_graph.parsefromstring ( ev.graph_def ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_graph )
__label__0 def get_symbol_from_name ( name : str ) - > optional [ any ] : return _name_to_symbol_mapping.get ( name )
__label__0 _fn ( ) self.assertequal ( 0 , mock_warning.call_count ) _fn ( arg=0 ) self.assertequal ( 1 , mock_warning.call_count ) _fn ( arg=1 ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 def testinitializingdecoratedclasswithinitparamsdoesntraise ( self ) : try : testdecoratedclass ( 2 ) except typeerror : self.assertfail ( )
__label__0 __getattr__ = deprecation.deprecate_moved_module ( __name__ , new_module , `` 2.9 '' ) # adjust version number . `` `
__label__0 def map_structure_with_paths ( func , * structure , * * kwargs ) : `` '' '' applies ` func ` to each entry in ` structure ` and returns a new structure .
__label__0 # save checkpoint from which to warm-start . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : variable_scope.get_variable ( `` linear_model/sc_hash/weights '' , shape= [ 15 , 1 ] , initializer=norms ( ) ) variable_scope.get_variable ( `` some_other_name '' , shape= [ 4 , 1 ] , initializer=rand ( ) ) variable_scope.get_variable ( `` linear_model/sc_vocab/weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 2 . ] , [ 3 . ] ] ) self._write_checkpoint ( sess )
__label__0 returns : a ` variable ` object. `` '' '' # scope the slot name in the namespace of the primary variable . # set `` primary.op.name + '/ ' + name '' as default name , so the scope name of # optimizer can be shared when reuse is true . meanwhile when reuse is false # and the same name has been previously used , the scope name will add '_n ' # as suffix for unique identifications . validate_shape = shape.is_fully_defined ( ) if isinstance ( primary , variables.variable ) : prefix = primary._shared_name # pylint : disable=protected-access else : prefix = primary.op.name with variable_scope.variable_scope ( none , prefix + `` / '' + name ) : if colocate_with_primary : distribution_strategy = distribute_lib.get_strategy ( ) with distribution_strategy.extended.colocate_vars_with ( primary ) : return _create_slot_var ( primary , initializer , `` '' , validate_shape , shape , dtype , copy_xla_sharding=copy_xla_sharding ) else : return _create_slot_var ( primary , initializer , `` '' , validate_shape , shape , dtype , copy_xla_sharding=copy_xla_sharding )
__label__0 this function exports the graph , saver , and collection objects into ` metagraphdef ` protocol buffer with the intention of it being imported at a later time or location to restart training , run inference , or be a subgraph .
__label__0 class sequenceexample ( typing.namedtuple ) : context : dict [ str , feature ] feature_lists : featurelists `` `
__label__0 with graph1.as_default ( ) , self.session ( graph=graph1 ) as sess : saver1 = saver_module.saver ( var_list=var_list_1 , max_to_keep=1 ) saver1.restore ( sess , saver0_ckpt ) saver2 = saver_module.saver ( var_list=var_list_2 , max_to_keep=1 ) saver2.restore ( sess , saver0_ckpt ) self.assertallclose ( expected , sess.run ( `` hidden1/relu:0 '' ) ) self.assertallclose ( expected , sess.run ( `` hidden2/relu:0 '' ) )
__label__0 text = `` f ( a , b , c , d ) \n '' _ , new_text = self._upgrade ( ast_edits.noupdatespec ( ) , text ) self.assertequal ( new_text , text )
__label__0 example : variables , placeholders , and independent operations can also be stored , as shown in the following example .
__label__0 def test_load_variable ( self ) : text = `` tf.contrib.framework.load_variable ( ' a ' ) '' expected_text = ( `` tf.train.load_variable ( ' a ' ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) text = `` tf.contrib.framework.load_variable ( checkpoint_dir= ' a ' ) '' expected_text = ( `` tf.train.load_variable ( ckpt_dir_or_file= ' a ' ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # wait till 1 seconds have elapsed so s1 will be old enough to keep . # sleep may return early , do n't trust it . mock_time.time.return_value = start_time + 1.0 s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s1 ] , save.last_checkpoints )
__label__0 the ` cached_saver ` argument allows the user to pass in a previously created saver , so multiple ` saver.restore ( ) ` calls do n't pollute the graph when graph building . this assumes that keys are consistent , meaning that the 1 ) ` checkpoint_path ` checkpoint , and 2 ) checkpoint used to create the ` cached_saver ` are the same type of object-based checkpoint . if this argument is set , this function will simply validate that all variables have been remapped by the checkpoint at ` checkpoint_path ` .
__label__0 def test_is_tensor_direct_import_upgrade ( self ) : text = `` contrib_framework.is_tensor ( x ) '' expected = `` tf.is_tensor ( x ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 def __init__ ( self , arg ) : myclass.init_args.append ( arg )
__label__0 args : checkpoint_path : string , path to object-based checkpoint
__label__0 def test_function ( self ) : decorator_utils.validate_callable ( _test_function , `` test '' )
__label__0 meta_graph_def = save.export_meta_graph ( strip_default_attrs=true ) node_def = test_util.get_node_def_from_graph ( `` complex '' , meta_graph_def.graph_def ) self.assertnotin ( `` t '' , node_def.attr ) self.assertnotin ( `` tout '' , node_def.attr )
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import compat
__label__0 check_types = kwargs.pop ( `` check_types '' , true ) expand_composites = kwargs.pop ( `` expand_composites '' , false )
__label__0 class initclass ( object ) :
__label__0 def __gt__ ( self , other ) : self._assert_type ( other ) return id ( self._wrapped ) > id ( other._wrapped ) # pylint : disable=protected-access
__label__0 args : target : the final callable to be wrapped . decorator_func : the wrapper function . decorator_name : the name of the decorator . if ` none ` , the name of the function calling make_decorator . decorator_doc : documentation specific to this application of ` decorator_func ` to ` target ` . decorator_argspec : override the signature using fullargspec .
__label__0 raw tensorflow stack traces involve many internal frames , which can be challenging to read through , while not being actionable for end users . by default , tensorflow filters internal frames in most exceptions that it raises , to keep stack traces short , readable , and focused on what 's actionable for end users ( their own code ) .
__label__0 prerequisites : 1. bazel is installed . 2. running in github repo of tensorflow . 3. configure has been run .
__label__0 with ops.graph ( ) .as_default ( ) : w_vector = variable_v1.variablev1 ( [ 1 , 2 , 3 ] , name= '' w '' ) with session.session ( server.target , config=sharing_config ) as sess : with self.assertraises ( errors_impl.failedpreconditionerror ) : sess.run ( w_vector ) sess.run ( w_vector.initializer ) self.assertallequal ( [ 1 , 2 , 3 ] , sess.run ( w_vector ) )
__label__0 s2 = save.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s1 , s2 ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s1 , s2 ] , save_dir=save_dir )
__label__0 from packaging import version import tensorflow as tf import yaml
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertfalse ( initialized ) sess.run ( v.initializer ) self.assertequal ( 1 , sess.run ( v ) ) saver.save ( sess , os.path.join ( checkpoint_dir , `` recover_session_checkpoint '' ) ) # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 2 , name= '' v '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm2.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.asserttrue ( initialized ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) )
__label__0 import numpy as np from tensorflow.core.protobuf import meta_graph_pb2 from tensorflow.core.protobuf import saver_pb2 from tensorflow.core.protobuf import trackable_object_graph_pb2 from tensorflow.python.checkpoint import checkpoint_management from tensorflow.python.client import session from tensorflow.python.eager import context from tensorflow.python.framework import constant_op from tensorflow.python.framework import device as pydev from tensorflow.python.framework import errors from tensorflow.python.framework import meta_graph from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import gen_io_ops from tensorflow.python.ops import io_ops from tensorflow.python.ops import string_ops from tensorflow.python.ops import variables from tensorflow.python.platform import gfile from tensorflow.python.platform import tf_logging as logging from tensorflow.python.saved_model.pywrap_saved_model import metrics from tensorflow.python.trackable import base as trackable from tensorflow.python.training import py_checkpoint_reader from tensorflow.python.training import training_util from tensorflow.python.training.saving import saveable_object from tensorflow.python.training.saving import saveable_object_util from tensorflow.python.util import compat from tensorflow.python.util.tf_export import tf_export
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` cublas_ver_major '' , `` cublas_ver_minor '' , `` cublas_ver_patch '' ) ) return `` . `` .join ( version )
__label__0 for example :
__label__0 def testrealdequeueenqueue ( self ) : with self.cached_session ( ) as sess : q0 = data_flow_ops.fifoqueue ( 3 , dtypes.float32 ) enqueue0 = q0.enqueue ( ( 10.0 , ) ) close0 = q0.close ( ) q1 = data_flow_ops.fifoqueue ( 30 , dtypes.float32 ) enqueue1 = q1.enqueue ( ( q0.dequeue ( ) , ) ) dequeue1 = q1.dequeue ( ) qr = queue_runner_impl.queuerunner ( q1 , [ enqueue1 ] ) threads = qr.create_threads ( sess ) for t in threads : t.start ( ) # enqueue 2 values , then close queue0 . enqueue0.run ( ) enqueue0.run ( ) close0.run ( ) # wait for the queue runner to terminate . for t in threads : t.join ( ) # it should have terminated cleanly . self.assertequal ( 0 , len ( qr.exceptions_raised ) ) # the 2 values should be in queue1 . self.assertequal ( 10.0 , self.evaluate ( dequeue1 ) ) self.assertequal ( 10.0 , self.evaluate ( dequeue1 ) ) # and queue1 should now be closed . with self.assertraisesregex ( errors_impl.outofrangeerror , `` is closed '' ) : self.evaluate ( dequeue1 )
__label__1 from collections import defaultdict
__label__0 returns : a context manager that yields a ` session ` restored from the latest checkpoint or initialized from scratch if not checkpoint exists . the session is closed when the ` with ` block exits. `` '' '' try : sess = self.prepare_or_wait_for_session ( master=master , config=config , start_standard_services=start_standard_services ) yield sess except exception as e : self.request_stop ( e ) finally : try : # request all the threads to stop and wait for them to do so . any # exception raised by the threads is raised again from stop ( ) . # passing stop_grace_period_secs is for blocked enqueue/dequeue # threads which are not checking for ` should_stop ( ) ` . they # will be stopped when we close the session further down . self.stop ( close_summary_writer=close_summary_writer ) finally : # close the session to finish up all pending calls . we do not care # about exceptions raised when closing . this takes care of # blocked enqueue/dequeue calls . try : sess.close ( ) except exception : # silently ignore exceptions raised by close ( ) . pass
__label__0 the ` local_init_op ` is an ` operation ` that is run always after a new session was created . if ` none ` , this step is skipped .
__label__0 @ compatibility ( eager ) queues are not compatible with eager execution . to ingest data when eager execution is enabled , use the ` tf.data ` api . @ end_compatibility `` '' '' if context.executing_eagerly ( ) : raise runtimeerror ( `` queues are not compatible with eager execution . '' ) if queue_runners is none : queue_runners = self._graph.get_collection ( ops.graphkeys.queue_runners ) threads = [ ] for qr in queue_runners : threads.extend ( qr.create_threads ( sess , coord=self._coord , daemon=true , start=true ) ) return threads
__label__0 from tensorflow.python.training.basic_session_run_hooks import get_or_create_steps_per_run_variable from tensorflow.python.training.basic_session_run_hooks import secondorsteptimer from tensorflow.python.training.basic_session_run_hooks import loggingtensorhook from tensorflow.python.training.basic_session_run_hooks import stopatstephook from tensorflow.python.training.basic_session_run_hooks import checkpointsaverhook from tensorflow.python.training.basic_session_run_hooks import checkpointsaverlistener from tensorflow.python.training.basic_session_run_hooks import stepcounterhook from tensorflow.python.training.basic_session_run_hooks import nanlossduringtrainingerror from tensorflow.python.training.basic_session_run_hooks import nantensorhook from tensorflow.python.training.basic_session_run_hooks import summarysaverhook from tensorflow.python.training.basic_session_run_hooks import globalstepwaiterhook from tensorflow.python.training.basic_session_run_hooks import finalopshook from tensorflow.python.training.basic_session_run_hooks import feedfnhook from tensorflow.python.training.basic_session_run_hooks import profilerhook from tensorflow.python.training.basic_loops import basic_train_loop from tensorflow.python.trackable.python_state import pythonstate from tensorflow.python.checkpoint.checkpoint import checkpoint from tensorflow.python.checkpoint.checkpoint_view import checkpointview from tensorflow.python.training.checkpoint_utils import init_from_checkpoint from tensorflow.python.training.checkpoint_utils import list_variables from tensorflow.python.training.checkpoint_utils import load_checkpoint from tensorflow.python.training.checkpoint_utils import load_variable
__label__0 class recoverlastcheckpointstest ( test.testcase ) :
__label__0 def _get_default_rocm_path ( ) : return `` /opt/rocm ''
__label__0 def _partitioner ( shape , dtype ) : # pylint : disable=unused-argument # partition each var into 2 equal slices . partitions = [ 1 ] * len ( shape ) partitions [ 0 ] = min ( 2 , shape.dims [ 0 ] .value ) return partitions
__label__0 args : dtype : type of tensor , must of one of the following types : float16 , float32 , float64 , int32 , or int64 min_size : minimum size of returned tensor max_size : maximum size of returned tensor min_val : minimum value in returned tensor max_val : maximum value in returned tensor
__label__0 def make_decorator ( target , decorator_func , decorator_name=none , decorator_doc= '' , decorator_argspec=none ) : `` '' '' make a decorator from a wrapper and a target .
__label__0 _for_subclass_implementers = `` _tf_docs_tools_for_subclass_implementers ''
__label__0 @ test_util.run_v1_only ( `` this exercises tensor lookup via names which is not supported in v2 . '' ) def test2workers ( self ) : num_workers = 2 replicas_to_aggregate = 2 num_ps = 2 workers , _ = create_local_cluster ( num_workers=num_workers , num_ps=num_ps )
__label__0 @ def_function.function def fn_disable_copy_on_read ( ) : ret = constant_op.constant ( 0 , dtypes.int32 ) for i in math_ops.range ( num_iter ) : op1 = resource_variable_ops.disable_copy_on_read ( var.handle ) op2 = resource_variable_ops.disable_copy_on_read ( accum.handle ) with ops.control_dependencies ( [ op1 , op2 ] ) : ret += i return ret
__label__0 def func ( a=1 , b=2 ) : return ( a , b )
__label__0 def testreturnsresultfromyield ( self ) : with test_yield_return_x_plus_1 ( 3 ) as result : self.assertequal ( 4 , result )
__label__0 args : func : the function or method needed to be decorated .
__label__0 the implementation of the different semantics use a common utility to avoid / minimize further divergence between the two apis over time. `` '' ''
__label__0 class typespec ( object ) : `` '' '' interface for internal isinstance checks to framework/type_spec.py .
__label__0 args : master : name of the tensorflow master to use . see the ` tf.compat.v1.session ` constructor for how this is interpreted . config : optional ` configproto ` proto used to configure the session . passed as-is to create the session . start_standard_services : whether to start the standard services , such as checkpoint , summary and step counter . close_summary_writer : whether to close the summary writer when closing the session . defaults to true .
__label__0 returns : sum of args. `` '' '' return arg0 + arg1
__label__0 args : cls : the class to register .
__label__0 # shallow tree ends at string . input_tree = [ [ ( `` a '' , 1 ) , [ ( `` b '' , 2 ) , [ ( `` c '' , 3 ) , [ ( `` d '' , 4 ) ] ] ] ] ] shallow_tree = [ [ `` level_1 '' , [ `` level_2 '' , [ `` level_3 '' , [ `` level_4 '' ] ] ] ] ] ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) input_tree_flattened_paths = [ p for p , _ in nest.flatten_with_tuple_paths ( input_tree ) ] input_tree_flattened = nest.flatten ( input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( 0 , 0 ) , ( 0 , 1 , 0 ) , ( 0 , 1 , 1 , 0 ) , ( 0 , 1 , 1 , 1 , 0 ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ( `` a '' , 1 ) , ( `` b '' , 2 ) , ( `` c '' , 3 ) , ( `` d '' , 4 ) ] )
__label__0 def testinitializers ( self ) : text = ( `` tf.zeros_initializer ; tf.zeros_initializer ( ) \n '' `` tf.ones_initializer ; tf.ones_initializer ( ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.zeros_initializer ( ) ; tf.zeros_initializer ( ) \n '' `` tf.ones_initializer ( ) ; tf.ones_initializer ( ) \n '' )
__label__0 def get_random_numeric_tensor ( self , dtype=none , min_size=_min_size , max_size=_max_size , min_val=_min_int , max_val=_max_int ) : `` '' '' return a tensor of random shape and values .
__label__0 def _another_group_active ( self , group_id ) : return any ( c > 0 for g , c in enumerate ( self._group_member_counts ) if g ! = group_id )
__label__0 def _find_rocm_config ( rocm_install_path ) :
__label__0 def visit_importfrom ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting an import-from node in the ast .
__label__0 structures_have_mismatching_lengths = ( nest_util.structures_have_mismatching_lengths )
__label__0 @ keyword_args.keyword_args_only def func_with_decorator ( a , b ) : return func_without_decorator ( a , b )
__label__0 parser.add_argument ( `` -- key_value '' , type=str , nargs= '' * '' , help= '' list of key=value pairs . '' )
__label__0 # generates metagraphdef . meta_graph_def = saver_module.export_meta_graph ( filename ) meta_graph_def0 = saver0.export_meta_graph ( ) meta_graph_def1 = saver1.export_meta_graph ( clear_extraneous_savers=true )
__label__0 class fencedcellparser ( doctest.doctestparser ) : `` '' '' implements test parsing for `` ` fenced cells .
__label__0 implementation note : - this method should not invoke any tensorflow ops . - this method only needs to unflatten the current level . if the object has an attribute that also need custom unflattening , nest functions will utilize this method to do recursive unflattening. `` '' ''
__label__0 def __ge__ ( self , other : set [ any ] ) - > bool : if not isinstance ( other , set ) : return notimplemented if len ( self ) < len ( other ) : return false for item in other : if item not in self : return false return true
__label__1 def maxenvelopes ( envelopes ) : # sort envelopes based on widths in ascending order envelopes.sort ( key=lambda x : ( x [ 0 ] , -x [ 1 ] ) ) # use dynamic programming to find the longest increasing subsequence of heights dp = [ ] for _ , h in envelopes : left , right = 0 , len ( dp ) while left < right : mid = left + ( right - left ) // 2 if dp [ mid ] < h : left = mid + 1 else : right = mid if right == len ( dp ) : dp.append ( h ) else : dp [ right ] = h return len ( dp ) # test cases print ( maxenvelopes ( [ [ 5,4 ] , [ 6,4 ] , [ 6,7 ] , [ 2,3 ] ] ) ) # output : 3 print ( maxenvelopes ( [ [ 1,1 ] , [ 1,1 ] , [ 1,1 ] ] ) ) # output : 1
__label__0 if not signature_checkers : signature = _signature_from_annotations ( dispatch_target ) checker = _make_signature_checker ( api_signature , signature ) dispatcher.register ( checker , dispatch_target ) _type_based_dispatch_signatures [ api ] [ dispatch_target ] .append ( signature )
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { 'my_feature ' : tf.io.raggedfeature ( dtype=tf.int64 ) } ) { 'my_feature ' : < tf.tensor : shape= ( 4 , ) , dtype=float32 , numpy=array ( [ 1 , 2 , 3 , 4 ] , dtype=int64 ) > }
__label__0 return node
__label__0 def testinitcapturesdecoratorname ( self ) : self.assertequal ( 'decorator name ' , tf_decorator.tfdecorator ( 'decorator name ' , test_function ) .decorator_name )
__label__0 def _saveandload ( self , var_name , var_value , other_value , save_path ) : with self.session ( graph=ops_lib.graph ( ) ) as sess : var = resource_variable_ops.resourcevariable ( var_value , name=var_name ) save = saver_module.saver ( { var_name : var } ) if not context.executing_eagerly ( ) : self.evaluate ( var.initializer ) val = save.save ( sess , save_path ) self.assertequal ( save_path , val ) with self.session ( graph=ops_lib.graph ( ) ) as sess : var = resource_variable_ops.resourcevariable ( other_value , name=var_name ) save = saver_module.saver ( { var_name : var } ) save.restore ( sess , save_path ) self.assertallclose ( var_value , self.evaluate ( var ) )
__label__0 class callable ( object ) :
__label__0 @ deprecation.deprecated_arg_values ( date , instructions , warn_once=false , deprecated=true ) def _fn ( arg0 , arg1 , deprecated=true ) : return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 _min_int = -10000 _max_int = 10000
__label__0 def _get_write_histogram_proto ( ) : proto_bytes = metrics.getcheckpointwritedurations ( api_label=api_label ) histogram_proto = summary_pb2.histogramproto ( ) histogram_proto.parsefromstring ( proto_bytes ) return histogram_proto
__label__0 args : ckpt_to_initialize_from : [ required ] a string specifying the directory with checkpoint file ( s ) or path to checkpoint from which to warm-start the model parameters . vars_to_warm_start : [ optional ] one of the following :
__label__0 similar to the python library function ` os.path.walk ` .
__label__0 text = `` slim.xavier_initializer ( uniform= ( true or false ) ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution= ( \ '' uniform\ '' if true or false else `` `` \ '' truncated_normal\ '' ) ) \n '' , )
__label__0 import typing from absl.testing import parameterized
__label__0 with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : v0 = variable_v1.variablev1 ( 111 , name= '' v0 '' ) with sess.graph.device ( `` /cpu:1 '' ) : v1 = variable_v1.variablev1 ( 222 , name= '' v1 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 } , sharded=true , max_to_keep=2 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertequal ( [ ] , save.last_checkpoints )
__label__0 > > > global_batch_size = 4 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > train_dataset = tf.data.dataset.from_tensors ( ( [ 1. ] , [ 1 . ] ) ) .repeat ( 50 ) .batch ( global_batch_size ) > > > train_dist_dataset = strategy.experimental_distribute_dataset ( train_dataset ) > > > @ tf.function ... def distributed_train_step ( dataset_inputs ) : ... def train_step ( input ) : ... loss = tf.constant ( 0.1 ) ... return loss ... per_replica_losses = strategy.run ( train_step , args= ( dataset_inputs , ) ) ... return strategy.reduce ( tf.distribute.reduceop.sum , per_replica_losses , axis=none ) > > > epochs = 2 > > > steps = 3 > > > for epoch in range ( epochs ) : ... total_loss = 0.0 ... num_batches = 0 ... dist_dataset_iterator = iter ( train_dist_dataset ) ... for _ in range ( steps ) : ... total_loss += distributed_train_step ( next ( dist_dataset_iterator ) ) ... num_batches += 1 ... average_train_loss = total_loss / num_batches ... template = ( `` epoch { } , loss : { : .4f } '' ) ... print ( template.format ( epoch+1 , average_train_loss ) ) epoch 1 , loss : 0.2000 epoch 2 , loss : 0.2000
__label__0 ` perreplica ` values exist on the worker devices , with a different value for each replica . they can be produced many ways , often by iterating through a distributed dataset returned by ` tf.distribute.strategy.experimental_distribute_dataset ` and ` tf.distribute.strategy.distribute_datasets_from_function ` . they are also the typical result returned by ` tf.distribute.strategy.run ` . `` '' ''
__label__0 if inspect.ismethod ( innermost_decorator ) : # bound methods ca n't be assigned attributes . thankfully , they seem to # be just proxies for their unbound counterpart , and we can modify that . if hasattr ( innermost_decorator , '__func__ ' ) : innermost_decorator.__func__.__wrapped__ = new_target elif hasattr ( innermost_decorator , 'im_func ' ) : innermost_decorator.im_func.__wrapped__ = new_target else : innermost_decorator.__wrapped__ = new_target else : innermost_decorator.__wrapped__ = new_target
__label__0 args : graph_fn : takes a single float tensor argument as input , outputs a single tensor `` '' '' test_dir = self._get_test_dir ( `` nested_control_flow '' ) filename = os.path.join ( test_dir , `` metafile '' ) saver_ckpt = os.path.join ( test_dir , `` saver.ckpt '' )
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _object ( ) ._fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 for each component , we check that it exports everything under its own subpackage .
__label__0 def _walk_fields ( proto : message.message , fields : fieldtypes ) : `` '' '' yields fields in a proto .
__label__0 return make_tf_decorator
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but ` input_tree ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` . `` '' '' return nest_util.flatten_up_to ( nest_util.modality.core , shallow_tree , input_tree , check_types , expand_composites , )
__label__0 args : var_list : a list of ` variable ` / ` saveableobject ` , or a dictionary mapping names to ` saveableobject ` s . if ` none ` , defaults to the list of all saveable objects . reshape : if ` true ` , allows restoring parameters from a checkpoint where the variables have a different shape . sharded : if ` true ` , shard the checkpoints , one per device . max_to_keep : maximum number of recent checkpoints to keep . defaults to 5. keep_checkpoint_every_n_hours : how often to keep checkpoints . defaults to 10,000 hours . name : string . optional name to use as a prefix when adding operations . restore_sequentially : a ` bool ` , which if true , causes restore of different variables to happen sequentially within each device . this can lower memory usage when restoring very large models . saver_def : optional ` saverdef ` proto to use instead of running the builder . this is only useful for specialty code that wants to recreate a ` saver ` object for a previously built ` graph ` that had a ` saver ` . the ` saver_def ` proto should be the one returned by the ` as_saver_def ( ) ` call of the ` saver ` that was created for that ` graph ` . builder : optional ` saverbuilder ` to use if a ` saver_def ` was not provided . defaults to ` bulksaverbuilder ( ) ` . defer_build : if ` true ` , defer adding the save and restore ops to the ` build ( ) ` call . in that case ` build ( ) ` should be called before finalizing the graph or using the saver . allow_empty : if ` false ` ( default ) raise an error if there are no variables in the graph . otherwise , construct the saver anyway and make it a no-op . write_version : controls what format to use when saving checkpoints . it also affects certain filepath matching logic . the v2 format is the recommended choice : it is much more optimized than v1 in terms of memory required and latency incurred during restore . regardless of this flag , the saver is able to restore from both v2 and v1 checkpoints . pad_step_number : if true , pads the global step number in the checkpoint filepaths to some fixed width ( 8 by default ) . this is turned off by default . save_relative_paths : if ` true ` , will write relative paths to the checkpoint state file . this is needed if the user wants to copy the checkpoint directory and reload from the copied directory . filename : if known at graph construction time , filename used for variable loading/saving .
__label__0 args : obj : a callable , possibly decorated .
__label__0 def testgetargspeconinitclass ( self ) :
__label__0 see the [ checkpoint compatibility ] ( https : //www.tensorflow.org/guide/migrate # checkpoint_compatibility ) section of the migration guide for more details .
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 args : node : current node `` '' '' if not node.module : self.generic_visit ( node ) return
__label__0 return decorator_utils.add_notice_to_docstring ( doc , instructions , 'deprecated function argument values ' , ' ( deprecated argument values ) ' , [ 'some argument values are deprecated : ` ( % s ) ` . ' 'they will be removed % s . ' % ( deprecation_string , when ) , 'instructions for updating : ' ] , notice_type='deprecated ' )
__label__0 # arg information parser.add_argument ( `` -- filename '' , help= '' path to whl file we are copying '' , required=true ) parser.add_argument ( `` -- new_py_ver '' , help= '' two digit py version eg . 27 or 33 '' , required=true )
__label__0 @ tf_contextlib.contextmanager def test_yield_return_x_plus_1 ( x ) : yield x + 1
__label__0 def wrapper ( * args , * * kwargs ) : return func ( * args , * * kwargs )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 this modality makes two changes : 1. it removes support for lists as a level of nesting in nested structures . 2. it adds support for ` sparsetensorvalue ` as an atomic element .
__label__0 if parent_desc is none : raise valueerror ( f '' unable to find fields : { fields } in proto of type { type ( proto ) } . '' )
__label__0 > > > import collections > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( `` bar '' , `` a b '' ) ( 1 , 2 ) , ... collections.namedtuple ( `` foo '' , `` a b '' ) ( 2 , 3 ) , ... check_types=false )
__label__0 returns : consumed string based on input bytes and constraints. `` '' '' return self.fdp.consumestring ( byte_count )
__label__0 def test_keras_experimental_export_warning ( self ) : text = `` tf.keras.experimental.export_saved_model '' _ , report , _ , _ = self._upgrade ( text ) expected_info = `` please use model.save '' self.assertin ( expected_info , report )
__label__0 def restore ( self , restore_tensors , shapes ) : return gen_lookup_ops.lookup_table_import_v2 ( self.op.table_ref , restore_tensors [ 0 ] , restore_tensors [ 1 ] ) # pylint : enable=protected-access
__label__0 _min_float = -10000.0 _max_float = 10000.0
__label__0 `` ` /parent.md # method1 # method2 /child.md # method1 # method2 `` `
__label__0 results = [ func ( * tensors ) for tensors in zip ( * all_flattened_up_to ) ] return _tf_data_pack_sequence_as ( structure=shallow_tree , flat_sequence=results )
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` nv_tensorrt_major '' , `` nv_tensorrt_minor '' , `` nv_tensorrt_patch '' ) ) # ` version ` is a generator object , so we convert it to a list before using # it ( muitiple times below ) . version = list ( version ) if not all ( version ) : return none # versions not found , make _matches_version returns false . return `` . `` .join ( version )
__label__0 if not structure : raise valueerror ( `` must provide at least one structure '' )
__label__0 class tfshouldusetest ( test.testcase ) :
__label__0 args : checkpoint_paths : a list of checkpoint paths. `` '' '' checkpoints_with_mtimes = [ ] for checkpoint_path in checkpoint_paths : try : mtime = checkpoint_management.get_checkpoint_mtimes ( [ checkpoint_path ] ) except errors.notfounderror : # it 's fine if some other thread/process is deleting some older # checkpoint concurrently . continue if mtime : checkpoints_with_mtimes.append ( ( checkpoint_path , mtime [ 0 ] ) ) self.set_last_checkpoints_with_time ( checkpoints_with_mtimes )
__label__0 from google.protobuf import json_format from google.protobuf import text_format from tensorflow.core.util import test_log_pb2 from tensorflow.python.platform import gfile from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging from tensorflow.tools.test import run_and_gather_logs_lib
__label__0 with open ( file_a , `` a '' ) as f : f.write ( `` import foo as f '' ) os.symlink ( file_a , file_b )
__label__0 def _fix_chunks ( self ) - > none : `` '' '' fixes chunk indices in the chunkedmessage . '' '' '' if not self._fix_chunk_order : return
__label__0 class iterator ( object ) : `` '' '' interface for distributed iterators . '' '' ''
__label__0 raises : valueerror : if nest and structure have different element counts. `` '' '' if not ( _tf_data_is_nested ( flat_sequence ) or isinstance ( flat_sequence , list ) ) : raise typeerror ( `` argument ` flat_sequence ` must be a sequence . got `` f '' ' { type ( flat_sequence ) .__name__ } ' . '' )
__label__0 # check if we have a labels keyword arg for karg in node.keywords : if karg.arg == `` labels '' : if _wrap_label ( karg , karg.value ) : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing labels arg of `` `` tf.nn.softmax_cross_entropy_with_logits to `` `` tf.stop_gradient ( labels ) . please check this `` `` transformation.\n '' ) ) return node return node
__label__0 @ dispatch.dispatch_for_unary_elementwise_apis ( maskedtensor ) def handler ( api_func , x ) : return maskedtensor ( api_func ( x.values ) , x.mask )
__label__0 def test_format_bytes ( self ) : self.assertequal ( util.format_bytes ( 1024 ) , `` 1kib '' ) self.assertequal ( util.format_bytes ( 5632 ) , `` 5.5kib '' ) self.assertequal ( util.format_bytes ( 53432 ) , `` 52.18kib '' )
__label__0 def main ( ) : `` '' '' this script copies binaries .
__label__0 # stored metagraphdef ev = next ( rr ) ev_meta_graph = meta_graph_pb2.metagraphdef ( ) ev_meta_graph.parsefromstring ( ev.meta_graph_def ) self.assertprotoequals ( meta_graph_def , ev_meta_graph ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_meta_graph.graph_def ) # the next one should have the values from the summary . ev = next ( rr ) self.assertprotoequals ( `` '' '' value { tag : 'c1 ' simple_value : 1.0 } value { tag : 'c2 ' simple_value : 2.0 } value { tag : 'c3 ' simple_value : 3.0 } `` '' '' , ev.summary )
__label__0 # this list should contain all modules _immediately_ under tensorflow _top_level_modules = [ `` tensorflow._api '' , `` tensorflow.python '' , `` tensorflow.tools '' , `` tensorflow.core '' , `` tensorflow.compiler '' , `` tensorflow.lite '' , `` tensorflow.keras '' , `` tensorflow.compat '' , `` tensorflow.summary '' , # tensorboard `` tensorflow.examples '' , ]
__label__0 for fname in list_files_by_mtime ( opts.datadir ) : fpath = os.path.join ( opts.datadir , fname ) try : with open ( fpath , `` r '' ) as fd : if trylock ( fd ) : upload_benchmark_data ( client , fd.read ( ) ) shutil.move ( fpath , os.path.join ( opts.archivedir , fname ) ) # unlock ( fd ) -- when `` with open ( ) '' closes fd , the lock is released . except exception as e : # pylint : disable=broad-except print ( `` can not process ' % s ' , skipping . error : % s '' % ( fpath , e ) )
__label__0 note : this is implemented by adding a hidden attribute on the object , so it can not be used on objects which do not allow new attributes to be added . so this decorator must go * below * ` @ property ` , ` @ classmethod ` , or ` @ staticmethod ` :
__label__0 # sometimes , people leave \n at the end of cell # in order to migrate only related things , and make the diff # the smallest - > here is another hack if ( line_idx == len ( cell_lines ) - 1 ) and code_line.endswith ( `` \n '' ) : code_line = code_line.replace ( `` \n '' , `` # # # === '' )
__label__0 old_value = keyword_arg.value new_value = _get_distribution ( keyword_arg.value )
__label__0 class vlogtest ( test.testcase ) :
__label__0 `` ` python with tf.graph ( ) .as_default ( ) : ... add operations to the graph ... # create a supervisor that will checkpoint the model in '/tmp/mydir ' . sv = supervisor ( logdir='/tmp/mydir ' ) # get a tensorflow session managed by the supervisor . with sv.managed_session ( flags.master ) as sess : # use the session to train the graph . while not sv.should_stop ( ) : sess.run ( < my_train_op > ) `` `
__label__0 def as_cluster_def ( self ) : `` '' '' returns a ` tf.train.clusterdef ` protocol buffer based on this cluster . '' '' '' return self._cluster_def
__label__0 print ( `` ignored files count : % d '' % ignored_files_count ) print ( `` denylisted dependencies count : % d '' % denylisted_dependencies_count ) if missing_dependencies : print ( `` missing the following dependencies from pip_packages : '' ) for missing_dependency in missing_dependencies : print ( `` \nmissing dependency : % s `` % missing_dependency ) print ( `` affected tests : '' ) rdep_query = ( `` rdeps ( kind ( py_test , % s ) , % s ) '' % ( `` + `` .join ( python_targets ) , missing_dependency ) ) affected_tests = subprocess.check_output ( [ `` bazel '' , `` cquery '' , `` -- experimental_cc_shared_library '' , rdep_query ] ) affected_tests_list = affected_tests.split ( `` \n '' ) [ : -2 ] print ( `` \n '' .join ( affected_tests_list ) )
__label__0 distribution_value = `` \ '' uniform\ '' '' # parse with pasta instead of ast to avoid emitting a spurious trailing \n . ast_value = pasta.parse ( distribution_value ) node.keywords.append ( ast.keyword ( arg= '' distribution '' , value=ast_value ) )
__label__0 from tensorflow.python.training.device_setter import replica_device_setter from tensorflow.python.training.monitored_session import scaffold from tensorflow.python.training.monitored_session import monitoredtrainingsession from tensorflow.python.training.monitored_session import sessioncreator from tensorflow.python.training.monitored_session import chiefsessioncreator from tensorflow.python.training.monitored_session import workersessioncreator from tensorflow.python.training.monitored_session import monitoredsession from tensorflow.python.training.monitored_session import singularmonitoredsession from tensorflow.python.training.saver import saver from tensorflow.python.checkpoint.checkpoint_management import checkpoint_exists from tensorflow.python.checkpoint.checkpoint_management import generate_checkpoint_state_proto from tensorflow.python.checkpoint.checkpoint_management import get_checkpoint_mtimes from tensorflow.python.checkpoint.checkpoint_management import get_checkpoint_state from tensorflow.python.checkpoint.checkpoint_management import latest_checkpoint from tensorflow.python.checkpoint.checkpoint_management import update_checkpoint_state from tensorflow.python.training.saver import export_meta_graph from tensorflow.python.training.saver import import_meta_graph from tensorflow.python.training.saving import saveable_object_util from tensorflow.python.training.session_run_hook import sessionrunhook from tensorflow.python.training.session_run_hook import sessionrunargs from tensorflow.python.training.session_run_hook import sessionruncontext from tensorflow.python.training.session_run_hook import sessionrunvalues from tensorflow.python.training.session_manager import sessionmanager from tensorflow.python.training.summary_io import summary_iterator from tensorflow.python.training.supervisor import supervisor from tensorflow.python.training.training_util import write_graph from tensorflow.python.training.training_util import global_step from tensorflow.python.training.training_util import get_global_step from tensorflow.python.training.training_util import assert_global_step from tensorflow.python.training.training_util import create_global_step from tensorflow.python.training.training_util import get_or_create_global_step from tensorflow.python.training.warm_starting_util import vocabinfo from tensorflow.python.training.warm_starting_util import warm_start from tensorflow.python.training.py_checkpoint_reader import newcheckpointreader from tensorflow.python.util.tf_export import tf_export
__label__0 # we can only create logical devices before initializing tensorflow . this is # called by unittest framework before running any test . # https : //docs.python.org/3/library/unittest.html # setupmodule-and-teardownmodule def setupmodule ( ) : setup_gpu ( flags.required_gpus )
__label__0 it throws an exception when the symbol was not hidden in the first place .
__label__0 def testwarmstartmoresettings ( self ) : # create old and new vocabs for sparse column `` sc_vocab '' . prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' ) # create feature columns . sc_hash = fc.categorical_column_with_hash_bucket ( `` sc_hash '' , hash_bucket_size=15 ) sc_keys = fc.categorical_column_with_vocabulary_list ( `` sc_keys '' , vocabulary_list= [ `` a '' , `` b '' , `` c '' , `` e '' ] ) sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=new_vocab_path , vocabulary_size=6 ) all_linear_cols = [ sc_hash , sc_keys , sc_vocab ]
__label__0 `` ` python input_tree = [ [ [ 2 , 2 ] , [ 3 , 3 ] ] , [ [ 4 , 9 ] , [ 5 , 5 ] ] ] shallow_tree = [ [ true , true ] , [ false , true ] ]
__label__0 visitor = _pastaeditvisitor ( self._api_change_spec ) visitor.visit ( t )
__label__0 # find the floats in the string returned by the test _ , self.got_floats = self.extract_floats ( got )
__label__0 # creates a graph with 2 variables . with ops.graph ( ) .as_default ( ) : v0 = variables.variable ( 1.0 , name= '' v0 '' ) v1 = variables.variable ( 2.0 , name= '' v0 '' )
__label__0 transforms a class method into a property whose value is computed once and then cached as a normal attribute for the life of the class . example usage :
__label__0 @ property def init_op ( self ) : `` '' '' return the init op used by the supervisor .
__label__0 def teststartqueuerunnersnondefaultgraph ( self ) : # countupto will raise out_of_range when it reaches the count . graph = ops.graph ( ) with graph.as_default ( ) : zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) init_op = variables.global_variables_initializer ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) queue_runner_impl.add_queue_runner ( qr ) with self.session ( graph=graph ) as sess : init_op.run ( ) threads = queue_runner_impl.start_queue_runners ( sess ) for t in threads : t.join ( ) self.assertequal ( 0 , len ( qr.exceptions_raised ) ) # the variable should be 3. self.assertequal ( 3 , self.evaluate ( var ) )
__label__0 save.restore ( none , ckpt_prefix ) self.assertnear ( 3.14 , self.evaluate ( v1 ) , 1e-5 ) self.assertallequal ( [ 1 , 2 ] , self.evaluate ( v2 ) )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 # and update the docstring of the class . cls.__doc__ = _add_deprecated_function_notice_to_docstring ( cls.__doc__ , date , instructions )
__label__0 to recover the latest checkpoint ,
__label__0 def __hash__ ( self ) : return self.hash_value
__label__0 
__label__0 def testgetmembers ( self ) : self.assertequal ( inspect.getmembers ( testdecoratedclass ) , tf_inspect.getmembers ( testdecoratedclass ) )
__label__0 `` ` python cdf = tf.config.experimental.clusterdevicefilters ( ) for i in range ( num_workers ) : cdf.set_device_filters ( 'worker ' , i , [ '/job : ps ' ] ) for i in range ( num_ps ) : cdf.set_device_filters ( 'ps ' , i , [ '/job : worker ' ] )
__label__0 returns : global step tensor .
__label__0 def __init__ ( self , api_change_spec ) : self._api_change_spec = api_change_spec self._log = [ ] # holds 4-tuples : severity , line , col , msg . self._stack = [ ] # allow easy access to parents .
__label__0 note that in general , ` tf.train.checkpoint ` should be used to restore/save an object-based checkpoint .
__label__0 # # # # # launching additional services
__label__0 with self.session ( graph=g ) as sess : self.evaluate ( a.initializer ) save_path = a_saver.save ( sess=sess , save_path=checkpoint_prefix )
__label__0 from tensorflow.tools.docs import fenced_doctest_lib
__label__0 def handle ( self , args , kwargs ) : # pylint : disable=unused-argument `` '' '' handle this dispatcher 's operation with the specified arguments .
__label__0 def testwithlambda ( self ) : anon_fn = lambda x : x self.assertequal ( ' < lambda > ' , function_utils.get_func_name ( anon_fn ) )
__label__0 returns : a string. `` '' '' return self._save_path
__label__0 `` `` '' utilities for collecting objects based on `` is '' comparison . '' '' '' # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== from typing import any , set import weakref
__label__0 def _extract_from_parse_example ( parse_example_op , sess ) : `` '' '' extract exampleparserconfig from parseexample op . '' '' '' config = example_parser_configuration_pb2.exampleparserconfiguration ( )
__label__0 see ` coordinator.should_stop ( ) ` .
__label__0 class _countdowntimer : `` '' '' a timer that tracks a duration since creation . '' '' ''
__label__0 for other in structure [ 1 : ] : _tf_data_assert_same_structure ( structure [ 0 ] , other , check_types=check_types )
__label__0 raises : typeerror : if the input is not a dictionary . valueerror : if any key and value do not have the same structure layout , or if keys are not unique. `` '' '' return _pywrap_nest.flattendictitems ( dictionary )
__label__0 def sequence_like ( instance , args ) : `` '' '' converts the sequence ` args ` to the same type as ` instance ` .
__label__0 to achieve a performance improvement , you can also wrap the ` strategy.run ` call with a ` tf.range ` inside a ` tf.function ` . this runs multiple steps in a ` tf.function ` . autograph will convert it to a ` tf.while_loop ` on the worker . however , it is less flexible comparing with running a single step inside ` tf.function ` . for example , you can not run things eagerly or arbitrary python code within the steps .
__label__0 flags = flags.flags
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassassertshallowstructure ( self ) : # these assertions are expected to pass : two dataclasses with the same # component size are considered to have the same shallow structure . mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt2 = maskedtensor ( mask=false , value=constant_op.constant ( [ 2 , 3 ] ) ) nest.assert_shallow_structure ( shallow_tree=mt , input_tree=mt2 , check_types=true ) nest.assert_shallow_structure ( shallow_tree=mt2 , input_tree=mt , check_types=true )
__label__0 visitor = public_api.publicapivisitor ( symbol_collector ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v2 , visitor )
__label__0 args : name : benchmark target identifier . test_name : a unique bazel target , e.g . `` //path/to : test '' test_args : a string containing all arguments to run the target with . benchmark_type : a string representing the benchmarktype enum ; the benchmark type for this target . start_time : test starting time ( epoch ) run_time : wall time that the test ran for log_files : paths to the log files
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 def _get_saver_or_default ( ) : `` '' '' returns the saver from savers collection , or creates a default one .
__label__0 raises : valueerror : if the signatures are incompatible . two signatures are considered compatible if they have the same number of parameters , and all corresponding parameters have the same ` name ` and ` kind ` . ( parameters are not required to have the same default value or the same annotation . ) `` `` '' # special case : if func_signature is ( * args , * * kwargs ) , then assume it 's ok. func_argspec = tf_inspect.getargspec ( func ) if ( func_argspec.varargs is not none and func_argspec.keywords is not none and not func_argspec.args ) : return
__label__0 args : node : nodedef with const value to extract . size : size of nodedef ( for error reporting ) .
__label__0 test_constant1 = 123 test_constant2 = 'abc ' test_constant3 = 0.5
__label__0 self.assertequal ( { ' a ' : 13 } , tf_inspect.getcallargs ( func ) )
__label__0 return tf_decorator.make_decorator ( func , wrapper )
__label__0 `` ` python b `` `
__label__0 def _assert_type ( self , other ) : if not isinstance ( other , _objectidentitywrapper ) : raise typeerror ( `` can not compare wrapped object with unwrapped object '' )
__label__0 where ` x ` and ` y ` are the first two arguments to the elementwise api , and ` api_func ` is a tensorflow function that takes two parameters and performs the elementwise operation ( e.g. , ` tf.add ` ) .
__label__0 `` ` python y = tf.variable ( tf.zeros ( [ 10 , 20 ] , dtype=tf.float32 ) )
__label__0 # # # usage :
__label__0 __slots__ = [ `` _lock '' , `` _group_id '' ]
__label__0 from tensorflow.python.framework import constant_op from tensorflow.python.framework import indexed_slices from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.framework.test_util import create_local_cluster from tensorflow.python.ops import variable_v1 from tensorflow.python.platform import test from tensorflow.python.training import adam from tensorflow.python.training import gradient_descent from tensorflow.python.training import training
__label__0 this definition may be used in user code . additional types may be added in the future as more input types are supported .
__label__0 self.assertequal ( [ tf_inspect.parameter ( ' x ' , tf_inspect.parameter.positional_or_keyword ) ] , list ( signature.parameters.values ( ) ) )
__label__0 def map_structure ( modality , func , * structure , * * kwargs ) : `` '' '' creates a new structure by applying ` func ` to each atom in ` structure ` .
__label__0 return sessions , graphs , train_ops
__label__0 def getfullargspec ( obj ) : `` '' '' tfdecorator-aware replacement for ` inspect.getfullargspec ` .
__label__0 def _is_ast_true ( node ) : if hasattr ( ast , `` nameconstant '' ) : return isinstance ( node , ast.nameconstant ) and node.value is true else : return isinstance ( node , ast.name ) and node.id == `` true ''
__label__0 nest.assert_same_structure ( output , inp_a ) self.assertshapeequal ( np.zeros ( ( 3 , 4 ) ) , output [ 0 ] ) self.assertshapeequal ( np.zeros ( ( 3 , 7 ) ) , output [ 1 ] )
__label__0 def testbasics ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` sharded_basics '' )
__label__1 def maxsumsubmatrix ( matrix , k ) : m , n = len ( matrix ) , len ( matrix [ 0 ] ) max_sum = float ( '-inf ' ) for left in range ( n ) : prefix = [ 0 ] * m for right in range ( left , n ) : for i in range ( m ) : prefix [ i ] += matrix [ i ] [ right ] # find the maximum subarray sum of prefix such that the sum is less than or equal to k curr_sum = 0 prefix_sum = [ 0 ] for num in prefix : curr_sum += num # find the smallest prefix_sum such that curr_sum - prefix_sum < = k idx = bisect.bisect_left ( prefix_sum , curr_sum - k ) if idx < len ( prefix_sum ) : max_sum = max ( max_sum , curr_sum - prefix_sum [ idx ] ) # insert curr_sum into prefix_sum array in sorted order bisect.insort ( prefix_sum , curr_sum ) return max_sum # test cases print ( maxsumsubmatrix ( [ [ 1,0,1 ] , [ 0 , -2,3 ] ] , 2 ) ) # output : 2 print ( maxsumsubmatrix ( [ [ 2,2 , -1 ] ] , 3 ) ) # output : 3
__label__0 with self.assertraisesregex ( typeerror , `` same sequence type '' ) : nest.map_structure ( lambda x , y : none , ( ( 3 , 4 ) , 5 ) , [ ( 3 , 4 ) , 5 ] )
__label__0 def loop_body ( it , biases2 ) : biases2 += constant_op.constant ( 0.1 , shape= [ 32 ] ) return it + 1 , biases2
__label__0 structures_have_mismatching_lengths = ( `` the two structures do n't have the same sequence length . input `` `` structure has length { input_length } , while shallow structure has length `` `` { shallow_length } . '' )
__label__0 this decorator allows to force document a private method/function .
__label__0 # # shallow non-list edge-case . # using iterable elements . input_tree = [ `` input_tree '' ] shallow_tree = `` shallow_tree '' flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' decorator that provides a warning if the wrapped object is never used . '' '' '' import copy import sys import textwrap import traceback import types
__label__0 def testgetargspecreturnsoutermostdecoratorthatchangesargspec ( self ) : outer_argspec = tf_inspect.fullargspec ( args= [ ' a ' ] , varargs=none , varkw=none , defaults= ( ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } , ) inner_argspec = tf_inspect.fullargspec ( args= [ ' b ' ] , varargs=none , varkw=none , defaults= ( ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } , )
__label__0 goodbye `` '' '' ) , ( 'multiline ' , [ ( ' a\nb ' , ' a\nb ' ) ] , `` '' '' hello
__label__0 def fullargspec_to_signature ( fullargspec : inspect.fullargspec ) - > inspect.signature : `` '' '' repackages fullargspec information into an equivalent inspect.signature . '' '' '' defaults = _make_default_values ( fullargspec ) parameters = [ ]
__label__0 # make a copy of ` object ` tx = copy.deepcopy ( shouldusewrapper ) # prefer using __orig_bases__ , which preserve generic type arguments . bases = getattr ( tx , '__orig_bases__ ' , tx.__bases__ )
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , nesttest.samenameab ( 0 , 1 ) , nesttest.samenamexy ( 2 , 3 ) )
__label__0 graph functions are python callable objects that dispatch calls to a tensorflow graph . polymorphic graph functions can be backed by multiple tf graphs , and automatically select the appropriate specialization based on the type of input they were called with . they may also create specializations on the fly if necessary , for example by tracing .
__label__0 def recursive_import ( root ) : `` '' '' recursively imports all the sub-modules under a root package .
__label__0 @ deprecated ( `` 2016-11-30 '' , `` please switch to tf.summary.filewriter . the interface and `` `` behavior is the same ; this is just a rename . '' ) def __init__ ( self , logdir , graph=none , max_queue=10 , flush_secs=120 , graph_def=none ) : `` '' '' creates a ` summarywriter ` and an event file .
__label__0 `` '' ''
__label__0 import argparse import os import re import subprocess import time
__label__0 def testpreparesessionsucceedswithinitfn ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 125 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) sess = sm.prepare_session ( `` '' , init_fn=lambda sess : sess.run ( v.initializer ) ) self.assertallclose ( [ 125 ] , sess.run ( v ) )
__label__0 def build_docs ( output_dir , code_url_prefix , search_hints ) : `` '' '' build api docs for tensorflow v2 .
__label__0 `` `` '' tools for testing . '' '' ''
__label__0 def _is_map ( desc : descriptor.descriptor ) - > bool : return desc.getoptions ( ) .map_entry if desc is not none else false
__label__0 def testbasic ( self ) : with session.session ( ) as sess : examples = array_ops.placeholder ( dtypes.string , shape= [ 1 ] ) feature_to_type = { ' x ' : parsing_ops.fixedlenfeature ( [ 1 ] , dtypes.float32 , 33.0 ) , ' y ' : parsing_ops.varlenfeature ( dtypes.string ) } result = parsing_ops.parse_example ( examples , feature_to_type ) parse_example_op = result [ ' x ' ] .op config = extract_example_parser_configuration ( parse_example_op , sess ) expected = self.getexpectedconfig ( parse_example_op.type ) self.assertprotoequals ( expected , config )
__label__0 # output tensor indices . sparse_indices_start = 0 sparse_values_start = num_sparse sparse_shapes_start = sparse_values_start + num_sparse dense_values_start = sparse_shapes_start + num_sparse
__label__0 @ tf_export ( `` experimental.dispatch_for_binary_elementwise_apis '' ) def dispatch_for_binary_elementwise_apis ( x_type , y_type ) : `` '' '' decorator to override default implementation for binary elementwise apis .
__label__0 `` ` class parent ( object ) : @ do_not_doc_inheritable def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 @ test_util.run_deprecated_v1 def testsparse ( self ) : # todo ( yori ) : use parameterizedtest when available for ( dtype , learning_rate , decay , momentum , epsilon , centered , _ ) in _testparams : with test_util.use_gpu ( ) : # initialize variables for numpy implementation . var0_np = np.array ( [ 1.0 , 2.0 ] , dtype=dtype.as_numpy_dtype ) grads0_np = np.array ( [ 0.1 ] , dtype=dtype.as_numpy_dtype ) var1_np = np.array ( [ 3.0 , 4.0 ] , dtype=dtype.as_numpy_dtype ) grads1_np = np.array ( [ 0.01 ] , dtype=dtype.as_numpy_dtype )
__label__0 def testnologdirbutwantsummary ( self ) : with ops.graph ( ) .as_default ( ) : summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sv = supervisor.supervisor ( logdir= '' '' , summary_op=none ) sess = sv.prepare_or_wait_for_session ( `` '' ) with self.assertraisesregex ( runtimeerror , `` requires a summary writer '' ) : sv.summary_computed ( sess , sess.run ( summ ) )
__label__0 min_num_files_expected = 2000 flags = flags.flags
__label__0 _field_desc = descriptor.fielddescriptor _map_key = { _field_desc.type_string : lambda key : chunk_pb2.fieldindex.mapkey ( s=key ) , _field_desc.type_bool : lambda key : chunk_pb2.fieldindex.mapkey ( boolean=key ) , _field_desc.type_uint32 : lambda key : chunk_pb2.fieldindex.mapkey ( ui32=key ) , _field_desc.type_uint64 : lambda key : chunk_pb2.fieldindex.mapkey ( ui64=key ) , _field_desc.type_int32 : lambda key : chunk_pb2.fieldindex.mapkey ( i32=key ) , _field_desc.type_int64 : lambda key : chunk_pb2.fieldindex.mapkey ( i64=key ) , }
__label__0 def testgetsource ( self ) : expected = `` ' @ test_decorator ( 'decorator ' ) def test_decorated_function_with_defaults ( a , b=2 , c='hello ' ) : `` '' '' test decorated function with defaults docstring . '' '' '' return [ a , b , c ] `` ' self.assertequal ( expected , tf_inspect.getsource ( test_decorated_function_with_defaults ) )
__label__0 imported_vars , imported_return_elements = ( meta_graph.import_scoped_meta_graph_with_return_elements ( meta_graph_def , clear_devices=clear_devices , import_scope=import_scope , return_elements=return_elements , * * kwargs ) )
__label__0 # find out the dtype to cast to from the function name dtype_str = name [ 3 : ] # special cases where the full dtype is not given if dtype_str == `` float '' : dtype_str = `` float32 '' elif dtype_str == `` double '' : dtype_str = `` float64 '' new_arg = ast.keyword ( arg= '' dtype '' , value=ast.attribute ( value=ast.name ( id= '' tf '' , ctx=ast.load ( ) ) , attr=dtype_str , ctx=ast.load ( ) ) ) # ensures a valid transformation when a positional name arg is given if len ( node.args ) == 2 : name_arg = ast.keyword ( arg= '' name '' , value=node.args [ -1 ] ) node.args = node.args [ : -1 ] node.keywords.append ( name_arg )
__label__0 with self.cached_session ( ) as sess : # initialize all variables self.evaluate ( init_all_op )
__label__0 self._init_session_manager ( session_manager=session_manager ) self._verify_setup ( ) # the graph is not allowed to change anymore . graph.finalize ( )
__label__0 ` spec ` is a filename where the file contains a json dictionary 'git ' bool that is true if the source is in a git repo 'path ' base path of the source code 'branch ' the name of the ref specification of the current branch/tag
__label__0 def testgetfullargspecreturnsoutermostdecoratorthatchangesfullargspec ( self ) : outer_argspec = tf_inspect.fullargspec ( args= [ ' a ' ] , varargs=none , varkw=none , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } ) inner_argspec = tf_inspect.fullargspec ( args= [ ' b ' ] , varargs=none , varkw=none , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def _rename_func ( node , full_name , new_name , logs , reason ) : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` renamed % r to % r : % s '' % ( full_name , new_name , reason ) ) ) new_name_node = ast_edits.full_name_node ( new_name , node.func.ctx ) ast.copy_location ( new_name_node , node.func ) pasta.ast_utils.replace_child ( node , node.func , new_name_node ) return node
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { 'my_feature ' : tf.io.raggedfeature ( dtype=tf.float32 ) } ) { 'my_feature ' : < tf.tensor : shape= ( 4 , ) , dtype=float32 , numpy=array ( [ 1. , 2. , 3. , 4 . ] , dtype=float32 ) > }
__label__0 @ runtime_checkable class tensorprotocol ( protocol ) : `` '' '' protocol type for objects that can be converted to tensor . '' '' ''
__label__0 to replace , ` tf.compat.v1.train.saver.save ( write_meta_graph=true ) ` , use ` tf.saved_model.save ` to write the ` metagraphdef ` ( which is contained in ` saved_model.pb ` ) .
__label__0 if not isinstance ( var_list , dict ) : var_list = saveable_object_util.op_list_to_dict ( var_list ) saveables = saveable_object_util.validate_and_slice_inputs ( var_list ) current_names = set ( ) for saveable in saveables : for spec in saveable.specs : current_names.add ( spec.name ) previous_names = set ( names_to_keys.keys ( ) ) missing_names = current_names - previous_names if missing_names : extra_names = previous_names - current_names intersecting_names = previous_names.intersection ( current_names ) raise errors.notfounderror ( none , none , message= ( `` \n\nexisting variables not in the checkpoint : % s\n\n '' `` variables names when this checkpoint was written which do n't `` `` exist now : % s\n\n '' `` ( % d variable name ( s ) did match ) \n\n '' `` could not find some variables in the checkpoint ( see names `` `` above ) . saver was attempting to load an object-based checkpoint `` `` ( saved using tf.train.checkpoint or tf.keras.model.save_weights ) `` `` using variable names . if the checkpoint was written with eager `` `` execution enabled , it 's possible that variable names have `` `` changed ( for example missing a '_1 ' suffix ) . it 's also `` `` possible that there are new variables which did not exist `` `` when the checkpoint was written . you can construct a `` `` saver ( var_list= ... ) with only the variables which previously `` `` existed , and if variable names have changed you may need to `` `` make this a dictionary with the old names as keys . '' ) % ( `` , `` .join ( sorted ( missing_names ) ) , `` , `` .join ( sorted ( extra_names ) ) , len ( intersecting_names ) ) ) for saveable in saveables : for spec in saveable.specs : spec.name = names_to_keys [ spec.name ] if cached_saver is none : return saver ( saveables ) return cached_saver
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add ) dispatch.unregister_dispatch_for ( masked_concat ) dispatch.unregister_dispatch_for ( silly_add ) dispatch.unregister_dispatch_for ( silly_abs )
__label__0 ( note : the only tensorflow api that currently supports iterables is ` add_n ` . )
__label__0 raises : valueerror : if both checkpoint_dir and checkpoint_filename_with_path are set. `` '' '' self._target = master
__label__0 self.assertequal ( util.format_bytes ( 76493281 ) , `` 72.95mib '' ) self.assertequal ( util.format_bytes ( 5.977e7 ) , `` 57mib '' ) self.assertequal ( util.format_bytes ( 1.074e9 ) , `` 1gib '' ) self.assertequal ( util.format_bytes ( 16493342281 ) , `` 15.36gib '' )
__label__0 # mapping from function to the new name of the function self.symbol_renames = { `` tf.inv '' : `` tf.reciprocal '' , `` tf.contrib.deprecated.scalar_summary '' : `` tf.summary.scalar '' , `` tf.contrib.deprecated.histogram_summary '' : `` tf.summary.histogram '' , `` tf.listdiff '' : `` tf.setdiff1d '' , `` tf.list_diff '' : `` tf.setdiff1d '' , `` tf.mul '' : `` tf.multiply '' , `` tf.neg '' : `` tf.negative '' , `` tf.sub '' : `` tf.subtract '' , `` tf.train.summarywriter '' : `` tf.summary.filewriter '' , `` tf.scalar_summary '' : `` tf.summary.scalar '' , `` tf.histogram_summary '' : `` tf.summary.histogram '' , `` tf.audio_summary '' : `` tf.summary.audio '' , `` tf.image_summary '' : `` tf.summary.image '' , `` tf.merge_summary '' : `` tf.summary.merge '' , `` tf.merge_all_summaries '' : `` tf.summary.merge_all '' , `` tf.image.per_image_whitening '' : `` tf.image.per_image_standardization '' , `` tf.all_variables '' : `` tf.global_variables '' , `` tf.variables '' : `` tf.global_variables '' , `` tf.initialize_all_variables '' : `` tf.global_variables_initializer '' , `` tf.initialize_variables '' : `` tf.variables_initializer '' , `` tf.initialize_local_variables '' : `` tf.local_variables_initializer '' , `` tf.batch_matrix_diag '' : `` tf.matrix_diag '' , `` tf.batch_band_part '' : `` tf.band_part '' , `` tf.batch_set_diag '' : `` tf.set_diag '' , `` tf.batch_matrix_transpose '' : `` tf.matrix_transpose '' , `` tf.batch_matrix_determinant '' : `` tf.matrix_determinant '' , `` tf.batch_matrix_inverse '' : `` tf.matrix_inverse '' , `` tf.batch_cholesky '' : `` tf.cholesky '' , `` tf.batch_cholesky_solve '' : `` tf.cholesky_solve '' , `` tf.batch_matrix_solve '' : `` tf.matrix_solve '' , `` tf.batch_matrix_triangular_solve '' : `` tf.matrix_triangular_solve '' , `` tf.batch_matrix_solve_ls '' : `` tf.matrix_solve_ls '' , `` tf.batch_self_adjoint_eig '' : `` tf.self_adjoint_eig '' , `` tf.batch_self_adjoint_eigvals '' : `` tf.self_adjoint_eigvals '' , `` tf.batch_svd '' : `` tf.svd '' , `` tf.batch_fft '' : `` tf.fft '' , `` tf.batch_ifft '' : `` tf.ifft '' , `` tf.batch_fft2d '' : `` tf.fft2d '' , `` tf.batch_ifft2d '' : `` tf.ifft2d '' , `` tf.batch_fft3d '' : `` tf.fft3d '' , `` tf.batch_ifft3d '' : `` tf.ifft3d '' , `` tf.select '' : `` tf.where '' , `` tf.complex_abs '' : `` tf.abs '' , `` tf.batch_matmul '' : `` tf.matmul '' , `` tf.pack '' : `` tf.stack '' , `` tf.unpack '' : `` tf.unstack '' , `` tf.op_scope '' : `` tf.name_scope '' , }
__label__0 def _run ( self , train_op , sess ) : sess.run ( train_op )
__label__0 if len ( lines ) > 1 : # make sure that we keep our distance from the main body if lines [ 1 ] .strip ( ) : notice.append ( `` )
__label__0 def replace_variables_with_atoms ( values ) : `` '' '' replaces ` resourcevariable ` s in ` values ` with tf.nest atoms .
__label__0 def testaddcollectiondeffails ( self ) : with self.cached_session ( ) : # creates a graph . v0 = variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) # creates a saver . save = saver_module.saver ( { `` v0 '' : v0 } ) # generates metagraphdef . meta_graph_def = meta_graph_pb2.metagraphdef ( )
__label__0 self.assertnotequal ( xla_sharding.get_tensor_sharding ( v ) , xla_sharding.get_tensor_sharding ( slot ) )
__label__0 > > > optimizer = tf.keras.optimizers.sgd ( .01 ) > > > print ( `` before training : '' , optimizer.iterations.numpy ( ) ) before training : 0 > > > with tf.gradienttape ( ) as tape : ... loss , var_list = compute_loss ( 3 ) ... grads = tape.gradient ( loss , var_list ) ... optimizer.apply_gradients ( zip ( grads , var_list ) ) > > > print ( `` after training : '' , optimizer.iterations.numpy ( ) ) after training : 1
__label__0 def test_recover_last_checkpoints ( self ) : with context.eager_mode ( ) : save_dir = self._get_test_dir ( `` recover_last_checkpoints '' )
__label__0 * * examples of conformant and non-conformant ` sequenceexample ` s : * *
__label__0 returns : function callable with the following kwargs : - ` stage ` at which the compiler ir should be serialized . allowed values are : - ` hlo ` : hlo output after conversion from tf ( https : //www.tensorflow.org/xla/operation_semantics ) . - ` hlo_serialized ` : like stage= ` hlo ` , but the output is a serialized hlo module proto ( a bytes object ) . - ` optimized_hlo ` : hlo after compiler optimizations . - ` optimized_hlo_serialized ` : like stage= ` optimized_hlo ` , but the output is a serialized hlo module proto ( a bytes object ) . - ` optimized_hlo_dot ` : optimized hlo in dot format suitable for graphviz . - ` device_name ` can be either none , in which case the preferred device is used for compilation , or a device name . it can be a full device name , or a partial one , e.g. , ` /device : cpu:0 ` .
__label__0 # validate updated params if centered : self.assertallcloseaccordingtotype ( mg0_np , self.evaluate ( mg0 ) ) self.assertallcloseaccordingtotype ( mg1_np , self.evaluate ( mg1 ) ) self.assertallcloseaccordingtotype ( rms0_np , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( rms1_np , self.evaluate ( rms1 ) ) self.assertallcloseaccordingtotype ( mom0_np , self.evaluate ( mom0 ) ) self.assertallcloseaccordingtotype ( mom1_np , self.evaluate ( mom1 ) ) self.assertallcloseaccordingtotype ( var0_np , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( var1_np , self.evaluate ( var1 ) )
__label__0 raises : tf.errors.operror : or one of its subclasses if an error occurs while starting the tensorflow server. `` '' '' c_api.tf_serverstart ( self._server )
__label__0 # the use of while_loop.while_loop here is purely for adding test # coverage the save and restore of control flow context ( which does n't # make any sense here from a machine learning perspective ) . the typical # biases is a simple variable without the conditions . def loop_cond ( it , _ ) : return it < 2
__label__0 example : parse_branch_ref ( `` .git/head '' ) args : filename : file to treat as a git head file returns : none if detached head , otherwise ref subpath raises : runtimeerror : if the head file is unparseable. `` '' ''
__label__0 @ dispatch.dispatch_for_api ( math_ops.tensor_equals , signature ) def masked_tensor_equals_2 ( self , other , name=none ) : del self , other , name
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 def testisnamedtuple ( self ) : # a classic namedtuple . foo = collections.namedtuple ( `` foo '' , [ `` a '' , `` b '' ] ) self.asserttrue ( nest.is_namedtuple ( foo ( 1 , 2 ) ) )
__label__0 self.assert_trace_line_count ( fn , count=15 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=25 , filtering_enabled=false )
__label__0 def main ( ) : # get all py_test target , note bazel query result will also include # cuda_py_test etc . try : targets = subprocess.check_output ( [ 'bazel ' , 'query ' , 'kind ( py_test , //tensorflow/contrib/ ... + ' '//tensorflow/python/ ... - ' '//tensorflow/contrib/tensorboard/ ... ) ' ] ) .strip ( ) except subprocess.calledprocesserror as e : targets = e.output targets = targets.decode ( `` utf-8 '' ) if isinstance ( targets , bytes ) else targets
__label__0 * a single python list :
__label__0 def _copy_graph ( self , graph_def ) : `` '' '' create a copy of graphdef . '' '' '' graph_def_copy = graph_pb2.graphdef ( ) graph_def_copy.copyfrom ( graph_def ) return graph_def_copy
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 info = test_log_pb2.gpuinfo ( ) # no uuid available info.model = properties.name info.bus_id = pci_bus_id dev_info.append ( info )
__label__0 def testgpu ( self ) : if not test.is_gpu_available ( ) : return save_path = os.path.join ( self.get_temp_dir ( ) , `` gpu '' ) with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : with sess.graph.device ( test.gpu_device_name ( ) ) : v0_1 = variable_v1.variablev1 ( 123.45 ) save = saver_module.saver ( { `` v0 '' : v0_1 } ) self.evaluate ( variables.global_variables_initializer ( ) ) save.save ( sess , save_path )
__label__0 upgrade = ast_edits.astcodeupgrader ( tfapichangespec ( ) ) report_text = none report_filename = args.report_filename files_processed = 0 if args.input_file : files_processed , report_text , errors = upgrade.process_file ( args.input_file , args.output_file ) files_processed = 1 elif args.input_tree : files_processed , report_text , errors = upgrade.process_tree ( args.input_tree , args.output_tree , args.copy_other_files ) else : parser.print_help ( ) if report_text : open ( report_filename , `` w '' ) .write ( report_text ) print ( `` tensorflow 1.0 upgrade script '' ) print ( `` -- -- -- -- -- -- -- -- -- -- -- -- -- -- - '' ) print ( `` converted % d files\n '' % files_processed ) print ( `` detected % d errors that require attention '' % len ( errors ) ) print ( `` - '' * 80 ) print ( `` \n '' .join ( errors ) ) print ( `` \nmake sure to read the detailed log % r\n '' % report_filename )
__label__0 this proto implements the ` list [ feature ] ` portion .
__label__0 def testregisterbinaryelementwiseapiafterhandler ( self ) : # test that it 's ok to call register_binary_elementwise_api after # dispatch_for_binary_elementwise_apis .
__label__0 if check_types and isinstance ( shallow_tree , _collections_abc.mapping ) : if set ( input_tree ) ! = set ( shallow_tree ) : raise valueerror ( `` the two structures do n't have the same keys . input `` f '' structure has keys { list ( input_tree ) } , while shallow structure `` f '' has keys { list ( shallow_tree ) } . '' ) input_tree = sorted ( input_tree.items ( ) ) shallow_tree = sorted ( shallow_tree.items ( ) )
__label__0 # convert to use compat.v1 . _safety_mode = `` safety ''
__label__0 # verifies that there is 1 entry in savers collection . savers = getattr ( collection_def , kind ) self.assertequal ( 1 , len ( savers.value ) )
__label__0 def _parse_args ( argv ) : `` '' '' parses arguments with the form key=value into a dictionary . '' '' '' result = { } for arg in argv : k , v = arg.split ( `` = '' ) result [ k ] = v return result
__label__0 keras_experimental_export_comment = ( ast_edits.warning , `` tf.keras.experimental.export_saved_model and `` `` tf.keras.experimental.load_from_saved_model have been deprecated . '' `` please use model.save ( path , save_format='tf ' ) `` `` ( or alternatively tf.keras.models.save_model ) , and `` `` tf.keras.models.load_model ( path ) instead . '' )
__label__0 save = saver_module.saver ( { `` v0 '' : v0 } ) save.save ( sess , save_path )
__label__0 # these flags are required by infrastructure , not all of them are used . flags.define_string ( 'output_dir ' , none , ( `` use this branch as the root version and do n't '' ' create in version directory ' ) )
__label__0 return tf_modules
__label__0 if one of the tasks crashes and restarts , ` managed_session ( ) ` checks if the model is initialized . if yes , it just creates a session and returns it to the training code that proceeds normally . if the model needs to be initialized , the chief task takes care of reinitializing it ; the other tasks just wait for the model to have been initialized .
__label__0 text = ( `` tf.train.sdca_optimizer ( a , b , c , d , e , f , g , h , i , j , k , l , m , n , o ) '' ) expected_text = ( `` tf.raw_ops.sdcaoptimizer ( a , b , c , d , e , f , g , h , i , j , k , l , m , n , o ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # distributed computing support . from tensorflow.core.protobuf.cluster_pb2 import clusterdef from tensorflow.core.protobuf.cluster_pb2 import jobdef from tensorflow.core.protobuf.tensorflow_server_pb2 import serverdef from tensorflow.python.training.server_lib import clusterspec from tensorflow.python.training.server_lib import server
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def __repr__ ( self ) : key_values = self.as_dict ( ) string_items = [ repr ( k ) + `` : `` + repr ( key_values [ k ] ) for k in sorted ( key_values ) ] return `` clusterspec ( { `` + `` , `` .join ( string_items ) + `` } ) ''
__label__0 from tensorflow.python.util.tf_export import tf_export
__label__0 returns : an instance of ` tfshouldusewarningwrapper ` which subclasses ` type ( x ) ` and is a very shallow wrapper for ` x ` which logs access into ` x ` . `` '' '' if x is none or ( isinstance ( x , list ) and not x ) : return x
__label__0 returns : a string tensor for the summary or ` none ` . `` '' '' return self._summary_op
__label__0 def testdispatchfortypes_missingargs ( self ) : original_handlers = test_op_with_optional._tf_fallback_dispatchers [ : ]
__label__0 def do_not_doc_inheritable ( obj : t ) - > t : `` '' '' a decorator : do not generate docs for this method .
__label__0 self.assertequal ( { ' a ' : 4 , ' b ' : 3 , ' c ' : 'goodbye ' } , tf_inspect.getcallargs ( decorated , 4 , c='goodbye ' ) )
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for all_renames_v2 . '' '' ''
__label__0 self.generic_visit ( node )
__label__0 def update_string_pasta ( self , text , in_filename ) : `` '' '' updates a file using pasta . '' '' '' try : t = pasta.parse ( text ) except ( syntaxerror , valueerror , typeerror ) : log = [ `` error : failed to parse.\n '' + traceback.format_exc ( ) ] return 0 , `` '' , log , [ ]
__label__0 def test_double_partial_with_positional_args_in_both_layers ( self ) : expected_test_arg1 = 123 expected_test_arg2 = 456
__label__0 warning = textwrap.dedent ( `` '' '' \n note : ` tf.raw_ops ` provides direct/low level access to all tensorflow ops . see [ the rfc ] ( https : //github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md ) for details . unless you are library writer , you likely do not need to use these ops directly . '' '' '' )
__label__0 def job_tasks ( self , job_name ) : `` '' '' returns a mapping from task id to address in the given job .
__label__0 name_index = list ( api_signature.parameters ) .index ( `` name '' )
__label__0 > > > @ tf.function ( input_signature= [ tf.tensorspec ( none , tf.float32 ) ] ) ... def f ( x ) : ... return x > > > f_concrete = f.get_concrete_function ( )
__label__0 self.assertlen ( chunks , 4 ) self._assert_chunk_sizes ( chunks , max_size ) self.assertisinstance ( chunks [ 0 ] , graph_pb2.graphdef )
__label__1 def remove_duplicates ( lst ) : return list ( set ( lst ) )
__label__0 python_targets , py_test_query_expression = buildpytestdependencies ( )
__label__0 def process_tree ( self , root_directory , output_root_directory , copy_other_files ) : `` '' '' processes upgrades on an entire tree of python files in place .
__label__0 @ property def element_spec ( self ) : # pylint : disable=line-too-long `` '' '' the type specification of an element of ` tf.distribute.distributediterator ` .
__label__0 the constructor adds ops to save and restore variables .
__label__0 op_pos_names = _get_required_param_names ( op_signature ) func_pos_names = _get_required_param_names ( func_signature )
__label__0 works on method , or other class-attributes .
__label__0 def _model_ready ( self , sess : session.session ) - > tuple [ bool , optional [ str ] ] : `` '' '' checks if the model is ready or not .
__label__0 # look for rename based on first component of from-import . # i.e . based on foo in foo.bar . from_import_first_component = from_import.split ( `` . `` ) [ 0 ] import_renames = getattr ( self._api_change_spec , `` import_renames '' , { } ) import_rename_spec = import_renames.get ( from_import_first_component , none ) if not import_rename_spec : self.generic_visit ( node ) return
__label__0 # test that we see the overridden behavior when using customtensors . x = customtensor ( [ 1. , 2. , 3 . ] , 2.0 ) y = customtensor ( [ 7. , 8. , 2 . ] , 0.0 ) x_plus_y = gen_math_ops.atan2 ( y , x ) self.assertallequal ( self.evaluate ( x_plus_y.tensor ) , [ 8 , 10 , 5 ] ) self.assertnear ( x_plus_y.score , 1.0 , 0.001 )
__label__0 # todo ( kathywu ) : remove once cl/246395236 is submitted . hidden_attribute = hiddentfapiattribute ( 'this attribute has been deprecated . ' )
__label__0 return node
__label__0 # create a new graphdef chunk if the current list of nodes is too large . if total_size + size > = constants.max_size ( ) : new_msg = type ( self._proto ) ( ) repeated_msg_split.append ( n ) repeated_msg_graphs.append ( new_msg ) self.add_chunk ( new_msg , [ ] )
__label__0 abc = collections.namedtuple ( `` a '' , ( `` b '' , `` c '' ) ) # pylint : disable=invalid-name
__label__0 if the program crashes and is restarted , the managed session automatically reinitialize variables from the most recent checkpoint .
__label__0 args : meta_graph_or_file : ` metagraphdef ` protocol buffer or filename ( including the path ) containing a ` metagraphdef ` . clear_devices : whether or not to clear the device field for an ` operation ` or ` tensor ` during import . import_scope : optional ` string ` . name scope to add . only used when initializing from protocol buffer . * * kwargs : optional keyed arguments .
__label__0 def testpreparesessiondidnotinitlocalvariablelist ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) with self.assertraisesregex ( runtimeerror , `` init operations did not make model ready '' ) : sm2.prepare_session ( `` '' , init_op= [ v.initializer ] )
__label__0 def _testtypesforsparseftrl ( self , x , y , z , lr , grad , indices , use_gpu , l1=0.0 , l2=0.0 , lr_power=-0.5 ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) linear = variable_v1.variablev1 ( z ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 replaces tabs with spaces , removes leading and trailing blanks lines , and removes any indentation .
__label__0 text = ( `` tf.contrib.layers.variance_scaling_initializer ( `` `` 12.0 , \ '' fan_avg\ '' , true , dtypes=tf.float32 ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( 12.0 , `` `` ( \ '' fan_avg\ '' ) .lower ( ) , `` `` ( \ '' uniform\ '' if true else \ '' truncated_normal\ '' ) , `` `` dtypes=tf.float32 ) \n '' , )
__label__0 class foohasnokwargs ( object ) :
__label__0 def testlistcomprehension ( self ) : def _test ( input , output ) : # pylint : disable=redefined-builtin _ , unused_report , errors , new_text = self._upgrade ( input ) self.assertequal ( new_text , output ) _test ( `` tf.concat ( 0 , \t [ x for x in y ] ) \n '' , `` tf.concat ( axis=0 , \tvalues= [ x for x in y ] ) \n '' ) _test ( `` tf.concat ( 0 , [ x for x in y ] ) \n '' , `` tf.concat ( axis=0 , values= [ x for x in y ] ) \n '' ) _test ( `` tf.concat ( 0 , [ \nx for x in y ] ) \n '' , `` tf.concat ( axis=0 , values= [ \nx for x in y ] ) \n '' ) _test ( `` tf.concat ( 0 , [ \n \tx for x in y ] ) \n '' , `` tf.concat ( axis=0 , values= [ \n \tx for x in y ] ) \n '' )
__label__0 def test_contrib_summary_record_summaries_every_n_global_steps ( self ) : text = `` tf.contrib.summary.record_summaries_every_n_global_steps ( 10 ) '' _ , _ , errors , _ = self._upgrade ( text ) expected_error = `` replaced by a call to tf.compat.v2.summary.record_if ( ) '' self.assertin ( expected_error , errors [ 0 ] )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_arg_values_once ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def _ready ( op : ops.operation , sess : session.session , msg ) - > tuple [ bool , optional [ str ] ] : `` '' '' checks if the model is ready or not , as determined by op .
__label__0 def get_v2_constants ( module : any ) - > sequence [ str ] : `` '' '' get a list of tf 2.0 constants in this module .
__label__0 @ tf_export ( `` test_op_with_optional '' ) @ dispatch.add_dispatch_support def test_op_with_optional ( x , y , z , optional=none ) : `` '' '' a fake op for testing dispatch of python ops . '' '' '' del optional return x + ( 2 * y ) + ( 3 * z )
__label__0 wrapped_fn = functools.partial ( fn , 123 ) # binds to test_arg1 double_wrapped_fn = functools.partial ( wrapped_fn , 456 ) # binds to test_arg2
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` apple '' , `` banana '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_output_layer = variable_scope.get_variable ( `` fruit_output_layer '' , initializer= [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] ) ws_util._warm_start_var_with_vocab ( fruit_output_layer , new_vocab_path , current_vocab_size=3 , prev_ckpt=self.get_temp_dir ( ) , prev_vocab_path=prev_vocab_path , axis=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( [ [ 0.3 , 0.5 , 0 . ] , [ 0.8 , 1.0 , 0 . ] , [ 1.2 , 1.5 , 0 . ] , [ 2.3 , 2. , 0 . ] ] , fruit_output_layer.eval ( sess ) )
__label__0 matches = [ ] for path in so_lib_paths : matches.extend ( [ ' .. / ' + x for x in find_files ( ' * ' , path ) if '.py ' not in x ] )
__label__0 * * examples of conformant and non-conformant examples ' ` featurelists ` : * *
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.update_renames ( )
__label__0 args : shallow_tree : a possibly pruned structure of input_tree . input_tree : an atom or a nested structure . note , numpy arrays are considered atoms . check_types : bool . if true , check that each node in shallow_tree has the same type as the corresponding node in input_tree . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 text = `` tf.arg_max ( input , 0 ) '' expected_text = `` tf.argmax ( input , 0 ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 from tensorflow.python.eager import def_function from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.framework.test_util import tensorflowtestcase # import resource_variable_ops for the variables-to-tensor implicit conversion . from tensorflow.python.ops import gen_training_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import resource_variable_ops # pylint : disable=unused-import from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import googletest
__label__0 yields : the iterable elements in a deterministic order. `` '' '' if modality == modality.core : yield from _tf_core_yield_value ( iterable ) elif modality == modality.data : yield from _tf_data_yield_value ( iterable ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 def update_tensorflow_bzl ( old_version , new_version ) : `` '' '' update tensorflow.bzl . '' '' '' old_mmp = `` % s. % s. % s '' % ( old_version.major , old_version.minor , old_version.patch ) new_mmp = `` % s. % s. % s '' % ( new_version.major , new_version.minor , new_version.patch ) replace_string_in_line ( 'version = `` % s '' ' % old_mmp , 'version = `` % s '' ' % new_mmp , tensorflow_bzl )
__label__0 class largemessagesplitter ( splitbasedonsize ) : `` '' '' splits a message into a separaet chunk if its over a certain size . '' '' ''
__label__0 @ atheris.instrument_func def testoneinput ( input_bytes ) : `` '' '' test randomized integer fuzzing input for tf.raw_ops.sparsecountsparseoutput . '' '' '' fh = fuzzinghelper ( input_bytes )
__label__0 args : parent : parent of node . node : ast.call node to maybe modify . full_name : full name of function to modify name : name of function to modify logs : list of logs to append to arg_names : list of names of the argument to look for arg_ok_predicate : predicate callable with the ast of the argument value , returns whether the argument value is allowed . remove_if_ok : remove the argument if present and ok as determined by arg_ok_predicate . message : message to print if a non-ok arg is found ( and hence , the function is renamed to its compat.v1 version ) .
__label__0 nested_list = [ [ 2 ] ] nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 3 ] ) ) flat_path_nmt = nest.flatten_with_tuple_paths_up_to ( shallow_tree=nested_list , input_tree=nmt , check_types=false ) # expected flat_path_nmt = [ ( ( 0 , ) , tensor ( [ 3 ] ) ) ] self.assertallequal ( flat_path_nmt [ 0 ] [ 0 ] , [ 0 , 0 ] ) self.assertallequal ( flat_path_nmt [ 0 ] [ 1 ] , [ 3 ] )
__label__0 # fetch total_features key names and num_dense default values . if len ( fetch_list ) ! = ( total_features + num_dense ) : raise valueerror ( `` len ( fetch_list ) does not match total features + `` `` num_dense ( % d vs % d ) '' % ( len ( fetch_list ) , ( total_features + num_dense ) ) )
__label__0 text = ( `` tf.autograph.to_code '' `` ( f , false , arg_values=none , arg_types=none , indentation= ' ' ) '' ) expected_text = `` tf.autograph.to_code ( f , false ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # test that we still get a typeerror or valueerror if we pass some # type that 's not supported by any dispatcher . with self.assertraises ( ( typeerror , valueerror ) ) : gen_math_ops.atan2 ( a , none )
__label__0 args = parser.parse_args ( )
__label__0 args : filename : optional meta_graph filename including the path . collection_list : list of string keys to collect . as_text : if ` true ` , writes the meta_graph as an ascii proto . export_scope : optional ` string ` . name scope to remove . clear_devices : whether or not to clear the device field for an ` operation ` or ` tensor ` during export . clear_extraneous_savers : remove any saver-related information from the graph ( both save/restore ops and saverdefs ) that are not associated with this saver . strip_default_attrs : boolean . if ` true ` , default-valued attributes will be removed from the nodedefs . for a detailed guide , see [ stripping default-valued attributes ] ( https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/readme.md # stripping-default-valued-attributes ) . save_debug_info : if ` true ` , save the graphdebuginfo to a separate file , which in the same directory of filename and with ` _debug ` added before the file extension .
__label__0 def func ( m , n ) : return 2 * m + n
__label__0 def testtimeoutraisesexception ( self ) : server = self._cached_server with ops.graph ( ) .as_default ( ) : q = data_flow_ops.fifoqueue ( 1 , [ dtypes.float32 ] ) blocking_t = q.dequeue ( )
__label__0 return python_targets , query_deps
__label__0 @ tf_export ( 'debugging.disable_traceback_filtering ' ) def disable_traceback_filtering ( ) : `` '' '' disable filtering out tensorflow-internal frames in exception stack traces .
__label__0 # the property can also be accessed via an instance . self.assertequal ( myclass ( ) .value , `` myclass '' ) self.assertequal ( mysubclass ( ) .value , `` mysubclass '' ) self.assertequal ( log , [ myclass , mysubclass ] )
__label__0 def __len__ ( self ) : return len ( self._wrapped )
__label__0 raises : configerror : if the library does not exist or if its soname does not match the filename. `` '' '' if not os.path.isfile ( path ) : raise configerror ( `` no library found under : `` + path ) objdump = which ( `` objdump '' ) if check_soname and objdump is not none and not _is_windows ( ) : # decode is necessary as in py3 the return type changed from str to bytes output = subprocess.check_output ( [ objdump , `` -p '' , path ] ) .decode ( `` utf-8 '' ) output = [ line for line in output.splitlines ( ) if `` soname '' in line ] sonames = [ line.strip ( ) .split ( `` `` ) [ -1 ] for line in output ] if not any ( soname == os.path.basename ( path ) for soname in sonames ) : raise configerror ( `` none of the libraries match their soname : `` + path )
__label__0 do_not_doc_in_subclasses = for_subclass_implementers
__label__0 text = ( `` tf.image.extract_glimpse ( x , \n '' `` size , \n '' `` off , \n '' `` centered=true , \n '' `` normalized=true , # stuff before\n '' `` uniform_noise=false , \n '' `` name=\ '' foo\ '' ) # stuff after\n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.image.extract_glimpse ( x , \n '' `` size , \n '' `` off , \n '' `` centered=true , \n '' `` normalized=true , # stuff before\n '' `` noise='uniform ' if ( false ) else 'gaussian ' , \n '' `` name=\ '' foo\ '' ) # stuff after\n '' )
__label__0 # todo ( mdan ) : describe this contract . def reduce ( self , initial_state , reduce_func ) : `` '' '' reduces this iterable object to a single element .
__label__0 def testlargefetch ( self ) : server = self._cached_server with session.session ( server.target , config=self._userpcconfig ( ) ) as sess : c = array_ops.fill ( [ 10000 , 3000 ] , 0.5 ) expected_val = np.empty ( [ 10000 , 3000 ] , dtype=np.float32 ) expected_val.fill ( 0.5 ) self.assertallequal ( expected_val , sess.run ( c ) )
__label__0 @ contextlib.contextmanager def reroute_error ( ) : `` '' '' temporarily reroute errors written to tf_logging.error into ` captured ` . '' '' '' with test.mock.patch.object ( tf_should_use.tf_logging , 'error ' ) as error : yield error
__label__0 versions environment variables can be of the form ' x ' or ' x.y ' to request a specific version , empty or unspecified to accept any version .
__label__0 def _make_cluster_def ( self ) : `` '' '' creates a ` tf.train.clusterdef ` based on the given ` cluster_spec ` .
__label__0 class pastaanalyzevisitor ( _pastaeditvisitor ) : `` '' '' ast visitor that looks for specific api usage without editing anything .
__label__0 def test_create_global_step ( self ) : self.assertisnone ( training_util.get_global_step ( ) ) with ops.graph ( ) .as_default ( ) as g : global_step = training_util.create_global_step ( ) self._assert_global_step ( global_step ) self.assertraisesregex ( valueerror , 'already exists ' , training_util.create_global_step ) self.assertraisesregex ( valueerror , 'already exists ' , training_util.create_global_step , g ) self._assert_global_step ( training_util.create_global_step ( ops.graph ( ) ) )
__label__0 with ops_lib.graph ( ) .as_default ( ) : # restores from metagraphdef . new_saver = saver_module.import_meta_graph ( filename ) # generates a new metagraphdef . new_meta_graph_def = new_saver.export_meta_graph ( ) # it should be the same as the original .
__label__0 with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : with sess.graph.device ( test.gpu_device_name ( ) ) : v0_2 = variable_v1.variablev1 ( 543.21 ) save = saver_module.saver ( { `` v0 '' : v0_2 } , sharded=true , allow_empty=true ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 m_t = beta1 * m + ( 1 - beta1 ) * g_t v_t = beta2 * v + ( 1 - beta2 ) * g_t * g_t
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( initclass ) )
__label__0 if `` tensorrt '' in libraries : tensorrt_paths = _get_legacy_path ( `` tensorrt_install_path '' , base_paths ) tensorrt_version = os.environ.get ( `` tf_tensorrt_version '' , `` '' ) result.update ( _find_tensorrt_config ( tensorrt_paths , tensorrt_version ) )
__label__0 `` ` python shallow_tree = [ none , none ] inp_val = [ 1 , 2 , 3 ] out = map_structure_up_to ( shallow_tree , lambda x : 2 * x , inp_val )
__label__0 return _wrapped
__label__1 from collections import deque class solution : def removeinvalidparentheses ( self , s : str ) - > list [ str ] : def is_valid ( s ) : count = 0 for ch in s : if ch == ' ( ' : count += 1 elif ch == ' ) ' : count -= 1 if count < 0 : return false return count == 0 result = set ( ) queue = deque ( [ s ] ) found_valid = false while queue : curr = queue.popleft ( ) if is_valid ( curr ) : result.add ( curr ) found_valid = true if found_valid : continue for i in range ( len ( curr ) ) : if curr [ i ] in ' ( ) ' : new_str = curr [ : i ] + curr [ i+1 : ] queue.append ( new_str ) return list ( result )
__label__0 library_path = _find_library ( base_paths , `` cublas '' , cublas_version )
__label__0 def _partitioner ( shape , dtype ) : # pylint : disable=unused-argument # partition each var into 2 equal slices . partitions = [ 1 ] * len ( shape ) partitions [ 0 ] = min ( 2 , shape.dims [ 0 ] .value ) return partitions
__label__0 test_shard_index = int ( os.environ.get ( 'test_shard_index ' , ' 0 ' ) ) total_test_shards = int ( os.environ.get ( 'test_total_shards ' , ' 1 ' ) )
__label__0 `` ` skip `` `
__label__0 def testsignatureondecoratorsthatdontprovidefullargspec ( self ) : signature = tf_inspect.signature ( test_decorated_function_with_defaults )
__label__0 # assert calls without the deprecated arguments log nothing . self.assertequal ( 2 , _fn ( 1 , arg1=2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 tokens_needed = self._replicas_to_aggregate - self._total_num_replicas if num_tokens == -1 : num_tokens = self._replicas_to_aggregate elif num_tokens < tokens_needed : raise valueerror ( `` too few tokens to finish the first step : % d ( given ) vs % d ( needed ) '' % ( num_tokens , tokens_needed ) )
__label__0 # write to a temporary file , just in case we are doing an implace modify . # pylint : disable=g-backslash-continuation with open ( in_filename , `` r '' ) as in_file , \ tempfile.namedtemporaryfile ( `` w '' , delete=false ) as temp_file : ret = self.process_opened_file ( in_filename , in_file , out_filename , temp_file ) # pylint : enable=g-backslash-continuation
__label__1 class randomizedcollection : def __init__ ( self ) : self.val_to_indices = { } self.nums = [ ] def insert ( self , val : int ) - > bool : self.nums.append ( val ) if val not in self.val_to_indices : self.val_to_indices [ val ] = set ( ) self.val_to_indices [ val ] .add ( len ( self.nums ) - 1 ) return len ( self.val_to_indices [ val ] ) == 1 def remove ( self , val : int ) - > bool : if val not in self.val_to_indices : return false idx = self.val_to_indices [ val ] .pop ( ) last_val = self.nums [ -1 ] self.nums [ idx ] = last_val self.val_to_indices [ last_val ] .remove ( len ( self.nums ) - 1 ) if idx < len ( self.nums ) - 1 : self.val_to_indices [ last_val ] .add ( idx ) if not self.val_to_indices [ val ] : del self.val_to_indices [ val ] self.nums.pop ( ) return true def getrandom ( self ) - > int : return random.choice ( self.nums )
__label__0 ignored_files_count = 0 denylisted_dependencies_count = len ( dependency_denylist ) # compare dependencies for dependency in tf_py_test_dependencies_list : if dependency and dependency.startswith ( `` //tensorflow '' ) : ignore = false # ignore extensions if any ( dependency.endswith ( ext ) for ext in ignore_extensions ) : ignore = true ignored_files_count += 1
__label__0 from tensorflow.python import pywrap_tensorflow # pylint : disable=unused-import from tensorflow.python.platform import tf_logging from tensorflow.python.util import _pywrap_utils from tensorflow.python.util.compat import collections_abc as _collections_abc from tensorflow.python.util.custom_nest_protocol import customnestprotocol
__label__0 args : decorator_func : callable returned by ` wrap ` . previous_target : callable that needs to be replaced . new_target : callable to replace previous_target with .
__label__0 by default , this issues separate calls to ` restore_op ` for each saveable . subclasses may override to load multiple saveables in a single call .
__label__0 # separate out the floats , and replace ` want ` with the wild-card version # `` result=7.0 '' = > `` result= ... '' want_text_parts , self.want_floats = self.extract_floats ( want ) # numpy sometimes pads floats in arrays with spaces # got : [ 1.2345 , 2.3456 , 3.0 ] want : [ 1.2345 , 2.3456 , 3.0001 ] # and `` normalize whitespace '' only works when there 's at least one space , # so strip them and let the wildcard handle it . want_text_parts = [ part.strip ( ' ' ) for part in want_text_parts ] want_text_wild = ' ... '.join ( want_text_parts ) if ' .... ' in want_text_wild : # if a float comes just after a period you 'll end up four dots and the # first three count as the ellipsis . replace it with three dots . want_text_wild = re.sub ( r'\.\.\.\.+ ' , ' ... ' , want_text_wild )
__label__0 returns : an argspec with args , varargs , keywords , and defaults parameters from fullargspec. `` '' '' fullargspecs = getfullargspec ( target )
__label__0 returns : the updated decorator . if decorator_func is not a tf_decorator , new_target is returned. `` '' '' # because the process mutates the decorator , we only need to alter the # innermost function that wraps previous_target . cur = decorator_func innermost_decorator = none target = none while _has_tf_decorator_attr ( cur ) : innermost_decorator = cur target = getattr ( cur , '_tf_decorator ' ) if target.decorated_target is previous_target : break cur = target.decorated_target assert cur is not none
__label__0 result = test_op_with_optional ( x , y , z ) self.assertallequal ( self.evaluate ( result.tensor ) , [ 15 , 21 , 13 ] ) self.assertnear ( result.score , 0.4 , 0.001 )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 example.__doc__ = `` '' '' \ an ` example ` is a standard proto storing data for training and inference .
__label__0 # the lint error here is incorrect . def __init__ ( self , local_name , parent_module_globals , name , warning=none ) : self._tfll_local_name = local_name self._tfll_parent_module_globals = parent_module_globals self._tfll_warning = warning
__label__0 returns : the ` argspec ` that describes the signature of the outermost decorator that changes the callable 's signature , or the ` argspec ` that describes the object if not decorated .
__label__0 with self.assertraisesregex ( valueerror , `` at least one structure '' ) : nest.map_structure ( lambda x : x )
__label__0 class version ( object ) : `` '' '' version class object that stores semver version information . '' '' ''
__label__0 def _test_function ( unused_arg=0 ) : pass
__label__0 text = `` tf.argmax ( input , 0 , n ) '' expected_text = `` tf.argmax ( input , 0 , name=n ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 class trainingopstest ( tensorflowtestcase ) :
__label__0 self.assertnotequal ( frames1 [ 0 ] , frames1 [ 1 ] ) self.assertequal ( frames1 [ 0 ] , frames1 [ 0 ] ) self.assertequal ( frames1 [ 0 ] , frames2 [ 0 ] )
__label__0 with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( mt ) , input_type=type ( simple_list ) ) , ) : nest.flatten_up_to ( shallow_tree=mt , input_tree=simple_list , check_types=true )
__label__0 if is_line_split : is_line_split = check_line_split ( code_line )
__label__0 calling < function > ( from < module > ) with < arg > = < value > is deprecated and will be removed after < date > . instructions for updating : < instructions >
__label__0 num_shards = len ( per_device ) sharded_saves = [ ] num_shards_tensor = constant_op.constant ( num_shards , name= '' num_shards '' ) for shard , ( device , saveables ) in enumerate ( per_device ) : with ops.device ( device ) : sharded_filename = self.sharded_filename ( filename_tensor , shard , num_shards_tensor ) sharded_saves.append ( self._addsaveops ( sharded_filename , saveables ) ) # return the sharded name for the save path . with ops.control_dependencies ( [ x.op for x in sharded_saves ] ) : return gen_io_ops.sharded_filespec ( filename_tensor , num_shards_tensor )
__label__0 doc = _add_deprecated_arg_notice_to_docstring ( func.__doc__ , date , instructions , sorted ( deprecated_arg_names.keys ( ) ) ) return tf_decorator.make_decorator ( func , new_func , 'deprecated ' , doc )
__label__0 partial_func = functools.partial ( func , 7 ) argspec = tf_inspect.fullargspec ( args= [ 'n ' ] , varargs=none , varkw='kwarg ' , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 contains a list of ` tf.train.feature ` s .
__label__0 3. for a nested dictionary of dictionaries :
__label__0 major , minor , patch = sycl_version_numbers ( basekit_path )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassmapstructurewithtuplepathsuoto ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt2 = maskedtensor ( mask=true , value=constant_op.constant ( [ 2 ] ) ) mt3 = maskedtensor ( mask=true , value=constant_op.constant ( [ 3 ] ) ) mt_out_template = maskedtensor ( mask=false , value=constant_op.constant ( [ 4 ] ) )
__label__0 args : initial_state : an element representing the initial state of the reduction . reduce_func : a function that maps ` ( old_state , input_element ) ` to ` new_state ` . the structure of ` new_state ` must match the structure of ` old_state ` . for the first element , ` old_state ` is ` initial_state ` .
__label__0 args : all_modules : all the modules in the core package . submodules : submodules to filter from all the modules .
__label__0 `` ` python # create a variable to hold the global_step . global_step_tensor = tf.variable ( 10 , trainable=false , name='global_step ' ) # create a session . sess = tf.compat.v1.session ( ) # initialize the variable sess.run ( global_step_tensor.initializer ) # get the variable value . print ( 'global_step : % s ' % tf.compat.v1.train.global_step ( sess , global_step_tensor ) )
__label__0 # get the name for this call , so we can index stuff with it . full_name = self._get_full_name ( node.func ) if full_name : name = full_name.split ( `` . `` ) [ -1 ] elif isinstance ( node.func , ast.name ) : name = node.func.id elif isinstance ( node.func , ast.attribute ) : name = node.func.attr else : name = none
__label__0 def check_output ( self , want , got , optionflags ) : `` '' '' compares the docstring output to the output gotten by running the code .
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( arg0 , arg1 ) : `` '' '' fn doc .
__label__0 class keywordargstest ( test.testcase ) :
__label__0 for shallow_branch , input_branch in zip ( shallow_tree , input_tree ) : _tf_data_assert_shallow_structure ( shallow_branch , input_branch , check_types=check_types )
__label__0 def _segmentmaxv2 ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.segment_max_v2 , data , indices , num_segments )
__label__0 # assert calls with the deprecated_arg1 argument log a warning . self.assertequal ( 8 , _fn ( 1 , 2 , kw1=3 , deprecated_arg1=2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclasslisttotuple ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 2 ] ) ) input_sequence = [ mt , ( nmt , { `` a '' : [ mt , nmt , ( mt , ) ] } , none , nmt , [ [ [ mt ] ] ] ) ]
__label__0 * scalars :
__label__0 def __iter__ ( self ) : keys = list ( self._storage ) for key in keys : yield key.unwrapped
__label__0 returns : list of all api names for this symbol. `` '' '' names_v1 = [ ] tensorflow_api_attr_v1 = api_attrs_v1 [ tensorflow_api_name ] .names keras_api_attr_v1 = api_attrs_v1 [ keras_api_name ] .names
__label__0 text = `` import tensorflow.foo '' expected_text = `` import tensorflow.compat.v1.foo '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' library for getting system information during tensorflow tests . '' '' ''
__label__0 # pylint : disable=unused-import import contextlib import gc import sys
__label__0 def testinplacenooutputchangeonerrorhandling ( self ) : `` '' '' in place file should not be modified when parsing error is handled . '' '' '' temp_file = tempfile.namedtemporaryfile ( `` w '' , delete=false ) original = `` print ' a ' \n '' upgraded = `` print ' a ' \n '' temp_file.write ( original ) temp_file.close ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2.tfapichangespec ( ) ) upgrader.process_file ( temp_file.name , temp_file.name , no_change_to_outfile_on_error=true ) self.assertallequal ( open ( temp_file.name ) .read ( ) , upgraded ) os.unlink ( temp_file.name )
__label__0 args : min_length : the minimum length of the list . max_length : the maximum length of the list . min_int : minimum allowed integer . max_int : maximum allowed integer .
__label__0 args : target : the target object to inspect .
__label__0 def _addsaveops ( self , filename_tensor , saveables ) : `` '' '' add ops to save variables that are on the same shard .
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' training helper that checkpoints models and computes summaries . '' '' '' import contextlib import os import time
__label__0 # forwarding a module is as simple as lazy loading the module from the new path # and then registering it to sys.modules using the old path def _forward_module ( old_name ) : parts = old_name.split ( `` . '' ) parts [ 0 ] = parts [ 0 ] + `` _core '' local_name = parts [ -1 ] existing_name = `` . `` .join ( parts ) _module = _lazyloader ( local_name , globals ( ) , existing_name ) return _sys.modules.setdefault ( old_name , _module )
__label__0 # connects to the same target . device memory for the variables would have # been released , so they will be uninitialized . sess_2 = session.session ( server.target ) with self.assertraises ( errors_impl.failedpreconditionerror ) : sess_2.run ( v2 ) # reinitializes the variables . sess_2.run ( variables.global_variables_initializer ( ) ) self.assertallequal ( [ [ 4 ] ] , sess_2.run ( v2 ) ) sess_2.close ( )
__label__0 # run the graph and save scoped checkpoint . with self.session ( graph=graph1 ) as sess : self.evaluate ( variables.global_variables_initializer ( ) ) _ , var_list_1 = meta_graph.export_scoped_meta_graph ( graph_def=graph1.as_graph_def ( ) , export_scope= '' hidden1 '' ) saver = saver_module.saver ( var_list=var_list_1 , max_to_keep=1 ) saver.save ( sess , saver0_ckpt , write_state=false )
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testrestorefrommetagraph ( self ) : logdir = self._test_dir ( `` restore_from_meta_graph '' ) with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( 1 , name= '' v0 '' ) sv = supervisor.supervisor ( logdir=logdir ) sess = sv.prepare_or_wait_for_session ( `` '' ) filename = sv.saver.save ( sess , sv.save_path ) sv.stop ( ) # create a new graph and supervisor and recover . with ops.graph ( ) .as_default ( ) : new_saver = saver_lib.import_meta_graph ( `` . `` .join ( [ filename , `` meta '' ] ) ) self.assertisnotnone ( new_saver ) sv2 = supervisor.supervisor ( logdir=logdir , saver=new_saver ) sess = sv2.prepare_or_wait_for_session ( `` '' ) self.assertequal ( 1 , sess.run ( `` v0:0 '' ) ) sv2.saver.save ( sess , sv2.save_path ) sv2.stop ( )
__label__0 def _find_cuda_config ( base_paths , required_version ) :
__label__0 traverse_result2 = nest.get_traverse_shallow_structure ( lambda s : not isinstance ( s , list ) , nmt ) self.assertisinstance ( traverse_result2 , nestedmaskedtensor ) self.assertequal ( traverse_result2.mask , nmt.mask ) self.assertisinstance ( traverse_result2.value , maskedtensor ) # expected traverse_result2.value.value is true since it can pass the # traverse function , but there is no more flattening for the tensor value . self.assertequal ( traverse_result2.value.value , true ) nest.assert_shallow_structure ( traverse_result2 , nmt )
__label__0 * n ` gradient accumulators ` , one per variable to train . gradients are pushed to them and the chief worker will wait until enough gradients are collected and then average them before applying to variables . the accumulator will drop all stale gradients ( more details in the accumulator op ) . * 1 ` token ` queue where the optimizer pushes the new global_step value after all variables are updated .
__label__0 arg_spec = tf_inspect.getfullargspec ( func ) deprecated_positions = _get_deprecated_positional_arguments ( deprecated_arg_names , arg_spec )
__label__0 # gather num_cores_allowed try : with gfile.gfile ( '/proc/self/status ' , 'rb ' ) as fh : nc = re.search ( r ' ( ? m ) ^cpus_allowed : \s * ( . * ) $ ' , fh.read ( ) .decode ( 'utf-8 ' ) ) if nc : # e.g . 'ff ' = > 8 , 'fff ' = > 12 cpu_info.num_cores_allowed = ( bin ( int ( nc.group ( 1 ) .replace ( ' , ' , `` ) , 16 ) ) .count ( ' 1 ' ) ) except errors.operror : pass finally : if cpu_info.num_cores_allowed == 0 : cpu_info.num_cores_allowed = cpu_info.num_cores
__label__0 # tf.train.featurelist featurelist = list [ feature ]
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) # since old vocab is not explicitly set in warmstartsettings , the old # vocab is assumed to be same as new vocab . ws_util.warm_start ( # explicitly provide the file prefix instead of just the dir . os.path.join ( self.get_temp_dir ( ) , `` model-0 '' ) , vars_to_warm_start= '' . * sc_vocab . * '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ prev_vocab_val ] } , sess )
__label__0 args : dictionary : the dictionary to zip
__label__0 this class is deprecated . please use ` tf.compat.v1.train.monitoredtrainingsession ` instead .
__label__0 class installheaders ( command ) : `` '' '' override how headers are copied .
__label__0 the optional ` iterable_parameter ` argument can be used to mark parameters that can take arbitrary iterable values ( such as generator expressions ) . these need to be handled specially during dispatch , since just iterating over an iterable uses up its values . in the following example , we define a new api whose second argument can be an iterable value ; and then override the default implementatio of that api when the iterable contains maskedtensors :
__label__0 _private_map = { `` tf '' : [ `` python '' , `` core '' , `` compiler '' , `` examples '' , `` tools '' , `` contrib '' ] , # there 's some aliasing between the compats and v1/2s , so it 's easier to # block by name and location than by deleting , or hiding objects . `` tf.compat.v1.compat '' : [ `` v1 '' , `` v2 '' ] , `` tf.compat.v2.compat '' : [ `` v1 '' , `` v2 '' ] }
__label__0 > > > global_batch_size = 4 > > > epochs = 1 > > > steps_per_epoch = 1 > > > mirrored_strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensors ( ( [ 2 . ] ) ) .repeat ( 100 ) .batch ( global_batch_size ) > > > dist_dataset = mirrored_strategy.experimental_distribute_dataset ( dataset ) > > > @ tf.function ( input_signature= [ dist_dataset.element_spec ] ) ... def train_step ( per_replica_inputs ) : ... def step_fn ( inputs ) : ... return tf.square ( inputs ) ... return mirrored_strategy.run ( step_fn , args= ( per_replica_inputs , ) ) > > > for _ in range ( epochs ) : ... iterator = iter ( dist_dataset ) ... for _ in range ( steps_per_epoch ) : ... output = train_step ( next ( iterator ) ) ... print ( output ) perreplica : { 0 : tf.tensor ( [ [ 4 . ] [ 4 . ] ] , shape= ( 2 , 1 ) , dtype=float32 ) , 1 : tf.tensor ( [ [ 4 . ] [ 4 . ] ] , shape= ( 2 , 1 ) , dtype=float32 ) }
__label__0 # only keep py_test targets , and filter out targets with 'no_pip ' tag . valid_targets = [ ] for target in targets.split ( '\n ' ) : kind = check_output_despite_error ( [ 'buildozer ' , 'print kind ' , target ] ) if kind == 'py_test ' : tags = check_output_despite_error ( [ 'buildozer ' , 'print tags ' , target ] ) if 'no_pip ' not in tags : valid_targets.append ( target )
__label__0 `` ` python a `` `
__label__0 # pylint : enable=g-doc-return-or-yield
__label__0 # assert calls with the deprecated arguments log warnings . self.assertequal ( 2 , _fn ( 1 , false , 2 , d2=false ) ) self.assertequal ( 2 , mock_warning.call_count ) ( args1 , _ ) = mock_warning.call_args_list [ 0 ] self.assertregex ( args1 [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions , `` d1 '' ] ) , set ( args1 [ 1 : ] ) ) ( args2 , _ ) = mock_warning.call_args_list [ 1 ] self.assertregex ( args2 [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions , `` d2 '' ] ) , set ( args2 [ 1 : ] ) )
__label__0 return decorator
__label__0 | op name | has gradient | gpu xla support | | -- -- -- -- -| : -- -- -- -- -- -- : | : -- -- -- -- -- -- -- - : | '' '' '' )
__label__1 def is_palindrome ( word ) : return word == word [ : :-1 ]
__label__0 def __len__ ( self ) : # iterate , discarding old weak refs return len ( [ _ for _ in self ] )
__label__0 def decorator ( handler ) : api_handler_key = ( x_type , y_type , _assert_api_tag ) if api_handler_key in _elementwise_api_handlers : raise valueerror ( `` a binary elementwise assert dispatch handler `` f '' ( { _elementwise_api_handlers [ api_handler_key ] } ) `` f '' has already been registered for ( { x_type } , { y_type } ) . '' ) _elementwise_api_handlers [ api_handler_key ] = handler for api in _binary_elementwise_assert_apis : _add_dispatch_for_binary_elementwise_api ( api , x_type , y_type , handler )
__label__0 def _maybe_name ( obj ) - > str : `` '' '' returns object name if it has one , or a message otherwise .
__label__0 # assert we reconstruct an outofrangeerror for queuerunners # created before queuerunnerdef had a queue_closed_exception_types field . del qr0_proto.queue_closed_exception_types [ : ] qr0_legacy_recon = queue_runner_impl.queuerunner.from_proto ( qr0_proto ) self.assertequal ( `` queue '' , qr0_legacy_recon.queue.name ) self.assertequal ( 1 , len ( qr0_legacy_recon.enqueue_ops ) ) self.assertequal ( enqueue_op , qr0_legacy_recon.enqueue_ops [ 0 ] ) self.assertequal ( close_op , qr0_legacy_recon.close_op ) self.assertequal ( cancel_op , qr0_legacy_recon.cancel_op ) self.assertequal ( ( errors_impl.outofrangeerror , ) , qr0_legacy_recon.queue_closed_exception_types )
__label__0 addons_symbol_mappings = { `` tf.contrib.layers.poincare_normalize '' : `` tfa.layers.poincarenormalize '' , `` tf.contrib.layers.maxout '' : `` tfa.layers.maxout '' , `` tf.contrib.layers.group_norm '' : `` tfa.layers.groupnormalization '' , `` tf.contrib.layers.instance_norm '' : `` tfa.layers.instancenormalization '' , `` tf.contrib.sparsemax.sparsemax '' : `` tfa.activations.sparsemax '' , `` tf.contrib.losses.metric_learning.contrastive_loss '' : `` tfa.losses.contrastiveloss '' , `` tf.contrib.losses.metric_learning.lifted_struct_loss '' : `` tfa.losses.liftedstructloss '' , `` tf.contrib.sparsemax.sparsemax_loss '' : `` tfa.losses.sparsemaxloss '' , `` tf.contrib.losses.metric_learning.triplet_semihard_loss '' : `` tfa.losses.tripletsemihardloss '' , `` tf.contrib.opt.lazyadamoptimizer '' : `` tfa.optimizers.lazyadam '' , `` tf.contrib.opt.movingaverageoptimizer '' : `` tfa.optimizers.movingaverage '' , `` tf.contrib.opt.momentumwoptimizer '' : `` tfa.optimizers.sgdw '' , `` tf.contrib.opt.adamwoptimizer '' : `` tfa.optimizers.adamw '' , `` tf.contrib.opt.extend_with_decoupled_weight_decay '' : `` tfa.optimizers.extend_with_decoupled_weight_decay '' , `` tf.contrib.text.skip_gram_sample '' : `` tfa.text.skip_gram_sample '' , `` tf.contrib.text.skip_gram_sample_with_text_vocab '' : `` tfa.text.skip_gram_sample_with_text_vocab '' , `` tf.contrib.image.dense_image_warp '' : `` tfa.image.dense_image_warp '' , `` tf.contrib.image.adjust_hsv_in_yiq '' : `` tfa.image.adjust_hsv_in_yiq '' , `` tf.contrib.image.compose_transforms '' : `` tfa.image.compose_transforms '' , `` tf.contrib.image.random_hsv_in_yiq '' : `` tfa.image.random_hsv_in_yiq '' , `` tf.contrib.image.angles_to_projective_transforms '' : `` tfa.image.angles_to_projective_transforms '' , `` tf.contrib.image.matrices_to_flat_transforms '' : `` tfa.image.matrices_to_flat_transforms '' , `` tf.contrib.image.rotate '' : `` tfa.image.rotate '' , `` tf.contrib.image.transform '' : `` tfa.image.transform '' , `` tf.contrib.rnn.nascell '' : `` tfa.rnn.nascell '' , `` tf.contrib.rnn.layernormbasiclstmcell '' : `` tfa.rnn.layernormlstmcell '' }
__label__0 this function works with the ast call node format of python3.5+ as well as the different ast format of earlier versions of python .
__label__0 end = time.time ( )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities for exporting tensorflow symbols to the api .
__label__0 def testnonchiefcannotwriteevents ( self ) :
__label__0 input_tree_flattened_as_shallow_tree = flatten_up_to ( shallow_tree , input_tree ) input_tree_flattened = flatten ( input_tree )
__label__0 def testpartitionedresourcevariable ( self ) : self._testpartitionedvariables ( use_resource=true )
__label__0 def _tf_core_flatten ( structure , expand_composites=false ) : `` '' '' see comments for flatten ( ) in tensorflow/python/util/nest.py . '' '' '' if structure is none : return [ none ] expand_composites = bool ( expand_composites ) return _pywrap_utils.flatten ( structure , expand_composites )
__label__0 @ property def is_chief ( self ) : `` '' '' return true if this is a chief supervisor .
__label__0 def bar ( self , a , b ) : return a + b
__label__0 def _find_hipruntime_config ( rocm_install_path ) :
__label__0 `` ` python x_ref = reference ( x ) print ( x is x_ref.deref ( ) ) == > true `` ` `` '' '' return self._wrapped
__label__0 def __call__ ( self , a , b ) : return a + b
__label__0 def thread_fn ( thread_id ) : time.sleep ( random.random ( ) * 0.1 ) group_id = thread_id % num_groups with lock.group ( group_id ) : time.sleep ( random.random ( ) * 0.1 ) self.assertgreater ( lock._group_member_counts [ group_id ] , 0 ) for g , c in enumerate ( lock._group_member_counts ) : if g ! = group_id : self.assertequal ( 0 , c ) finished.add ( thread_id )
__label__0 def _at_least_version ( actual_version , required_version ) : actual = [ int ( v ) for v in actual_version.split ( `` . '' ) ] required = [ int ( v ) for v in required_version.split ( `` . '' ) ] return actual > = required
__label__0 def testcachedclassproperty ( self ) : log = [ ] # log all calls to ` myclass.value ` .
__label__0 def testexportgraphdefwithscope ( self ) : test_dir = self._get_test_dir ( `` export_graph_def '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) graph1 = ops_lib.graph ( ) with graph1.as_default ( ) : with ops_lib.name_scope ( `` hidden1 '' ) : images = constant_op.constant ( 1.0 , dtypes.float32 , shape= [ 3 , 2 ] , name= '' images '' ) weights1 = variable_v1.variablev1 ( [ [ 1.0 , 2.0 , 3.0 ] , [ 4.0 , 5.0 , 6.0 ] ] , name= '' weights '' ) biases1 = variable_v1.variablev1 ( [ 0.1 ] * 3 , name= '' biases '' ) nn_ops.relu ( math_ops.matmul ( images , weights1 ) + biases1 , name= '' relu '' )
__label__0 with self.assertraisesregex ( valueerror , ( `` the two structures do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\n '' `` second structure : . * \n\n '' r'more specifically : substructure `` type=list str=\ [ 0 , 1\ ] '' ' r'is a sequence , while substructure `` type=ndarray str=\ [ 0 1\ ] '' ' `` is not '' ) ) : nest.assert_same_structure ( [ 0 , 1 ] , np.array ( [ 0 , 1 ] ) )
__label__0 # connects to the same target . device memory for the v0 would have # been released , so it will be uninitialized . but v1 should still # be valid . sess = session.session ( server.target ) with self.assertraises ( errors_impl.failedpreconditionerror ) : sess.run ( v0 ) self.assertallequal ( 2.0 , sess.run ( v1 ) )
__label__0 need_to_bind_api_args = ( len ( api_signature.parameters ) > 2 or `` name '' not in api_signature.parameters )
__label__0 the unary elementwise apis are :
__label__0 expected = np.reshape ( [ [ 5.0999999 , 7.0999999 , 9.10000038 ] * 3 ] , ( 3 , 3 ) )
__label__0 # todo ( allenl ) : track down python3 reference cycles in these tests . @ test_util.run_in_graph_and_eager_modes def testnotsaveablebutistrackable ( self ) : v = _ownsavariablesimple ( ) test_dir = self.get_temp_dir ( ) prefix = os.path.join ( test_dir , `` ckpt '' ) for saver in ( saver_module.saver ( var_list= [ v ] ) , saver_module.saver ( var_list= { `` v '' : v } ) ) : with self.cached_session ( ) as sess : self.evaluate ( v.non_dep_variable.assign ( 42 . ) ) save_path = saver.save ( sess , prefix ) self.evaluate ( v.non_dep_variable.assign ( 43 . ) ) saver.restore ( sess , save_path ) self.assertequal ( 42. , self.evaluate ( v.non_dep_variable ) )
__label__0 def __repr__ ( self ) : if self.args is none and self.kwargs is none : return self.name else : args = [ str ( x ) for x in self.args ] args += sorted ( [ `` { } = { } '' .format ( name , x ) for ( name , x ) in self.kwargs.items ( ) ] ) return `` { } ( { } ) '' .format ( self.name , `` , `` .join ( args ) )
__label__0 args : symbol_name : a string representing the full absolute path of the symbol . target_module : if specified , the module in which to restore the symbol. `` '' '' if symbol_name not in _hidden_attributes : raise lookuperror ( 'symbol % s is not a hidden symbol ' % symbol_name ) symbol_basename = symbol_name.split ( ' . ' ) [ -1 ] ( original_module , attr_value ) = _hidden_attributes [ symbol_name ] if not target_module : target_module = original_module setattr ( target_module , symbol_basename , attr_value )
__label__0 def end ( self , session ) : print ( 'done with the session . ' )
__label__0 # set max_wait_secs to allow us to try a few times . with self.assertraises ( errors.deadlineexceedederror ) : sm.wait_for_session ( master= '' '' , max_wait_secs=3 )
__label__0 child = repeatedstringsplitter ( proto.rs [ 0 ] , parent_splitter=splitter , fields_in_parent= [ `` rs '' , 0 ] ) child.build_chunks ( )
__label__0 flatten_with_tuple_paths_up_to ( 0 , [ 0 , 1 , 2 ] ) # output : [ ( ) , [ 0 , 1 , 2 ] ]
__label__0 `` ` python shallow_tree = [ none , none ] inp_val = [ 1 , 2 , 3 ] out = map_structure_up_to ( shallow_tree , lambda x : 2 * x , inp_val )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : typing.tuple [ maskedtensor ] } ) def my_add ( x , y , name=none ) : # pylint : disable=unused-variable del x , y , name
__label__0 returns : ` func ` `` '' '' _binary_elementwise_assert_apis.append ( func ) for args , handler in _elementwise_api_handlers.items ( ) : if len ( args ) == 3 and args [ 2 ] is _assert_api_tag : _add_dispatch_for_binary_elementwise_api ( func , args [ 0 ] , args [ 1 ] , handler ) return func
__label__0 args : args : a list of command line args .
__label__0 args : node : the ast.call node to check arg values for .
__label__0 def __set__ ( self , obj , value ) : raise attributeerror ( 'property % s is read-only ' % self._func.__name__ )
__label__0 with ops.device ( global_step.device ) , ops.name_scope ( `` '' ) : # replicas have to wait until they can get a token from the token queue . with ops.control_dependencies ( train_ops ) : token = sync_token_queue.dequeue ( ) train_op = state_ops.assign ( self._local_step , token )
__label__0 # ` enable_interactive_logging ` must come after ` enable_v2_behavior ` . logging_ops.enable_interactive_logging ( )
__label__0 from tensorflow.core.protobuf import saver_pb2 from tensorflow.python.client import session from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import saver
__label__0 `` ` python @ tf.function ( jit_compile=true ) def f ( x ) : return x + 1
__label__0 def _create_saver_from_imported_meta_graph ( meta_graph_def , import_scope , imported_vars ) : `` '' '' return a saver for restoring variable values to an imported metagraph . '' '' '' if meta_graph_def.hasfield ( `` saver_def '' ) : # infer the scope that is prepended by ` import_scoped_meta_graph ` . scope = import_scope var_names = list ( imported_vars.keys ( ) ) if var_names : sample_key = var_names [ 0 ] sample_var = imported_vars [ sample_key ] scope = sample_var.name [ : -len ( sample_key ) ]
__label__0 def testpreparesessionfails ( self ) : checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` prepare_session '' ) checkpoint_dir2 = os.path.join ( self.get_temp_dir ( ) , `` prepare_session2 '' ) try : gfile.deleterecursively ( checkpoint_dir ) gfile.deleterecursively ( checkpoint_dir2 ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` arg0 '' , `` arg1 '' , warn_once=true ) def _fn ( arg0=0 , arg1=0 ) : # pylint : disable=unused-argument pass
__label__0 def testfastdictapis ( self ) : module = childfastmodule ( `` test '' ) # at first `` bar '' does not exist in the module 's attributes self.assertfalse ( module._fastdict_key_in ( `` bar '' ) ) with self.assertraisesregex ( keyerror , `` module has no attribute 'bar ' '' ) : module._fastdict_get ( `` bar '' )
__label__0 class splitrepeatedstringtest ( test.testcase ) :
__label__0 def _is_windows ( ) : return platform.system ( ) == `` windows ''
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for queuerunner . '' '' ''
__label__0 # gives ` keras.optimizers.adam ` print ( tf_export.get_canonical_name_for_symbol ( cls , api_name='keras ' ) ) `` `
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for serialization functions . '' '' ''
__label__0 sv0.stop ( ) sv1.stop ( )
__label__0 def flatten_with_joined_string_paths ( structure , separator= '' / '' , expand_composites=false ) : `` '' '' returns a list of ( string path , atom ) tuples .
__label__0 # the isinstance check is enough -- a bare attribute is never root . i = 2 while isinstance ( self._stack [ -i ] , ast.attribute ) : i += 1 whole_name = pasta.dump ( self._stack [ - ( i-1 ) ] )
__label__0 @ tf_export ( v1= [ `` train.rmspropoptimizer '' ] ) class rmspropoptimizer ( optimizer.optimizer ) : `` '' '' optimizer that implements the rmsprop algorithm ( tielemans et al .
__label__0 tf.__doc__ = `` '' '' # # tensorflow
__label__0 the new api is
__label__0 def testcreateslotfromtensor ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = constant_op.constant ( [ 1.0 , 2.5 ] , name= '' const '' ) slot = slot_creator.create_slot ( v , v * 2 , name= '' slot '' )
__label__0 exclude ` external ` files and move vendored xla/tsl files accordingly .
__label__0 args : node : the ast.call node to extract arg values from . arg_name : the name of the argument to extract . arg_pos : the position of the argument ( in case it 's passed as a positional argument ) .
__label__0 full_name = self._get_full_name ( node ) if full_name : parent = self._stack [ -2 ]
__label__0 import_header = ( `` from tensorflow import foo as tf\n '' `` from tensorflow.compat import v1 as tf_v1\n '' `` from tensorflow.compat import v2 as tf_v2\n '' ) text = import_header + old_symbol expected_header = ( `` from tensorflow.compat.v2 import foo as tf\n '' `` from tensorflow.compat import v1 as tf_v1\n '' `` from tensorflow.compat import v2 as tf_v2\n '' ) expected_text = expected_header + new_symbol _ , _ , _ , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 `` ` python flatten_up_to ( 0 , 0 ) # output : [ 0 ] flatten_up_to ( 0 , [ 0 , 1 , 2 ] ) # output : [ [ 0 , 1 , 2 ] ] flatten_up_to ( [ 0 , 1 , 2 ] , 0 ) # output : typeerror flatten_up_to ( [ 0 , 1 , 2 ] , [ 0 , 1 , 2 ] ) # output : [ 0 , 1 , 2 ]
__label__0 migrating to a keras optimizer :
__label__0 # cuda.cuda is only valid in oss try : from cuda.cuda import cuda_config # pylint : disable=g-import-not-at-top except importerror : cuda_config = none
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 return config
__label__0 with self.session ( ) as sess : saver3 = saver_module.saver ( var_list=new_var_list_1 , max_to_keep=1 ) saver3.restore ( sess , saver0_ckpt ) self.assertallclose ( expected , sess.run ( `` new_hidden1/relu:0 '' ) )
__label__0 def _create_linear_model ( self , feature_cols , partitioner ) : cols_to_vars = { } with variable_scope.variable_scope ( `` '' , partitioner=partitioner ) : # create the variables . fc.linear_model ( features=self._create_dummy_inputs ( ) , feature_columns=feature_cols , units=1 , cols_to_vars=cols_to_vars ) # return a dictionary mapping each column to its variable . return cols_to_vars
__label__0 this decorator logs a deprecation warning whenever the decorated function is called with the deprecated argument values . it has the following format :
__label__0 def basicsaverestore ( self , variable_op ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` basic_save_restore '' )
__label__0 args : graph : the graph in which to create the global step read tensor . if missing , use default graph .
__label__0 def get_disabled_rewriter_config ( ) : global _rewriter_config_optimizer_disabled if _rewriter_config_optimizer_disabled is none : config = config_pb2.configproto ( ) rewriter_config = config.graph_options.rewrite_options rewriter_config.disable_meta_optimizer = true _rewriter_config_optimizer_disabled = config.serializetostring ( ) return _rewriter_config_optimizer_disabled
__label__0 # verifies that we can save the subgraph under `` hidden1 '' and restore it # into `` new_hidden1 '' in the new graph . def testscopedsaveandrestore ( self ) : test_dir = self._get_test_dir ( `` scoped_export_import '' ) ckpt_filename = `` ckpt '' self._testscopedsave ( test_dir , `` exported_hidden1.pbtxt '' , ckpt_filename ) self._testscopedrestore ( test_dir , `` exported_hidden1.pbtxt '' , `` exported_new_hidden1.pbtxt '' , ckpt_filename )
__label__0 class g3doctesttest ( parameterized.testcase ) :
__label__0 self.assertprotoequals ( expected_proto , cluster_spec.as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_cluster_def ( ) ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_dict ( ) ) .as_cluster_def ( ) )
__label__0 `` ` python optimizer = tf.keras.optimizers.rmsprop ( learning_rate=learning_rate , rho=decay , momentum=momentum , epsilon=epsilon ) `` `
__label__0 slot_sharding = xla_sharding.get_tensor_sharding ( slot ) slot_proto = xla_data_pb2.opsharding ( ) slot_proto.parsefromstring ( slot_sharding ) self.assertequal ( slot_proto , xla_data_pb2.opsharding ( type=xla_data_pb2.opsharding.other , tile_assignment_dimensions= [ 4 ] , tile_assignment_devices=range ( 4 ) ) )
__label__0 for functions , it returns a function wrapped by ` tf_decorator.make_decorator ` . that function prints a warning when used , and has a deprecation notice in its docstring . this is more or less equivalent ( the deprecation warning has slightly different text ) to writing :
__label__0 for file in deps : for path , val in path_to_replace.items ( ) : if path in file : copy_file ( file , os.path.join ( srcs_dir , val ) , path ) break else : # exclude external py files if `` external '' not in file : copy_file ( file , srcs_dir )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) # since old vocab is not explicitly set in warmstartsettings , the old # vocab is assumed to be same as new vocab . ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * sc_vocab . * '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ prev_vocab_val ] } , sess )
__label__0 def setup ( self ) : super ( ) .setup ( ) self._modules = [ ]
__label__0 def flatten ( modality , structure , expand_composites=false ) : `` '' '' flattens a nested structure .
__label__0 _fn ( arg0= '' allowed '' , arg1= '' also allowed '' ) self.assertequal ( 0 , mock_warning.call_count ) _fn ( arg0= '' forbidden '' , arg1= '' disallowed '' ) self.assertequal ( 2 , mock_warning.call_count ) _fn ( arg0= '' forbidden '' , arg1= '' allowed '' ) self.assertequal ( 2 , mock_warning.call_count ) _fn ( arg0= '' forbidden '' , arg1= '' disallowed '' ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 `` ` class example ( object ) : @ property @ do_not_doc_inheritable def x ( self ) : return self._x `` `
__label__0 text = `` tf.train.sdca_fprint ( input , name=n ) '' expected_text = `` tf.raw_ops.sdcafprint ( input , name=n ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def _assert_chunk_sizes ( self , chunks , max_size ) : `` '' '' asserts that all chunk proto sizes are < = max_size . '' '' '' for chunk in chunks : if isinstance ( chunk , message.message ) : self.assertlessequal ( chunk.bytesize ( ) , max_size )
__label__0 # verifies various reset failures . def testresetfails ( self ) : with ops.graph ( ) .as_default ( ) : # creates variable with container name . with ops.container ( `` test0 '' ) : v0 = variable_v1.variablev1 ( 1.0 , name= '' v0 '' ) # creates variable with default container . v1 = variable_v1.variablev1 ( 2.0 , name= '' v1 '' ) # verifies resetting the non-existent target returns error . with self.assertraises ( errors_impl.notfounderror ) : session.session.reset ( `` nonexistent '' , [ `` test0 '' ] )
__label__0 this check would work for mocked object as well since it would check if returned attribute has the right type .
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( arg0 , arg1 ) : return arg0 + arg1
__label__0 def testsaverestoreandvalidatevariabledtype ( self ) : for variable_op in [ variables.variable , resource_variable_ops.resourcevariable ] : save_path = os.path.join ( self.get_temp_dir ( ) , `` basic_save_restore '' )
__label__0 subprocess.check_call ( [ 'bazel ' , 'build ' , 'tensorflow/cc : cc_ops ' ] , cwd=tensorflow_root ) subprocess.check_call ( [ 'cp ' , ' -- dereference ' , '-r ' , 'bazel-bin ' , output_dir / 'bazel-genfiles ' ] , cwd=tensorflow_root )
__label__0 raises : typeerror : if 'summary ' is not a summary proto or a string . runtimeerror : if the supervisor was created without a ` logdir ` . `` '' '' if not self._summary_writer : raise runtimeerror ( `` writing a summary requires a summary writer . '' ) if global_step is none and self.global_step is not none : global_step = training_util.global_step ( sess , self.global_step ) self._summary_writer.add_summary ( summary , global_step )
__label__0 check_all_files ( ) old_version = get_current_semver_version ( )
__label__0 @ property def stop_requested ( self ) : `` '' '' returns whether a stop is requested or not .
__label__0 args : saver : a ` saver ` object . if set to use_default , create one that saves all the variables. `` '' '' if saver is supervisor.use_default : saver = self._get_first_op_from_collection ( ops.graphkeys.savers ) if saver is none and variables.global_variables ( ) : saver = saver_mod.saver ( ) ops.add_to_collection ( ops.graphkeys.savers , saver ) self._saver = saver
__label__0 tf_modules = sorted ( tf_modules , key=lambda mod : mod.__name__ ) for n , module in enumerate ( tf_modules ) : if ( n % total_test_shards ) ! = test_shard_index : continue
__label__0 # the splitter algorithm is n't extremely precise , so the max is set to a little # less than 2gb . _max_size = ( 1 < < 31 ) - 500
__label__0 this does not close the session .
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 `` ` python input_tree = [ [ [ 2 , 2 ] , [ 3 , 3 ] ] , [ [ 4 , 9 ] , [ 5 , 5 ] ] ] shallow_tree = [ [ true , true ] , [ false , true ] ]
__label__0 def testnestedwith ( self ) : x = [ ] with test_yield_append_before_and_after_yield ( x , 'before ' , 'after ' ) : with test_yield_append_before_and_after_yield ( x , 'inner ' , 'outer ' ) : with test_yield_return_x_plus_1 ( 1 ) as var : x.append ( var ) self.assertequal ( [ 'before ' , 'inner ' , 2 , 'outer ' , 'after ' ] , x )
__label__0 def __init__ ( self , wrapped ) : self._wrapped = wrapped
__label__0 if hasattr ( tf.compat , `` v1 '' ) :
__label__0 def fn ( a , test_arg1 , test_arg2 ) : if test_arg1 ! = expected_test_arg1 or test_arg2 ! = expected_test_arg2 : return valueerror ( 'partial does not work correctly ' ) return a
__label__0 class samevariablesnocleartest ( test.testcase ) :
__label__0 class child ( parent ) : def method1 ( self ) : pass def method2 ( self ) : pass `` ` this will produce the following docs :
__label__0 > > > d1 = { `` hello '' : 24 , `` world '' : 76 } > > > d2 = { `` hello '' : 36 , `` world '' : 14 } > > > tf.nest.map_structure ( lambda p1 , p2 : p1 + p2 , d1 , d2 ) { 'hello ' : 60 , 'world ' : 90 }
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' run doctests for tensorflow . '' '' ''
__label__0 @ property def original_args ( self ) : `` '' '' a ` sessionrunargs ` object holding the original arguments of ` run ( ) ` .
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 returns : a saverdef proto .
__label__0 def assertcheckpointstate ( self , model_checkpoint_path , all_model_checkpoint_paths , save_dir ) : checkpoint_state = checkpoint_management.get_checkpoint_state ( save_dir ) self.assertequal ( checkpoint_state.model_checkpoint_path , model_checkpoint_path ) self.assertequal ( checkpoint_state.all_model_checkpoint_paths , all_model_checkpoint_paths )
__label__0 def testcolocategradientswithopsminimize ( self ) : text = `` optimizer.minimize ( a , foo=false ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( text , new_text ) self.assertequal ( errors , [ ] )
__label__0 returns : the effective argument that should be used . raises : valueerror : if new_value and old_value are both non-null `` '' '' if old_value is not none : if new_value is not none : raise valueerror ( f '' can not specify both ' { old_name } ' and ' { new_name } ' . '' ) return old_value return new_value
__label__0 _name_to_symbol_mapping : dict [ str , any ] = dict ( )
__label__0 args : op : python function : the operation that should be overridden . * types : the argument types for which this function should be used. `` '' ''
__label__0 return rnd
__label__0 this file is autogenerated : to update , please run : bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map this file should be updated whenever a function is added to self.reordered_function_names in tf_upgrade_v2.py. `` '' '' reorders = { 'tf.argmax ' : [ none , none , 'name ' , 'dimension ' , 'output_type ' ] , 'tf.argmin ' : [ none , none , 'name ' , 'dimension ' , 'output_type ' ] , 'tf.batch_to_space ' : [ none , 'crops ' , 'block_size ' , 'name ' , 'block_shape ' ] , 'tf.boolean_mask ' : [ none , none , 'name ' , 'axis ' ] , 'tf.cond ' : [ none , none , none , 'strict ' , 'name ' , 'fn1 ' , 'fn2 ' ] , 'tf.confusion_matrix ' : [ none , none , none , 'dtype ' , 'name ' , 'weights ' ] , 'tf.convert_to_tensor ' : [ none , none , 'name ' , 'preferred_dtype ' , 'dtype_hint ' ] , 'tf.data.experimental.raggedtensorstructure ' : [ 'dtype ' , 'shape ' , 'ragged_rank ' ] , 'tf.data.experimental.sparsetensorstructure ' : [ 'dtype ' , 'shape ' ] , 'tf.data.experimental.tensorarraystructure ' : [ 'dtype ' , 'element_shape ' , 'dynamic_size ' , 'infer_shape ' ] , 'tf.data.experimental.tensorstructure ' : [ 'dtype ' , 'shape ' ] , 'tf.debugging.assert_all_finite ' : [ 't ' , 'msg ' , 'name ' , ' x ' , 'message ' ] , 'tf.decode_csv ' : [ none , none , none , none , 'name ' , 'na_value ' , 'select_cols ' ] , 'tf.depth_to_space ' : [ none , none , 'name ' , 'data_format ' ] , 'tf.feature_column.categorical_column_with_vocabulary_file ' : [ none , none , none , 'num_oov_buckets ' , 'default_value ' , 'dtype ' ] , 'tf.gather_nd ' : [ none , none , 'name ' , 'batch_dims ' ] , 'tf.gradients ' : [ none , none , none , none , 'colocate_gradients_with_ops ' , 'gate_gradients ' , 'aggregation_method ' , 'stop_gradients ' , 'unconnected_gradients ' ] , 'tf.hessians ' : [ none , none , 'name ' , 'colocate_gradients_with_ops ' , 'gate_gradients ' , 'aggregation_method ' ] , 'tf.image.sample_distorted_bounding_box ' : [ none , none , none , 'seed2 ' , 'min_object_covered ' , 'aspect_ratio_range ' , 'area_range ' , 'max_attempts ' , 'use_image_if_no_bounding_boxes ' , 'name ' ] , 'tf.initializers.uniform_unit_scaling ' : [ 'factor ' , 'seed ' , 'dtype ' ] , 'tf.io.decode_csv ' : [ none , none , none , none , 'name ' , 'na_value ' , 'select_cols ' ] , 'tf.io.parse_example ' : [ none , none , 'name ' , 'example_names ' ] , 'tf.io.parse_single_example ' : [ none , none , 'name ' , 'example_names ' ] , 'tf.io.serialize_many_sparse ' : [ none , 'name ' , 'out_type ' ] , 'tf.io.serialize_sparse ' : [ none , 'name ' , 'out_type ' ] , 'tf.linalg.norm ' : [ none , none , none , none , none , 'keep_dims ' ] , 'tf.manip.gather_nd ' : [ none , none , 'name ' , 'batch_dims ' ] , 'tf.math.argmax ' : [ none , none , 'name ' , 'dimension ' , 'output_type ' ] , 'tf.math.argmin ' : [ none , none , 'name ' , 'dimension ' , 'output_type ' ] , 'tf.math.confusion_matrix ' : [ none , none , none , 'dtype ' , 'name ' , 'weights ' ] , 'tf.math.in_top_k ' : [ 'predictions ' , 'targets ' , ' k ' , 'name ' ] , 'tf.math.reduce_all ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_any ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_logsumexp ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_max ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_mean ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_min ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_prod ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.math.reduce_sum ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.multinomial ' : [ none , none , 'seed ' , 'name ' , 'output_dtype ' ] , 'tf.nn.avg_pool ' : [ 'value ' , 'ksize ' , 'strides ' , 'padding ' , 'data_format ' , 'name ' , 'input ' ] , 'tf.nn.avg_pool2d ' : [ 'value ' , 'ksize ' , 'strides ' , 'padding ' , 'data_format ' , 'name ' , 'input ' ] , 'tf.nn.conv1d ' : [ 'value ' , 'filters ' , 'stride ' , 'padding ' , 'use_cudnn_on_gpu ' , 'data_format ' , 'name ' , 'input ' , 'dilations ' ] , 'tf.nn.conv2d ' : [ none , 'filter ' , 'strides ' , 'padding ' , 'use_cudnn_on_gpu ' , 'data_format ' , 'dilations ' , 'name ' , 'filters ' ] , 'tf.nn.conv2d_backprop_input ' : [ 'input_sizes ' , 'filter ' , 'out_backprop ' , 'strides ' , 'padding ' , 'use_cudnn_on_gpu ' , 'data_format ' , 'dilations ' , 'name ' , 'filters ' ] , 'tf.nn.convolution ' : [ none , 'filter ' , 'padding ' , 'strides ' , 'dilation_rate ' , 'name ' , 'data_format ' , 'filters ' , 'dilations ' ] , 'tf.nn.crelu ' : [ none , 'name ' , 'axis ' ] , 'tf.nn.ctc_beam_search_decoder ' : [ 'inputs ' , 'sequence_length ' , 'beam_width ' , 'top_paths ' , 'merge_repeated ' ] , 'tf.nn.depth_to_space ' : [ none , none , 'name ' , 'data_format ' ] , 'tf.nn.depthwise_conv2d ' : [ none , none , none , none , 'rate ' , 'name ' , 'data_format ' , 'dilations ' ] , 'tf.nn.embedding_lookup ' : [ none , none , 'partition_strategy ' , 'name ' , 'validate_indices ' , 'max_norm ' ] , 'tf.nn.embedding_lookup_sparse ' : [ none , none , none , 'partition_strategy ' , 'name ' , 'combiner ' , 'max_norm ' , 'allow_fast_lookup ' ] , 'tf.nn.fractional_avg_pool ' : [ 'value ' , 'pooling_ratio ' , 'pseudo_random ' , 'overlapping ' , 'deterministic ' , 'seed ' , 'seed2 ' , 'name ' ] , 'tf.nn.fractional_max_pool ' : [ 'value ' , 'pooling_ratio ' , 'pseudo_random ' , 'overlapping ' , 'deterministic ' , 'seed ' , 'seed2 ' , 'name ' ] , 'tf.nn.in_top_k ' : [ 'predictions ' , 'targets ' , ' k ' , 'name ' ] , 'tf.nn.max_pool ' : [ 'value ' , 'ksize ' , 'strides ' , 'padding ' , 'data_format ' , 'name ' , 'input ' ] , 'tf.nn.moments ' : [ none , none , none , 'name ' , 'keep_dims ' , 'keepdims ' ] , 'tf.nn.pool ' : [ none , none , none , 'padding ' , 'dilation_rate ' , 'strides ' , 'name ' , 'data_format ' , 'dilations ' ] , 'tf.nn.separable_conv2d ' : [ none , none , none , none , none , 'rate ' , 'name ' , 'data_format ' , 'dilations ' ] , 'tf.nn.softmax_cross_entropy_with_logits ' : [ 'labels ' , 'logits ' , 'dim ' , 'name ' , 'axis ' ] , 'tf.nn.space_to_batch ' : [ none , 'paddings ' , 'block_size ' , 'name ' , 'block_shape ' ] , 'tf.nn.space_to_depth ' : [ none , none , 'name ' , 'data_format ' ] , 'tf.nn.weighted_moments ' : [ none , none , none , 'name ' , 'keep_dims ' , 'keepdims ' ] , 'tf.norm ' : [ none , none , none , none , none , 'keep_dims ' ] , 'tf.pad ' : [ none , none , none , 'name ' , 'constant_values ' ] , 'tf.parse_example ' : [ none , none , 'name ' , 'example_names ' ] , 'tf.parse_single_example ' : [ none , none , 'name ' , 'example_names ' ] , 'tf.quantize_v2 ' : [ none , none , none , none , none , 'name ' , 'round_mode ' , 'narrow_range ' , 'axis ' , 'ensure_minimum_range ' ] , 'tf.random.multinomial ' : [ none , none , 'seed ' , 'name ' , 'output_dtype ' ] , 'tf.random.poisson ' : [ 'lam ' , 'shape ' , 'dtype ' , 'seed ' , 'name ' ] , 'tf.random_poisson ' : [ 'lam ' , 'shape ' , 'dtype ' , 'seed ' , 'name ' ] , 'tf.reduce_all ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_any ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_join ' : [ none , none , 'keep_dims ' , 'separator ' , 'name ' , 'reduction_indices ' , 'keepdims ' ] , 'tf.reduce_logsumexp ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_max ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_mean ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_min ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_prod ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reduce_sum ' : [ none , none , none , none , 'reduction_indices ' , 'keep_dims ' ] , 'tf.reverse_sequence ' : [ none , none , none , none , none , 'seq_dim ' , 'batch_dim ' ] , 'tf.serialize_many_sparse ' : [ none , 'name ' , 'out_type ' ] , 'tf.serialize_sparse ' : [ none , 'name ' , 'out_type ' ] , 'tf.shape ' : [ none , 'name ' , 'out_type ' ] , 'tf.size ' : [ none , 'name ' , 'out_type ' ] , 'tf.space_to_batch ' : [ none , 'paddings ' , 'block_size ' , 'name ' , 'block_shape ' ] , 'tf.space_to_depth ' : [ none , none , 'name ' , 'data_format ' ] , 'tf.sparse.add ' : [ none , none , none , 'thresh ' ] , 'tf.sparse.concat ' : [ none , none , 'name ' , 'expand_nonconcat_dim ' , 'concat_dim ' , 'expand_nonconcat_dims ' ] , 'tf.sparse.reduce_max ' : [ none , none , none , 'reduction_axes ' , 'keep_dims ' ] , 'tf.sparse.segment_mean ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse.segment_sqrt_n ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse.segment_sum ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse.split ' : [ 'keyword_required ' , 'sp_input ' , 'num_split ' , 'axis ' , 'name ' , 'split_dim ' ] , 'tf.sparse_add ' : [ none , none , none , 'thresh ' ] , 'tf.sparse_concat ' : [ none , none , 'name ' , 'expand_nonconcat_dim ' , 'concat_dim ' , 'expand_nonconcat_dims ' ] , 'tf.sparse_matmul ' : [ none , none , none , none , 'a_is_sparse ' , 'b_is_sparse ' , 'name ' ] , 'tf.sparse_reduce_max ' : [ none , none , none , 'reduction_axes ' , 'keep_dims ' ] , 'tf.sparse_segment_mean ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse_segment_sqrt_n ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse_segment_sum ' : [ none , none , none , 'name ' , 'num_segments ' , 'sparse_gradient ' ] , 'tf.sparse_split ' : [ 'keyword_required ' , 'sp_input ' , 'num_split ' , 'axis ' , 'name ' , 'split_dim ' ] , 'tf.strings.length ' : [ none , 'name ' , 'unit ' ] , 'tf.strings.reduce_join ' : [ none , none , 'keep_dims ' , 'separator ' , 'name ' , 'reduction_indices ' , 'keepdims ' ] , 'tf.strings.substr ' : [ none , none , none , 'name ' , 'unit ' ] , 'tf.substr ' : [ none , none , none , 'name ' , 'unit ' ] , 'tf.test.assert_equal_graph_def ' : [ 'actual ' , 'expected ' , 'checkpoint_v2 ' , 'hash_table_shared_name ' ] , 'tf.transpose ' : [ none , none , 'name ' , 'conjugate ' ] , 'tf.tuple ' : [ none , 'name ' , 'control_inputs ' ] , 'tf.uniform_unit_scaling_initializer ' : [ 'factor ' , 'seed ' , 'dtype ' ] , 'tf.verify_tensor_all_finite ' : [ 't ' , 'msg ' , 'name ' , ' x ' , 'message ' ] , 'tf.while_loop ' : [ 'cond ' , 'body ' , 'loop_vars ' , 'shape_invariants ' , 'parallel_iterations ' , 'back_prop ' , 'swap_memory ' , 'name ' , 'maximum_iterations ' , 'return_same_structure ' ] }
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for fenced_doctest . '' '' '' from typing import list , optional , tuple
__label__0 use case :
__label__0 returns : ` args ` with the type of ` instance ` . `` '' '' if _is_mutable_mapping ( instance ) : # pack dictionaries in a deterministic order by sorting the keys . # notice this means that we ignore the original order of ` ordereddict ` # instances . this is intentional , to avoid potential bugs caused by mixing # ordered and plain dicts ( e.g. , flattening a dict but using a # corresponding ` ordereddict ` to pack it back ) . result = dict ( zip ( _tf_core_sorted ( instance ) , args ) ) instance_type = type ( instance ) if instance_type == _collections.defaultdict : d = _collections.defaultdict ( instance.default_factory ) else : d = instance_type ( ) for key in instance : d [ key ] = result [ key ] return d elif _is_mapping ( instance ) : result = dict ( zip ( _tf_core_sorted ( instance ) , args ) ) instance_type = type ( instance ) if not getattr ( instance_type , `` __supported_by_tf_nest__ '' , false ) : tf_logging.log_first_n ( tf_logging.warn , `` mapping types may not work well with tf.nest. `` `` prefer using mutablemapping for { } '' .format ( instance_type ) , 1 , ) try : return instance_type ( ( key , result [ key ] ) for key in instance ) except typeerror as err : # pylint : disable=raise-missing-from raise typeerror ( `` error creating an object of type { } like { } . note that `` `` it must accept a single positional argument `` `` representing an iterable of key-value pairs , in `` `` addition to self . cause : { } '' .format ( type ( instance ) , instance , err ) ) elif _is_mapping_view ( instance ) : # we ca n't directly construct mapping views , so we create a list instead return list ( args ) elif is_namedtuple ( instance ) or _is_attrs ( instance ) : if isinstance ( instance , _wrapt.objectproxy ) : instance_type = type ( instance.__wrapped__ ) else : instance_type = type ( instance ) return instance_type ( * args ) elif _is_composite_tensor ( instance ) : assert len ( args ) == 1 spec = instance._type_spec # pylint : disable=protected-access return spec._from_components ( args [ 0 ] ) # pylint : disable=protected-access elif _is_type_spec ( instance ) : # pack a compositetensor 's components according to a typespec . assert len ( args ) == 1 return instance._from_components ( args [ 0 ] ) # pylint : disable=protected-access elif isinstance ( instance , range ) : return sequence_like ( list ( instance ) , args ) elif isinstance ( instance , _wrapt.objectproxy ) : # for object proxies , first create the underlying type and then re-wrap it # in the proxy type . return type ( instance ) ( sequence_like ( instance.__wrapped__ , args ) ) elif isinstance ( instance , customnestprotocol ) : metadata = instance.__tf_flatten__ ( ) [ 0 ] return instance.__tf_unflatten__ ( metadata , tuple ( args ) ) else : # not a namedtuple return type ( instance ) ( args )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow.python.util.module_wrapper . '' '' ''
__label__0 def func ( m , n , * * kwarg ) : return m * n + len ( kwarg )
__label__0 def decorator ( dispatch_target ) : `` '' '' decorator that registers the given dispatch target . '' '' '' if not callable ( dispatch_target ) : raise typeerror ( `` expected dispatch_target to be callable ; `` f '' got { dispatch_target ! r } '' ) dispatch_target = _add_name_scope_wrapper ( dispatch_target , api_signature ) _check_signature ( api_signature , dispatch_target )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testattrsflattenandpack ( self ) : if attr is none : self.skiptest ( `` attr module is unavailable . '' )
__label__0 to_fix = [ self._chunked_message ] while to_fix : for field in to_fix.pop ( ) .chunked_fields : if field.message.chunked_fields : to_fix.append ( field.message ) if not field.message.hasfield ( `` chunk_index '' ) : continue chunk_addr = self._add_chunk_order [ field.message.chunk_index ] assert ( chunk_addr in chunk_indices ) , f '' found unexpected chunk { chunk_addr } '' new_chunk_index = chunk_indices [ chunk_addr ] field.message.chunk_index = new_chunk_index
__label__0 # assert function docs are properly updated . self.assertequal ( `` fn doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : \n % s '' `` \n '' `` \nargs : '' `` \n arg0 : arg 0 . '' `` \n arg1 : arg 1 . '' `` \n '' `` \nreturns : '' `` \n sum of args . '' % ( date , instructions ) , getattr ( _object , `` _fn '' ) .__doc__ )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor , `` y '' : maskedtensor } ) def masked_add ( * args , * * kwargs ) : self.assertallequal ( kwargs [ `` x '' ] .values , x.values ) self.assertallequal ( kwargs [ `` y '' ] .values , y.values ) self.assertempty ( args ) return `` stub ''
__label__0 # verify that the mapped names are present in the saved file and can be # restored using remapped names . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = variable_op ( -1.0 , name= '' v0 '' ) v1 = variable_op ( -1.0 , name= '' v1 '' )
__label__0 def testaddduplicateapidisptachererror ( self ) : some_op = lambda x : x some_op = dispatch.add_type_based_api_dispatcher ( some_op ) with self.assertraisesregex ( valueerror , `` . * already has a type-based api dispatcher . `` ) : some_op = dispatch.add_type_based_api_dispatcher ( some_op )
__label__0 def get_filtered_filenames ( self ) : return empty_set
__label__0 @ classmethod def _overload_operator ( cls , operator ) : # pylint : disable=invalid-name `` '' '' overload an operator with the same overloading as ` tensor_lib.tensor ` . '' '' '' tensor_oper = getattr ( tensor_lib.tensor , operator )
__label__0 class maskedtensor ( extension_type.extensiontype ) : `` '' '' simple extensiontype for testing v2 dispatch . '' '' '' values : tensor_lib.tensor mask : tensor_lib.tensor
__label__0 `` `` ''
__label__0 from tensorflow.python.framework import ops from tensorflow.python.util import tf_export from tensorflow.python.util import tf_inspect
__label__0 # this copies the prefix and suffix on old_value to new_value . pasta.ast_utils.replace_child ( parent , old_value , new_value ) ast.copy_location ( new_value , old_value ) return true
__label__0 mock_tf_v1 = mockmodule ( 'tensorflow.compat.v1 ' ) mock_tf_v1_wrapped = module_wrapper.tfmodulewrapper ( mock_tf_v1 , 'test ' , public_apis=apis ) self.assertfalse ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded ) mock_tf_v1_wrapped.cosh # pylint : disable=pointless-statement self.asserttrue ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded )
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 4 ] ) ) nmt2 = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=false , inner_value=constant_op.constant ( [ 5 ] ) ) nmt_combined_with_path = nest.map_structure_with_paths ( path_sum , nmt , nmt2 ) self.assertisinstance ( nmt_combined_with_path , nestedmaskedtensor ) self.assertequal ( nmt_combined_with_path.mask , true ) self.assertequal ( nmt_combined_with_path.value.mask , false ) self.assertallequal ( nmt_combined_with_path.value.value [ 0 ] , `` 0/0 '' ) self.assertallequal ( nmt_combined_with_path.value.value [ 1 ] , [ 9 ] )
__label__0 def testaddshouldusewarningwhenusedwithgetshape ( self ) : def get_shape ( h ) : _ = h.shape self._testaddshouldusewarningwhenused ( get_shape , name='blah_get_name ' ) gc.collect ( ) self.assertfalse ( gc.garbage )
__label__0 new_file_a = os.path.join ( output_dir , `` a.py '' ) new_file_b = os.path.join ( output_dir , `` b.py '' ) self.asserttrue ( os.path.islink ( new_file_b ) ) self.assertequal ( new_file_a , os.readlink ( new_file_b ) ) with open ( new_file_a , `` r '' ) as f : self.assertequal ( `` import bar as f '' , f.read ( ) )
__label__0 text = contrib_alias + `` layers.xavier_initializer_conv2d ( false , 12 ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution= ( \ '' uniform\ '' if false else \ '' truncated_normal\ '' ) , `` `` seed=12 ) \n '' , )
__label__0 # create a first session and then stop . sess = sv.prepare_or_wait_for_session ( `` '' ) sv.stop ( ) sess.close ( ) self.asserttrue ( sv.should_stop ( ) )
__label__0 nt = nesttest.abtuple ( a= ( `` something '' , `` something_else '' ) , b= '' yet another thing '' ) rev_nt = nest.map_structure ( lambda x : x [ : :-1 ] , nt ) # check the output is the correct structure , and all strings are reversed . nest.assert_same_structure ( nt , rev_nt ) self.assertequal ( nt.a [ 0 ] [ : :-1 ] , rev_nt.a [ 0 ] ) self.assertequal ( nt.a [ 1 ] [ : :-1 ] , rev_nt.a [ 1 ] ) self.assertequal ( nt.b [ : :-1 ] , rev_nt.b )
__label__0 def _find_sycl_config ( basekit_path ) : # pylint : disable=missing-function-docstring
__label__0 def make_type_checker ( annotation ) : `` '' '' builds a pytypechecker for the given type annotation . '' '' '' if type_annotations.is_generic_union ( annotation ) : type_args = type_annotations.get_generic_type_args ( annotation )
__label__0 `` `` '' all user ops . '' '' ''
__label__0 import typing from typing import protocol
__label__0 text = `` tf.name_scope ( name=none , values=stuff ) '' _ , _ , errors , _ = self._upgrade ( text ) self.assertin ( `` name_scope call with neither name nor default_name '' , errors [ 0 ] )
__label__0 def testiteratorsunshardedrestore ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` restore_unsharded_iterators '' )
__label__0 def handle ( self , args , kwargs ) : if self._handles ( args , kwargs ) : return self._override_func ( * args , * * kwargs ) else : return self.not_supported
__label__0 contrib_tpu_strategy_warning = ( ast_edits.error , `` ( manual edit required ) tf.contrib.distribute.tpustrategy has `` `` been migrated to tf.distribute.tpustrategy . note the `` `` slight changes in constructor. `` + distribute_strategy_api_changes )
__label__0 args : timer_interval_secs : number . time boundaries at which to call ` target ` . target : a callable object . args : optional arguments to pass to ` target ` when calling it . kwargs : optional keyword arguments to pass to ` target ` when calling it .
__label__0 if uses_star_kwargs_in_call ( node ) : self.add_log ( warning , node.lineno , node.col_offset , `` ( manual check required ) upgrading % s may require `` `` renaming or removing call arguments , but it was passed `` `` variable-length * args or * * kwargs . the upgrade `` `` script can not handle these automatically . '' % ( full_name or name ) ) modified = false new_keywords = [ ] for keyword in node.keywords : argkey = keyword.arg if argkey in renamed_keywords : modified = true if renamed_keywords [ argkey ] is none : lineno = getattr ( keyword , `` lineno '' , node.lineno ) col_offset = getattr ( keyword , `` col_offset '' , node.col_offset ) self.add_log ( info , lineno , col_offset , `` removed argument % s for function % s '' % ( argkey , full_name or name ) ) else : keyword.arg = renamed_keywords [ argkey ] lineno = getattr ( keyword , `` lineno '' , node.lineno ) col_offset = getattr ( keyword , `` col_offset '' , node.col_offset ) self.add_log ( info , lineno , col_offset , `` renamed keyword argument for % s from % s to % s '' % ( full_name , argkey , renamed_keywords [ argkey ] ) ) new_keywords.append ( keyword ) else : new_keywords.append ( keyword )
__label__0 def __reduce__ ( self ) : return importlib.import_module , ( self.__name__ , )
__label__0 returns : packed : ` flat_sequence ` converted to have the same recursive structure as ` structure ` .
__label__0 # proto.nested_map_bool [ false ] .map_field_int64 ret = util.get_field_tag ( proto , [ `` nested_map_bool '' , false , `` map_field_int64 '' ] )
__label__0 `` ` python generate2.py -- output_dir=/tmp/out `` `
__label__0 def get_load_model_function ( ) : global _keras_load_model_function return _keras_load_model_function
__label__0 # the signature of this method changes in py3.10 so we override to enforce it . @ classmethod def from_callable ( cls , obj , * , follow_wrapped=true ) : return super ( ) .from_callable ( obj , follow_wrapped=follow_wrapped )
__label__0 def testgetargspeconpartialwithdecoratorthatchangesargspec ( self ) : `` '' '' tests getargspec on partial function with decorated argspec . '' '' ''
__label__0 parse the given benchmark data from the serialized json-format used to write the test results file . create the different datastore entities from that data and upload them to the datastore in a batch using the client connection .
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) : # initialize with zeros . var = variable_scope.get_variable ( `` v1 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) var2 = variable_scope.get_variable ( `` v2 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= [ `` v1 '' , `` v2 '' ] , var_name_to_prev_var_name=dict ( v2= '' v1 '' ) ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started ( init overridden to ones ) . self.assertallequal ( var , prev_int_val ) self.assertallequal ( var2 , prev_int_val )
__label__0 examples :
__label__0 def _validate_group_id ( self , group_id ) : if group_id < 0 or group_id > = self._num_groups : raise valueerror ( `` argument ` group_id ` should verify ` 0 < = group_id < num_groups ` `` f '' ( with ` num_groups= { self._num_groups } ` ) . `` f '' received : group_id= { group_id } '' )
__label__0 def patch_so ( srcs_dir : str ) - > none : `` '' '' patch .so files . we must patch some of .so files otherwise auditwheel will fail . args : srcs_dir : target directory with .so files to patch. `` '' '' to_patch = { `` tensorflow/python/_pywrap_tensorflow_internal.so '' : ( `` $ origin/ .. / .. /tensorflow/compiler/xla/tsl/python/lib/core '' ) , ( `` tensorflow/compiler/mlir/quantization/tensorflow/python/ '' `` pywrap_function_lib.so '' ) : `` $ origin/ .. / .. / .. / .. / .. /python '' , ( `` tensorflow/compiler/mlir/quantization/tensorflow/python/ '' `` pywrap_quantize_model.so '' ) : `` $ origin/ .. / .. / .. / .. / .. /python '' , ( `` tensorflow/compiler/mlir/tensorflow_to_stablehlo/python/ '' `` pywrap_tensorflow_to_stablehlo.so '' ) : `` $ origin/ .. / .. / .. / .. /python '' , ( `` tensorflow/compiler/mlir/lite/python/_pywrap_converter_api.so '' ) : `` $ origin/ .. / .. / .. / .. /python '' , } for file , path in to_patch.items ( ) : rpath = subprocess.check_output ( [ `` patchelf '' , `` -- print-rpath '' , `` { } / { } '' .format ( srcs_dir , file ) ] ) .decode ( ) .strip ( ) new_rpath = rpath + `` : '' + path subprocess.run ( [ `` patchelf '' , `` -- set-rpath '' , new_rpath , `` { } / { } '' .format ( srcs_dir , file ) ] , check=true ) subprocess.run ( [ `` patchelf '' , `` -- shrink-rpath '' , `` { } / { } '' .format ( srcs_dir , file ) ] , check=true )
__label__0 def assert_shallow_structure ( shallow_tree , input_tree , check_types=true , expand_composites=false ) : `` '' '' asserts that ` shallow_tree ` is a shallow structure of ` input_tree ` .
__label__0 def _testpartitionedvariables ( self , use_resource ) : var_full_shape = [ 10 , 3 ] # allows save/restore mechanism to work w/ different slicings . var_name = `` my_var '' saved_dir = self._get_test_dir ( `` partitioned_variables '' ) saved_path = os.path.join ( saved_dir , `` ckpt '' )
__label__0 # clean up test_op_with_optional._tf_fallback_dispatchers = original_handlers
__label__0 class getqualifiednametest ( test.testcase ) :
__label__0 argspec = tf_inspect.fullargspec ( args= [ 'self ' , ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def __init ( self ) : pass
__label__0 if __name__ == `` __main__ '' : parser = argparse.argumentparser ( formatter_class=argparse.rawdescriptionhelpformatter , description= '' '' '' convert a tensorflow python file to 1.0
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 def testgetargspecondecoratorthatchangesargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } , )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.raggedcountsparseoutput . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 # if we have detected the presence of imports of specific tf versions , # we want to modify the update spec to check only module deprecations # and skip all other conversions . if detections : self.function_handle = { } self.function_reorders = { } self.function_keyword_renames = { } self.symbol_renames = { } self.function_warnings = { } self.change_to_function = { } self.module_deprecations = module_deprecations_v2.module_deprecations self.function_transformers = { } self.import_renames = { } return root_node , visitor.log , visitor.warnings_and_errors
__label__0 returns : obj `` '' ''
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # lint.ifchange `` '' '' tensorflow root package '' '' ''
__label__0 if tensorrt_config : build_info.update ( tensorrt_config.config )
__label__0 def _maybedeleteoldcheckpoints ( self , meta_graph_suffix= '' meta '' ) : `` '' '' deletes old checkpoints if necessary .
__label__0 the list of libraries to find is specified as arguments . supported libraries are cuda ( includes cublas ) , cudnn , nccl , and tensorrt .
__label__0 shutil.move ( os.path.join ( srcs_dir , `` tensorflow/tools/pip_package/xla_build/cmakelists.txt '' ) , os.path.join ( srcs_dir , `` cmakelists.txt '' ) , )
__label__0 when it is deleted it will emit a warning or error if its ` sate ` method has not been called by time of deletion , and tensorflow is not executing eagerly or inside a tf.function ( which use autodeps and resolve the main issues this wrapper warns about ) . `` '' ''
__label__0 elif is_repeated ( field_desc ) and i < len ( fields ) : # the next field is the index within the list . index = fields [ i ] try : field_proto = field_proto [ index ] if field_proto is not none else none except indexerror : field_proto = none i += 1
__label__0 def testlazyload ( self ) : module = mockmodule ( 'test ' ) apis = { 'cmd ' : ( `` , 'cmd ' ) , 'abcmeta ' : ( 'abc ' , 'abcmeta ' ) } wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' , public_apis=apis , deprecation=false ) import cmd as _cmd # pylint : disable=g-import-not-at-top from abc import abcmeta as _abcmeta # pylint : disable=g-import-not-at-top , g-importing-member self.assertfalse ( wrapped_module._fastdict_key_in ( 'cmd ' ) ) self.assertequal ( wrapped_module.cmd , _cmd ) # verify that the apis are added to the cache of fastmoduletype object self.asserttrue ( wrapped_module._fastdict_key_in ( 'cmd ' ) ) self.assertfalse ( wrapped_module._fastdict_key_in ( 'abcmeta ' ) ) self.assertequal ( wrapped_module.abcmeta , _abcmeta ) self.asserttrue ( wrapped_module._fastdict_key_in ( 'abcmeta ' ) )
__label__0 return { `` tensorrt_version '' : header_version , `` tensorrt_include_dir '' : os.path.dirname ( header_path ) , `` tensorrt_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 self._ready.acquire ( ) self._group_member_counts [ group_id ] -= 1 if self._group_member_counts [ group_id ] == 0 : self._ready.notify_all ( ) self._ready.release ( )
__label__0 def _call_location ( outer=false ) : `` '' '' returns call location given level up from current call . '' '' '' # two up : < _call_location > , < _call_location 's caller > # tf_inspect is not required here . please ignore the lint warning by adding # disable_import_inspect_check=true to your cl description . using it caused # test timeouts ( b/189384061 ) . f = inspect.currentframe ( ) .f_back.f_back parent = f and f.f_back if outer and parent is not none : f = parent return ' { } : { } '.format ( f.f_code.co_filename , f.f_lineno )
__label__0 def testunsortedsegmentsum1dindices1ddata ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 1 , 3 , 2 , 9 ] , dtype=dtype ) , self._unsortedsegmentsum ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 3 , 0 , 2 , 1 , 3 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 # check also that the ordereddict was created , with the correct key order . unflattened_ordered_dict = unflattened [ 2 ] [ `` c '' ] [ 1 ] self.assertisinstance ( unflattened_ordered_dict , collections.ordereddict ) self.assertequal ( list ( unflattened_ordered_dict.keys ( ) ) , [ `` b '' , `` a '' ] )
__label__0 _flags_warning = ( ast_edits.error , `` tf.flags and tf.app.flags have been removed , please use the argparse or `` `` absl modules if you need command line parsing . '' )
__label__0 `` ` python d `` ` * second
__label__0 def testwarmstartvarstowarmstartisnone ( self ) : # create old and new vocabs for sparse column `` sc_vocab '' . prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' ) # create feature columns . sc_hash = fc.categorical_column_with_hash_bucket ( `` sc_hash '' , hash_bucket_size=15 ) sc_keys = fc.categorical_column_with_vocabulary_list ( `` sc_keys '' , vocabulary_list= [ `` a '' , `` b '' , `` c '' , `` e '' ] ) sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=new_vocab_path , vocabulary_size=6 ) all_linear_cols = [ sc_hash , sc_keys , sc_vocab ]
__label__0 expected_message = `` the two structures do n't have the same . * '' with self.assertraisesregex ( valueerror , expected_message ) : nest.assert_same_structure ( nesttest.samenameab ( 0 , nesttest.samenameab2 ( 1 , 2 ) ) , nesttest.samenameab2 ( nesttest.samenameab ( 0 , 1 ) , 2 ) )
__label__0 # if we used the alias , it should get renamed text = `` g2 ( a , b , kw1_alias=x , c=c , d=d ) \n '' acceptable_outputs = [ `` g2 ( a , b , kw1=x , c=c , d=d ) \n '' , `` g2 ( a , b , c=c , d=d , kw1=x ) \n '' , `` g2 ( a=a , b=b , kw1=x , c=c , d=d ) \n '' , `` g2 ( a=a , b=b , c=c , d=d , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 `` `
__label__0 class tensortracer ( object ) : `` '' '' an object used to trace tensorflow graphs .
__label__0 self._make_cluster_device_filters ( ) return self._cluster_device_filters
__label__0 with self.assertraisesregex ( assertionerror , `` dispatched op is called with argument ` optional ` set to a non-default '' `` value , which is not supported by the decorated function '' , ) : test_op_with_optional ( x , y , z , optional=3 )
__label__0 the tensorflow apis that may be overridden by ` @ dispatch_for_api ` are :
__label__0 expected_structure3 = collections.defaultdict ( list ) expected_structure3 [ `` a '' ] = [ 2 , 3 , 4 , 5 ] expected_structure3 [ `` b '' ] = [ 3 , 4 , 5 , 6 ] self.assertequal ( expected_structure3 , nest.map_structure ( lambda x : x + 1 , structure3 ) )
__label__0 def _upgrade ( self , spec , old_file_text ) : in_file = io.stringio ( old_file_text ) out_file = io.stringio ( ) upgrader = ast_edits.astcodeupgrader ( spec ) count , report , errors = ( upgrader.process_opened_file ( `` test.py '' , in_file , `` test_out.py '' , out_file ) ) return ( count , report , errors ) , out_file.getvalue ( )
__label__0 class reference ( _objectidentitywrapper ) : `` '' '' reference that refers an object .
__label__0 def testconcatreordernested ( self ) : text = `` tf.concat ( a , tf.concat ( c , d ) ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.concat ( axis=a , values=tf.concat ( axis=c , values=d ) ) \n '' )
__label__0 def testgetfullargspeconcallableobject ( self ) :
__label__0 signature = dict ( [ ( name , param.annotation ) for ( name , param ) in func_signature.parameters.items ( ) if param.annotation ! = tf_inspect.parameter.empty ] ) if not signature : raise valueerror ( `` the dispatch_for_api decorator must be called with at `` `` least one signature , or applied to a function that `` `` has type annotations on its parameters . '' ) return signature
__label__0 subclasses of ` composablechunks ` should only need to override this method .
__label__0 if transformers : if uses_star_args_or_kwargs_in_call ( node ) : self.add_log ( warning , node.lineno , node.col_offset , `` ( manual check required ) upgrading % s may require `` `` modifying call arguments , but it was passed `` `` variable-length * args or * * kwargs . the upgrade `` `` script can not handle these automatically . '' % ( full_name or name ) )
__label__0 def testaddshoulduseexceptionineagerandfunction ( self ) : def in_this_function ( ) : c = constant_op.constant ( 0 , name='blah0 ' ) h = tf_should_use._add_should_use_warning ( c , warn_in_eager=true , error_in_function=true ) del h if context.executing_eagerly ( ) : with reroute_error ( ) as error : in_this_function ( ) msg = '\n'.join ( error.call_args [ 0 ] ) self.assertin ( 'object was never used ' , msg ) self.assertin ( 'in_this_function ' , msg ) self.assertfalse ( gc.garbage )
__label__0 `` ` class parent ( object ) : def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 - using it in any capacity ( e.g . ` y = t + 0 ` , ` sess.run ( t ) ` ) - accessing a property ( e.g . getting ` t.name ` or ` t.op ` ) . - calling ` t.mark_used ( ) ` .
__label__0 def _load ( self ) : `` '' '' import the target module and insert it into the parent 's namespace . '' '' '' module = _importlib.import_module ( self.__name__ ) self._parent_module_globals [ self._local_name ] = module self.__dict__.update ( module.__dict__ ) return module
__label__0 def skip_magic ( code_line , magic_list ) : `` '' '' checks if the cell has magic , that is not python-based .
__label__0 flags = flags.flags
__label__0 # * * kwargs passed in that we can not inspect , should warn text = `` f ( a , b , kw1=c , * * kwargs ) \n '' ( _ , report , _ ) , _ = self._upgrade ( reorderandrenamekeywordspec ( ) , text ) self.assertin ( `` manual check required '' , report )
__label__0 self.generic_visit ( node )
__label__0 def get_filtered_filenames ( self ) : raise notimplementederror ( 'subclasses need to override this ' )
__label__0 returns : the zipped dictionary .
__label__0 # this header file has an explicit # define for hip_version , whose value # is ( hip_version_major * 100 + hip_version_minor ) # retreive the major + minor and re-calculate here , since we do not # want get into the business of parsing arith exprs major = _get_header_version ( version_file , `` hip_version_major '' ) minor = _get_header_version ( version_file , `` hip_version_minor '' ) return 100 * major + minor
__label__0 def testflatten_stringisnotflattened ( self ) : structure = `` lots of letters '' flattened = nest.flatten ( structure ) self.assertlen ( flattened , 1 ) unflattened = nest.pack_sequence_as ( `` goodbye '' , flattened ) self.assertequal ( structure , unflattened )
__label__0 # the local step for both workers should still be 0 because the initial # tokens in the token queue are 0s . this means that the following # computation of the gradients will be wasted as local_step is smaller than # the current global step . however , this only happens once when the system # just starts and this is necessary to make the system robust for the case # when chief gets restarted by errors/preemption/ ... self.assertallequal ( 0 , sessions [ 0 ] .run ( local_step_0 ) ) self.assertallequal ( 0 , sessions [ 1 ] .run ( local_step_1 ) )
__label__0 text = `` from tensorflow.google.compat import v2 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 def wrapper ( * args , * * kwargs ) : new_args = [ ] found = false for arg in args : if not found and isinstance ( arg , int ) : new_args.append ( arg + 1 ) found = true else : new_args.append ( arg ) return target ( * new_args , * * kwargs )
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) checkpoint_filename = os.path.join ( checkpoint_dir , `` prepare_session_checkpoint '' ) saver.save ( sess , checkpoint_filename ) # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : # renames the checkpoint directory . os.rename ( checkpoint_dir , checkpoint_dir2 ) gfile.makedirs ( checkpoint_dir ) v = variable_v1.variablev1 ( [ 6.0 , 7.0 , 8.0 ] , name= '' v '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) # this should fail as there 's no checkpoint within 2 seconds . with self.assertraisesregex ( runtimeerror , `` no init_op or init_fn or local_init_op was given '' ) : sess = sm.prepare_session ( `` '' , init_op=none , saver=saver , checkpoint_dir=checkpoint_dir , wait_for_checkpoint=true , max_wait_secs=2 ) # rename the checkpoint directory back . gfile.deleterecursively ( checkpoint_dir ) os.rename ( checkpoint_dir2 , checkpoint_dir ) # this should succeed as there 's checkpoint . sess = sm.prepare_session ( `` '' , init_op=none , saver=saver , checkpoint_dir=checkpoint_dir , wait_for_checkpoint=true , max_wait_secs=2 ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) )
__label__0 flattened_input_tree = flatten_with_tuple_paths_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = flatten_with_tuple_paths_up_to ( shallow_tree , shallow_tree )
__label__0 def test_call_forward ( self ) : visitor = self.testvisitor ( ) children = [ ( 'name1 ' , 'thing1 ' ) , ( 'name2 ' , 'thing2 ' ) ] public_api.publicapivisitor ( visitor ) ( 'test ' , 'dummy ' , children ) self.assertequal ( set ( [ 'test ' ] ) , visitor.symbols ) self.assertequal ( 'dummy ' , visitor.last_parent ) self.assertequal ( [ ( 'name1 ' , 'thing1 ' ) , ( 'name2 ' , 'thing2 ' ) ] , visitor.last_children )
__label__0 empty_strings = not bool ( proto.strings ) super ( ) .__init__ ( proto , proto_as_initial_chunk=empty_strings , * * kwargs )
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` nccl_major '' , `` nccl_minor '' , `` nccl_patch '' ) ) return `` . `` .join ( version )
__label__0 def __init__ ( self , learning_rate , decay=0.9 , momentum=0.0 , epsilon=1e-10 , use_locking=false , centered=false , name= '' rmsprop '' ) : `` '' '' construct a new rmsprop optimizer .
__label__0 args : job_name : the string name of a job in this cluster .
__label__0 def initialized_value ( var ) : return cond.cond ( variable_v1.is_variable_initialized ( var ) , var.read_value , lambda : var.initial_value )
__label__0 raises : valueerror : if date is not none or in iso 8601 format , or instructions are empty. `` '' '' _validate_deprecation_args ( date , instructions ) if not deprecated_kwargs : raise valueerror ( 'specify which argument values are deprecated . ' )
__label__0 # avoid long waits in cases of pretty unambiguous failure . if tf_inspect.ismodule ( parent ) and len ( path.split ( ' . ' ) ) > 10 : raise runtimeerror ( 'modules nested too deep : \n % s. % s\n\nthis is likely a ' 'problem with an accidental public import . ' % ( self._root_name , path ) )
__label__0 else : approx_node_size -= node_splitter.build_chunks ( ) if library_size + approx_node_size > constants.max_size ( ) : library_size -= function_splitter.build_chunks ( )
__label__0 def testboundfuncwithmanyparamsanddefaults ( self ) :
__label__0 def _init_session_manager ( self , session_manager=none ) : if session_manager is none : self._session_manager = session_manager_mod.sessionmanager ( local_init_op=self._local_init_op , ready_op=self._ready_op , ready_for_local_init_op=self._ready_for_local_init_op , graph=self._graph , recovery_wait_secs=self._recovery_wait_secs , local_init_run_options=self._local_init_run_options ) else : self._session_manager = session_manager
__label__0 @ test_util.run_deprecated_v1 def testminimizesparseresourcevariablecentered ( self ) : for dtype in [ dtypes.float32 , dtypes.float64 ] : with self.cached_session ( ) : var0 = resource_variable_ops.resourcevariable ( [ [ 1.0 , 2.0 ] ] , dtype=dtype ) x = constant_op.constant ( [ [ 4.0 ] , [ 5.0 ] ] , dtype=dtype ) pred = math_ops.matmul ( embedding_ops.embedding_lookup ( [ var0 ] , [ 0 ] ) , x ) loss = pred * pred sgd_op = rmsprop.rmspropoptimizer ( learning_rate=1.0 , decay=0.0 , momentum=0.0 , epsilon=1.0 , centered=true ) .minimize ( loss ) self.evaluate ( variables.global_variables_initializer ( ) ) # fetch params to validate initial values self.assertallcloseaccordingtotype ( [ [ 1.0 , 2.0 ] ] , self.evaluate ( var0 ) ) # run 1 step of sgd self.evaluate ( sgd_op ) # validate updated params self.assertallcloseaccordingtotype ( [ [ -111 , -138 ] ] , self.evaluate ( var0 ) , atol=0.01 )
__label__0 examples :
__label__0 def testspacetobatch ( self ) : text = `` tf.space_to_batch_nd ( input , shape , paddings , name ) '' expected_text = `` tf.space_to_batch ( input , shape , paddings , name ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # assert calls with the deprecated arguments do n't log warnings if # the value matches the 'ok_val ' . mock_warning.reset_mock ( ) self.assertequal ( 3 , _fn ( 1 , none , 2 , d2= '' my_ok_val '' ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 # restore the values saved earlier in the parameter nodes . save2 = saver_module.saver ( { `` v0 '' : v0_2 , `` v1 '' : v1_2 , `` v2 '' : v2_2.saveable } ) save2.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0_2 ) ) self.assertequal ( 20.0 , self.evaluate ( v1_2 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2_2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2_2.values ( ) ) )
__label__0 # verifies that v1 is still valid . self.assertallequal ( 2.0 , sess_1.run ( v1 ) )
__label__0 symbols in this file are deprecated . see replacements in tensorflow/python/training/trackable and tensorflow/python/training/saving. `` '' '' import collections import glob import os.path import threading import time
__label__0 args : graph : the graph in which to create the global step read tensor . if missing , use default graph .
__label__0 to create a cluster with two jobs and five tasks , you specify the mapping from job names to lists of network addresses ( typically hostname-port pairs ) .
__label__0 # keywords are reordered , so we should reorder arguments too text = `` f ( a , b , c , d ) \n '' acceptable_outputs = [ `` f ( a , b , d , c ) \n '' , `` f ( a=a , b=b , kw1=c , kw3=d ) \n '' , `` f ( a=a , b=b , kw3=d , kw1=c ) \n '' , ] ( _ , report , _ ) , new_text = self._upgrade ( reorderandrenamekeywordspec ( ) , text ) self.assertin ( new_text , acceptable_outputs ) self.assertnotin ( `` manual check required '' , report )
__label__0 partial_func = functools.partial ( func , 7 , 10 ) argspec = tf_inspect.fullargspec ( args= [ ] , varargs=none , varkw=none , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def testshoulduseresultwhennotreallyused ( self ) : @ tf_should_use.should_use_result ( warn_in_eager=true ) def return_const ( value ) : return constant_op.constant ( value , name='blah3 ' ) with reroute_error ( ) as error : with self.cached_session ( ) : return_const ( 0.0 ) # creating another op and executing it does not mark the # unused op as being `` used '' . v = constant_op.constant ( 1.0 , name='meh ' ) self.evaluate ( v ) msg = '\n'.join ( error.call_args [ 0 ] ) self.assertin ( 'object was never used ' , msg ) if not context.executing_eagerly ( ) : self.assertin ( 'blah3:0 ' , msg ) self.assertin ( 'return_const ' , msg ) gc.collect ( ) self.assertfalse ( gc.garbage )
__label__0 this proto implements the ` union ` .
__label__0 # this assertion is expected to fail , when ` check_types=true ` , because the # shallow_tree type is not the same as input_tree . with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( mt ) , input_type=type ( nmt ) ) , ) : nest.assert_shallow_structure ( shallow_tree=mt , input_tree=nmt , check_types=true )
__label__0 @ doc_controls.do_not_generate_docs def reduce ( self , initial_state , reduce_func ) : raise notimplementederror ( `` distributeddataset.reduce must be implemented in descendants . '' )
__label__0 with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( x ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( v_res ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w_res ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( x_res ) .eval ( ) ) sm2 = session_manager.sessionmanager ( local_init_op= [ w.initializer , x.initializer , w_res.initializer , x_res.initializer ] ) sess = sm2.prepare_session ( `` '' , init_op=none ) self.assertequal ( false , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` x:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( w ) ) self.assertequal ( 3 , sess.run ( x ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v_res:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w_res:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` x_res:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( w_res ) ) self.assertequal ( 3 , sess.run ( x_res ) )
__label__0 target.decorated_target = new_target
__label__0 def flatten_up_to ( modality , shallow_tree , input_tree , check_types=true , expand_composites=false , ) : # pylint : disable=g-doc-return-or-yield , g-doc-args `` '' '' flattens ` input_tree ` up to ` shallow_tree ` .
__label__0 ` visitor ` should be a callable suitable as a visitor for ` traverse ` . it will be called only for members of the public tensorflow api .
__label__0 args : run_context : a ` sessionruncontext ` object .
__label__0 config.name = `` xla '' config.suffixes = [ `` .cc '' , `` .hlo '' , `` .hlotxt '' , `` .json '' , `` .mlir '' , `` .pbtxt '' , `` .py '' ]
__label__0 attempt to get the sha from environment variable git_commit , which should be available on jenkins build agents .
__label__0 new_alias = ast.alias ( name=new_name , asname=new_asname ) new_aliases.append ( new_alias ) import_updated = true found_update = true
__label__0 def test_injectable_decorator_square ( target ) :
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf 2.0 upgrader . '' '' ''
__label__0 use ` tf.train.checkpointmanager ` to manage checkpoints in tf2 . ` tf.train.checkpointmanager ` offers equivalent ` keep_checkpoint_every_n_hours ` and ` max_to_keep ` parameters .
__label__0 def finalize_options ( self ) : self.set_undefined_options ( 'install ' , ( 'install_headers ' , 'install_dir ' ) , ( 'force ' , 'force ' ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassisnested ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) self.asserttrue ( nest.is_nested ( mt ) )
__label__0 returns : function that accepts wrapper function as an argument and returns ` tfdecorator ` instance. `` '' ''
__label__0 2. for a nested python tuple :
__label__0 with ops.name_scope ( name , `` save '' , [ saveable.op for saveable in saveables ] ) as name : # add a placeholder string tensor for the filename . filename_tensor = array_ops.placeholder_with_default ( filename or `` model '' , shape= ( ) , name= '' filename '' ) # keep the name `` const '' for backwards compatibility . filename_tensor = array_ops.placeholder_with_default ( filename_tensor , shape= ( ) , name= '' const '' )
__label__0 def _create_prev_run_var ( self , var_name , shape=none , initializer=none , partitioner=none ) : with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : var = variable_scope.get_variable ( var_name , shape=shape , initializer=initializer , partitioner=partitioner ) self._write_checkpoint ( sess ) if partitioner : self.asserttrue ( isinstance ( var , variables.partitionedvariable ) ) var = var._get_variable_list ( ) return var , self.evaluate ( var )
__label__0 with self.assertraisesregex ( valueerror , `` do n't have the same set of keys '' ) : nest.assert_same_structure ( { `` a '' : 1 } , { `` b '' : 1 } )
__label__0 the decorated function ( known as the `` dispatch target '' ) will override the default implementation for the api when the api is called with parameters that match a specified type signature . signatures are specified using dictionaries that map parameter names to type annotations . e.g. , in the following example , ` masked_add ` will be called for ` tf.add ` if both ` x ` and ` y ` are ` maskedtensor ` s :
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=true ) class mystr ( str ) :
__label__0 if v1_output and v2_output : assert type ( v2_output ) == type ( v1_output ) # pylint : disable=unidiomatic-typecheck assert v2_output.args == v1_output.args
__label__0 def testne ( self ) : self.assertnotequal ( server_lib.clusterspec ( { } ) , server_lib.clusterspec ( { `` job '' : [ `` host:2223 '' ] } ) , ) self.assertnotequal ( server_lib.clusterspec ( { `` job1 '' : [ `` host:2222 '' ] } ) , server_lib.clusterspec ( { `` job2 '' : [ `` host:2222 '' ] } ) , ) self.assertnotequal ( server_lib.clusterspec ( { `` job '' : [ `` host:2222 '' ] } ) , server_lib.clusterspec ( { `` job '' : [ `` host:2223 '' ] } ) , ) self.assertnotequal ( server_lib.clusterspec ( { `` job '' : [ `` host:2222 '' , `` host:2223 '' ] } ) , server_lib.clusterspec ( { `` job '' : [ `` host:2223 '' , `` host:2222 '' ] } ) , )
__label__0 num_sparse = parse_example_op.get_attr ( `` nsparse '' ) num_dense = parse_example_op.get_attr ( `` ndense '' ) total_features = num_dense + num_sparse
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 def _tf_core_yield_value ( iterable ) : for _ , v in _tf_core_yield_sorted_items ( iterable ) : yield v
__label__0 @ property def private_map ( self ) : `` '' '' a map from parents to symbols that should not be included at all .
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testflattenandpack_withdicts ( self ) : # a nice messy mix of tuples , lists , dicts , and ` ordereddict ` s . mess = [ `` z '' , nesttest.abc ( 3 , 4 ) , { `` d '' : _custommapping ( { 41 : 4 } ) , `` c '' : [ 1 , collections.ordereddict ( [ ( `` b '' , 3 ) , ( `` a '' , 2 ) , ] ) , ] , `` b '' : 5 } , 17 ]
__label__0 tf.ones ( [ 4 , 5 ] ) except attributeerror : pass `` '' '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # gather source code information git_sha = get_git_commit_sha ( ) if git_sha : results.commit_id.hash = git_sha
__label__0 returns : true if the node uses starred variadic positional args or keyword args . false if it does not. `` '' '' return uses_star_args_in_call ( node ) or uses_star_kwargs_in_call ( node )
__label__0 def __init__ ( self , tensor , score ) : self.tensor = ops.convert_to_tensor ( tensor ) self.score = score
__label__0 from tensorflow.python.framework import composite_tensor from tensorflow.python.framework import ops from tensorflow.python.util import _pywrap_utils from tensorflow.python.util import nest
__label__0 decorator_utils.validate_callable ( func , 'deprecated ' )
__label__0 args : func : a callable that accepts as many arguments as there are structures . * structure : atom or nested structure . * * kwargs : valid keyword args are : * ` check_types ` : if set to ` true ` ( default ) the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` . note that namedtuples with identical name and fields are always considered to have the same shallow structure . * ` expand_composites ` : if set to ` true ` , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors . if ` false ` ( the default ) , then composite tensors are not expanded .
__label__0 def testmaxtokeepeager ( self ) : with context.eager_mode ( ) : save_dir = self._get_test_dir ( `` max_to_keep_eager '' )
__label__0 if not found : raise valueerror ( f '' function { dispatch_target } was not registered using `` `` a ` @ dispatch_for_ * ` decorator . '' )
__label__0 flags.define_string ( `` name '' , `` '' , `` '' '' benchmark target identifier . '' '' '' ) flags.define_string ( `` test_name '' , `` '' , `` '' '' test target to run . '' '' '' ) flags.define_multi_string ( `` test_args '' , `` '' , `` '' '' \ test arguments , space separated . may be specified more than once , in which case the args are all appended . '' '' '' ) flags.define_boolean ( `` test_log_output_use_tmpdir '' , false , `` whether to store the log output into tmpdir . '' ) flags.define_string ( `` benchmark_type '' , `` '' , `` '' '' benchmark type ( benchmarktype enum string ) . '' '' '' ) flags.define_string ( `` compilation_mode '' , `` '' , `` '' '' mode used during this build ( e.g . opt , dbg ) . '' '' '' ) flags.define_string ( `` cc_flags '' , `` '' , `` '' '' cc flags used during this build . '' '' '' ) flags.define_string ( `` test_log_output_dir '' , `` '' , `` '' '' directory for benchmark results output . '' '' '' ) flags.define_string ( `` test_log_output_filename '' , `` '' , `` '' '' filename to write output benchmark results to . if the filename is not specified , it will be automatically created . '' '' '' ) flags.define_boolean ( `` skip_export '' , false , `` whether to skip exporting test results . '' )
__label__0 args : structure : nested structure , whose structure is given by nested lists , tuples , and dicts . note : numpy arrays and strings are considered scalars . flat_sequence : flat sequence to pack . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 2. for a nested python tuple :
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import nest from tensorflow.python.util import object_identity
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 import os import subprocess
__label__0 returns : a ` str ` object .
__label__0 from tensorflow.python.util import tf_decorator
__label__0 example :
__label__0 def test_class ( self ) : visitor = testvisitor ( ) traverse.traverse ( testvisitor , visitor ) self.assertequal ( testvisitor , visitor.call_log [ 0 ] [ 1 ] ) # there are a bunch of other members , but make sure that the ones we know # about are there . self.assertin ( '__init__ ' , [ name for name , _ in visitor.call_log [ 0 ] [ 2 ] ] ) self.assertin ( '__call__ ' , [ name for name , _ in visitor.call_log [ 0 ] [ 2 ] ] )
__label__0 functiontype inherits from inspect.signature which canonically represents the structure ( and optionally type ) information of input parameters and output of a python function . additionally , it integrates with the tf.function type system ( ` tf.types.experimental.tracetype ` ) to provide a holistic representation of the the i/o contract of the callable . it is used for : - canonicalization and type-checking of python input arguments - type-based dispatch to concrete functions - packing/unpacking structured python values to tensors - generation of structured placeholder values for tracing `` '' ''
__label__0 class summarywriter ( _filewriter ) :
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' prints cuda library and header directories and versions found on the system .
__label__0 import ast import io import os
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=false ) def _fn ( ) : pass
__label__0 for i , task_device_filters in sorted ( tasks.items ( ) ) : for tdf in task_device_filters : try : tdf = compat.as_bytes ( tdf ) except typeerror : raise typeerror ( `` device filter % r must be bytes or unicode '' % tdf ) jdf.tasks [ i ] .device_filters.append ( tdf )
__label__0 def test_fail_non_callable ( self ) : x = 0 self.assertraises ( valueerror , decorator_utils.validate_callable , x , `` test '' )
__label__0 def run_loop ( self ) : if self._sv.global_step is not none : summary_strs , global_step = self._sess.run ( [ self._sv.summary_op , self._sv.global_step ] ) else : summary_strs = self._sess.run ( self._sv.summary_op ) global_step = none if self._sv.summary_writer : logging.info ( `` recording summary at step % s . `` , global_step ) self._sv.summary_writer.add_summary ( summary_strs , global_step )
__label__0 if name is none : return elementwise_api_handler ( tensor_api , x , y ) else : with ops.name_scope ( name , none , [ x , y ] ) : return elementwise_api_handler ( tensor_api , x , y )
__label__0 # we ca n't import implib-gen directly because it has a dash in its name . implib = importlib.import_module ( 'implib-gen ' )
__label__0 # this key is defined in tf/core/util/reporter.h as # testreporter : :ktestreporterenv . os.environ [ `` test_report_file_prefix '' ] = test_file_prefix start_time = time.time ( ) subprocess.check_call ( [ test_executable ] + test_args ) if skip_processing_logs : return none , test_adjusted_name run_time = time.time ( ) - start_time log_files = gfile.glob ( `` { } * '' .format ( test_file_prefix ) ) if not log_files : raise missinglogserror ( `` no log files found at % s . '' % test_file_prefix )
__label__0 sess = session.session ( self._target , graph=self._graph , config=config ) if checkpoint_dir and checkpoint_filename_with_path : raise valueerror ( `` can not provide both checkpoint_dir and `` `` checkpoint_filename_with_path . '' ) # if either saver or checkpoint_ * is not specified , can not restore . just # return . if not saver or not ( checkpoint_dir or checkpoint_filename_with_path ) : return sess , false
__label__0 if not structure : raise valueerror ( `` must provide at least one structure '' )
__label__0 seed = self.get_int ( ) shape = self.get_int_list ( min_length=min_size , max_length=max_size , min_int=min_size , max_int=max_size )
__label__0 keras_version = `` tf_keras '' if self._tfll_mode == `` v1 '' : package_name = `` tf_keras.api._v1.keras '' else : package_name = `` tf_keras.api._v2.keras '' except importerror : logging.warning ( `` your environment has tf_use_legacy_keras set to true , but you `` `` do not have the tf_keras package installed . you must install it `` `` in order to use the legacy tf.keras . install it via : `` `` ` pip install tf_keras ` `` ) else : try : import keras # pylint : disable=g-import-not-at-top
__label__0 def main ( ) : parser = argparse.argumentparser ( description='generates stubs for cuda libraries . ' ) parser.add_argument ( 'symbols ' , help='file containing a list of symbols . ' ) parser.add_argument ( ' -- outdir ' , '-o ' , help='path to create wrapper at ' , default= ' . ' ) parser.add_argument ( ' -- target ' , help='target platform name , e.g . x86_64 , aarch64 . ' , required=true , ) args = parser.parse_args ( )
__label__0 for i , kw in enumerate ( node.keywords ) : if kw.arg == `` source '' : kw.arg = `` input ''
__label__0 # assert calling new fn with non-deprecated value logs nothing . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=false ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 a none value is returned if no variables exist in the ` metagraphdef ` ( i.e. , there are no variables to restore ) .
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' unit tests for tf_contextlib . '' '' ''
__label__0 def as_text ( bytes_or_text , encoding='utf-8 ' ) : `` '' '' converts any string-like python input types to unicode .
__label__0 def test_metrics_save_restore ( self ) : api_label = saver_module._saver_label
__label__0 def testwaitforsessionreturnsnoneaftertimeout ( self ) : with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) , recovery_wait_secs=1 )
__label__0 `` ` python optimizer = tf.compat.v1.train.rmspropoptimizer ( learning_rate=learning_rate , decay=decay , momentum=momentum , epsilon=epsilon ) `` `
__label__0 # pylint : disable=g-doc-return-or-yield , broad-except @ contextlib.contextmanager def managed_session ( self , master= '' '' , config=none , start_standard_services=true , close_summary_writer=true ) : `` '' '' returns a context manager for a managed session .
__label__0 with deprecation.silence ( ) : _fn ( ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 def testdropoutexpr ( self ) : text = `` tf.nn.dropout ( x , 1 - func ( 3 + 4 . ) , name=\ '' foo\ '' ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.nn.dropout ( x , rate=1 - ( 1 - func ( 3 + 4 . ) ) , name=\ '' foo\ '' ) \n '' , )
__label__0 if not prev_tensor_name : # assume tensor name remains the same . prev_tensor_name = _infer_var_name ( var )
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertfalse ( initialized ) sess.run ( v.initializer ) self.assertequal ( 1 , sess.run ( v ) ) saver.save ( sess , os.path.join ( checkpoint_dir , `` recover_session_checkpoint '' ) ) # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 2 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( variables.global_variables ( ) ) , local_init_op=w.initializer ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm2.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.asserttrue ( initialized ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) ) self.assertequal ( 1 , sess.run ( w ) )
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import constant_op from tensorflow.python.framework import strict_mode from tensorflow.python.framework import tensor from tensorflow.python.framework import test_util from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import deprecation from tensorflow.python.util import tf_inspect
__label__0 returns : obj `` '' '' setattr ( obj , _do_not_doc_inheritable , none ) return obj
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 # save the initialized values in the file at `` save_path '' val = save.save ( sess , save_path1 ) self.assertisinstance ( val , str ) self.assertequal ( save_path1 , val )
__label__0 import numpy as np
__label__0 header_path , header_version = _find_header ( base_paths , `` cusparse.h '' , required_version , get_header_version ) cusparse_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 def testcolocategradientswithopscomputegradients ( self ) : text = `` optimizer.compute_gradients ( a , foo=false ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( text , new_text ) self.assertequal ( errors , [ ] )
__label__0 def _testgraphextensionrestore ( self , test_dir ) : filename = os.path.join ( test_dir , `` metafile '' ) train_filename = os.path.join ( test_dir , `` train_metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : # restores from metagraphdef . new_saver = saver_module.import_meta_graph ( filename ) # generates a new metagraphdef . new_saver.export_meta_graph ( ) # restores from checkpoint . new_saver.restore ( sess , saver0_ckpt ) # adds loss and train . labels = constant_op.constant ( 0 , dtypes.int32 , shape= [ 100 ] , name= '' labels '' ) batch_size = array_ops.size ( labels ) labels = array_ops.expand_dims ( labels , 1 ) indices = array_ops.expand_dims ( math_ops.range ( 0 , batch_size ) , 1 ) concated = array_ops.concat ( [ indices , labels ] , 1 ) onehot_labels = sparse_ops.sparse_to_dense ( concated , array_ops_stack.stack ( [ batch_size , 10 ] ) , 1.0 , 0.0 ) logits = ops_lib.get_collection ( `` logits '' ) [ 0 ] cross_entropy = nn_ops.softmax_cross_entropy_with_logits ( labels=onehot_labels , logits=logits , name= '' xentropy '' ) loss = math_ops.reduce_mean ( cross_entropy , name= '' xentropy_mean '' )
__label__0 def main ( ) : options = parse_cmd_line ( )
__label__0 def testsamenamedtuples ( self ) : # a classic namedtuple and an equivalent cppy . foo1 = collections.namedtuple ( `` foo '' , [ `` a '' , `` b '' ] ) foo2 = collections.namedtuple ( `` foo '' , [ `` a '' , `` b '' ] ) self.asserttrue ( nest.same_namedtuples ( foo1 ( 1 , 2 ) , foo1 ( 3 , 4 ) ) ) self.asserttrue ( nest.same_namedtuples ( foo1 ( 1 , 2 ) , foo2 ( 3 , 4 ) ) )
__label__0 `` ` python def main ( ... ) : while true try : train ( ) except tf.errors.aborted : pass `` `
__label__0 returns : a tuple whose first element is an list of tfdecorator-derived objects that were applied to the final callable target , and whose second element is the final undecorated callable target . if the ` maybe_tf_decorator ` parameter is not decorated by any tfdecorators , the first tuple element will be an empty list . the ` tfdecorator ` list is ordered from outermost to innermost decorators. `` '' '' decorators = [ ] cur = maybe_tf_decorator while true : if isinstance ( cur , tfdecorator ) : decorators.append ( cur ) elif _has_tf_decorator_attr ( cur ) : decorators.append ( getattr ( cur , '_tf_decorator ' ) ) else : break if not hasattr ( decorators [ -1 ] , 'decorated_target ' ) : break cur = decorators [ -1 ] .decorated_target return decorators , cur
__label__0 returns : a summary_iterator `` '' '' event_paths = sorted ( glob.glob ( os.path.join ( test_dir , `` event * '' ) ) ) return summary_iterator.summary_iterator ( event_paths [ -1 ] )
__label__0 def testgetargspeconpartialargumentwithconvertibletofalse ( self ) : `` '' '' tests getargspec on partial function with args that convert to false . '' '' ''
__label__0 return sycl_config
__label__0 if the arg is not found , the call site is left alone .
__label__0 def testgetattrcallback ( self ) : # tests that functionality of __getattr__ can be set as a callback . module = childfastmodule ( `` test '' ) fastmoduletype.set_getattribute_callback ( module , childfastmodule._getattribute2 ) fastmoduletype.set_getattr_callback ( module , childfastmodule._getattr ) self.assertequal ( 3 , module.foo )
__label__0 def testtensorflowfromimport ( self ) : text = `` from tensorflow import foo '' expected_text = `` from tensorflow.compat.v1 import foo '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def testtwoops ( self ) : with self.cached_session ( ) as sess : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var0 = variable_v1.variablev1 ( zero64 ) count_up_to_3 = var0.count_up_to ( 3 ) var1 = variable_v1.variablev1 ( zero64 ) count_up_to_30 = var1.count_up_to ( 30 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to_3 , count_up_to_30 ] ) threads = qr.create_threads ( sess ) self.assertequal ( sorted ( t.name for t in threads ) , [ `` queuerunnerthread-fifo_queue-countupto:0 '' , `` queuerunnerthread-fifo_queue-countupto_1:0 '' ] ) self.evaluate ( variables.global_variables_initializer ( ) ) for t in threads : t.start ( ) for t in threads : t.join ( ) self.assertequal ( 0 , len ( qr.exceptions_raised ) ) self.assertequal ( 3 , self.evaluate ( var0 ) ) self.assertequal ( 30 , self.evaluate ( var1 ) )
__label__0 floatlist.__doc__ = `` '' '' \ used in ` tf.train.example ` protos . holds a list of floats .
__label__0 * entry : ( test , entry , start , timing ) is indexed to use projection and only fetch the recent ( start , timing ) data for a given test/entry benchmark .
__label__0 the other arguments to the constructor control the asynchronous writes to the event file :
__label__0 # pylint : disable=missing-function-docstring def _tf_core_map_structure ( func , * structure , * * kwargs ) : if not callable ( func ) : raise typeerror ( `` func must be callable , got : % s '' % func )
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.function_keyword_renames [ `` g '' ] = { `` kw1_alias '' : `` kw1 '' } self.function_keyword_renames [ `` g2 '' ] = { `` kw1_alias '' : `` kw1 '' }
__label__0 try : x = maskedtensor ( [ 1 , -2 , -3 ] , [ true , true , false ] ) # test calls with positional & keyword argument ( & combinations ) abs_x = math_ops.abs ( x ) sign_x = math_ops.sign ( x=x ) neg_x = math_ops.negative ( x , `` neg_x '' ) invert_x = bitwise_ops.invert ( x , name= '' invert_x '' ) ones_like_x = array_ops.ones_like ( x , name= '' ones_like_x '' ) ones_like_x_float = array_ops.ones_like ( x , dtypes.float32 , name= '' ones_like_x_float '' ) self.assertallequal ( abs_x.values , [ 1 , 2 , 3 ] ) self.assertallequal ( sign_x.values , [ 1 , -1 , -1 ] ) self.assertallequal ( neg_x.values , [ -1 , 2 , 3 ] ) self.assertallequal ( invert_x.values , [ -2 , 1 , 2 ] ) self.assertallequal ( ones_like_x.values , [ 1 , 1 , 1 ] ) self.assertallequal ( ones_like_x_float.values , [ 1. , 1. , 1 . ] ) for result in [ abs_x , sign_x , neg_x , invert_x , ones_like_x , ones_like_x_float ] : self.assertallequal ( result.mask , [ true , true , false ] ) if not context.executing_eagerly ( ) : # names not defined in eager mode . self.assertregex ( neg_x.values.name , r '' ^neg_x/neg : . * '' ) self.assertregex ( invert_x.values.name , r '' ^invert_x/ . * '' ) self.assertregex ( ones_like_x.values.name , r '' ^ones_like_x/ . * '' ) self.assertregex ( ones_like_x_float.values.name , r '' ^ones_like_x_float/ . * '' )
__label__0 def test_get_or_create_global_step ( self ) : with ops.graph ( ) .as_default ( ) as g : self.assertisnone ( training_util.get_global_step ( ) ) self._assert_global_step ( training_util.get_or_create_global_step ( ) ) self._assert_global_step ( training_util.get_or_create_global_step ( g ) )
__label__0 self._check_saver_def ( ) if not context.executing_eagerly ( ) : # updates next checkpoint time . # set in __init__ when executing eagerly . self._next_checkpoint_time = ( time.time ( ) + self.saver_def.keep_checkpoint_every_n_hours * 3600 )
__label__0 def test_import_analysis ( self ) : old_symbol = `` tf.conj ( a ) '' new_symbol = `` tf.math.conj ( a ) ''
__label__0 def testposonly ( self ) : self.assertequal ( self._matmul_func.canonicalize ( 2 , 3 ) , [ 2 , 3 , false , false , false , false , false , false , none ] )
__label__0 self.assertequal ( len ( math_ops.atan2._tf_fallback_dispatchers ) , len ( original_handlers ) + 1 )
__label__0 # whether chunks have been created . see ` build_chunks ( ) ` . self._built = false
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities for accessing python generic type annotations ( typing. * ) . '' '' ''
__label__0 # runs train_op . self.evaluate ( train_op )
__label__0 with session.session ( graph=ops_lib.graph ( ) ) as sess : saver_module.import_meta_graph ( meta_graph_def , clear_devices=false , import_scope= '' new_model '' ) # device refers to gpu , which is not available here . with self.assertraises ( errors_impl.invalidargumenterror ) : self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 self.assertfalse ( nest.is_namedtuple ( somethingelsewithfields ( ) ) )
__label__0 # 'reset ' the status before testing against 'tensorflow.compat.v2.compat.v1 ' module_wrapper.tfmodulewrapper.compat_v1_usage_recorded = false mock_tf_v2_v1 = mock_tf_v1 = mockmodule ( 'tensorflow.compat.v2.compat.v1 ' ) mock_tf_v2_v1_wrapped = module_wrapper.tfmodulewrapper ( mock_tf_v2_v1 , 'test ' , public_apis=apis ) self.assertfalse ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded ) mock_tf_v2_v1_wrapped.cosh # pylint : disable=pointless-statement self.asserttrue ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded )
__label__0 self.assertequal ( { ' a ' : 3 } , tf_inspect.getcallargs ( func , a=3 ) )
__label__0 * ` keep_checkpoint_every_n_hours ` : in addition to keeping the most recent ` max_to_keep ` checkpoint files , you might want to keep one checkpoint file for every n hours of training . this can be useful if you want to later analyze how a model progressed during a long training session . for example , passing ` keep_checkpoint_every_n_hours=2 ` ensures that you keep one checkpoint file for every 2 hours of training . the default value of 10,000 hours effectively disables the feature .
__label__0 args : results : the return values from ` session.run ( ) ` corresponding to the fetches attribute returned in the runargs . note that this has the same shape as the runargs fetches . for example : fetches = global_step_tensor = > results = nparray ( int ) fetches = [ train_op , summary_op , global_step_tensor ] = > results = [ none , nparray ( string ) , nparray ( int ) ] fetches = { 'step ' : global_step_tensor , 'summ ' : summary_op } = > results = { 'step ' : nparray ( int ) , 'summ ' : nparray ( string ) } options : ` runoptions ` from the ` session.run ( ) ` call . run_metadata : ` runmetadata ` from the ` session.run ( ) ` call. `` '' ''
__label__0 def __call__ ( self ) : pass
__label__0 self.assertfalse ( function_utils.has_kwargs ( double_wrapped_fn ) ) some_arg = 1 self.assertequal ( double_wrapped_fn ( some_arg ) , some_arg ) # pylint : disable=no-value-for-parameter
__label__0 def __init__ ( self , version ) : self.log_level = ast_edits.info self.log_message = ( `` not upgrading symbols because ` tensorflow . '' + version + `` ` was directly imported as ` tf ` . '' )
__label__0 hipsparse_config = { `` hipsparse_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 `` ` # tf.train.feature feature = union [ list [ bytes ] , list [ int64 ] , list [ float ] ]
__label__0 @ tf_export ( `` __internal__.nest.list_to_tuple '' , v1= [ ] ) def list_to_tuple ( structure ) : `` '' '' replace all lists with tuples .
__label__0 > > > dict = { `` key3 '' : `` value3 '' , `` key1 '' : `` value1 '' , `` key2 '' : `` value2 '' } > > > tf.nest.flatten ( dict ) [ 'value1 ' , 'value2 ' , 'value3 ' ]
__label__0 if cuda_config : build_info.update ( cuda_config.config )
__label__0 def testwarmstartvarbothvarspartitioned ( self ) : _ , weights = self._create_prev_run_var ( `` old_scope/fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) prev_val = np.concatenate ( [ weights [ 0 ] , weights [ 1 ] ] , axis=0 ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` new_scope/fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) self.asserttrue ( isinstance ( fruit_weights , variables.partitionedvariable ) ) prev_tensor_name , var = ws_util._get_var_info ( fruit_weights , prev_tensor_name= '' old_scope/fruit_weights '' ) checkpoint_utils.init_from_checkpoint ( self.get_temp_dir ( ) , { prev_tensor_name : var } ) self.evaluate ( variables.global_variables_initializer ( ) ) fruit_weights = fruit_weights._get_variable_list ( ) new_val = np.concatenate ( [ fruit_weights [ 0 ] .eval ( sess ) , fruit_weights [ 1 ] .eval ( sess ) ] , axis=0 ) self.assertallclose ( prev_val , new_val )
__label__0 `` ` bash result `` `
__label__0 def testmultipledeprecatedendpoint ( self ) : @ deprecation.deprecated_endpoints ( `` foo1 '' , `` foo2 '' ) def foo ( ) : pass self.assertequal ( ( `` foo1 '' , `` foo2 '' ) , foo._tf_deprecated_api_names )
__label__0 def _unsortedsegmentsum ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.unsorted_segment_sum , data , indices , num_segments )
__label__0 example :
__label__0 temp_directory = tempfile.mkdtemp ( prefix= '' run_and_gather_logs '' ) mangled_test_name = ( test_adjusted_name.strip ( `` / '' ) .replace ( `` | '' , `` _ '' ) .replace ( `` / '' , `` _ '' ) .replace ( `` : '' , `` _ '' ) ) test_file_prefix = os.path.join ( temp_directory , mangled_test_name ) test_file_prefix = `` % s . '' % test_file_prefix
__label__0 class noopsplitter ( split.composablesplitter ) :
__label__0 def testexportsinglefunctionv1only ( self ) : export_decorator = tf_export.tf_export ( v1= [ 'namea ' , 'nameb ' ] ) decorated_function = export_decorator ( self._test_function ) self.assertequal ( decorated_function , self._test_function ) self.assertallequal ( ( 'namea ' , 'nameb ' ) , decorated_function._tf_api_names_v1 ) self.assertallequal ( [ 'namea ' , 'nameb ' ] , tf_export.get_v1_names ( decorated_function ) ) self.assertequal ( [ ] , tf_export.get_v2_names ( decorated_function ) ) self.assertequal ( tf_export.get_symbol_from_name ( 'compat.v1.namea ' ) , decorated_function ) self.assertequal ( tf_export.get_symbol_from_name ( 'compat.v1.nameb ' ) , decorated_function ) self.assertequal ( tf_export.get_symbol_from_name ( tf_export.get_canonical_name_for_symbol ( decorated_function , add_prefix_to_v1_names=true ) ) , decorated_function )
__label__0 if ` session.run ( ) ` raises exception other than outofrangeerror or stopiteration then ` end ( ) ` is not called . note the difference between ` end ( ) ` and ` after_run ( ) ` behavior when ` session.run ( ) ` raises outofrangeerror or stopiteration . in that case ` end ( ) ` is called but ` after_run ( ) ` is not called .
__label__0 return self._chief_queue_runner
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 > > > tf.nest.assert_same_structure ( [ 0 , 1 ] , np.array ( [ 0 , 1 ] ) ) traceback ( most recent call last ) : ... valueerror : the two structures do n't have the same nested structure
__label__0 with self.assertraisesregex ( valueerror , `` same nested structure '' ) : nest.map_structure ( lambda x , y : none , ( ( 3 , 4 ) , 5 ) , ( 3 , ( 4 , 5 ) ) , check_types=false )
__label__0 tensor.enable_tensor_equality ( ) initial_count = mock_warning.call_count # check that we avoid error from explicit ` var == none ` check . _fn ( arg0=variables.variable ( 0 ) ) self.assertequal ( initial_count , mock_warning.call_count ) _fn ( arg0=none ) self.assertequal ( initial_count + 1 , mock_warning.call_count ) tensor.disable_tensor_equality ( )
__label__0 args : args : tuple containing positional arguments kwargs : dict containing keyword arguments captures : tuple of tensors supplying captured tensor values .
__label__0 config.hostname = gather_hostname ( )
__label__0 # creates the workers and return their sessions , graphs , train_ops . def get_workers ( num_workers , replicas_to_aggregate , workers ) : sessions = [ ] graphs = [ ] train_ops = [ ] for worker_id in range ( num_workers ) : graph = ops.graph ( ) is_chief = ( worker_id == 0 ) with graph.as_default ( ) : with ops.device ( `` /job : ps/task:0 '' ) : global_step = variable_v1.variablev1 ( 0 , name= '' global_step '' , trainable=false ) var_0 = variable_v1.variablev1 ( 0.0 , name= '' v0 '' ) with ops.device ( `` /job : ps/task:1 '' ) : var_1 = variable_v1.variablev1 ( 1.0 , name= '' v1 '' ) var_sparse = variable_v1.variablev1 ( [ [ 3.0 ] , [ 4.0 ] ] , name= '' v_sparse '' )
__label__0 # if the graph contains resources loaded from a savedmodel , they are not # restored when calling ` saver.restore ` . thus , the savedmodel initializer must # be called with ` saver.restore ` to properly initialize the model .
__label__0 `` '' ''
__label__0 updates function calls from old api version to new api version using a given change spec. `` '' ''
__label__0 def testcolocategradientswithops ( self ) : text = `` tf.gradients ( yx=a , foo=false ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( text , new_text ) self.assertequal ( errors , [ ] )
__label__0 # now you can call ` minimize ( ) ` or ` compute_gradients ( ) ` and # ` apply_gradients ( ) ` normally training_op = opt.minimize ( total_loss , global_step=self.global_step )
__label__0 def get_git_commit_sha ( ) : `` '' '' get git commit sha for this build .
__label__0 for ( i , index ) in enumerate ( indices ) : self.assertallcloseaccordingtotype ( x [ index ] - lr * grad [ i ] * ( y [ index ] + grad [ i ] * grad [ i ] ) * * ( lr_power ) , self.evaluate ( var ) [ index ] ) self.assertallcloseaccordingtotype ( y [ index ] + grad [ i ] * grad [ i ] , self.evaluate ( accum ) [ index ] )
__label__0 input_tree = 0 shallow_tree = [ 9 , 8 ] with self.assertraiseswithliteralmatch ( typeerror , nest.if_shallow_is_seq_input_must_be_seq.format ( type ( input_tree ) ) , ) : ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree_paths , [ ( 0 , ) , ( 1 , ) ] ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 def update ( self ) : self.internal_map.update_to ( tuple ( self.get_effective_source_map ( ) .items ( ) ) )
__label__0 # now create a second session and test that we do n't stay stopped , until # we ask to stop again . sess2 = sv.prepare_or_wait_for_session ( `` '' ) self.assertfalse ( sv.should_stop ( ) ) sv.stop ( ) sess2.close ( ) self.asserttrue ( sv.should_stop ( ) )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 json.dump ( spec , open ( os.path.join ( gen_path , `` spec.json '' ) , `` w '' ) , indent=2 ) if debug : print ( `` gen_git_source.py : list % s '' % gen_path ) print ( `` gen_git_source.py : % s '' + repr ( os.listdir ( gen_path ) ) ) print ( `` gen_git_source.py : spec is % r '' % spec )
__label__0 pasta.base.formatting.set ( keyword_arg.value , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( keyword_arg.value , `` suffix '' , `` ) '' )
__label__0 if not ( iterable_parameters is none or ( isinstance ( iterable_parameters , ( list , tuple ) ) and all ( isinstance ( p , str ) for p in iterable_parameters ) ) ) : raise typeerror ( `` iterable_parameters should be a list or tuple of string . '' )
__label__0 with ops_lib.graph ( ) .as_default ( ) as g : a = variable_v1.variablev1 ( [ 1 . ] , name= '' a '' ) a_saver = saver_module.saver ( [ a ] ) with self.session ( graph=g ) as sess : with self.assertraisesregex ( errors.invalidargumenterror , `` a mismatch between the current graph and the graph '' ) : a_saver.restore ( sess=sess , save_path=save_path )
__label__0 simple usage : tf_convert.py -- infile foo.py -- outfile bar.py tf_convert.py -- intree ~/code/old -- outtree ~/code/new `` '' '' ) parser.add_argument ( `` -- infile '' , dest= '' input_file '' , help= '' if converting a single file , the name of the file `` `` to convert '' ) parser.add_argument ( `` -- outfile '' , dest= '' output_file '' , help= '' if converting a single file , the output filename . '' ) parser.add_argument ( `` -- intree '' , dest= '' input_tree '' , help= '' if converting a whole tree of files , the directory `` `` to read from ( relative or absolute ) . '' ) parser.add_argument ( `` -- outtree '' , dest= '' output_tree '' , help= '' if converting a whole tree of files , the output `` `` directory ( relative or absolute ) . '' ) parser.add_argument ( `` -- copyotherfiles '' , dest= '' copy_other_files '' , help= ( `` if converting a whole tree of files , whether to `` `` copy the other files . `` ) , type=bool , default=false ) parser.add_argument ( `` -- reportfile '' , dest= '' report_filename '' , help= ( `` the name of the file where the report log is `` `` stored . '' `` ( default : % ( default ) s ) '' ) , default= '' report.txt '' ) args = parser.parse_args ( )
__label__0 args : fn : function , or function-like object ( e.g. , result of ` functools.partial ` ) .
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > global_batch_size = 2 > > > steps_per_loop = 2 > > > dataset = tf.data.dataset.range ( 10 ) .batch ( global_batch_size ) > > > distributed_iterator = iter ( ... strategy.experimental_distribute_dataset ( dataset ) ) > > > def step_fn ( x ) : ... # train the model with inputs ... return x > > > @ tf.function ... def train_fn ( distributed_iterator ) : ... for _ in tf.range ( steps_per_loop ) : ... optional_data = distributed_iterator.get_next_as_optional ( ) ... if not optional_data.has_value ( ) : ... break ... per_replica_results = strategy.run ( step_fn , args= ( optional_data.get_value ( ) , ) ) ... tf.print ( strategy.experimental_local_results ( per_replica_results ) ) > > > train_fn ( distributed_iterator ) ... # ( [ 0 1 ] , [ 2 3 ] ) ... # ( [ 4 ] , [ ] )
__label__0 
__label__0 import numpy as np import wrapt
__label__0 - all conformant context features ` k ` must obey the same conventions as a conformant example 's features ( see above ) .
__label__0 def deprecated_alias ( deprecated_name , name , func_or_class , warn_once=true ) : `` '' '' deprecate a symbol in favor of a new name with identical semantics .
__label__1 class solution : def maxnumber ( self , nums1 : list [ int ] , nums2 : list [ int ] , k : int ) - > list [ int ] : def maxnumberfromonearray ( nums , k ) : stack = [ ] drop = len ( nums ) - k for num in nums : while drop and stack and stack [ -1 ] < num : stack.pop ( ) drop -= 1 stack.append ( num ) return stack [ : k ] def merge ( nums1 , nums2 ) : merged = [ ] while nums1 or nums2 : if nums1 > nums2 : merged.append ( nums1.pop ( 0 ) ) else : merged.append ( nums2.pop ( 0 ) ) return merged result = [ ] for i in range ( max ( 0 , k - len ( nums2 ) ) , min ( k , len ( nums1 ) ) + 1 ) : maxnum1 = maxnumberfromonearray ( nums1 , i ) maxnum2 = maxnumberfromonearray ( nums2 , k - i ) merged = merge ( maxnum1 , maxnum2 ) result = max ( result , merged ) return result
__label__0 if this decorator is set on a child class 's method whose parent 's method contains ` do_not_doc_in_subclasses ` , then that will be overriden and the child method will get documented . all classes inherting from the child will also document that method .
__label__0 `` ` python ... create a graph ... # launch the graph in a session . sess = tf.compat.v1.session ( ) # create a summary writer , add the 'graph ' to the event file . writer = tf.compat.v1.summary.filewriter ( < some-directory > , sess.graph ) `` `
__label__0 # expected chunks : # graphdef.nodes [ :3 ] , graphdef.nodes [ 3 : ] , fn1 , functiondeflibrary self.assertlen ( chunks , 4 ) self._assert_chunk_sizes ( chunks , max_size )
__label__0 # a typing.namedtuple . class typedfoo ( namedtuple ) : a : int b : int self.asserttrue ( nest.is_namedtuple ( typedfoo ( 1 , 2 ) ) )
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import partitioned_variables from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import saver
__label__0 5. how to inspect per-replica values locally :
__label__0 # utility op . open source . todo ( touts ) : move to nn ? from tensorflow.python.training.learning_rate_decay import * # pylint : enable=wildcard-import
__label__0 # make sure we do n't have a confusing `` during handling of the above # exception '' block in python 3. self.assertnotin ( `` newcheckpointreader '' , cs.exception.message )
__label__0 # function warnings . < function name > placeholder inside warnings will be # replaced by function name . # you can use * . to add items which do not check the fqn , and apply to e.g. , # methods . self.function_warnings = { `` * .save '' : keras_default_save_format_comment , `` tf.assert_equal '' : assert_return_type_comment , `` tf.assert_none_equal '' : assert_return_type_comment , `` tf.assert_negative '' : assert_return_type_comment , `` tf.assert_positive '' : assert_return_type_comment , `` tf.assert_non_negative '' : assert_return_type_comment , `` tf.assert_non_positive '' : assert_return_type_comment , `` tf.assert_near '' : assert_return_type_comment , `` tf.assert_less '' : assert_return_type_comment , `` tf.assert_less_equal '' : assert_return_type_comment , `` tf.assert_greater '' : assert_return_type_comment , `` tf.assert_greater_equal '' : assert_return_type_comment , `` tf.assert_integer '' : assert_return_type_comment , `` tf.assert_type '' : assert_return_type_comment , `` tf.assert_scalar '' : assert_return_type_comment , `` tf.assert_rank '' : assert_rank_comment , `` tf.assert_rank_at_least '' : assert_rank_comment , `` tf.assert_rank_in '' : assert_rank_comment , `` tf.contrib.layers.layer_norm '' : contrib_layers_layer_norm_comment , `` tf.contrib.saved_model.load_keras_model '' : keras_experimental_export_comment , `` tf.contrib.saved_model.save_keras_model '' : keras_experimental_export_comment , `` tf.contrib.summary.all_summary_ops '' : contrib_summary_comment , `` tf.contrib.summary.audio '' : contrib_summary_comment , `` tf.contrib.summary.create_file_writer '' : contrib_create_file_writer_comment , `` tf.contrib.summary.generic '' : contrib_summary_comment , `` tf.contrib.summary.graph '' : contrib_summary_graph_comment , `` tf.contrib.summary.histogram '' : contrib_summary_comment , `` tf.contrib.summary.import_event '' : contrib_summary_import_event_comment , `` tf.contrib.summary.image '' : contrib_summary_comment , `` tf.contrib.summary.record_summaries_every_n_global_steps '' : contrib_summary_record_every_n_comment , `` tf.contrib.summary.scalar '' : contrib_summary_comment , `` tf.debugging.assert_equal '' : assert_return_type_comment , `` tf.debugging.assert_greater '' : assert_return_type_comment , `` tf.debugging.assert_greater_equal '' : assert_return_type_comment , `` tf.debugging.assert_integer '' : assert_return_type_comment , `` tf.debugging.assert_less '' : assert_return_type_comment , `` tf.debugging.assert_less_equal '' : assert_return_type_comment , `` tf.debugging.assert_near '' : assert_return_type_comment , `` tf.debugging.assert_negative '' : assert_return_type_comment , `` tf.debugging.assert_non_negative '' : assert_return_type_comment , `` tf.debugging.assert_non_positive '' : assert_return_type_comment , `` tf.debugging.assert_none_equal '' : assert_return_type_comment , `` tf.debugging.assert_positive '' : assert_return_type_comment , `` tf.debugging.assert_type '' : assert_return_type_comment , `` tf.debugging.assert_scalar '' : assert_return_type_comment , `` tf.debugging.assert_rank '' : assert_rank_comment , `` tf.debugging.assert_rank_at_least '' : assert_rank_comment , `` tf.debugging.assert_rank_in '' : assert_rank_comment , `` tf.train.exponential_decay '' : decay_function_comment , `` tf.train.piecewise_constant_decay '' : decay_function_comment , `` tf.train.polynomial_decay '' : decay_function_comment , `` tf.train.natural_exp_decay '' : decay_function_comment , `` tf.train.inverse_time_decay '' : decay_function_comment , `` tf.train.cosine_decay '' : decay_function_comment , `` tf.train.cosine_decay_restarts '' : decay_function_comment , `` tf.train.linear_cosine_decay '' : decay_function_comment , `` tf.train.noisy_linear_cosine_decay '' : decay_function_comment , `` tf.nn.embedding_lookup '' : deprecate_partition_strategy_comment , `` tf.nn.embedding_lookup_sparse '' : deprecate_partition_strategy_comment , `` tf.nn.nce_loss '' : deprecate_partition_strategy_comment , `` tf.nn.safe_embedding_lookup_sparse '' : deprecate_partition_strategy_comment , `` tf.nn.sampled_softmax_loss '' : deprecate_partition_strategy_comment , `` tf.keras.experimental.export_saved_model '' : keras_experimental_export_comment , `` tf.keras.experimental.load_from_saved_model '' : keras_experimental_export_comment , `` tf.keras.initializers.zeros '' : initializers_no_dtype_comment , `` tf.keras.initializers.zeros '' : initializers_no_dtype_comment , `` tf.keras.initializers.ones '' : initializers_no_dtype_comment , `` tf.keras.initializers.ones '' : initializers_no_dtype_comment , `` tf.keras.initializers.constant '' : initializers_no_dtype_comment , `` tf.keras.initializers.constant '' : initializers_no_dtype_comment , `` tf.keras.initializers.variancescaling '' : initializers_no_dtype_comment , `` tf.keras.initializers.orthogonal '' : initializers_no_dtype_comment , `` tf.keras.initializers.orthogonal '' : initializers_no_dtype_comment , `` tf.keras.initializers.identity '' : initializers_no_dtype_comment , `` tf.keras.initializers.identity '' : initializers_no_dtype_comment , `` tf.keras.initializers.glorot_uniform '' : initializers_no_dtype_comment , `` tf.keras.initializers.glorot_normal '' : initializers_no_dtype_comment , `` tf.initializers.zeros '' : initializers_no_dtype_comment , `` tf.zeros_initializer '' : initializers_no_dtype_comment , `` tf.initializers.ones '' : initializers_no_dtype_comment , `` tf.ones_initializer '' : initializers_no_dtype_comment , `` tf.initializers.constant '' : initializers_no_dtype_comment , `` tf.constant_initializer '' : initializers_no_dtype_comment , `` tf.initializers.random_uniform '' : initializers_no_dtype_comment , `` tf.random_uniform_initializer '' : initializers_no_dtype_comment , `` tf.initializers.random_normal '' : initializers_no_dtype_comment , `` tf.random_normal_initializer '' : initializers_no_dtype_comment , `` tf.initializers.truncated_normal '' : initializers_no_dtype_comment , `` tf.truncated_normal_initializer '' : initializers_no_dtype_comment , `` tf.initializers.variance_scaling '' : initializers_no_dtype_comment , `` tf.variance_scaling_initializer '' : initializers_no_dtype_comment , `` tf.initializers.orthogonal '' : initializers_no_dtype_comment , `` tf.orthogonal_initializer '' : initializers_no_dtype_comment , `` tf.initializers.identity '' : initializers_no_dtype_comment , `` tf.glorot_uniform_initializer '' : initializers_no_dtype_comment , `` tf.initializers.glorot_uniform '' : initializers_no_dtype_comment , `` tf.glorot_normal_initializer '' : initializers_no_dtype_comment , `` tf.initializers.glorot_normal '' : initializers_no_dtype_comment , `` tf.losses.absolute_difference '' : losses_comment , `` tf.losses.add_loss '' : losses_comment , `` tf.losses.compute_weighted_loss '' : losses_comment , `` tf.losses.cosine_distance '' : losses_comment , `` tf.losses.get_losses '' : losses_comment , `` tf.losses.get_regularization_loss '' : losses_comment , `` tf.losses.get_regularization_losses '' : losses_comment , `` tf.losses.get_total_loss '' : losses_comment , `` tf.losses.hinge_loss '' : losses_comment , `` tf.losses.huber_loss '' : losses_comment , `` tf.losses.log_loss '' : losses_comment , `` tf.losses.mean_pairwise_squared_error '' : losses_comment , `` tf.losses.mean_squared_error '' : losses_comment , `` tf.losses.sigmoid_cross_entropy '' : losses_comment , `` tf.losses.softmax_cross_entropy '' : losses_comment , `` tf.losses.sparse_softmax_cross_entropy '' : losses_comment , `` tf.metrics.accuracy '' : metrics_comment , `` tf.metrics.auc '' : metrics_comment , `` tf.metrics.average_precision_at_k '' : metrics_comment , `` tf.metrics.false_negatives '' : metrics_comment , `` tf.metrics.false_negatives_at_thresholds '' : metrics_comment , `` tf.metrics.false_positives '' : metrics_comment , `` tf.metrics.false_positives_at_thresholds '' : metrics_comment , `` tf.metrics.mean '' : metrics_comment , `` tf.metrics.mean_absolute_error '' : metrics_comment , `` tf.metrics.mean_cosine_distance '' : metrics_comment , `` tf.metrics.mean_iou '' : metrics_comment , `` tf.metrics.mean_per_class_accuracy '' : metrics_comment , `` tf.metrics.mean_relative_error '' : metrics_comment , `` tf.metrics.mean_squared_error '' : metrics_comment , `` tf.metrics.mean_tensor '' : metrics_comment , `` tf.metrics.percentage_below '' : metrics_comment , `` tf.metrics.precision '' : metrics_comment , `` tf.metrics.precision_at_k '' : metrics_comment , `` tf.metrics.precision_at_thresholds '' : metrics_comment , `` tf.metrics.precision_at_top_k '' : metrics_comment , `` tf.metrics.recall '' : metrics_comment , `` tf.metrics.recall_at_k '' : metrics_comment , `` tf.metrics.recall_at_thresholds '' : metrics_comment , `` tf.metrics.recall_at_top_k '' : metrics_comment , `` tf.metrics.root_mean_squared_error '' : metrics_comment , `` tf.metrics.sensitivity_at_specificity '' : metrics_comment , `` tf.metrics.sparse_average_precision_at_k '' : metrics_comment , `` tf.metrics.sparse_precision_at_k '' : metrics_comment , `` tf.metrics.specificity_at_sensitivity '' : metrics_comment , `` tf.metrics.true_negatives '' : metrics_comment , `` tf.metrics.true_negatives_at_thresholds '' : metrics_comment , `` tf.metrics.true_positives '' : metrics_comment , `` tf.metrics.true_positives_at_thresholds '' : metrics_comment , `` tf.get_variable '' : ( ast_edits.warning , `` < function name > returns resourcevariables by default in 2.0 , `` `` which have well-defined semantics and are stricter about shapes. `` `` you can disable this behavior by passing use_resource=false , or `` `` by calling tf.compat.v1.disable_resource_variables ( ) . `` ) , `` tf.pywrap_tensorflow '' : ( ast_edits.error , `` < function name > can not be converted automatically. `` `` ` tf.pywrap_tensorflow ` will not be distributed with `` `` tensorflow 2.0 , please consider an alternative in public `` `` tensorflow apis . `` ) , `` tf.contrib.distribute.mirroredstrategy '' : contrib_mirrored_strategy_warning , `` tf.distribute.mirroredstrategy '' : core_mirrored_strategy_warning , `` tf.contrib.distribute.onedevicestrategy '' : contrib_one_device_strategy_warning , `` tf.contrib.distribute.tpustrategy '' : contrib_tpu_strategy_warning , `` tf.contrib.distribute.collectiveallreducestrategy '' : contrib_collective_strategy_warning , `` tf.contrib.distribute.parameterserverstrategy '' : contrib_ps_strategy_warning , `` tf.summary.filewriter '' : summary_api_comment , `` tf.summary.filewritercache '' : summary_api_comment , `` tf.summary.summary '' : summary_api_comment , `` tf.summary.audio '' : summary_api_comment , `` tf.summary.histogram '' : summary_api_comment , `` tf.summary.image '' : summary_api_comment , `` tf.summary.merge '' : summary_api_comment , `` tf.summary.merge_all '' : summary_api_comment , `` tf.summary.scalar '' : summary_api_comment , `` tf.summary.tensor_summary '' : summary_api_comment , `` tf.summary.text '' : summary_api_comment , `` tf.saved_model.load '' : saved_model_load_warning , `` tf.saved_model.loader.load '' : saved_model_load_warning , } all_renames_v2.add_contrib_direct_import_support ( self.function_warnings )
__label__0 class removedeprecatedaliasandreorderrest ( removedeprecatedaliaskeyword ) : `` '' '' a specification where kw1_alias is removed in g .
__label__0 args : value : a object that can be converted to ` str ` . encoding : encoding for ` bytes ` typed inputs .
__label__0 if not context.executing_eagerly ( ) : with self.assertraisesoperror ( `` uninitialized '' ) : self.evaluate ( v0 ) with self.assertraisesoperror ( `` uninitialized '' ) : self.evaluate ( v1 )
__label__0 def testsoftmaxcrossentropywithlogitsdoesntnest ( self ) : text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( labels ) , logits=logits , dim=2 ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( labels ) , logits=logits , axis=2 ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 > > > a = { `` hello '' : 24 , `` world '' : 76 } > > > tf.nest.map_structure ( lambda p : p * 2 , a ) { 'hello ' : 48 , 'world ' : 152 }
__label__0 returns : a ` variable ` object. `` '' '' # scope the slot name in the namespace of the primary variable . # set primary 's name + '/ ' + name as default name , so the scope name of # optimizer can be shared when reuse is true . meanwhile when reuse is false # and the same name has been previously used , the scope name will add '_n ' # as suffix for unique identifications . validate_shape = val.get_shape ( ) .is_fully_defined ( ) if isinstance ( primary , variables.variable ) : prefix = primary._shared_name # pylint : disable=protected-access else : prefix = primary.op.name with variable_scope.variable_scope ( none , prefix + `` / '' + name ) : if colocate_with_primary : distribution_strategy = distribute_lib.get_strategy ( ) with distribution_strategy.extended.colocate_vars_with ( primary ) : return _create_slot_var ( primary , val , `` '' , validate_shape , none , none , copy_xla_sharding=copy_xla_sharding ) else : return _create_slot_var ( primary , val , `` '' , validate_shape , none , none , copy_xla_sharding=copy_xla_sharding )
__label__0 def testtensorflowgooglefromimport ( self ) : text = `` from tensorflow.google.compat import v1 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 import threading
__label__0 return _tf_data_pack_sequence_as ( structure [ 0 ] , [ func ( * x ) for x in entries ] )
__label__0 with self.assertraisesregex ( valueerror , `` ` split ` method should not be called directly '' ) : child.split ( ) path = os.path.join ( self.create_tempdir ( ) , `` na-split '' ) with self.assertraisesregex ( valueerror , `` ` write ` method should not be called directly '' ) : child.write ( path )
__label__0 def __init__ ( self , visitor ) : `` '' '' constructor .
__label__0 tf_decorator.make_decorator ( simple_parametrized_wrapper , wrapped_fn )
__label__0 if ` ready_op ` is ` none ` , the model is not checked for readiness .
__label__0 def testnonexistentpath ( self ) : with self.assertraisesregex ( errors.notfounderror , `` unsuccessful tensorslicereader '' ) : py_checkpoint_reader.newcheckpointreader ( `` non-existent '' )
__label__0 try : # fall back on using libcudart return _gather_gpu_devices_cudart ( ) except ( oserror , valueerror , notimplementederror , errors.operror ) : return [ ]
__label__0 def __next__ ( self ) : pass
__label__0 returns : the final state of the transformation. `` '' ''
__label__0 @ tf_export ( `` types.experimental.functiontype '' ) class functiontype ( inspect.signature , metaclass=abc.abcmeta ) : `` '' '' represents the type of a tensorflow callable .
__label__0 self.assertgreater ( trace_line_count , 0 )
__label__0 class callcounter ( tf_decorator.tfdecorator ) : def __init__ ( self , target ) : super ( callcounter , self ) .__init__ ( 'count_calls ' , target ) self.call_count = 0
__label__0 @ deprecation.deprecated_arg_values ( date , instructions , warn_once=true , arg0= '' forbidden '' , arg1= '' disallowed '' ) def _fn ( arg0 , arg1 ) : # pylint : disable=unused-argument pass
__label__0 to train with replicas you deploy the same program in a ` cluster ` . one of the tasks must be identified as the * chief * : the task that handles initialization , checkpoints , summaries , and recovery . the other tasks depend on the * chief * for these services .
__label__0 returns : the started thread. `` '' '' looper = coordinator.looperthread ( self._coord , timer_interval_secs , target=target , args=args , kwargs=kwargs ) looper.start ( ) return looper
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } }
__label__0 global_step_read_tensor = global_step_value + 0 ops.add_to_collection ( global_step_read_key , global_step_read_tensor ) return _get_global_step_read ( graph )
__label__0 if _at_least_version ( cuda_version , `` 11.0 '' ) :
__label__0 for op_name in sorted ( dir ( tf.raw_ops ) ) : try : ops._gradient_registry.lookup ( op_name ) # pylint : disable=protected-access has_gradient = `` \n { heavy check mark } \n { variation selector-16 } '' except lookuperror : has_gradient = `` \n { cross mark } ''
__label__0 def __eq__ ( self , other ) : if other is none : return false self._assert_type ( other ) return self._wrapped is other._wrapped # pylint : disable=protected-access
__label__0 partial_func = functools.partial ( func , 7 , 8 ) argspec = tf_inspect.fullargspec ( args= [ ] , varargs='arg ' , varkw=none , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 # save from eager mode and restore from graph mode . eager_ckpt_prefix = os.path.join ( self.get_temp_dir ( ) , `` eager_ckpt '' ) with context.eager_mode ( ) : ops_lib._default_graph_stack.reset ( ) # pylint : disable=protected-access ops_lib.reset_default_graph ( )
__label__0 example usages :
__label__0 @ tf_export ( `` nest.pack_sequence_as '' ) def pack_sequence_as ( structure , flat_sequence , expand_composites=false ) : `` '' '' returns a given flattened sequence packed into a given structure .
__label__0 # dense features . for i in range ( num_dense ) : key = fetched [ dense_keys_start + i ] feature_config = config.feature_map [ key ] # convert the default value numpy array fetched from the session run # into a tensorproto . fixed_config = feature_config.fixed_len_feature
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.function_keyword_renames [ `` h '' ] = { `` kw1_alias '' : `` kw1 '' , `` kw2_alias '' : `` kw2 '' , }
__label__0 shutil.move ( temp_file.name , out_filename )
__label__1 def remove_duplicates ( lst ) : return list ( dict.fromkeys ( lst ) )
__label__0 def _testtypesforsparseftrlmultiplylinearbylr ( self , x , y , z , lr , grad , indices , l1=0.0 , l2=0.0 , lr_power=-0.5 ) : self.setup ( ) with self.session ( use_gpu=false ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) linear = variable_v1.variablev1 ( z ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def add_contrib_direct_import_support ( symbol_dict ) : `` '' '' add support for ` tf.contrib . * ` alias ` contrib_ * . ` updates dict in place . '' '' '' for symbol_name in list ( symbol_dict.keys ( ) ) : symbol_alias = symbol_name.replace ( `` tf.contrib . `` , `` contrib_ '' ) symbol_dict [ symbol_alias ] = symbol_dict [ symbol_name ]
__label__0 class _object ( object ) :
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for function args retrieval utils . '' '' ''
__label__0 input_tree = [ 0 , 1 ] shallow_tree = 9 ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 total_size += size
__label__0 the new api is
__label__0 mt_combined_with_path = nest.map_structure_up_to ( mt_out_template , sum_tensors , mt , mt2 , mt3 ) self.assertisinstance ( mt_combined_with_path , maskedtensor ) # metadata uses the one from the first arg ( mt_out_template ) . self.assertequal ( mt_combined_with_path.mask , false ) # sum of all input tensors . self.assertallequal ( mt_combined_with_path.value , [ 6 ] )
__label__0 # exercise the third helper .
__label__0 def test_decorator ( decorator_name , decorator_doc=none ) :
__label__0 # assert calling new fn with non-deprecated value logs nothing . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=false ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 def testemptyclusterspecisfalse ( self ) : self.assertfalse ( server_lib.clusterspec ( { } ) )
__label__0 def _export ( self ) : return gen_lookup_ops.lookup_table_export_v2 ( self.table_ref , dtypes.string , dtypes.float32 )
__label__0 > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( `` bar '' , `` a b '' ) ( 1 , 2 ) , ... { `` a '' : 1 , `` b '' : 2 } , ... check_types=false )
__label__0 def __bool__ ( self ) : return bool ( self._cluster_spec )
__label__0 def truncate ( value , length ) : value_str = str ( value ) return value_str [ : length ] + ( value_str [ length : ] and `` ... '' )
__label__0 def testexceptionscaptured ( self ) : with self.cached_session ( ) as sess : queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) qr = queue_runner_impl.queuerunner ( queue , [ _mockop ( `` i fail '' ) , _mockop ( `` so fail '' ) ] ) threads = qr.create_threads ( sess ) self.evaluate ( variables.global_variables_initializer ( ) ) for t in threads : t.start ( ) for t in threads : t.join ( ) exceptions = qr.exceptions_raised self.assertequal ( 2 , len ( exceptions ) ) self.asserttrue ( `` operation not in the graph '' in str ( exceptions [ 0 ] ) ) self.asserttrue ( `` operation not in the graph '' in str ( exceptions [ 1 ] ) )
__label__0 # lazy load all of the _top_level_modules , we do n't need their names anymore for _m in _top_level_modules : _forward_module ( _m )
__label__0 argspec = tf_inspect.argspec ( args= [ 'self ' , ' a ' , ' b ' , ' c ' ] , varargs=none , keywords=none , defaults= ( 1 , 'hello ' ) )
__label__0 this can not introspect * args or * * args , but it safely handles * args in python3.5+ .
__label__0 this way the outputs feel like notebooks .
__label__0 def get_base_dirs_and_prefixes ( code_url_prefix ) : `` '' '' returns the base_dirs and code_prefixes for oss tensorflow api gen. '' '' '' base_dir = pathlib.path ( tf.__file__ ) .parent
__label__0 library_path = _find_library ( base_paths , `` nccl '' , nccl_version )
__label__0 args : node : a node of type attribute .
__label__0 ` feature_lists ` :
__label__0 input_tree_flattened_as_shallow_tree = flatten_up_to ( shallow_tree , input_tree ) input_tree_flattened = flatten ( input_tree )
__label__0 args : left : first arg right : second arg `` '' '' self.assertequal ( new_docs , new_docs_ref )
__label__0 class scopedgraphtest ( test.testcase ) :
__label__0 else : header_version = cuda_version header_path = _find_file ( base_paths , _header_paths ( ) , `` cufft.h '' ) cufft_version = required_version
__label__0 @ parameterized.parameters ( { `` mapping_type '' : collections.ordereddict } , { `` mapping_type '' : _custommapping } ) def testpackdictorder ( self , mapping_type ) : `` '' '' packing orders dicts by key , including ordereddicts . '' '' '' custom = mapping_type ( [ ( `` d '' , 0 ) , ( `` b '' , 0 ) , ( `` a '' , 0 ) , ( `` c '' , 0 ) ] ) plain = { `` d '' : 0 , `` b '' : 0 , `` a '' : 0 , `` c '' : 0 } seq = [ 0 , 1 , 2 , 3 ] custom_reconstruction = nest.pack_sequence_as ( custom , seq ) plain_reconstruction = nest.pack_sequence_as ( plain , seq ) self.assertisinstance ( custom_reconstruction , mapping_type ) self.assertisinstance ( plain_reconstruction , dict ) self.assertequal ( mapping_type ( [ ( `` d '' , 3 ) , ( `` b '' , 1 ) , ( `` a '' , 0 ) , ( `` c '' , 2 ) ] ) , custom_reconstruction ) self.assertequal ( { `` d '' : 3 , `` b '' : 1 , `` a '' : 0 , `` c '' : 2 } , plain_reconstruction )
__label__0 def _initialize ( self ) : `` '' '' resolve the keras version to use and initialize the loader . '' '' '' self._tfll_initialized = true package_name = none keras_version = none if os.environ.get ( `` tf_use_legacy_keras '' , none ) in ( `` true '' , `` true '' , `` 1 '' ) : try : import tf_keras # pylint : disable=g-import-not-at-top , unused-import
__label__0 @ functools.wraps ( func ) def new_func ( * args , * * kwargs ) : `` '' '' deprecation wrapper . '' '' '' # todo ( apassos ) figure out a way to have reasonable performance with # deprecation warnings and eager mode . if is_in_graph_mode.is_in_graph_mode ( ) and _print_deprecation_warnings : invalid_args = [ ] named_args = tf_inspect.getcallargs ( func , * args , * * kwargs ) for arg_name , spec in iter ( deprecated_positions.items ( ) ) : if ( spec.position < len ( args ) and not ( spec.has_ok_value and _same_value ( named_args [ arg_name ] , spec.ok_value ) ) ) : invalid_args.append ( arg_name ) if is_varargs_deprecated and len ( args ) > len ( arg_spec.args ) : invalid_args.append ( arg_spec.varargs ) if is_kwargs_deprecated and kwargs : invalid_args.append ( arg_spec.varkw ) for arg_name in deprecated_arg_names : if ( arg_name in kwargs and not ( deprecated_positions [ arg_name ] .has_ok_value and _same_value ( named_args [ arg_name ] , deprecated_positions [ arg_name ] .ok_value ) ) ) : invalid_args.append ( arg_name ) for arg_name in invalid_args : if ( func , arg_name ) not in _printed_warning : if warn_once : _printed_warning [ ( func , arg_name ) ] = true _log_deprecation ( 'from % s : calling % s ( from % s ) with % s is deprecated and will ' 'be removed % s.\ninstructions for updating : \n % s ' , _call_location ( ) , decorator_utils.get_qualified_name ( func ) , func.__module__ , arg_name , 'in a future version ' if date is none else ( 'after % s ' % date ) , instructions ) return func ( * args , * * kwargs )
__label__0 import sys as _sys import importlib as _importlib import types as _types
__label__0 the ` context ` contains features which apply to the entire example . the ` feature_lists ` contain a key , value map where each key is associated with a repeated set of ` tf.train.features ` ( a ` tf.train.featurelist ` ) . a ` featurelist ` represents the values of a feature identified by its key over time / frames .
__label__0 doc_controls.set_deprecated ( new_func ) new_func = tf_decorator.make_decorator ( func , new_func , 'deprecated ' , _add_deprecated_function_notice_to_docstring ( func.__doc__ , date , instructions ) ) new_func.__signature__ = inspect.signature ( func )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def _restore_checkpoint_and_maybe_run_saved_model_initializers ( sess : session.session , saver : saver_lib.saver , path : str ) : `` '' '' restores checkpoint values and savedmodel initializers if found . '' '' '' # note : all references to savedmodel refer to savedmodels loaded from the # load_v2 api ( which does not require the ` sess ` argument ) .
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 raises : typeerror : the nest is or contains a dict with non-sortable keys. `` '' '' return nest_util.flatten ( nest_util.modality.core , structure , expand_composites )
__label__0 @ abc.abstractmethod def split ( self , ) - > tuple [ sequence [ union [ message.message , bytes ] ] , chunk_pb2.chunkedmessage ] : `` '' '' splits proto message into a sequence of protos/bytes . '' '' ''
__label__0 # includes self._root_name full_path = ' . '.join ( [ self._root_name , path ] ) if path else self._root_name
__label__0 * test : - test : unique name of this test ( string ) - start : start time of this test run ( datetime ) - info : json-encoded test metadata ( string , not indexed )
__label__0 return node
__label__0 aggregated_grads_and_vars = zip ( aggregated_grad , var_list )
__label__0 def test_substr ( self ) : text = `` tf.substr ( input , pos , len , name , unit ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( `` tf.strings.substr ( input , pos , len , name=name , `` `` unit=unit ) \n '' , new_text ) self.assertequal ( errors , [ ] )
__label__0 args : proto : proto message to split . proto_as_initial_chunk : whether to initialize chunks with the user-provided proto as the initial chunk . parent_splitter : the parent ` composablesplitter ` object . fields_in_parent : fields to access ` proto ` from the parent splitter 's proto. `` '' '' self._proto = proto self._parent_splitter = parent_splitter self._fields_in_parent = fields_in_parent
__label__0 import abc from typing import any , iterator , list , optional , sequence
__label__0 the ` tf.compat ` module contains two sets of compatibility functions .
__label__0 codeline = collections.namedtuple ( `` codeline '' , [ `` cell_number '' , `` code '' ] )
__label__0 @ deprecation.deprecated_args ( date , instructions , ( `` d1 '' , none ) , ( `` d2 '' , `` my_ok_val '' ) ) def _fn ( arg0 , d1=none , arg1=2 , d2=none ) : return arg0 + arg1 if d1 else arg1 + arg0 if d2 else arg0 * arg1
__label__0 - for modality.core : see comments for _tf_core_yield_flat_up_to ( ) below - for modality.data : see comments for _tf_data_yield_flat_up_to ( ) below
__label__0 def testgetfile ( self ) : self.asserttrue ( 'tf_inspect_test.py ' in tf_inspect.getfile ( test_decorated_function_with_defaults ) ) self.asserttrue ( 'tf_decorator.py ' in tf_inspect.getfile ( test_decorator ( 'decorator ' ) ( tf_decorator.unwrap ) ) )
__label__0 def _matches_version ( actual_version , required_version ) : `` '' '' checks whether some version meets the requirements .
__label__0 args : want : the output in the docstring . got : the output generated after running the snippet . optionflags : flags passed to the doctest .
__label__0 try : del python except nameerror : pass
__label__0 # fetch params to validate initial values self.assertallclose ( [ 1.0 , 2.0 ] , self.evaluate ( var0 ) ) self.assertallclose ( [ 3.0 , 4.0 ] , self.evaluate ( var1 ) )
__label__0 the type annotations in type signatures may be type objects ( e.g. , ` maskedtensor ` ) , ` typing.list ` values , or ` typing.union ` values . for example , the following will register ` masked_concat ` to be called if ` values ` is a list of ` maskedtensor ` values :
__label__0 file_count = 0 tree_errors = { } report = `` '' report += ( `` = '' * 80 ) + `` \n '' report += `` input tree : % r\n '' % root_directory report += ( `` = '' * 80 ) + `` \n ''
__label__0 initializers = [ `` zeros '' , `` ones '' , `` ones '' , `` zeros '' , `` constant '' , `` constant '' , `` variancescaling '' , `` orthogonal '' , `` orthogonal '' , `` identity '' , `` identity '' , `` glorot_uniform '' , `` glorot_normal '' , `` lecun_normal '' , `` lecun_uniform '' , `` he_normal '' , `` he_uniform '' , `` truncatednormal '' , `` truncated_normal '' , `` randomuniform '' , `` uniform '' , `` random_uniform '' , `` randomnormal '' , `` normal '' , `` random_normal '' , ] self.verify_compat_v1_rename_correctness ( initializers , ns_prefix= '' keras.initializers '' )
__label__0 def testwaitforsessionwithreadyforlocalinitopfailstoreadylocal ( self ) : with ops.graph ( ) .as_default ( ) as graph : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) sm = session_manager.sessionmanager ( graph=graph , ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( ) , local_init_op=w.initializer )
__label__0 entry % a_inference_f_120__.8 ( arg0.1 : f32 [ 10,20 ] , arg1.2 : f32 [ 10,20 ] ) - > f32 [ 10,20 ] { % arg0.1 = f32 [ 10,20 ] { 1,0 } parameter ( 0 ) , parameter_replication= { false } , metadata= { op_name= '' xla_args '' } % reshape.3 = f32 [ 10,20 ] { 1,0 } reshape ( f32 [ 10,20 ] { 1,0 } % arg0.1 ) % arg1.2 = f32 [ 10,20 ] { 1,0 } parameter ( 1 ) , parameter_replication= { false } , metadata= { op_name= '' xla_args '' } % add.4 = f32 [ 10,20 ] { 1,0 } add ( f32 [ 10,20 ] { 1,0 } % reshape.3 , f32 [ 10,20 ] { 1,0 } % arg1.2 ) , metadata= { op_type= '' addv2 '' op_name= '' add '' source_file= '' < ipython-input-16-ea04879c1873 > '' source_line=4 } % reshape.5 = f32 [ 10,20 ] { 1,0 } reshape ( f32 [ 10,20 ] { 1,0 } % add.4 ) , metadata= { op_name= '' xla_retvals '' } % tuple.6 = ( f32 [ 10,20 ] { 1,0 } ) tuple ( f32 [ 10,20 ] { 1,0 } % reshape.5 ) , metadata= { op_name= '' xla_retvals '' } root % get-tuple-element.7 = f32 [ 10,20 ] { 1,0 } get-tuple-element ( ( f32 [ 10,20 ] { 1,0 } ) % tuple.6 ) , index=0 , metadata= { op_name= '' xla_retvals '' } } `` `
__label__0 def func ( m , n , * * kwarg ) : return m * n + len ( kwarg )
__label__0 # although ` check_types=false ` is set , this assertion would fail because the # shallow_tree component has a deeper structure than the input_tree # component . with self.assertraisesregex ( # pylint : disable=g-error-prone-assert-raises typeerror , `` if shallow structure is a sequence , input must also be a sequence '' , ) : nest.flatten_up_to ( shallow_tree=nested_list2 , input_tree=nmt , check_types=false )
__label__0 return new_notebook
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated argument values ) '' `` \n '' `` \ndeprecated : some argument values are deprecated : ` ( deprecated=true ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 def _is_exported_function ( s ) : return ( s [ 'bind ' ] ! = 'local ' and s [ 'type ' ] == 'func ' and s [ 'ndx ' ] ! = 'und ' and s [ 'name ' ] not in [ `` , '_init ' , '_fini ' ] and s [ 'default ' ] )
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 feature.__doc__ = `` '' '' \ used in ` tf.train.example ` protos . contains a list of values .
__label__0 class _ownsmirroredvariables ( trackable_base.trackable ) : `` '' '' a trackable object which returns a more complex saveableobject . '' '' ''
__label__0 tf.config.experimental_connect_to_cluster ( cluster_def , cluster_device_filters=cdf ) `` `
__label__0 * ` @ dispatch_for_api ( tf.concat , { 'values ' : list [ maskedtensor ] } ) ` : will not dispatch to the decorated dispatch target when the user calls ` tf.concat ( [ ] ) ` .
__label__0 def _normalize_docstring ( docstring ) : `` '' '' normalizes the docstring .
__label__0 s3 = save.save ( sess , os.path.join ( save_dir , `` s3 '' ) ) self.assertequal ( [ s2 , s3 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertcheckpointstate ( model_checkpoint_path=s3 , all_model_checkpoint_paths= [ s2 , s3 ] , save_dir=save_dir )
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( 1 , 2 , a=true , b=false ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 raises : typeerror : if ` obj ` can not be serialized. `` '' '' # if obj is a serializable keras class instance # e.g . optimizer , layer if hasattr ( obj , 'get_config ' ) : return { 'class_name ' : obj.__class__.__name__ , 'config ' : obj.get_config ( ) }
__label__0 raises : valueerror : if ` job_name ` does not name a job in this cluster , or no task with index ` task_index ` is defined in that job. `` '' '' try : job = self._cluster_spec [ job_name ] except keyerror : raise valueerror ( `` no such job in cluster : % r '' % job_name ) try : return job [ task_index ] except keyerror : raise valueerror ( `` no task with index % r in job % r '' % ( task_index , job_name ) )
__label__0 ` tf.train.checkpointmanager ` also writes a [ ` checkpointstate ` proto ] ( https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_state.proto ) which contains the timestamp when each checkpoint was created .
__label__0 class serverdeftest ( test.testcase ) :
__label__0 returns : ` cls ` . `` '' '' _api_dispatcher.register_dispatchable_type ( cls ) return cls
__label__0 text = ( `` tf.multinomial ( logits , samples , seed , name , output_dtype ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) expected_text = ( `` tf.random.categorical ( logits , samples , seed=seed , name=name , `` `` dtype=output_dtype ) \n '' ) self.assertequal ( new_text , expected_text )
__label__0 import collections import enum
__label__0 # serialized protobuf for cluster device filters . self._cluster_device_filters = none
__label__0 transforms : tf.contrib.layers.variance_scaling_initializer ( factor , mode , uniform , seed , dtype ) to tf.compat.v1.keras.initializers.variancescaling ( scale=factor , mode=mode.lower ( ) , distribution= ( `` uniform '' if uniform else `` truncated_normal '' ) , seed=seed , dtype=dtype )
__label__0 def test_contrib_summary_image ( self ) : text = `` tf.contrib.summary.image ( 'foo ' , myval , red , 3 , 'fam ' , 42 ) '' expected = ( `` tf.compat.v2.summary.image ( name='foo ' , data=myval , `` `` max_outputs=3 , step=42 ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'bad_color ' argument '' , errors [ 0 ] ) self.assertin ( `` 'family ' argument '' , errors [ 1 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 2 ] )
__label__1 def count_occurrences ( lst , element ) : return lst.count ( element )
__label__0 args : master : ` string ` representation of the tensorflow master to use . saver : a ` saver ` object used to restore a model . checkpoint_dir : path to the checkpoint files . the latest checkpoint in the dir will be used to restore . checkpoint_filename_with_path : full file name path to the checkpoint file . wait_for_checkpoint : whether to wait for checkpoint to become available . max_wait_secs : maximum time to wait for checkpoints to become available . config : optional ` configproto ` proto used to configure the session .
__label__0 return ( false , none )
__label__0 def testgetapiswithtypebaseddispatch ( self ) : dispatch_apis = dispatch.apis_with_type_based_dispatch ( ) self.assertin ( math_ops.add , dispatch_apis ) self.assertin ( array_ops.concat , dispatch_apis )
__label__0 override = dispatch.dispatch_for_types ( test_op , customtensor ) ( override_for_test_op )
__label__0 6 . ` tf.raggedtensor ` : this is a composite tensor thats representation consists of a flattened list of 'values ' and a list of 'row_splits ' which indicate how to chop up the flattened list into different rows . for more details on ` tf.raggedtensor ` , please visit https : //www.tensorflow.org/api_docs/python/tf/raggedtensor .
__label__0 def testcreatezerosslotfromtensor ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = constant_op.constant ( [ 1.0 , 2.5 ] , name= '' const '' ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 return hipsparse_config
__label__0 class splitter ( abc.abc ) : `` '' '' an abstract class for splitting and writing protos that are > 2gb .
__label__0 # call transformers . these have the ability to modify the node , and if they # do , will return the new node they created ( or the same node if they just # changed it ) . the are given the parent , but we will take care of # integrating their changes into the parent if they return a new node . # # these are matched on the old name , since renaming is performed by the # attribute visitor , which happens later . transformers = self._get_applicable_entries ( `` function_transformers '' , full_name , name )
__label__0 as a rule of thumb , private ( beginning with ` _ ` ) methods/functions are not documented .
__label__0 def test_child_splitter ( self ) : proto = test_message_pb2.repeatedrepeatedstring ( rs= [ test_message_pb2.repeatedstring ( strings= [ `` a '' , `` b '' , `` c '' ] ) , test_message_pb2.repeatedstring ( strings= [ `` d '' , `` e '' ] ) , ] ) splitter = noopsplitter ( proto )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 implementation note : - this method should not invoke any tensorflow ops . - this method only needs to flatten the current level . if current object has an attribute that also need custom flattening , nest functions ( such as ` nest.flatten ` ) will utilize this method to do recursive flattening . - components must ba a ` tuple ` , not a ` list ` `` '' ''
__label__0 for modality.core refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure . note the method does not check the types of atoms inside the structures .
__label__0 def wrapper ( x ) : return wrapper.__wrapped__ ( x ) + 1
__label__0 __slots__ = [ `` _wrapped '' , `` __weakref__ '' ]
__label__0 def test_max_pool_2d ( self ) : text = `` tf.nn.max_pool ( value=4 ) '' expected_text = `` tf.nn.max_pool2d ( input=4 ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 header_path , header_version = _find_header ( base_paths , `` curand.h '' , required_version , get_header_version ) curand_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 def testmissingpos2 ( self ) : with self.assertraisesregex ( typeerror , 'missing required positional argument ' ) : self._matmul_func.canonicalize ( transpose_a=true , transpose_b=true , adjoint_a=true )
__label__0 def testtensorflowdontchangecontrib ( self ) : text = `` import tensorflow.contrib as foo '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 returns : the result of the operation , or ` opdispatcher.not_supported ` if this dispatcher can not handle the given arguments. `` '' '' return self.not_supported
__label__0 args : node : the ast.call node to check arg values for .
__label__0 def __reduce__ ( self ) : return __import__ , ( self.__name__ , )
__label__0 5. tf.tensor ( considered a scalar ) :
__label__0 _tf_dtypes = [ tf.half , tf.float16 , tf.float32 , tf.float64 , tf.bfloat16 , tf.complex64 , tf.complex128 , tf.int8 , tf.uint8 , tf.uint16 , tf.uint32 , tf.uint64 , tf.int16 , tf.int32 , tf.int64 , tf.bool , tf.string , tf.qint8 , tf.quint8 , tf.qint16 , tf.quint16 , tf.qint32 , tf.resource , tf.variant ]
__label__0 self.assertequal ( _get_write_histogram_proto ( ) .num , num_writes_start + 1 ) time_after_one_save = metrics.gettrainingtimesaved ( api_label=api_label ) self.assertgreater ( time_after_one_save , time_start )
__label__0 args : sess : a session .
__label__0 @ attr.s class unsortedsampleattr ( object ) : field3 = attr.ib ( ) field1 = attr.ib ( ) field2 = attr.ib ( )
__label__0 if ` structure ` is an atom , ` flat_sequence ` must be a single-item list ; in this case the return value is ` flat_sequence [ 0 ] ` .
__label__0 self.assert_trace_line_count ( fn , count=10 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=25 , filtering_enabled=false )
__label__0 the binary elementwise apis are :
__label__0 # pylint : disable=undefined-variable tf_export ( `` train.byteslist '' ) ( byteslist ) tf_export ( `` train.clusterdef '' ) ( clusterdef ) tf_export ( `` train.example '' ) ( example ) tf_export ( `` train.feature '' ) ( feature ) tf_export ( `` train.features '' ) ( features ) tf_export ( `` train.featurelist '' ) ( featurelist ) tf_export ( `` train.featurelists '' ) ( featurelists ) tf_export ( `` train.floatlist '' ) ( floatlist ) tf_export ( `` train.int64list '' ) ( int64list ) tf_export ( `` train.jobdef '' ) ( jobdef ) tf_export ( v1= [ `` train.saverdef '' ] ) ( saverdef ) tf_export ( `` train.sequenceexample '' ) ( sequenceexample ) tf_export ( `` train.serverdef '' ) ( serverdef )
__label__0 # an entity which is initialized through a table_initializer . w = variable_v1.variablev1 ( [ 4 , 5 , 6 ] , trainable=false , collections= [ ] ) ops.add_to_collection ( ops.graphkeys.table_initializers , w.initializer )
__label__0 @ property def info ( self ) : return [ log for log in self._log if log [ 0 ] == info ]
__label__0 you can load a name-based checkpoint written by ` tf.compat.v1.train.saver ` using ` tf.train.checkpoint.restore ` or ` tf.keras.model.load_weights ` . however , you may have to change the names of the variables in your model to match the variable names in the name-based checkpoint , which can be viewed with ` tf.train.list_variables ( path ) ` .
__label__0 # utility classes for training . from tensorflow.python.training.coordinator import coordinator from tensorflow.python.training.coordinator import looperthread # go/tf-wildcard-import # pylint : disable=wildcard-import from tensorflow.python.training.queue_runner import *
__label__0 git_version = get_git_version ( source_dir , git_tag_override ) write_version_info ( output_file , git_version )
__label__0 # keep a list of chunk ids in the order in which they were added to the # list . self._add_chunk_order = [ ] self._fix_chunk_order = false
__label__0 import importlib import os import pkgutil import sys
__label__0 tf_export : exporttype = functools.partial ( api_export , api_name=tensorflow_api_name ) keras_export : exporttype = functools.partial ( api_export , api_name=keras_api_name )
__label__0 the decorator function must use ` < decorator name > .__wrapped__ ` instead of the wrapped function that is normally used :
__label__0 # todo ( b/169898786 ) : use the keras public api when tflite moves out of tf
__label__0 def deref ( self ) : `` '' '' returns the referenced object .
__label__0 if num_tokens > 0 : with ops.device ( self._global_step.device ) , ops.name_scope ( `` '' ) : tokens = array_ops.fill ( [ num_tokens ] , self._global_step ) init_tokens = self._sync_token_queue.enqueue_many ( ( tokens , ) ) else : init_tokens = control_flow_ops.no_op ( name= '' no_init_tokens '' )
__label__0 def _check_sated ( self , raise_error ) : `` '' '' check if the object has been sated . '' '' '' if self._sated : return creation_stack = `` .join ( [ line.rstrip ( ) for line in traceback.format_stack ( self._stack_frame , limit=5 ) ] ) if raise_error : try : raise runtimeerror ( 'object was never used ( type { } ) : { } . if you want to mark it as ' 'used call its `` mark_used ( ) '' method . it was originally created ' 'here : \n { } '.format ( self._type , self._repr , creation_stack ) ) finally : self.sate ( ) else : tf_logging.error ( '==================================\n ' 'object was never used ( type { } ) : \n { } \nif you want to mark it as ' 'used call its `` mark_used ( ) '' method.\nit was originally created ' 'here : \n { } \n ' '================================== ' .format ( self._type , self._repr , creation_stack ) )
__label__0 def testcreateslotwithcustomsplitxlasharding ( self ) : # slot_creator is used only in optimizer v1 . # we insert our own custom split xla sharding that overrides the spmd # sharding copied over by the slot_creator . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 , 10.0 , 15.1 ] , name= '' var '' ) v = xla_sharding.mesh_split ( v , np.array ( [ 0 , 1 ] ) , [ 0 ] , use_sharding_op=false ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 , copy_xla_sharding=true ) slot = xla_sharding.split ( slot , split_dimension=0 , num_devices=4 , use_sharding_op=false )
__label__0 the following code will raise an exception : `` ` python shallow_tree = [ `` a '' , `` b '' ] input_tree = [ `` c '' , [ `` d '' , `` e '' ] , `` f '' ] assert_shallow_structure ( shallow_tree , input_tree ) `` `
__label__0 import collections import copy import json import re import shutil import tempfile
__label__0 return hipsolver_config
__label__0 class _context ( object ) : `` '' '' context manager helper for ` grouplock ` . '' '' ''
__label__0 > > > structure = [ ' a ' ] > > > flat_sequence = [ np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) [ array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ]
__label__0 visit the [ tutorial ] ( https : //www.tensorflow.org/tutorials/distribute/input ) on distributed input for more examples and caveats. `` '' ''
__label__0 print ( x_ref1 == x_ref2 ) == > true
__label__0 def test_wrapper ( * args , * * kwargs ) : return test_function ( * args , * * kwargs )
__label__0 # write this : def simple_parametrized_wrapper ( * args , * * kwds ) : return simple_parametrized_wrapper.__wrapped__ ( * args , * * kwds )
__label__0 library_path = _find_library ( base_paths , `` curand '' , curand_version )
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' exports functions from tf_decorator.py to avoid cycles . '' '' ''
__label__0 def assert_same_structure ( modality , nest1 , nest2 , check_types=true , expand_composites=false ) : `` '' '' asserts that two structures are nested in the same way .
__label__0 # the next one should have the values from the summary . ev = next ( rr ) self.assertprotoequals ( `` '' '' value { tag : 'c1 ' simple_value : 1.0 } value { tag : 'c2 ' simple_value : 2.0 } value { tag : 'c3 ' simple_value : 3.0 } `` '' '' , ev.summary )
__label__0 upgrader = ast_edits.astcodeupgrader ( renameimports ( ) ) upgrader.process_tree_inplace ( upgrade_dir )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license .. # ============================================================================== `` '' '' tensorflow is an open source machine learning framework for everyone .
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 args : x_type : a type annotation indicating when the api handler should be called . y_type : a type annotation indicating when the api handler should be called .
__label__0 returns : a ` concretefunction ` . `` '' '' pass
__label__0 def __init__ ( self , lock , group_id ) : self._lock = lock self._group_id = group_id
__label__0 for more specific needs , you can create custom hooks : class examplehook ( sessionrunhook ) : def begin ( self ) : # you can add ops to the graph here . print ( 'starting the session . ' ) self.your_tensor = ...
__label__0 # if no default values are found , return argspec with defaults=none . if first_default is none : return argspec ( args , varargs , keywords , none )
__label__0 def end ( self , session ) : # pylint : disable=unused-argument `` '' '' called at the end of session .
__label__0 ` mirrored ` values are ` tf.distribute.distributedvalues ` for which we know that the value on all replicas is the same . ` mirrored ` values are kept synchronized by the distribution strategy in use , while ` tf.types.experimental.perreplica ` values are left unsynchronized . ` mirrored ` values typically represent model weights . we can safely read a ` mirrored ` value in a cross-replica context by using the value on any replica , while ` perreplica ` values should not be read or manipulated directly by the user in a cross-replica context. `` '' ''
__label__0 @ tf_export ( v1= [ 'train.assert_global_step ' ] ) def assert_global_step ( global_step_tensor ) : `` '' '' asserts ` global_step_tensor ` is a scalar int ` variable ` or ` tensor ` .
__label__0 def testfunctionlotsofnodes ( self ) : sizes = [ ] fn1 = [ 50 , 50 , 50 , 50 , 50 ] max_size = 200 constants.debug_set_max_size ( max_size )
__label__0 def testgraceperiod ( self ) : with self.cached_session ( ) as sess : # the enqueue will quickly block . queue = data_flow_ops.fifoqueue ( 2 , dtypes.float32 ) enqueue = queue.enqueue ( ( 10.0 , ) ) dequeue = queue.dequeue ( ) qr = queue_runner_impl.queuerunner ( queue , [ enqueue ] ) coord = coordinator.coordinator ( ) qr.create_threads ( sess , coord , start=true ) # dequeue one element and then request stop . dequeue.op.run ( ) time.sleep ( 0.02 ) coord.request_stop ( ) # we should be able to join because the requeststop ( ) will cause # the queue to be closed and the enqueue to terminate . coord.join ( stop_grace_period_secs=1.0 )
__label__0 when generating docs for a class 's arributes , the ` __mro__ ` is searched and the attribute will be skipped if this decorator is detected on the attribute on any class in the ` __mro__ ` .
__label__0 class _cachedclassproperty ( object ) : `` '' '' cached class property decorator .
__label__0 @ property def warnings ( self ) : return [ log for log in self._log if log [ 0 ] == warning ]
__label__0 1. its keys and values should have the same exact nested structure . 2. the set of all flattened keys of the dictionary must not contain repeated keys .
__label__0 # the wrapped method is called for each subclass . self.assertequal ( mysubclass.value , `` mysubclass '' ) self.assertequal ( log , [ myclass , mysubclass ] ) self.assertequal ( mysubclass.value , `` mysubclass '' ) self.assertequal ( mysubclass.value , `` mysubclass '' ) self.assertequal ( log , [ myclass , mysubclass ] )
__label__0 node.keywords = new_keywords + node.keywords
__label__0 with self.cached_session ( ) as sess : v = variable_v1.variablev1 ( np.int64 ( -1 ) , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } )
__label__0 def test_contrib_cudnn_rnn_deprecation ( self ) : _ , report , _ , _ = self._upgrade ( `` tf.contrib.cudnn_rnn '' ) self.assertin ( `` tf.contrib.cudnn_rnn . * has been deprecated '' , report )
__label__0 > > > tf.nest.map_structure ( lambda x , y : x + y , 3 , 4 ) 7
__label__0 dispatch.unregister_dispatch_for ( masked_tensor_equals ) # clean up .
__label__0 def _signature_from_annotations ( func ) : `` '' '' builds a dict mapping from parameter names to type annotations . '' '' '' func_signature = tf_inspect.signature ( func )
__label__0 with self.assertraisesregex ( typeerror , self.bad_pack_pattern ) : nest.pack_sequence_as ( [ 4 , 5 ] , `` bad_sequence '' )
__label__0 def testcast ( self ) : for ( name , dtype ) in [ ( `` int32 '' , `` int32 '' ) , ( `` int64 '' , `` int64 '' ) , ( `` float '' , `` float32 '' ) , ( `` double '' , `` float64 '' ) , ( `` complex64 '' , `` complex64 '' ) , ( `` complex128 '' , `` complex128 '' ) , ( `` bfloat16 '' , `` bfloat16 '' ) ] : text = `` tf.to_ % s ( x , name='test ' ) '' % name expected_text = `` tf.cast ( x , name='test ' , dtype=tf. % s ) '' % dtype _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # converts all symbols in the v1 namespace to the v2 namespace , raising # an error if the target of the conversion is not in the v2 namespace . # please regenerate the renames file or edit any manual renames if this # test fails . def conversion_visitor ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) api_names = tf_export.get_v1_names ( attr ) for name in api_names : _ , _ , _ , text = self._upgrade ( `` tf . '' + name ) if ( text and not text.startswith ( `` tf.compat.v1 '' ) and not text.startswith ( `` tf.compat.v2 '' ) and text not in self.v2_symbols and # ignore any symbol that contains __internal__ `` __internal__ '' not in text ) : self.assertfalse ( true , `` symbol % s generated from % s not in v2 api '' % ( text , name ) )
__label__0 field , field_desc = util.get_field ( proto , [ `` field_one '' , `` repeated_field '' ] ) self.assertisinstance ( field , iterable ) self.assertlen ( field , 2 ) self.assertequal ( `` repeated_field '' , field_desc.name ) self.assertequal ( 2 , field_desc.number ) self.assertprotoequals ( proto.field_one.repeated_field , field )
__label__0 def to_proto ( self , export_scope=none ) : `` '' '' converts this ` saver ` to a ` saverdef ` protocol buffer .
__label__0 # pylint : disable=unused-import from tensorflow.python.platform import test from tensorflow.python.util import tf_contextlib from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect
__label__0 config.memory_info.copyfrom ( gather_memory_info ( ) )
__label__0 returns : packed : ` flat_sequence ` converted to have the same recursive structure as ` structure ` .
__label__0 @ classproperty def value ( cls ) : return '123 '
__label__0 return cls
__label__0 parser.add_argument ( `` -- source_dir '' , type=str , help= '' base path of the source code ( used for cmake/make ) '' )
__label__0 args : local_init_op : an ` operation ` run immediately after session creation . usually used to initialize tables and local variables . ready_op : an ` operation ` to check if the model is initialized . ready_for_local_init_op : an ` operation ` to check if the model is ready to run local_init_op . graph : the ` graph ` that the model will use . recovery_wait_secs : seconds between checks for the model to be ready . local_init_run_options : runoptions to be passed to session.run when executing the local_init_op . local_init_feed_dict : optional session feed dictionary to use when running the local_init_op .
__label__0 def copy_binary ( directory , origin_tag , new_tag , version , package ) : `` '' '' rename and copy binaries for different python versions .
__label__0 @ end_compatibility
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 2 ] ) ) nmt_flat_paths = list ( nest.yield_flat_paths ( nmt ) ) self.assertequal ( nmt_flat_paths , [ ( 0 , 0 ) ] )
__label__0 pasta.base.formatting.set ( dist , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( dist , `` suffix '' , `` ) '' )
__label__0 from tensorflow.core.util import test_log_pb2 from tensorflow.python.platform import gfile from tensorflow.tools.test import gpu_info_lib from tensorflow.tools.test import system_info_lib
__label__0 this function works with the ast call node format of python3.5+ as well as the different ast format of earlier versions of python .
__label__0 def testexcludedimport ( self ) : # foo.baz module is excluded from changes . text = `` import foo.baz '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 text = `` tf.image.extract_glimpse ( x ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , text ) self.assertequal ( errors , [ ] )
__label__0 text = `` tf.batch_to_space ( input , crops , block_size , name ) '' expected_text = ( `` tf.batch_to_space ( input , crops=crops , block_shape=block_size , `` `` name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 traverses the tree of python objects starting with ` root ` , depth first . parent-child relationships in the tree are defined by membership in modules or classes . the function ` visit ` is called with arguments ` ( path , parent , children ) ` for each module or class ` parent ` found in the tree of python objects starting with ` root ` . ` path ` is a string containing the name with which ` parent ` is reachable from the current context . for example , if ` root ` is a local class called ` x ` which contains a class ` y ` , ` visit ` will be called with ` ( ' y ' , x.y , children ) ` ) .
__label__0 returns : a list of tensors resulting from reading 'saveable ' from 'filename ' .
__label__0 def _unsortedsegmentprod ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.unsorted_segment_prod , data , indices , num_segments )
__label__0 def run ( self ) : hdrs = self.distribution.headers if not hdrs : return
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' helper class for tf python fuzzing . '' '' ''
__label__0 def _find_rocblas_config ( rocm_install_path ) :
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ real_bucket ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { real_bucket : [ np.zeros ( [ 5 , 1 ] ) ] } , sess )
__label__0 # check whether arg is problematic ( and if not , maybe remove it ) . if arg_ok_predicate and arg_ok_predicate ( arg_value ) : if remove_if_ok : for i , kw in enumerate ( node.keywords ) : if kw.arg == arg_name : node.keywords.pop ( i ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` removed argument % s for function % s '' % ( arg_name , full_name or name ) ) ) break return node else : return
__label__0 def _maybe_change_to_function_call ( self , parent , node , full_name ) : `` '' '' wraps node ( typically , an attribute or expr ) in a call . '' '' '' if full_name in self._api_change_spec.change_to_function : if not isinstance ( parent , ast.call ) : # ast.call 's constructor is really picky about how many arguments it # wants , and also , it changed between py2 and py3 . new_node = ast.call ( node , [ ] , [ ] ) pasta.ast_utils.replace_child ( parent , node , new_node ) ast.copy_location ( new_node , node ) self.add_log ( info , node.lineno , node.col_offset , `` changed % r to a function call '' % full_name ) return true return false
__label__0 text = `` from tensorflow import contrib '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 def testshoulduseresult ( self ) : @ tf_should_use.should_use_result ( warn_in_eager=true ) def return_const ( value ) : return constant_op.constant ( value , name='blah2 ' ) with reroute_error ( ) as error : return_const ( 0.0 ) msg = '\n'.join ( error.call_args [ 0 ] ) self.assertin ( 'object was never used ' , msg ) if not context.executing_eagerly ( ) : self.assertin ( 'blah2:0 ' , msg ) self.assertin ( 'return_const ' , msg ) gc.collect ( ) self.assertfalse ( gc.garbage )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 with sess.graph.device ( `` /cpu:1 '' ) : ds1 = dataset_ops.dataset.range ( 20 ) it1 = dataset_ops.make_initializable_iterator ( ds1 ) get_next1 = it1.get_next ( ) saveable1 = iterator_ops._iteratorsaveable ( it1._iterator_resource , name= '' saveable_it1 '' ) saver = saver_module.saver ( { `` it0 '' : saveable0 , `` it1 '' : saveable1 } , write_version=self._write_version , sharded=true ) self.evaluate ( it0.initializer ) self.evaluate ( it1.initializer ) self.assertequal ( 0 , self.evaluate ( get_next0 ) ) self.assertequal ( 1 , self.evaluate ( get_next0 ) ) self.assertequal ( 0 , self.evaluate ( get_next1 ) ) val = saver.save ( sess , save_path ) self.assertequal ( save_path , val ) data_files = glob.glob ( save_path + `` .data * '' ) self.assertequal ( 2 , len ( data_files ) )
__label__0 # bad/buggy join versions . _bad_versions = ( )
__label__0 this is useful for names that apper in error messages . args : obj : object to get the name of . returns : name , `` none '' , or a `` no name '' message. `` '' '' if obj is none : return `` none '' elif hasattr ( obj , `` name '' ) : return obj.name else : return `` < no name for % s > '' % type ( obj )
__label__0 def test_contrib_summary_scalar_nostep ( self ) : text = `` tf.contrib.summary.scalar ( 'foo ' , myval ) '' expected = ( `` tf.compat.v2.summary.scalar ( name='foo ' , data=myval , `` `` step=tf.compat.v1.train.get_or_create_global_step ( ) ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'step ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 deprecatedargspec = collections.namedtuple ( 'deprecatedargspec ' , [ 'position ' , 'has_ok_value ' , 'ok_value ' ] )
__label__0 def _upgrade_multiple ( self , upgrade_compat_v1_import , old_file_texts ) : upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2.tfapichangespec ( true , upgrade_compat_v1_import ) ) results = [ ] for old_file_text in old_file_texts : in_file = io.stringio ( old_file_text ) out_file = io.stringio ( ) count , report , errors = ( upgrader.process_opened_file ( `` test.py '' , in_file , `` test_out.py '' , out_file ) ) results.append ( [ count , report , errors , out_file.getvalue ( ) ] ) return results
__label__0 class configerror ( exception ) : pass
__label__0 returns : a tuple ( test_results , mangled_test_name ) , where test_results : a test_log_pb2.testresults proto , or none if log processing is skipped . test_adjusted_name : unique benchmark name that consists of benchmark name optionally followed by gpu type .
__label__0 def testcallableparams ( self ) : with context.eager_mode ( ) : for dtype in [ dtypes.half , dtypes.float32 ] : var0 = resource_variable_ops.resourcevariable ( [ 1.0 , 2.0 ] , dtype=dtype ) var1 = resource_variable_ops.resourcevariable ( [ 3.0 , 4.0 ] , dtype=dtype ) grads0 = constant_op.constant ( [ 0.1 , 0.1 ] , dtype=dtype ) grads1 = constant_op.constant ( [ 0.01 , 0.01 ] , dtype=dtype )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 if flags.test_log_output_filename : file_name = flags.test_log_output_filename else : file_name = ( name.strip ( `` / '' ) .translate ( str.maketrans ( `` / : '' , `` __ '' ) ) + time.strftime ( `` % y % m % d % h % m % s '' , time.gmtime ( ) ) ) if flags.test_log_output_use_tmpdir : tmpdir = test.get_temp_dir ( ) output_path = os.path.join ( tmpdir , flags.test_log_output_dir , file_name ) else : output_path = os.path.join ( os.path.abspath ( flags.test_log_output_dir ) , file_name ) json_test_results = json_format.messagetojson ( test_results ) gfile.gfile ( output_path + `` .json '' , `` w '' ) .write ( json_test_results ) tf_logging.info ( `` test results written to : % s '' % output_path )
__label__0 returns : result of repeatedly applying ` func ` , with the same structure layout as ` shallow_tree ` . `` '' '' if modality == modality.core : return _tf_core_map_structure_with_tuple_paths_up_to ( shallow_tree , func , * inputs , * * kwargs ) elif modality == modality.data : return _tf_data_map_structure_up_to ( shallow_tree , func , * inputs ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 class _countingsaveable ( saver_module.basesaverbuilder.saveableobject ) :
__label__0 make_decorator = tf_export.tf_export ( '__internal__.decorator.make_decorator ' , v1= [ ] ) ( tf_decorator.make_decorator ) unwrap = tf_export.tf_export ( '__internal__.decorator.unwrap ' , v1= [ ] ) ( tf_decorator.unwrap )
__label__0 def _get_composite_version_number ( major , minor , patch ) : return 10000 * major + 100 * minor + patch
__label__0 there are a few pre-defined hooks : - stopatstephook : request stop based on global_step - checkpointsaverhook : saves checkpoint - loggingtensorhook : outputs one or more tensor values to log - nantensorhook : request stop if given ` tensor ` contains nans . - summarysaverhook : saves summaries to a summary writer
__label__0 # keywords are reordered , so we should reorder arguments too text = `` g2 ( a , b , x , c , d ) \n '' # do n't accept an output which does n't reorder c and d acceptable_outputs = [ `` g2 ( a , b , c , d , x ) \n '' , `` g2 ( a=a , b=b , kw1=x , c=c , d=d ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliasandreorderrest ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 # now create a second session and test that we do n't stay stopped , until # we ask to stop again . sess2 = sv.prepare_or_wait_for_session ( `` '' ) self.assertfalse ( sv.should_stop ( ) ) sv.stop ( ) sess2.close ( ) self.asserttrue ( sv.should_stop ( ) )
__label__0 args : obj : the ` functools.partial ` object returns : an ` inspect.argspec ` raises : valueerror : when callable 's signature can not be expressed with argspec. `` '' '' # when callable is a functools.partial object , we construct its argspec with # following strategy : # - if callable partial contains default value for positional arguments ( ie . # object.args ) , then final argspec does n't contain those positional arguments . # - if callable partial contains default value for keyword arguments ( ie . # object.keywords ) , then we merge them with wrapped target . default values # from callable partial takes precedence over those from wrapped target . # # however , there is a case where it is impossible to construct a valid # argspec . python requires arguments that have no default values must be # defined before those with default values . argspec structure is only valid # when this presumption holds true because default values are expressed as a # tuple of values without keywords and they are always assumed to belong to # last k arguments where k is number of default values present . # # since functools.partial can give default value to any argument , this # presumption may no longer hold in some cases . for example : # # def func ( m , n ) : # return 2 * m + n # partialed = functools.partial ( func , m=1 ) # # this example will result in m having a default value but n does n't . this is # usually not allowed in python and can not be expressed in argspec correctly . # # thus , we must detect cases like this by finding first argument with default # value and ensures all following arguments also have default values . when # this is not true , a valueerror is raised .
__label__0 if isinstance ( obj , collections_abc.mapping ) : return dict ( obj )
__label__0 def testunboundfuncwithtwoparamsdefaultonekeywordfirst ( self ) :
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( arg0 , arg1 ) : `` '' '' fn doc .
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utility to retrieve function args . '' '' ''
__label__0 def check_line_split ( code_line ) : r '' '' '' checks if a line was split with ` \ ` .
__label__0 def _get_wrapper ( x , tf_should_use_helper ) : `` '' '' create a wrapper for object x , whose class subclasses type ( x ) .
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 this is primarily intended for testing purposes .
__label__0 @ atheris.instrument_func def testoneinput ( input_bytes ) : `` '' '' test randomized integer/float fuzzing input for tf.raw_ops.raggedcountsparseoutput . '' '' '' fh = fuzzinghelper ( input_bytes )
__label__0 text = `` from foo import baz , a , c '' expected_text = `` '' '' from foo import baz from bar import a , c '' '' '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 # in v1 api we need to print deprecation messages if not isinstance ( _sys.modules [ __name__ ] , module_wrapper.tfmodulewrapper ) : _sys.modules [ __name__ ] = module_wrapper.tfmodulewrapper ( _sys.modules [ __name__ ] , `` '' )
__label__0 x = customtensor ( [ 1 , 2 , 3 ] , 0.2 ) y = customtensor ( [ 7 , 8 , 2 ] , 0.4 ) z = customtensor ( [ 0 , 1 , 2 ] , 0.6 )
__label__0 glob_name = `` * . '' + name if name else none transformers = function_transformers.get ( `` * '' , { } ) .copy ( ) transformers.update ( function_transformers.get ( glob_name , { } ) ) transformers.update ( function_transformers.get ( full_name , { } ) ) return transformers
__label__0 s1 = save.save ( none , os.path.join ( save_dir , `` ckpt-1 '' ) ) s2 = save.save ( none , os.path.join ( save_dir , `` ckpt-2 '' ) ) s3 = save.save ( none , os.path.join ( save_dir , `` ckpt-3 '' ) ) self.assertequal ( [ s1 , s2 , s3 ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertcheckpointstate ( model_checkpoint_path=s3 , all_model_checkpoint_paths= [ s1 , s2 , s3 ] , save_dir=save_dir )
__label__0 _ , var_list = meta_graph.export_scoped_meta_graph ( filename=os.path.join ( test_dir , exported_filename ) , graph=ops_lib.get_default_graph ( ) , export_scope= '' hidden1 '' ) self.assertequal ( [ `` biases:0 '' , `` weights:0 '' ] , sorted ( var_list.keys ( ) ) )
__label__0 _fn ( deprecated=false ) self.assertequal ( 0 , mock_warning.call_count ) _fn ( deprecated=true ) self.assertequal ( 1 , mock_warning.call_count ) _fn ( deprecated=true ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 report_text = none report_filename = args.report_filename files_processed = 0 if args.input_file : if not args.in_place and not args.output_file : raise valueerror ( `` -- outfile= < output file > argument is required when converting a `` `` single file . '' ) if args.in_place and args.output_file : raise valueerror ( `` -- outfile argument is invalid when converting in place '' ) output_file = args.input_file if args.in_place else args.output_file files_processed , report_text , errors = process_file ( args.input_file , output_file , upgrade ) errors = { args.input_file : errors } files_processed = 1 elif args.input_tree : if not args.in_place and not args.output_tree : raise valueerror ( `` -- outtree= < output directory > argument is required when converting a `` `` file tree . '' ) if args.in_place and args.output_tree : raise valueerror ( `` -- outtree argument is invalid when converting in place '' ) output_tree = args.input_tree if args.in_place else args.output_tree files_processed , report_text , errors = upgrade.process_tree ( args.input_tree , output_tree , args.copy_other_files ) else : parser.print_help ( ) if report_text : num_errors = 0 report = [ ] for f in errors : if errors [ f ] : num_errors += len ( errors [ f ] ) report.append ( `` - '' * 80 + `` \n '' ) report.append ( `` file : % s\n '' % f ) report.append ( `` - '' * 80 + `` \n '' ) report.append ( `` \n '' .join ( errors [ f ] ) + `` \n '' )
__label__0 def testdepthtospace ( self ) : text = `` tf.nn.depth_to_space ( input , block_size , name , data_format ) '' expected_text = ( `` tf.nn.depth_to_space ( input , block_size , name=name , `` `` data_format=data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # allow deprecation warnings to be silenced temporarily with a context manager . _print_deprecation_warnings = true
__label__0 final = source_ast.body [ -1 ] if isinstance ( final , ast.expr ) : # wrap the final expression as ` _print_if_not_none ( expr ) ` print_it = ast.expr ( lineno=-1 , col_offset=-1 , value=ast.call ( func=ast.name ( id='_print_if_not_none ' , ctx=ast.load ( ) , lineno=-1 , col_offset=-1 ) , lineno=-1 , col_offset=-1 , args= [ final ] , # wrap the final expression keywords= [ ] ) ) source_ast.body [ -1 ] = print_it
__label__0 # # # # use for a single program
__label__0 this is the inverse of ` full_name_node ` .
__label__0 @ abc.abstractmethod def most_specific_common_supertype ( self , others : sequence [ `` tracetype '' ] ) - > optional [ `` tracetype '' ] : `` '' '' returns the most specific supertype of ` self ` and ` others ` , if exists .
__label__0 def include_frame ( fname ) : for exclusion in _excluded_paths : if exclusion in fname : return false return true
__label__0 returns : a compatible function , which conducts the actions of ` func ` but can be called like ` op ` , given that : - the list of required arguments in ` func ` and ` op ` are the same . - there is no override of the default arguments of ` op ` that are not supported by ` func ` . `` '' '' op_signature = _remove_annotation ( tf_inspect.signature ( op ) ) func_signature = _remove_annotation ( tf_inspect.signature ( func ) )
__label__0 # assert calling new fn issues log warning . self.assertequal ( `` prop_no_doc '' , _object ( ) ._prop ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 > > > class maskedtensor ( tf.experimental.extensiontype ) : ... values : tf.tensor ... mask : tf.tensor > > > @ dispatch_for_unary_elementwise_apis ( maskedtensor ) ... def unary_elementwise_api_handler ( api_func , x ) : ... return maskedtensor ( api_func ( x.values ) , x.mask ) > > > mt = maskedtensor ( [ 1 , -2 , -3 ] , [ true , false , true ] ) > > > abs_mt = tf.abs ( mt ) > > > print ( f '' values= { abs_mt.values.numpy ( ) } , mask= { abs_mt.mask.numpy ( ) } '' ) values= [ 1 2 3 ] , mask= [ true false true ]
__label__0 def _maybe_add_warning ( self , node , full_name ) : `` '' '' adds an error to be printed about full_name at node . '' '' '' function_warnings = self._api_change_spec.function_warnings if full_name in function_warnings : level , message = function_warnings [ full_name ] message = message.replace ( `` < function name > '' , full_name ) self.add_log ( level , node.lineno , node.col_offset , `` % s requires manual check . % s '' % ( full_name , message ) ) return true else : return false
__label__0 # if no scale was provided , make tf 2.0 use slim 's default factor if not found_scale : # parse with pasta instead of ast to avoid emitting a spurious trailing \n . scale_value = pasta.parse ( `` 2.0 '' ) node.keywords = ( [ ast.keyword ( arg= '' scale '' , value=scale_value ) ] + node.keywords )
__label__0 base_dirs , code_url_prefixes = base_dir.get_base_dirs_and_prefixes ( code_url_prefix ) doc_generator = generate_lib.docgenerator ( root_title= '' tensorflow 2 '' , py_modules= [ ( `` tf '' , tf ) ] , base_dir=base_dirs , search_hints=search_hints , code_url_prefix=code_url_prefixes , site_path=site_path , visitor_cls=tfexportawarevisitor , private_map=_private_map , extra_docs=_extra_docs , callbacks=base_dir.get_callbacks ( ) )
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] iterable : an iterable .
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but one of ` * inputs ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` .
__label__0 def _safe_eq ( a , b ) : if a is none or b is none : return a is none and b is none return a == b
__label__0 collaborator_build = os.environ.get ( 'collaborator_build ' , false )
__label__0 raises : typeerror : if ` func ` is not callable or if the structures do not match each other by depth tree . valueerror : if no structure is provided or if the structures do not match each other by type . valueerror : if wrong keyword arguments are provided. `` '' '' return nest_util.map_structure ( nest_util.modality.core , func , * structure , * * kwargs )
__label__0 shallow_tree = { ' b ' : none } map_structure_with_tuple_paths_up_to ( shallow_tree , print_path_and_values , lowercase , uppercase , check_types=false ) path : ( ' b ' , 1 ) , values : ( ( 'bo ' , 'b1 ' ) , ( 'b0 ' , 'b1 ' ) )
__label__0 class child1 ( parent ) : @ doc_in_current_and_subclasses def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 features.__doc__ = `` '' '' \ used in ` tf.train.example ` protos . contains the mapping from keys to ` feature ` .
__label__0 contrib_mirrored_strategy_warning = ( ast_edits.error , `` ( manual edit required ) tf.contrib.distribute.mirroredstrategy has `` `` been migrated to tf.distribute.mirroredstrategy . things to note : `` `` constructor arguments have changed . if you are using `` `` mirroredstrategy with keras training framework , the input provided to `` `` ` model.fit ` will be assumed to have global batch size and split `` `` across the replicas. `` + distribute_strategy_api_changes )
__label__0 # verifies copy to the same graph with the same name fails . with graph1.as_default ( ) : with self.assertraiseswithpredicatematch ( valueerror , lambda e : `` need to be different '' in str ( e ) ) : meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden1 '' , to_scope= '' hidden1 '' )
__label__0 def insert ( self , keys , values ) : return gen_lookup_ops.lookup_table_insert_v2 ( self.table_ref , keys , values )
__label__0 sometimes we may wish to partially flatten a structure , retaining some of the nested structure . we achieve this by specifying a shallow structure , ` shallow_tree ` , we wish to flatten up to .
__label__0 with session.session ( server.target ) as sess : self.assertequal ( 1.0 , self.evaluate ( a ) )
__label__0 def get_filtered_filenames ( self ) : if self._cached_set is not none : return self._cached_set
__label__0 if isinstance ( field , int ) : try : field_desc = parent_desc.fields_by_number [ field ] except keyerror : raise keyerror ( # pylint : disable=raise-missing-from f '' unable to find field number { field } in { parent_desc.full_name } . `` f '' valid field numbers : { parent_desc.fields_by_number.keys ( ) } '' ) elif isinstance ( field , str ) : try : field_desc = parent_desc.fields_by_name [ field ] except keyerror : raise keyerror ( # pylint : disable=raise-missing-from f '' unable to find field ' { field } ' in { parent_desc.full_name } . `` f '' valid field names : { parent_desc.fields_by_name.keys ( ) } '' ) else : # bool ( only expected as map key ) raise typeerror ( `` unexpected bool found in field list . '' )
__label__0 try : tmpdir = tempfile.mkdtemp ( ) os.chdir ( tmpdir )
__label__0 # aliases for code which was moved but still has lots of users . variablesaveable = saveable_object_util.referencevariablesaveable resourcevariablesaveable = saveable_object_util.resourcevariablesaveable
__label__0 # pylint : disable=protected-access if isinstance ( primary , variables.variable ) and primary._save_slice_info : # primary is a partitioned variable , so we need to also indicate that # the slot is a partitioned variable . slots have the same partitioning # as their primaries . # for examples when using adamoptimizer in linear model , slot.name # here can be `` linear//weights/adam:0 '' , while primary.op.name is # `` linear//weight '' . we want to get 'adam ' as real_slot_name , so we # remove `` 'linear//weight ' + '/ ' '' and ':0 ' . real_slot_name = slot.name [ len ( primary.op.name + `` / '' ) : -2 ] slice_info = primary._save_slice_info # support slot 's shape not same as primary 's shape # example : primary 's shape = [ 10 , 20 , 30 ] , slot 's shape = # none , [ ] , [ 10 ] , [ 10 , 20 ] or [ 10 , 20 , 30 ] is allowed # slot 's shape = none or [ 10 , 20 , 30 ] , set slot 's slice_info same as primary # slot 's shape = [ ] , do n't set slot 's slice_info # slot 's shape = [ 10 ] or [ 10 , 20 ] , set slot 's slice_info according to ndims n = slot.shape.ndims if n is none or n > 0 : slot._set_save_slice_info ( variables.variable.savesliceinfo ( slice_info.full_name + `` / '' + real_slot_name , slice_info.full_shape [ : n ] , slice_info.var_offset [ : n ] , slice_info.var_shape [ : n ] ) ) # pylint : enable=protected-access
__label__0 if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( checkpoint_management.latest_checkpoint ( self.get_temp_dir ( ) ) , os.path.join ( self.get_temp_dir ( ) , `` sharded_basics- ? ? ? ? ? -of-00002 '' ) ) else : self.assertequal ( checkpoint_management.latest_checkpoint ( self.get_temp_dir ( ) ) , os.path.join ( self.get_temp_dir ( ) , `` sharded_basics '' ) )
__label__0 if cls is none : return new_func else : # insert the wrapped function as the constructor setattr ( cls , constructor_name , new_func )
__label__1 class treenode : def __init__ ( self , val=0 , left=none , right=none ) : self.val = val self.left = left self.right = right class codec : def serialize ( self , root : treenode ) - > str : `` '' encodes a tree to a single string . '' '' def preorder ( node ) : if not node : return [ `` null '' ] return [ str ( node.val ) ] + preorder ( node.left ) + preorder ( node.right ) return `` , '' .join ( preorder ( root ) ) def deserialize ( self , data : str ) - > treenode : `` decodes your encoded data to tree . '' def build_tree ( nodes ) : val = nodes.pop ( 0 ) if val == `` null '' : return none node = treenode ( int ( val ) ) node.left = build_tree ( nodes ) node.right = build_tree ( nodes ) return node nodes = data.split ( `` , '' ) return build_tree ( nodes ) # example usage : # root = treenode ( 1 ) # root.left = treenode ( 2 ) # root.right = treenode ( 3 ) # root.right.left = treenode ( 4 ) # root.right.right = treenode ( 5 ) # codec = codec ( ) # serialized_tree = codec.serialize ( root ) # print ( serialized_tree ) # output : `` 1,2 , null , null,3,4 , null , null,5 , null , null '' # deserialized_tree = codec.deserialize ( serialized_tree )
__label__0 def after_run ( self , run_context , run_values ) : print ( 'done running one step . the value of my tensor : % s ' , run_values.results ) if you-need-to-stop-loop : run_context.request_stop ( )
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 # these pylint warnings are a mistake . # pylint : disable=g-explicit-bool-comparison , g-bool-id-comparison
__label__0 return handler
__label__0 linux : /usr/local/cuda , /usr , and paths from 'ldconfig -p ' . windows : cuda_path environment variable , or c : \\program files\\nvidia gpu computing toolkit\\cuda\\ *
__label__0 param_t = param - alpha_t * m_t / ( np.sqrt ( v_t ) + epsilon ) return param_t , m_t , v_t
__label__0 partial = functools.partial ( test_function , x=1 ) # smoke test : this should not raise an exception , even though ` partial ` does # not have ` __name__ ` , ` __module__ ` , and ` __doc__ ` attributes . _ = tf_decorator.make_decorator ( partial , test_wrapper )
__label__0 class automodule ( types.moduletype ) :
__label__0 # copyright 2024 the openxla authors . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================ `` '' '' test utils for python tests in xla . '' '' '' import os import pathlib
__label__0 def testparens ( self ) : text = `` '' '' def _log_prob ( self , x ) : return tf.debugging.assert_all_finite ( ( self.mixture_distribution.logits + self.distribution.log_prob ( x [ ... , tf.newaxis ] ) ) , message='nans or infs found ' ) '' '' '' expected_text = `` '' '' def _log_prob ( self , x ) : return tf.debugging.assert_all_finite ( x= ( self.mixture_distribution.logits + self.distribution.log_prob ( x [ ... , tf.newaxis ] ) ) , message='nans or infs found ' ) '' '' '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def testprotodictdefequivalenceswithstringtaskindex ( self ) : cluster_spec = server_lib.clusterspec ( { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : { `` 1 '' : `` worker1:2222 '' } } )
__label__0 def loop_body ( it , biases ) : biases += constant_op.constant ( 0.1 , shape= [ 32 ] ) return it + 1 , biases
__label__0 must use the same fixed amount of tensors as ` to_tensors ` .
__label__0 class svstepcounterthread ( coordinator.looperthread ) : `` '' '' threads to count steps and measure their duration . '' '' ''
__label__0 # check that s1 is still here , but s2 is gone . self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s4 ) )
__label__0 # pylint : disable=missing-function-docstring def _tf_data_map_structure_up_to ( shallow_tree , func , * inputs ) : if not inputs : raise valueerror ( `` argument ` inputs ` is empty . can not map over no sequences . '' ) for input_tree in inputs : _tf_data_assert_shallow_structure ( shallow_tree , input_tree )
__label__0 def two_func ( self ) : return 2
__label__0 def _string_split_rtype_transformer ( parent , node , full_name , name , logs ) : `` '' '' update tf.strings.split arguments : result_type , source . '' '' '' # remove the `` result_type '' argument . need_to_sparse = true for i , kw in enumerate ( node.keywords ) : if kw.arg == `` result_type '' : if ( isinstance ( kw.value , ast.str ) and kw.value.s in ( `` raggedtensor '' , `` sparsetensor '' ) ) : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` removed argument result_type= % r for function % s '' % ( kw.value.s , full_name or name ) ) ) node.keywords.pop ( i ) if kw.value.s == `` raggedtensor '' : need_to_sparse = false else : return _rename_to_compat_v1 ( node , full_name , logs , `` % s no longer takes the result_type parameter . '' % full_name ) break
__label__0 def __init__ ( self ) : self._storage = { }
__label__0 `` ` python cluster = tf.train.clusterspec ( { `` worker '' : [ `` worker0.example.com:2222 '' , `` worker1.example.com:2222 '' , `` worker2.example.com:2222 '' ] , `` ps '' : [ `` ps0.example.com:2222 '' , `` ps1.example.com:2222 '' ] } ) `` `
__label__0 def main ( ) : cmake_vars = _parse_args ( sys.argv [ 1 : ] ) for line in sys.stdin : sys.stdout.write ( _expand_cmakedefines ( line , cmake_vars ) )
__label__0 def deprecate_moved_module ( deprecated_name , new_module , deletion_version ) : `` '' '' logs a warning when a module that has been moved to a new location is used .
__label__0 the fallback dispatch system is based on `` operation dispatchers '' , which can be used to override the behavior for tensorflow ops when they are called with otherwise unsupported argument types . in particular , when an operation is called with arguments that would cause it to raise a typeerror , it falls back on its registered operation dispatchers . if any registered dispatchers can handle the arguments , then its result is returned . otherwise , the original typeerror is raised .
__label__0 api docstring : tensorflow.nest `` '' ''
__label__0 # the next one has the graph and metagraph . ev = next ( rr ) self.asserttrue ( ev.graph_def )
__label__0 if not op_name.startswith ( `` _ '' ) : path = pathlib.path ( `` / '' ) / flags.site_path / `` tf/raw_ops '' / op_name path = path.with_suffix ( `` .md '' ) link = ( ' < a id= { op_name } href= '' { path } '' > { op_name } < /a > ' ) .format ( op_name=op_name , path=str ( path ) ) parts.append ( `` | { link } | { has_gradient } | '' .format ( link=link , has_gradient=has_gradient ) )
__label__0 import_header = ( `` import tensorflow as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) text = import_header + old_symbol expected_header = ( `` import tensorflow.compat.v2 as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) expected_text = expected_header + new_symbol _ , _ , _ , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 # remove the scope keyword or arg if it is present if scope_keyword : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` dropping scope arg from tf.contrib.layers.l2_regularizer , '' `` because it is unsupported in tf.keras.regularizers.l2\n '' ) ) node.keywords.remove ( scope_keyword ) if len ( node.args ) > 1 : node.args = node.args [ :1 ] logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` dropping scope arg from tf.contrib.layers.l2_regularizer , '' `` because it is unsupported in tf.keras.regularizers.l2\n '' ) )
__label__0 variadic_args = uses_star_args_or_kwargs_in_call ( node )
__label__0 with self.assertraisesregex ( typeerror , `` returned structure '' ) : nest.get_traverse_shallow_structure ( lambda _ : [ true ] , 0 )
__label__0 def map_structure_with_tuple_paths ( func , * structure , * * kwargs ) : `` '' '' applies ` func ` to each entry in ` structure ` and returns a new structure .
__label__0 def override_for_test_op ( x , y , z ) : # pylint : disable=unused-variable return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 )
__label__0 def testmultipleimports ( self ) : text = `` import foo.bar as a , foo.baz as b , foo.baz.c , foo.d '' expected_text = `` import bar.bar as a , foo.baz as b , foo.baz.c , bar.d '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) sparse_apply_adagrad = gen_training_ops.sparse_apply_adagrad ( var , accum , lr , grad , constant_op.constant ( indices , self._totype ( indices.dtype ) ) ) out = self.evaluate ( sparse_apply_adagrad ) self.assertshapeequal ( out , sparse_apply_adagrad )
__label__0 returns : the value casted to this tracetype .
__label__0 see the [ ` tf.train.example ` ] ( https : //www.tensorflow.org/tutorials/load_data/tfrecord # tftrainexample ) guide for usage details. `` '' ''
__label__0 from google.protobuf import descriptor from google.protobuf import message from tensorflow.tools.proto_splitter import chunk_pb2
__label__0 def gather_memory_info ( ) : `` '' '' gather memory info . '' '' '' mem_info = test_log_pb2.memoryinfo ( ) vmem = psutil.virtual_memory ( ) mem_info.total = vmem.total mem_info.available = vmem.available return mem_info
__label__0 see ` tf.estimator.warmstartsettings ` for examples of using vocabinfo to warm-start .
__label__0 with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : with sess.graph.device ( test.gpu_device_name ( ) ) : v0_2 = variable_v1.variablev1 ( 543.21 ) save = saver_module.saver ( { `` v0 '' : v0_2 } ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def _is_private ( self , path , name , obj=none ) : `` '' '' return whether a name is private . '' '' '' # todo ( wicke ) : find out what names to exclude . del obj # unused . return ( ( path in self._private_map and name in self._private_map [ path ] ) or ( name.startswith ( ' _ ' ) and not re.match ( '__ . * __ $ ' , name ) or name in [ '__base__ ' , '__class__ ' , '__next_in_mro__ ' ] ) )
__label__0 `` '' ''
__label__0 with ` expand_composites=false ` , we just return the raggedtensor as is .
__label__0 deprecated_module.a ( ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 def testsegmentprodnumsegmentsless ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 0 , 1 , 2 ] , dtype=dtype ) , self._segmentprodv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 3 ) )
__label__0 self._tfmw_deprecated_checked.add ( name )
__label__0 graph_def = self._make_graph_def_with_constant_nodes ( sizes , fn=fn1 ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , _ = s.split ( )
__label__0 an ` atomicfunction ` encapsulates a single graph function definition .
__label__0 # create while loop using ` outer_body_fn ` . with ops_lib.graph ( ) .as_default ( ) : var = variable_v1.variablev1 ( 0.0 ) var_name = var.name output = graph_fn ( var ) output_name = output.name init_op = variables.global_variables_initializer ( )
__label__0 def __tf_tracing_type__ ( self , context ) : return fruittracetype ( self ) `` `
__label__0 args : name : the name to translate to a node . ctx : what context this name is used in . defaults to load ( )
__label__0 def __iter__ ( self ) : pass
__label__0 @ tf_export ( '__internal__.register_load_model_function ' , v1= [ ] ) def register_load_model_function ( func ) : global _keras_load_model_function _keras_load_model_function = func
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def main ( ) : # make sure base_dir ends with tensorflow . if it does n't , we probably # computed the wrong directory . if os.path.split ( base_dir ) [ -1 ] ! = 'tensorflow ' : raise assertionerror ( `` base_dir = ' % s ' does n't end with tensorflow '' % base_dir )
__label__0 def isframe ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.ismodule . '' '' '' return _inspect.isframe ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 def _tf_core_map_structure_with_tuple_paths_up_to ( shallow_tree , func , * inputs , * * kwargs ) : `` '' '' see comments for map_structure_with_tuple_paths_up_to ( ) in tensorflow/python/util/nest.py . '' '' '' if not inputs : raise valueerror ( `` can not map over no sequences '' )
__label__0 wrapped_func = tf_decorator.make_decorator ( func , wrapped_func ) wrapped_func.__signature__ = func_signature.replace ( parameters= ( list ( func_signature.parameters.values ( ) ) + [ api_signature.parameters [ `` name '' ] ] ) ) del wrapped_func._tf_decorator return wrapped_func
__label__0 def test_flags_bare ( self ) : _ , _ , errors , _ = self._upgrade ( `` tf.flags '' ) self.assertin ( `` tf.flags and tf.app.flags have been removed '' , errors [ 0 ] )
__label__0 # assert that v1 function has valid v1 argument names . for from_name , _ in keyword_renames.items ( ) : self.assertin ( from_name , arg_names_v1 , `` % s not found in % s arguments : % s '' % ( from_name , name , str ( arg_names_v1 ) ) )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_new_class ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 the ` run_context ` argument is the same one send to ` before_run ` call . ` run_context.request_stop ( ) ` can be called to stop the iteration .
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ real_bucket ] , partitioner ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * real_bucketized . * '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { real_bucket : [ prev_bucket_val ] } , sess )
__label__0 structure_traverse_input = [ ( 1 , [ 2 ] ) , ( [ 1 ] , 2 ) ] structure_traverse_r = nest.get_traverse_shallow_structure ( lambda s : ( true , false ) if isinstance ( s , tuple ) else true , structure_traverse_input ) self.assertequal ( structure_traverse_r , [ ( true , false ) , ( [ true ] , false ) ] ) nest.assert_shallow_structure ( structure_traverse_r , structure_traverse_input )
__label__0 def _tf_data_yield_value ( iterable ) : `` '' '' yield elements of ` iterable ` in a deterministic order .
__label__0 each job may also be specified as a sparse mapping from task indices to network addresses . this enables a server to be configured without needing to know the identity of ( for example ) all other worker tasks :
__label__0 returns : a ` metagraphdef ` proto .
__label__0 try : _ = tf.raw_ops.add ( x=input_tensor_x , y=input_tensor_y ) except ( tf.errors.invalidargumenterror , tf.errors.unimplementederror ) : pass
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for ` tensorflow : :functionparametercanonicalizer ` . '' '' ''
__label__0 core_mirrored_strategy_warning = ( ast_edits.warning , `` ( manual edit may be required ) tf.distribute.mirroredstrategy api has `` `` changed. `` + distribute_strategy_api_changes )
__label__0 `` '' ''
__label__0 def testrecoversession ( self ) : # create a checkpoint . checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` recover_session '' ) try : gfile.deleterecursively ( checkpoint_dir ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 from absl.testing import parameterized
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow.tools.common.public_api . '' '' ''
__label__0 import numpy as np
__label__0 if proto.bytesize ( ) > constants.max_size ( ) : # since there are chunks with the `` library '' field tag , insert this # chunk before the other chunks at index 1 ( index 0 is reserved for the # base chunk ) . self.add_chunk ( proto.library , [ `` library '' ] , 1 ) proto.clearfield ( `` library '' )
__label__0 `` '' ''
__label__0 the following local variable is created : * ` sync_rep_local_step ` , one per replica . compared against the global_step in each accumulator to check for staleness of the gradients .
__label__0 # ifndef tensorflow_core_util_version_info_h_ # define tensorflow_core_util_version_info_h_
__label__0 def testoneinput ( data ) : `` '' '' test numeric randomized fuzzing input for tf.raw_ops.add . '' '' '' fh = fuzzinghelper ( data )
__label__0 # test flattening ordered_keys_flat = nest.flatten ( ordered.keys ( ) ) ordered_values_flat = nest.flatten ( ordered.values ( ) ) ordered_items_flat = nest.flatten ( ordered.items ( ) ) self.assertequal ( [ 3 , 1 , 0 , 2 ] , ordered_values_flat ) self.assertequal ( [ `` d '' , `` b '' , `` a '' , `` c '' ] , ordered_keys_flat ) self.assertequal ( [ `` d '' , 3 , `` b '' , 1 , `` a '' , 0 , `` c '' , 2 ] , ordered_items_flat )
__label__0 # the savedmodel init is stored in the `` saved_model_initializers '' collection . # this collection is part of the metagraph 's default_init_op , so it is already # called by monitoredsession as long as the saver does n't restore any # checkpoints from the working dir . saved_model_init_ops = ops.get_collection ( `` saved_model_initializers '' ) if saved_model_init_ops : sess.run ( saved_model_init_ops )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_silence ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def testwarmstart_listofvariables ( self ) : # save checkpoint from which to warm-start . _ , prev_int_val = self._create_prev_run_var ( `` v1 '' , shape= [ 10 , 1 ] , initializer=ones ( ) ) # verify we initialized the values correctly . self.assertallequal ( np.ones ( [ 10 , 1 ] ) , prev_int_val )
__label__0 this script runs bazel queries to see what python files are required by the tests and ensures they are in the pip package superset. `` '' ''
__label__0 self.assertequal ( { ' a ' : 10 , ' b ' : 20 } , tf_inspect.getcallargs ( func , 10 , 20 ) )
__label__0 import collections as _collections import enum
__label__0 inner_decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , inner_argspec ) outer_decorator = tf_decorator.tfdecorator ( `` , inner_decorator , `` , outer_argspec ) self.assertequal ( outer_argspec , tf_inspect.getfullargspec ( outer_decorator ) )
__label__0 # use this flag to disable bazel generation if you 're not setup for it . flags.define_bool ( 'gen_ops ' , true , 'enable/disable bazel-generated ops ' )
__label__0 for classes , it creates a new class which is functionally identical ( it inherits from the original , and overrides its constructor ) , but which prints a deprecation warning when an instance is created . it also adds a deprecation notice to the class ' docstring .
__label__0 featurelist.__doc__ = `` '' '' \ mainly used as part of a ` tf.train.sequenceexample ` .
__label__0 add_contrib_direct_import_support ( manual_symbol_renames )
__label__0 def testlazyloadcorrectlitemodule ( self ) : # if set , always load lite module from public api list . module = mockmodule ( 'test ' ) apis = { 'lite ' : ( `` , 'cmd ' ) } module.lite = 5 import cmd as _cmd # pylint : disable=g-import-not-at-top wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' , public_apis=apis , deprecation=false , has_lite=true ) self.assertequal ( wrapped_module.lite , _cmd )
__label__0 def testcacherereadsfile ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` cache_rereads '' ) # save and reload one variable named `` var0 '' . self._saveandload ( `` var0 '' , 0.0 , 1.0 , save_path ) # save and reload one variable named `` var1 '' in the same file . # the cached readers should know to re-read the file . self._saveandload ( `` var1 '' , 1.1 , 2.2 , save_path )
__label__0 with ` expand_composites=true ` , we expect that the flattened input contains the tensors making up the ragged tensor i.e . the values and row_splits tensors .
__label__0 @ test_util.run_deprecated_v1 def testwithoutmomentum ( self ) : for dtype in [ dtypes.half , dtypes.float32 ] : with test_util.use_gpu ( ) : var0 = variables.variable ( [ 1.0 , 2.0 ] , dtype=dtype ) var1 = variables.variable ( [ 3.0 , 4.0 ] , dtype=dtype ) grads0 = constant_op.constant ( [ 0.1 , 0.1 ] , dtype=dtype ) grads1 = constant_op.constant ( [ 0.01 , 0.01 ] , dtype=dtype ) opt = rmsprop.rmspropoptimizer ( learning_rate=2.0 , decay=0.9 , momentum=0.0 , epsilon=1.0 ) update = opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def _to_proto ( self , strings ) : return test_message_pb2.repeatedstring ( strings=strings )
__label__0 creates a ` session ` on 'master ' . if a ` saver ` object is passed in , and ` checkpoint_dir ` points to a directory containing valid checkpoint files , then it will try to recover the model from checkpoint . if no checkpoint files are available , and ` wait_for_checkpoint ` is ` true ` , then the process would check every ` recovery_wait_secs ` , up to ` max_wait_secs ` , for recovery to succeed .
__label__0 # warnings that should be printed if corresponding functions are used . self.function_warnings = { `` tf.reverse '' : ( ast_edits.error , `` tf.reverse has had its argument semantics changed `` `` significantly . the converter can not detect this reliably , so `` `` you need to inspect this usage manually.\n '' ) , }
__label__0 def testglobaldispatcher ( self ) : original_global_dispatchers = dispatch._global_dispatchers try : tensortraceropdispatcher ( ) .register ( )
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` curand_ver_major '' , `` curand_ver_minor '' , `` curand_ver_patch '' ) ) return `` . `` .join ( version )
__label__0 `` ` python saver.save ( sess , 'my-model ' , global_step=0 ) == > filename : 'my-model-0 ' ... saver.save ( sess , 'my-model ' , global_step=1000 ) == > filename : 'my-model-1000 ' `` `
__label__0 if self._tfmw_print_deprecation_warnings : self._tfmw_add_deprecation_warning ( name , attr ) return attr
__label__0 the type of the slot is determined by the given value .
__label__0 def _segmentreduction ( self , op , data , indices , num_segments ) : with self.session ( ) as sess , self.test_scope ( ) : d = array_ops.placeholder ( data.dtype , shape=data.shape ) if isinstance ( indices , int ) : i = array_ops.placeholder ( np.int32 , shape= [ ] ) else : i = array_ops.placeholder ( indices.dtype , shape=indices.shape ) return sess.run ( op ( d , i , num_segments ) , { d : data , i : indices } )
__label__0 def set_identifier_string ( self , identifier_string ) : self.identifier_string = identifier_string self._update_string ( )
__label__0 the input , ` input_tree ` , can be thought of as having the same structure layout as ` shallow_tree ` , but with leaf nodes that are themselves tree structures .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_function_alias ( self , mock_warning ) : deprecated_func = deprecation.deprecated_alias ( `` deprecated.func '' , `` real.func '' , logging.error )
__label__0 contrib_summary_import_event_comment = ( ast_edits.error , `` ( manual edit required ) tf.contrib.summary.import_event ( ) has no `` `` direct equivalent in tf 2.0. for a similar experimental feature , try `` `` tf.compat.v2.summary.experimental.write_raw_pb ( ) which also accepts `` `` serialized summary protocol buffer input , but for tf.summary `` `` protobufs rather than tf.events . '' )
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 def _gather_saveables_for_checkpoint ( self ) : def _saveable_factory ( name=self.non_dep_variable.name ) : return _mirroringsaveable ( primary_variable=self.non_dep_variable , mirrored_variable=self.mirrored , name=name ) return { trackable_base.variable_value_key : _saveable_factory }
__label__0 def testkeyword ( self ) : text = `` tf.reduce_any ( a , reduction_indices= [ 1 , 2 ] ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.reduce_any ( a , axis= [ 1 , 2 ] ) \n '' )
__label__0 args : master : name of the tensorflow master to use . see the ` tf.compat.v1.session ` constructor for how this is interpreted . config : optional configproto proto used to configure the session , which is passed as-is to create the session . wait_for_checkpoint : whether we should wait for the availability of a checkpoint before creating session . defaults to false . max_wait_secs : maximum time to wait for the session to become available . start_standard_services : whether to start the standard services and the queue runners .
__label__0 def extract_stack ( stacklevel=1 ) : `` '' '' an eager-friendly alternative to traceback.extract_stack .
__label__0 to export a function or a class use tf_export decorator . for e.g . : `` ` python @ tf_export ( 'foo ' , 'bar.foo ' ) def foo ( ... ) : ... `` `
__label__0 args : filename_tensor : a string tensor . shard : integer . the shard for the filename . num_shards : an int tensor for the number of shards .
__label__0 def testunsortedsegmentsum1dindices2ddatadisjoint ( self ) : for dtype in self.numeric_types : data = np.array ( [ [ 0 , 1 , 2 , 3 ] , [ 20 , 21 , 22 , 23 ] , [ 30 , 31 , 32 , 33 ] , [ 40 , 41 , 42 , 43 ] , [ 50 , 51 , 52 , 53 ] ] , dtype=dtype ) indices = np.array ( [ 8 , 1 , 0 , 3 , 7 ] , dtype=np.int32 ) num_segments = 10 y = self._unsortedsegmentsum ( data , indices , num_segments ) self.assertallclose ( np.array ( [ [ 30 , 31 , 32 , 33 ] , [ 20 , 21 , 22 , 23 ] , [ 0 , 0 , 0 , 0 ] , [ 40 , 41 , 42 , 43 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 50 , 51 , 52 , 53 ] , [ 0 , 1 , 2 , 3 ] , [ 0 , 0 , 0 , 0 ] ] , dtype=dtype ) , y )
__label__0 def contains_cls ( x ) : `` '' '' returns true if ` x ` contains ` cls ` . '' '' '' if isinstance ( x , dict ) : return any ( contains_cls ( v ) for v in x.values ( ) ) elif x is cls : return true elif ( type_annotations.is_generic_list ( x ) or type_annotations.is_generic_union ( x ) ) : type_args = type_annotations.get_generic_type_args ( x ) return any ( contains_cls ( arg ) for arg in type_args ) else : return false
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassmapstructurewithpaths ( self ) : mt = maskedtensor ( mask=false , value=constant_op.constant ( [ 1 ] ) ) mt2 = maskedtensor ( mask=true , value=constant_op.constant ( [ 2 ] ) ) mt3 = maskedtensor ( mask=true , value=constant_op.constant ( [ 3 ] ) )
__label__0 param_names = list ( api_signature.parameters ) for param_name , param_type in signature.items ( ) : # convert positional parameters to named parameters . if ( isinstance ( param_name , int ) and param_name < len ( api_signature.parameters ) ) : param_name = list ( api_signature.parameters.values ( ) ) [ param_name ] .name
__label__0 note that only python files . if you have custom code in other languages , you will need to manually upgrade those .
__label__0 # expected chunks ( max size = 500 ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # chunk # : contents # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 0 : graphdef # ( nodes [ 0:5 ] ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 1 : graphdef # ( nodes [ 5:10 ] ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 2 : graphdef # ( nodes [ 10:15 ] ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - graph_def = self._make_graph_def_with_constant_nodes ( sizes ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , chunked_message = s.split ( ) self.assertlen ( chunks , 3 ) self._assert_chunk_sizes ( chunks , max_size ) for node , chunk in zip ( graph_def.node , itertools.chain ( chunks [ 0 ] .node , chunks [ 1 ] .node , chunks [ 2 ] .node ) , ) : self.assertprotoequals ( node , chunk )
__label__0 * these nested structure vs. nested structure comparisons will pass :
__label__0 def _contrib_layers_l2_regularizer_transformer ( parent , node , full_name , name , logs ) : `` '' '' replace slim l2 regularizer with keras one , with l=0.5 * scale .
__label__0 structure_of_mess = [ 14 , nesttest.abc ( `` a '' , true ) , { `` d '' : _custommapping ( { 41 : 42 } ) , `` c '' : [ 0 , collections.ordereddict ( [ ( `` b '' , 9 ) , ( `` a '' , 8 ) , ] ) , ] , `` b '' : 3 } , `` hi everybody '' , ]
__label__0 processed_file , new_file_content , log , process_errors = ( upgrader.update_string_pasta ( `` \n '' .join ( raw_lines ) , in_filename ) )
__label__0 self.assertallequal ( [ [ 4 ] ] , sess_1.run ( e ) ) self.assertallequal ( [ [ 4 ] ] , sess_2.run ( e ) )
__label__0 def print_path_and_values ( path , * values ) : print ( `` path : { } , values : { } '' .format ( path , values ) )
__label__0 def upload_benchmark_files ( opts ) : `` '' '' find benchmark files , process them , and upload their data to the datastore .
__label__0 class castcontext ( ) : `` '' '' contains context info and rules for casting values to a typespec . '' '' ''
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] structure : - for modality.core : nested structure , whose structure is given by nested lists , tuples , and dicts . note : numpy arrays and strings are considered scalars . - for modality.data : tuple or list constructed of scalars and/or other tuples/lists , or a scalar . note : numpy arrays are considered scalars . flat_sequence : flat sequence to pack . expand_composites : arg valid for modality.core only . if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors . sequence_fn : arg valid for modality.core only .
__label__0 if len ( dense_types ) ! = num_dense : raise valueerror ( `` len ( dense_types ) attribute does not match `` `` ndense attribute ( % d vs % d ) '' % ( len ( dense_types ) , num_dense ) )
__label__0 args : x_type : a type annotation indicating when the api handler should be called . y_type : a type annotation indicating when the api handler should be called .
__label__0 # the next one should have the values from the summary . # but only once . ev = next ( rr ) self.assertprotoequals ( `` '' '' value { tag : 'c1 ' simple_value : 1.0 } value { tag : 'c2 ' simple_value : 2.0 } value { tag : 'c3 ' simple_value : 3.0 } `` '' '' , ev.summary )
__label__0 returns : none `` '' '' current_symbols = set ( dir ( _sys.modules [ module_name ] ) ) should_have = make_all ( module_name , doc_string_modules ) should_have += allowed_exception_list or [ ] extra_symbols = current_symbols - set ( should_have ) target_module = _sys.modules [ module_name ] for extra_symbol in extra_symbols : # skip over __file__ , etc . also preserves internal symbols . if extra_symbol.startswith ( ' _ ' ) : continue fully_qualified_name = module_name + ' . ' + extra_symbol _hidden_attributes [ fully_qualified_name ] = ( target_module , getattr ( target_module , extra_symbol ) ) delattr ( target_module , extra_symbol )
__label__0 class tfmakedecoratortest ( test.testcase ) :
__label__0 `` ` python # saving contents and operations . v1 = tf.placeholder ( tf.float32 , name= '' v1 '' ) v2 = tf.placeholder ( tf.float32 , name= '' v2 '' ) v3 = tf.math.multiply ( v1 , v2 ) vx = tf.variable ( 10.0 , name= '' vx '' ) v4 = tf.add ( v3 , vx , name= '' v4 '' ) saver = tf.train.saver ( [ vx ] ) sess = tf.session ( ) sess.run ( tf.global_variables_initializer ( ) ) sess.run ( vx.assign ( tf.add ( vx , vx ) ) ) result = sess.run ( v4 , feed_dict= { v1:12.0 , v2:3.3 } ) print ( result ) saver.save ( sess , `` ./model_ex1 '' ) `` `
__label__0 for an example , see ` tfapiimportanalysisspec ` . `` '' ''
__label__0 parser = argparse.argumentparser ( description=desc ) for opt in opts : parser.add_argument ( opt [ 0 ] , opt [ 1 ] , type=opt [ 2 ] , default=opt [ 3 ] , required=opt [ 4 ] , help=opt [ 5 ] ) return parser.parse_args ( )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_varargs ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def build_chunks ( self ) - > int : `` '' '' splits the proto , and returns the size of the chunks created . '' '' '' proto = self._proto
__label__0 def _maybe_rename ( self , parent , node , full_name ) : `` '' '' replace node ( attribute or name ) with a node representing full_name . '' '' '' new_name = self._api_change_spec.symbol_renames.get ( full_name , none ) if new_name : self.add_log ( info , node.lineno , node.col_offset , `` renamed % r to % r '' % ( full_name , new_name ) ) new_node = full_name_node ( new_name , node.ctx ) ast.copy_location ( new_node , node ) pasta.ast_utils.replace_child ( parent , node , new_node ) return true else : return false
__label__0 examples :
__label__0 def _not_found_error ( base_paths , relative_paths , filepattern ) : base_paths = `` '' .join ( [ `` \n ' % s ' '' % path for path in sorted ( base_paths ) ] ) relative_paths = `` '' .join ( [ `` \n ' % s ' '' % path for path in relative_paths ] ) return configerror ( `` could not find any % s in any subdirectory : % s\nof : % s\n '' % ( filepattern , relative_paths , base_paths ) )
__label__0 def testargmax ( self ) : text = `` tf.argmax ( input , name=n , dimension=1 , output_type=type ) '' expected_text = `` tf.argmax ( input , name=n , axis=1 , output_type=type ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 if isinstance ( target , type ) : try : return _getargspec ( target.__init__ ) except typeerror : pass
__label__0 `` `` '' start a simple interactive console with tensorflow available . '' '' ''
__label__0 tf_modules = find_modules ( )
__label__0 def testdispatchfortypes_opdoesnotsupportdispatch ( self ) :
__label__0 # exercise the second helper .
__label__0 elif isinstance ( annotation , type ) : if annotation not in _is_instance_checker_cache : checker = _api_dispatcher.makeinstancechecker ( annotation ) _is_instance_checker_cache [ annotation ] = checker return _is_instance_checker_cache [ annotation ]
__label__0 self.assertequal ( checkpoint_management.latest_checkpoint ( save_dir1 ) , save_path1 ) save_dir2 = os.path.join ( self.get_temp_dir ( ) , `` save_dir2 '' ) os.renames ( save_dir1 , save_dir2 ) save_path2 = os.path.join ( save_dir2 , `` save_copy_restore '' ) self.assertequal ( checkpoint_management.latest_checkpoint ( save_dir2 ) , save_path2 )
__label__0 def __init__ ( self ) : self.function_handle = { } self.function_reorders = { } self.function_keyword_renames = { } self.symbol_renames = { } self.function_warnings = { } self.change_to_function = { } self.module_deprecations = { } self.function_transformers = { } self.import_renames = { }
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 > > > d1 = { `` hello '' : 24 , `` world '' : 76 } > > > d2 = { `` hello '' : 36 , `` world '' : 14 } > > > tf.nest.map_structure ( lambda p1 , p2 : p1 + p2 , d1 , d2 ) { 'hello ' : 60 , 'world ' : 90 }
__label__0 # adding s2 again ( but helper is unaware of previous s2 ) s2 = save3.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s2 ] , save3.last_checkpoints ) # created by the first helper . self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) # deleted by the first helper . self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) # even though the file for s1 exists , this saver is n't aware of it , which # is why it does n't end up in the checkpoint state . self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s2 ] , save_dir=save_dir )
__label__0 with self.cached_session ( ) as sess : # initializes all the variables . self.evaluate ( init_all_op ) # runs to logit . self.evaluate ( logits ) # creates a saver . saver0 = saver_module.saver ( ) saver0.save ( sess , saver0_ckpt ) # generates metagraphdef . saver0.export_meta_graph ( filename )
__label__0 another option is to create an ` assignment_map ` that maps the name of the variables in the name-based checkpoint to the variables in your model , eg : `` ` { 'sequential/dense/bias ' : model.variables [ 0 ] , 'sequential/dense/kernel ' : model.variables [ 1 ] } `` ` and use ` tf.compat.v1.train.init_from_checkpoint ( path , assignment_map ) ` to restore the name-based checkpoint .
__label__0 def testimportintonamescopewithoutvariables ( self ) : # save a simple graph that contains no variables into a checkpoint . test_dir = self._get_test_dir ( `` no_vars_graph '' ) filename = os.path.join ( test_dir , `` ckpt '' ) graph_1 = ops_lib.graph ( ) with session.session ( graph=graph_1 ) as sess : constant_op.constant ( [ 1 , 2 , 3 ] , name= '' x '' ) constant_op.constant ( [ 1 , 2 , 3 ] , name= '' y '' ) saver = saver_module.saver ( allow_empty=true ) saver.save ( sess , filename )
__label__0 header_path , header_version = _find_header ( base_paths , `` cublas_api.h '' , required_version , get_header_version ) # cublas uses the major version only . cublas_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 def testdispatchapiwithnonamearg ( self ) : # note : the `` tensor_equals '' api has no `` name '' argument . signature = { `` self '' : maskedtensor , `` other '' : maskedtensor }
__label__0 class modality ( enum.enum ) : `` '' '' modality/semantic used for treating nested structures .
__label__0 example :
__label__0 def __get__ ( self , instance , owner ) : return self._decorated_target.__get__ ( instance , owner )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # now , saves a full variable and restores partitionedvariable . saved_full = _save ( ) restored_full = _restore ( partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=3 ) ) self.assertallequal ( saved_full , restored_full )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.acosh . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 def testprotodictdefequivalences ( self ) : cluster_spec = server_lib.clusterspec ( { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : [ `` worker0:2222 '' , `` worker1:2222 '' , `` worker2:2222 '' ] } )
__label__0 1. create a keras optimizer , which generates an ` iterations ` variable . this variable is automatically incremented when calling ` apply_gradients ` . 2. manually create and increment a ` tf.variable ` .
__label__0 raises : valueerror : if the grads_and_vars is empty . valueerror : if global step is not provided , the staleness can not be checked. `` '' '' if not grads_and_vars : raise valueerror ( `` must supply at least one variable '' )
__label__0 > > > tf.nest.is_nested ( `` 1234 '' ) false > > > tf.nest.is_nested ( [ 1 , 3 , [ 4 , 5 ] ] ) true > > > tf.nest.is_nested ( ( ( 7 , 8 ) , ( 5 , 6 ) ) ) true > > > tf.nest.is_nested ( [ ] ) true > > > tf.nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } ) true > > > tf.nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .keys ( ) ) true > > > tf.nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .values ( ) ) true > > > tf.nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .items ( ) ) true > > > tf.nest.is_nested ( set ( [ 1 , 2 ] ) ) false > > > ones = tf.ones ( [ 2 , 3 ] ) > > > tf.nest.is_nested ( ones ) false
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' help include git hash in tensorflow bazel build .
__label__0 _fn ( ) self.assertequal ( 1 , mock_warning.call_count ) _fn ( ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 class syncreplicasoptimizertest ( test.testcase ) :
__label__0 if args.mode == _safety_mode : change_spec = tf_upgrade_v2_safety.tfapichangespec ( ) else : if args.no_import_rename : change_spec = tf_upgrade_v2.tfapichangespec ( import_rename=false , upgrade_compat_v1_import=not args.no_upgrade_compat_v1_import ) else : change_spec = tf_upgrade_v2.tfapichangespec ( import_rename=_import_rename_default , upgrade_compat_v1_import=not args.no_upgrade_compat_v1_import ) upgrade = ast_edits.astcodeupgrader ( change_spec )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' base tfdecorator class and utility functions for working with decorators .
__label__0 ( _ , extracted_floats ) = extract_floats ( text )
__label__0 return decorator
__label__0 def testargmin ( self ) : text = `` tf.argmin ( input , name=n , dimension=1 , output_type=type ) '' expected_text = `` tf.argmin ( input , name=n , axis=1 , output_type=type ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 visitor = public_api.publicapivisitor ( conversion_visitor ) visitor.do_not_descend_map [ `` tf '' ] .append ( `` contrib '' ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v1 , visitor )
__label__0 def _load ( self ) : `` '' '' load the module and insert it into the parent 's globals . '' '' '' # import the target module and insert it into the parent 's namespace module = importlib.import_module ( self.__name__ ) self._tfll_parent_module_globals [ self._tfll_local_name ] = module
__label__0 def _partitioner ( shape , dtype ) : # pylint : disable=unused-argument # partition each var into 2 equal slices . partitions = [ 1 ] * len ( shape ) partitions [ 0 ] = min ( 2 , shape.dims [ 0 ] .value ) return partitions
__label__0 args : traverse_fn : function taking a substructure and returning either a scalar ` bool ` ( whether to traverse that substructure or not ) or a depth=1 shallow structure of the same type , describing which parts of the substructure to traverse . structure : the structure to traverse . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 def testsegmentmax ( self ) : for dtype in self.int_types | self.float_types : minval = dtypes.as_dtype ( dtype ) .min if dtype == np.float64 and self._finddevice ( `` tpu '' ) : minval = -np.inf self.assertallclose ( np.array ( [ 1 , minval , 2 , 5 ] , dtype=dtype ) , self._segmentmaxv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 # todo ( mdan ) : is this strictly needed ? only ops.py really uses it . class nativeobject ( object ) : `` '' '' types natively supported by various tf operations .
__label__0 def raise_error ( unused_self ) : raise attributeerror ( deprecation_message )
__label__0 def func ( ) : trace = tf_stack.extract_stack ( ) # comment frames = list ( trace.get_user_frames ( ) ) return frames
__label__0 if `` dev '' in tf.__version__ : keras_url_prefix = `` https : //github.com/keras-team/keras/tree/master/keras/src '' else : keras_url_prefix = ( f '' https : //github.com/keras-team/keras/tree/v { keras.__version__ } /keras/src '' )
__label__0 @ _wrap_decorator ( func_or_class.__init__ , 'deprecated_alias ' ) def __init__ ( self , * args , * * kwargs ) : if hasattr ( _newclass.__init__ , '__func__ ' ) : # python 2 _newclass.__init__.__func__.__doc__ = func_or_class.__init__.__doc__ else : # python 3 _newclass.__init__.__doc__ = func_or_class.__init__.__doc__
__label__0 def test_callable ( self ) :
__label__0 @ tf_export ( `` experimental.dispatch_for_binary_elementwise_assert_apis '' ) def dispatch_for_binary_elementwise_assert_apis ( x_type , y_type ) : `` '' '' decorator to override default implementation for binary elementwise assert apis .
__label__0 # running on worker with the old session should raise an exception since # the workersession of the old session has been garbage collected with self.assertraisesregex ( errors_impl.abortederror , `` session handle is not found '' ) : sess_old.run ( b )
__label__0 def testpicklesubmodule ( self ) : name = pickletest.__module__ # the current module is a submodule . module = module_wrapper.tfmodulewrapper ( mockmodule ( name ) , name ) restored = pickle.loads ( pickle.dumps ( module ) ) self.assertequal ( restored.__name__ , name ) self.assertisnotnone ( restored.pickletest )
__label__0 def testsingledeprecatedendpoint ( self ) : @ deprecation.deprecated_endpoints ( `` foo1 '' ) def foo ( ) : pass self.assertequal ( ( `` foo1 '' , ) , foo._tf_deprecated_api_names )
__label__0 this function could be used along with ` tf_inspect.getfullargspec ` to determine if the first argument of ` object ` argspec is self or cls . if the first argument is self or cls , it needs to be excluded from argspec when we compare the argspec to the input arguments and , if provided , the tf.function input_signature .
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] ) self.assertequal ( math_ops.add ( x , y ) , `` stub '' )
__label__0 # todo ( b/178822082 ) : revisit this api when tf.types gets more resource . @ tf_export ( `` __internal__.types.tensor '' , v1= [ ] ) class tensor ( object ) : `` '' '' the base class of all dense tensor objects .
__label__0 def _yield_sorted_items ( iterable ) : return nest_util.yield_sorted_items ( nest_util.modality.core , iterable )
__label__0 # ensures a valid transformation when a positional name arg is given if len ( node.args ) == 4 : pos_arg = ast.keyword ( arg= '' preserve_aspect_ratio '' , value=node.args [ -1 ] ) node.args = node.args [ : -1 ] node.keywords.append ( pos_arg ) if len ( node.args ) == 3 : pos_arg = ast.keyword ( arg= '' align_corners '' , value=node.args [ -1 ] ) node.args = node.args [ : -1 ]
__label__0 def testtensorflowimportalreadyhascompat ( self ) : text = `` import tensorflow.compat.v1 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 2 , name= '' v '' ) w = variable_v1.variablev1 ( 1 , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=none , local_init_op=w.initializer ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm2.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir , wait_for_checkpoint=false ) self.assertfalse ( initialized ) self.assertequal ( false , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( w ) )
__label__0 raises : valueerror : if the warmstartsettings contains prev_var_name or vocabinfo configuration for variable names that are not used . this is to ensure a stronger check for variable configuration than relying on users to examine the logs. `` '' '' logging.info ( `` warm-starting from : { } '' .format ( ckpt_to_initialize_from ) ) grouped_variables = _get_grouped_variables ( vars_to_warm_start )
__label__0 # adding s1 ( s3 should now be deleted as oldest in list ) s1 = save.save ( none , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s2 , s1 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s2 , s1 ] , save_dir=save_dir )
__label__0 user_options = [ ( 'install-dir= ' , 'd ' , 'directory to install header files to ' ) , ( 'force ' , ' f ' , 'force installation ( overwrite existing files ) ' ) , ]
__label__0 def _get_read_histogram_proto ( ) : proto_bytes = metrics.getcheckpointreaddurations ( api_label=api_label ) histogram_proto = summary_pb2.histogramproto ( ) histogram_proto.parsefromstring ( proto_bytes ) return histogram_proto
__label__0 this function allows replacing a function wrapped by ` decorator_func ` , assuming the decorator that wraps the function is written as described below .
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { int64_list : { value : [ 5 ] } } } } } `` `
__label__0 def testcreateslotwithoutxlasharding ( self ) : # slot_creator is used only in optimizer v1 . # the spmd sharding annotations should not be copied since the primary # variable and slot variable have different ranks . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) v = xla_sharding.mesh_split ( v , np.array ( [ 0 , 1 ] ) , [ 0 ] , use_sharding_op=false ) with ops.control_dependencies ( none ) : slot = slot_creator.create_slot ( v , constant_op.constant ( 10 , name= '' const '' ) , name= '' slot '' , copy_xla_sharding=true ) self.assertisnone ( xla_sharding.get_tensor_sharding ( slot ) ) self.assertnotequal ( xla_sharding.get_tensor_sharding ( v ) , xla_sharding.get_tensor_sharding ( slot ) )
__label__0 import random import time
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' library for getting system information during tensorflow tests . '' '' ''
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ np.zeros ( [ 2 , 1 ] ) ] } , sess )
__label__0 try : tf.estimator.estimator = doc_controls.inheritable_header ( textwrap.dedent ( `` '' '' \ warning : tensorflow 2.15 included the final release of the ` tf-estimator ` package . estimators will not be available in tensorflow 2.16 or after . see the [ migration guide ] ( https : //www.tensorflow.org/guide/migrate/migrating_estimator ) for more information about how to convert off of estimators . '' `` `` '' ) ) ( tf.estimator.estimator ) except attributeerror : pass
__label__0 finally : # clean up . dispatch._global_dispatchers = original_global_dispatchers
__label__0 # apply indentation to new node . pasta.base.formatting.set ( new_line_node , `` prefix '' , pasta.base.formatting.get ( node , `` prefix '' ) ) pasta.base.formatting.set ( new_line_node , `` suffix '' , os.linesep ) self.add_log ( info , node.lineno , node.col_offset , `` adding ` % s ` after import of % s '' % ( new_line_node , import_alias.name ) ) # find one match , break if found_update : break # no rename is found for all levels if not found_update : new_aliases.append ( import_alias ) # no change needed
__label__0 def __init__ ( self , sync_optimizer , is_chief , num_tokens ) : `` '' '' creates hook to handle syncreplicasoptimizer initialization ops .
__label__0 minus_one ( array_ops.identity ( v0 ) ) save = saver_module.saver ( { `` v0 '' : v0 } ) variables.global_variables_initializer ( )
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 this api is intended * only * for debugging as there are no guarantees on backwards compatibility of returned ir or the allowed values of ` stage ` .
__label__0 @ test_util.run_deprecated_v1 def testwithmomentum ( self ) : for dtype in [ dtypes.half , dtypes.float32 ] : with test_util.use_gpu ( ) : var0 = variables.variable ( [ 1.0 , 2.0 ] , dtype=dtype ) var1 = variables.variable ( [ 3.0 , 4.0 ] , dtype=dtype ) grads0 = constant_op.constant ( [ 0.1 , 0.1 ] , dtype=dtype ) grads1 = constant_op.constant ( [ 0.01 , 0.01 ] , dtype=dtype )
__label__0 raises : tf.deadlineexceedederror : if the session is not available after max_wait_secs. `` '' '' self._target = master
__label__0 x = tensortracer ( `` x '' ) y = tensortracer ( `` y '' ) trace = x [ y ] self.assertequal ( str ( trace ) , `` __operators__.getitem ( x , y ) '' )
__label__0 def testsourcemap ( self ) : source_map = tf_stack._tf_stack.pybindsourcemap ( )
__label__0 expected = np.reshape ( [ [ 5.0999999 , 7.0999999 , 9.10000038 ] * 3 ] , ( 3 , 3 ) )
__label__0 canonical_name = get_canonical_name ( api_names , deprecated_api_names ) if canonical_name : return canonical_name
__label__0 args : others : a sequence of tracetypes .
__label__0 flat_path_nested_list = nest.flatten_with_tuple_paths_up_to ( shallow_tree=nmt , input_tree=nested_list , check_types=false ) self.assertallequal ( flat_path_nested_list , [ [ ( 0 , 0 ) , 2 ] ] )
__label__0 x = tensortracer ( `` x '' ) y = tensortracer ( `` y '' ) trace = math_ops.reduce_sum ( math_ops.add ( math_ops.abs ( x ) , y ) , axis=3 ) self.assertequal ( str ( trace ) , `` math.reduce_sum ( math.add ( math.abs ( x ) , y ) , axis=3 ) '' )
__label__0 # tf_py_test_dependencies is the list of dependencies for all python # tests in tensorflow tf_py_test_dependencies = subprocess.check_output ( [ `` bazel '' , `` cquery '' , `` -- experimental_cc_shared_library '' , py_test_query_expression ] ) if isinstance ( tf_py_test_dependencies , bytes ) : tf_py_test_dependencies = tf_py_test_dependencies.decode ( `` utf-8 '' ) tf_py_test_dependencies_list = tf_py_test_dependencies.strip ( ) .split ( `` \n '' ) tf_py_test_dependencies_list = [ x.split ( ) [ 0 ] for x in tf_py_test_dependencies.strip ( ) .split ( `` \n '' ) ] print ( `` pytest dependency subset size : % d '' % len ( tf_py_test_dependencies_list ) )
__label__0 def testgetfullargspeconpartialwithvarargs ( self ) : `` '' '' tests getfullargspec on partial function with variable arguments . '' '' ''
__label__0 def testwarmstartvarwithvocabbothvarspartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] )
__label__0 def register ( self , op ) : `` '' '' register this dispatcher as a handler for ` op ` .
__label__0 def update_renames ( self ) : self.function_keyword_renames [ `` f '' ] = { `` kw2 '' : `` kw3 '' }
__label__0 field , field_desc = util.get_field ( proto , self.repeated_field ) if not util.is_repeated ( field_desc ) and field_desc.message_type : raise valueerror ( `` repeatedmessagesplitter can only be used on repeated fields. `` f '' got proto= { type ( proto ) } , field= ' { field_desc.name } ' '' )
__label__0 s3 = save.save ( sess , os.path.join ( save_dir , `` s3 '' ) ) self.assertequal ( [ s2 , s3 ] , save.last_checkpoints ) self.assertequal ( 0 , len ( gfile.glob ( s1 + `` * '' ) ) ) self.assertfalse ( gfile.exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( 2 , len ( gfile.glob ( s2 ) ) ) else : self.assertequal ( 4 , len ( gfile.glob ( s2 + `` * '' ) ) ) self.asserttrue ( gfile.exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( 2 , len ( gfile.glob ( s3 ) ) ) else : self.assertequal ( 4 , len ( gfile.glob ( s3 + `` * '' ) ) ) self.asserttrue ( gfile.exists ( checkpoint_management.meta_graph_filename ( s3 ) ) )
__label__0 def _tf_tensor_numpy_output ( self , string ) : modified_string = self._numpy_output_re.sub ( r'\1 ' , string ) return modified_string , modified_string ! = string
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for ast_edits which is used in tf upgraders .
__label__0 _cmake_define_regex = re.compile ( r '' \s * # cmakedefine\s+ ( [ a-za-z_0-9 ] * ) ( \s . * ) ? $ '' ) _cmake_define01_regex = re.compile ( r '' \s * # cmakedefine01\s+ ( [ a-za-z_0-9 ] * ) '' ) _cmake_var_regex = re.compile ( r '' \ $ { ( [ a-za-z_0-9 ] * ) } '' ) _cmake_atvar_regex = re.compile ( r '' @ ( [ a-za-z_0-9 ] * ) @ '' )
__label__0 def get_get_session_function ( ) : global _keras_get_session_function return _keras_get_session_function
__label__0 def __init__ ( self , input_bytes ) : `` '' '' fuzzinghelper initializer .
__label__0 def teststartqueuerunnersraisesifnotasession ( self ) : zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) init_op = variables.global_variables_initializer ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) queue_runner_impl.add_queue_runner ( qr ) with self.cached_session ( ) : init_op.run ( ) with self.assertraisesregex ( typeerror , `` tf.session '' ) : queue_runner_impl.start_queue_runners ( `` notasession '' )
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 args : func : the function to wrap . signature must match ` api_signature ` ( except the `` name '' parameter may be missing . api_signature : the signature of the original api ( used to find the index for the `` name '' parameter ) .
__label__0 # namedtuples . ab_tuple = nesttest.abtuple input_tree = ab_tuple ( a= [ 0 , 1 ] , b=2 ) shallow_tree = ab_tuple ( a=0 , b=1 ) input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ [ 0 , 1 ] , 2 ] )
__label__0 setup ( name=project_name , version=_version.replace ( '- ' , `` ) , description=doclines [ 0 ] , long_description='\n'.join ( doclines [ 2 : ] ) , long_description_content_type='text/markdown ' , url='https : //www.tensorflow.org/ ' , download_url='https : //github.com/tensorflow/tensorflow/tags ' , author='google inc. ' , author_email='packages @ tensorflow.org ' , install_requires=required_packages , extras_require=extra_packages , # add in any packaged data . zip_safe=false , # supported python versions python_requires= ' > =3.9 ' , # pypi package information . classifiers=sorted ( [ 'development status : : 5 - production/stable ' , # todo ( angerson ) add ifttt when possible 'environment : : gpu : : nvidia cuda : : 12 ' , 'environment : : gpu : : nvidia cuda : : 12 : : 12.2 ' , 'intended audience : : developers ' , 'intended audience : : education ' , 'intended audience : : science/research ' , 'license : : osi approved : : apache software license ' , 'programming language : : python : : 3 ' , 'programming language : : python : : 3.9 ' , 'programming language : : python : : 3.10 ' , 'programming language : : python : : 3.11 ' , 'programming language : : python : : 3.12 ' , 'programming language : : python : : 3 : : only ' , 'topic : : scientific/engineering ' , 'topic : : scientific/engineering : : mathematics ' , 'topic : : scientific/engineering : : artificial intelligence ' , 'topic : : software development ' , 'topic : : software development : : libraries ' , 'topic : : software development : : libraries : : python modules ' , ] ) , license='apache 2.0 ' , keywords='tensorflow tensor machine learning ' , * * collaborator_build_dependent_options )
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import test as test_lib from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import tf_upgrade_v2_safety
__label__0 > > > ones_mt = tf.ones_like ( mt , dtype=tf.float32 ) > > > print ( f '' values= { ones_mt.values.numpy ( ) } , mask= { ones_mt.mask.numpy ( ) } '' ) values= [ 1.0 1.0 1.0 ] , mask= [ true false true ]
__label__0 with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) ws_util.warm_start ( self.get_temp_dir ( ) ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( prev_val , self.evaluate ( fruit_weights ) )
__label__0 n_prune_args = len ( obj.args ) partial_keywords = obj.keywords or { }
__label__0 def testunwrapreturnslistofuniquetfdecorators ( self ) : decorators , _ = tf_decorator.unwrap ( test_decorated_function ) self.assertequal ( 3 , len ( decorators ) ) self.asserttrue ( isinstance ( decorators [ 0 ] , tf_decorator.tfdecorator ) ) self.asserttrue ( isinstance ( decorators [ 1 ] , tf_decorator.tfdecorator ) ) self.asserttrue ( isinstance ( decorators [ 2 ] , tf_decorator.tfdecorator ) ) self.assertisnot ( decorators [ 0 ] , decorators [ 1 ] ) self.assertisnot ( decorators [ 1 ] , decorators [ 2 ] ) self.assertisnot ( decorators [ 2 ] , decorators [ 0 ] )
__label__0 from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import gen_training_ops from tensorflow.python.ops import init_ops from tensorflow.python.ops import math_ops from tensorflow.python.training import optimizer from tensorflow.python.util.tf_export import tf_export
__label__0 returns : a ( string , array ) pair , where ` string ` has each float replaced by `` ... '' and ` array ` is a ` float32 ` ` numpy.array ` containing the extracted floats. `` '' '' texts = [ ] floats = [ ] for i , part in enumerate ( self._float_re.split ( string ) ) : if i % 2 == 0 : texts.append ( part ) else : floats.append ( float ( part ) )
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 structure1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] with self.assertraisesregex ( typeerror , `` same sequence type '' ) : nest.map_structure ( lambda x , y : none , structure1 , structure1_list )
__label__0 def testrenamepack ( self ) : text = `` tf.pack ( a ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.stack ( a ) \n '' ) text = `` tf.unpack ( a ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.unstack ( a ) \n '' )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tensor utility functions . '' '' '' import collections import functools import inspect import re
__label__1 import random
__label__0 def testreturnsemptywhenunboundfunchasnoparameters ( self ) :
__label__0 return `` \n '' .join ( parts )
__label__0 # dicts . inp_val = dict ( a=2 , b=3 ) inp_ops = dict ( a=dict ( add=1 , mul=2 ) , b=dict ( add=2 , mul=3 ) ) out = nest.map_structure_up_to ( inp_val , lambda val , ops : ( val + ops [ `` add '' ] ) * ops [ `` mul '' ] , inp_val , inp_ops ) self.assertequal ( out [ `` a '' ] , 6 ) self.assertequal ( out [ `` b '' ] , 15 )
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , nesttest.named0ab ( 3 , 4 ) , nesttest.named1ab ( 3 , 4 ) )
__label__0 user classes should implement the two methods defined in this protocol in order to be supported by nest functions . - ` __tf_flatten__ ` for generating the flattened components and the metadata of the current object . - ` __tf_unflatten__ ` for creating a new object based on the input metadata and the components . see the method doc for details .
__label__0 save = saver_module.saver ( { `` save_prefix/v0 '' : v0 , `` save_prefix/v1 '' : v1 } ) save.restore ( sess , save_path )
__label__0 with ` expand_composites=false ` , we treat raggedtensor as a scalar .
__label__0 def testnestedcontrolflowserdes ( self ) : # test while loop in a cond in a while loop . # pylint : disable=g-long-lambda def body ( i , x ) : cond_result = cond.cond ( i > 0 , lambda : while_loop.while_loop ( lambda j , y : j < 3 , lambda j , y : ( j + 1 , y + x ) , [ 0 , 0.0 ] ) [ 1 ] , lambda : x ) return i + 1 , cond_result # pylint : enable=g-long-lambda self._testwhileloopandgradientserdes ( body )
__label__0 def testpreparesessiondidnotinitlocalvariable ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) with self.assertraisesregex ( runtimeerror , `` init operations did not make model ready . * '' ) : sm2.prepare_session ( `` '' , init_op=v.initializer )
__label__0 a ` concretefunction ` encapsulates the original graph function definition with support for differentiability under ` tf.gradienttape ` contexts . in the process , it may generate new graph functions ( using the original ) to efficiently perform forwards and backwards passes. `` '' ''
__label__0 def __init__ ( self ) : removedeprecatedaliaskeyword.__init__ ( self ) # note that these should be in the old order . self.function_reorders [ `` g '' ] = [ `` a '' , `` b '' , `` kw1 '' , `` c '' ] self.function_reorders [ `` g2 '' ] = [ `` a '' , `` b '' , `` kw1 '' , `` c '' , `` d '' ]
__label__0 def testisforwardref ( self ) : tp = typing.union [ ' b ' , int ] tp_args = type_annotations.get_generic_type_args ( tp ) self.asserttrue ( type_annotations.is_forward_ref ( tp_args [ 0 ] ) ) self.assertfalse ( type_annotations.is_forward_ref ( tp_args [ 1 ] ) )
__label__0 def should_use_result ( fn=none , warn_in_eager=false , error_in_function=false ) : `` '' '' function wrapper that ensures the function 's output is used .
__label__0 @ tf_export ( `` __internal__.dispatch.opdispatcher '' , v1= [ ] ) class opdispatcher ( object ) : `` '' '' abstract base class for tensorflow operator dispatchers .
__label__0 save_path = os.path.join ( self.get_temp_dir ( ) , `` metrics_save_restore '' ) # values at beginning of unit test . time_start = metrics.gettrainingtimesaved ( api_label=api_label ) num_writes_start = _get_write_histogram_proto ( ) .num num_reads_start = _get_read_histogram_proto ( ) .num
__label__0 def _testtypesforsparseadagrad ( self , x , y , lr , grad , indices , use_gpu ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 mt_combined_with_path = nest.map_structure_with_paths ( path_sum , mt , mt2 , mt3 ) self.assertisinstance ( mt_combined_with_path , maskedtensor ) # metadata uses the one from the first input ( mt ) . self.assertequal ( mt_combined_with_path.mask , false ) # tesnor index is ' 0 ' for the only compoenent in maskedtensor . self.assertallequal ( mt_combined_with_path.value [ 0 ] , `` 0 '' ) # sum of all input tensors . self.assertallequal ( mt_combined_with_path.value [ 1 ] , [ 6 ] )
__label__0 return _newclass else : decorator_utils.validate_callable ( func_or_class , 'deprecated ' )
__label__0 # chief should have already initialized all the variables . var_0_g_1 = graphs [ 1 ] .get_tensor_by_name ( `` v0:0 '' ) var_1_g_1 = graphs [ 1 ] .get_tensor_by_name ( `` v1:0 '' ) local_step_1 = graphs [ 1 ] .get_tensor_by_name ( `` sync_rep_local_step:0 '' ) global_step = graphs [ 1 ] .get_tensor_by_name ( `` global_step:0 '' )
__label__0 def test_contrib_rnn_function ( self ) : api_symbols = [ `` static_rnn '' , `` static_state_saving_rnn '' , `` static_bidirectional_rnn '' ] for symbol in api_symbols : text = `` tf.contrib.rnn . '' + symbol expected_text = `` tf.compat.v1.nn . '' + symbol _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 returns : sum of args. `` '' '' return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 returns : a ` supervisor ` .
__label__0 the new api is
__label__0 @ tf_export ( `` __internal__.nest.flatten_up_to '' , v1= [ ] ) def flatten_up_to ( shallow_tree , input_tree , check_types=true , expand_composites=false ) : `` '' '' flattens ` input_tree ` up to ` shallow_tree ` .
__label__0 args : fn : the function to wrap . warn_in_eager : whether to create warnings in eager as well . error_in_function : whether to raise an error when creating a tf.function .
__label__0 > > > tf.nest.assert_same_structure ( [ 0 , 1 ] , np.array ( [ 0 , 1 ] ) ) traceback ( most recent call last ) : ... valueerror : the two structures do n't have the same nested structure
__label__0 def _get_applicable_dict ( self , transformer_field , full_name , name ) : `` '' '' get all dict entries indexed by name that apply to full_name or name . '' '' '' # transformers are indexed to full name , name , or no name # as a performance optimization . function_transformers = getattr ( self._api_change_spec , transformer_field , { } )
__label__0 @ test_util.run_in_graph_and_eager_modes def testresourcesaverestorecachingdevice ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` resource_cache '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : v = resource_variable_ops.resourcevariable ( [ 1 ] , caching_device= '' /cpu:0 '' , name= '' v '' ) if context.executing_eagerly ( ) : sess = none else : self.evaluate ( variables.global_variables_initializer ( ) ) save = saver_module.saver ( [ v ] ) save.save ( sess , save_path )
__label__0 this only works for save_relative_paths=true. `` '' '' save_dir1 = os.path.join ( self.get_temp_dir ( ) , `` save_dir1 '' ) os.mkdir ( save_dir1 ) save_path1 = os.path.join ( save_dir1 , `` save_copy_restore '' )
__label__0 # generates metagraphdef . meta_graph_def = save.export_meta_graph ( filename ) self.asserttrue ( meta_graph_def.hasfield ( `` saver_def '' ) ) self.asserttrue ( meta_graph_def.hasfield ( `` graph_def '' ) ) self.asserttrue ( meta_graph_def.hasfield ( `` meta_info_def '' ) ) self.assertnotequal ( meta_graph_def.meta_info_def.tensorflow_version , `` '' ) self.assertnotequal ( meta_graph_def.meta_info_def.tensorflow_git_version , `` '' ) collection_def = meta_graph_def.collection_def self.assertequal ( len ( collection_def ) , 12 )
__label__0 _default_filename = '/tmp/test.txt '
__label__0 with sess.graph.device ( `` /cpu:1 '' ) : ds1 = dataset_ops.dataset.range ( 20 ) it1 = dataset_ops.make_initializable_iterator ( ds1 ) get_next1 = it1.get_next ( ) saveable1 = iterator_ops._iteratorsaveable ( it1._iterator_resource , name= '' saveable_it1 '' ) saver = saver_module.saver ( { `` it0 '' : saveable0 , `` it1 '' : saveable1 } , write_version=self._write_version , sharded=true ) self.evaluate ( it0.initializer ) self.evaluate ( it1.initializer ) self.assertequal ( 0 , self.evaluate ( get_next0 ) ) self.assertequal ( 1 , self.evaluate ( get_next0 ) ) self.assertequal ( 0 , self.evaluate ( get_next1 ) ) val = saver.save ( sess , save_path ) self.assertequal ( save_path , val ) data_files = glob.glob ( save_path + `` .data * '' ) self.assertequal ( 2 , len ( data_files ) )
__label__0 @ compatibility ( tf2 ) with the deprecation of global graphs , tf no longer tracks variables in collections . in other words , there are no global variables in tf2 . thus , the global step functions have been removed ( ` get_or_create_global_step ` , ` create_global_step ` , ` get_global_step ` ) . you have two options for migrating :
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests vlog printing in tensorflow . '' '' ''
__label__0 * ` set ` ; ` { `` a '' , `` b '' } ` is an atom , while ` [ `` a '' , `` b '' ] ` is a nested structure . * [ ` dataclass ` classes ] ( https : //docs.python.org/library/dataclasses.html ) that do n't implement the custom flattening/unflattening methods mentioned above . * ` tf.tensor ` . * ` numpy.array ` .
__label__0 @ tf_export ( `` nest.flatten '' ) def flatten ( structure , expand_composites=false ) : `` '' '' returns a flat list from a given structure .
__label__0 def check_existence ( filename ) : `` '' '' check the existence of file or dir . '' '' '' if not os.path.exists ( filename ) : raise runtimeerror ( `` % s not found . are you under the tensorflow source root '' `` directory ? '' % filename )
__label__0 library version env variable additional base directories -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- cuda tf_cuda_version cuda_toolkit_path cublas tf_cublas_version cuda_toolkit_path cudnn tf_cudnn_version cudnn_install_path nccl tf_nccl_version nccl_install_path , nccl_hdr_path tensorrt tf_tensorrt_version tensorrt_install_path
__label__0 def testraiseswithnoncallableobject ( self ) : with self.assertraises ( valueerror ) : function_utils.get_func_name ( none )
__label__0 if args.configure is not none : if args.gen_root_path is none : raise runtimeerror ( `` must pass -- gen_root_path arg when running -- configure '' ) configure ( args.configure , args.gen_root_path , debug=args.debug ) elif args.generate is not none : generate ( args.generate , args.git_tag_override ) elif args.raw_generate is not none : source_path = `` . '' if args.source_dir is not none : source_path = args.source_dir raw_generate ( args.raw_generate , source_path , args.git_tag_override ) else : raise runtimeerror ( `` -- configure or -- generate or -- raw_generate `` `` must be used '' )
__label__0 returns : a value of this type. `` '' '' del tensors return self.placeholder_value ( placeholdercontext ( ) )
__label__0 def _test_recovered_variable ( self , checkpoint_dir=none , checkpoint_filename_with_path=none ) : # create a new graph and sessionmanager and recover from a checkpoint . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 2 , name= '' v '' ) with session_lib.session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm2.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir , checkpoint_filename_with_path=checkpoint_filename_with_path ) self.asserttrue ( initialized ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) )
__label__0 # in the following use case , it 's possible to have restore_ops be called # something else : # - build inference graph and export a meta_graph . # - import the inference meta_graph # - extend the inference graph to a train graph . # - export a new meta_graph . # now the second restore_op will be called `` restore_all_1 '' . # as such , comment out the assert for now until we know whether supporting # such usage model makes sense . # # assert restore_op.name.endswith ( `` restore_all '' ) , restore_op.name if context.executing_eagerly ( ) : # store the tensor values to the tensor_names . save_tensor_name = save_tensor.numpy ( ) if build_save else `` '' return saver_pb2.saverdef ( filename_tensor_name=filename_tensor.numpy ( ) , save_tensor_name=save_tensor_name , restore_op_name= '' '' , max_to_keep=max_to_keep , sharded=sharded , keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours , version=self._write_version ) else : graph = ops.get_default_graph ( ) # do some sanity checking on collections containing # partitionedvariables . if a saved collection has a partitionedvariable , # the graphdef needs to include concat ops to get the value ( or there 'll # be a lookup error on load ) . check_collection_list = graph.get_all_collection_keys ( ) for collection_type in check_collection_list : for element in graph.get_collection ( collection_type ) : if isinstance ( element , variables.partitionedvariable ) : try : graph.get_operation_by_name ( element.name ) except keyerror : # create a concat op for this partitionedvariable . the user may # not need it , but we 'll try looking it up on metagraph restore # since it 's in a collection . element.as_tensor ( ) return saver_pb2.saverdef ( filename_tensor_name=filename_tensor.name , save_tensor_name=save_tensor.name , restore_op_name=restore_op.name , max_to_keep=max_to_keep , sharded=sharded , keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours , version=self._write_version )
__label__0 def _tf_core_yield_flat_up_to ( shallow_tree , input_tree , is_nested_fn , path= ( ) ) : `` '' '' yields ( path , value ) pairs of input_tree flattened up to shallow_tree .
__label__0 def _dropout_transformer ( parent , node , full_name , name , logs ) : `` '' '' replace keep_prob with 1-rate . '' '' '' def _replace_keep_prob_node ( parent , old_value ) : `` '' '' replaces old_value with 1- ( old_value ) . '' '' '' one = ast.num ( n=1 ) one.lineno = 0 one.col_offset = 0 new_value = ast.binop ( left=one , op=ast.sub ( ) , right=old_value ) # this copies the prefix and suffix on old_value to new_value . pasta.ast_utils.replace_child ( parent , old_value , new_value ) ast.copy_location ( new_value , old_value ) # put parentheses around keep_prob.value ( and remove the old prefix/ # suffix , they should only be around new_value ) . pasta.base.formatting.set ( old_value , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( old_value , `` suffix '' , `` ) '' )
__label__0 within the ` with sv.managed_session ( ) ` block all variables in the graph have been initialized . in addition , a few services have been started to checkpoint the model and add summaries to the event log .
__label__0 def test_keyword_args_only ( self ) :
__label__0 def testmanagedsession ( self ) : logdir = self._test_dir ( `` managed_session '' ) with ops.graph ( ) .as_default ( ) : my_op = constant_op.constant ( 1.0 ) sv = supervisor.supervisor ( logdir=logdir ) with sv.managed_session ( `` '' ) : for _ in range ( 10 ) : self.evaluate ( my_op ) # supervisor has been stopped . self.asserttrue ( sv.should_stop ( ) )
__label__0 text = `` tf.name_scope ( default_name=default_name , values=stuff ) '' expected_text = `` tf.name_scope ( name=default_name ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 warns if necessary .
__label__0 rocblas_config = { `` rocblas_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 the ` inputs ` , can be thought of as having the same structure layout as ` shallow_tree ` , but with leaf nodes that are themselves tree structures .
__label__0 non-sequence edge cases :
__label__0 # proto.repeated_field [ 55 ] .nested_map_bool [ true ] .string_field ret = util.get_field_tag ( proto , [ 2 , 55 , 7 , true , 3 ] ) self.assertlen ( ret , 5 ) self.assertequal ( 2 , ret [ 0 ] .field ) self.assertequal ( 55 , ret [ 1 ] .index ) self.assertequal ( 7 , ret [ 2 ] .field ) self.assertequal ( true , ret [ 3 ] .map_key.boolean ) self.assertequal ( 3 , ret [ 4 ] .field )
__label__0 slot_sharding = xla_sharding.get_tensor_sharding ( slot ) slot_proto = xla_data_pb2.opsharding ( ) slot_proto.parsefromstring ( slot_sharding ) self.assertequal ( slot_proto , xla_data_pb2.opsharding ( type=xla_data_pb2.opsharding.replicated ) )
__label__0 class sillycallableclass ( object ) :
__label__0 def set_device_filters ( self , job_name , task_index , device_filters ) : `` '' '' set the device filters for given job name and task id . '' '' '' assert all ( isinstance ( df , str ) for df in device_filters ) self._device_filters.setdefault ( job_name , { } ) self._device_filters [ job_name ] [ task_index ] = [ df for df in device_filters ] # due to updates in data , invalidate the serialized proto cache . self._cluster_device_filters = none
__label__0 returns : node , if it was modified , else none. `` '' '' # check whether arg is there . arg_present , arg_value = ast_edits.get_arg_value ( node , arg_name ) if not arg_present : return
__label__0 `` '' ''
__label__0 `` ` pip install git+https : //github.com/tensorflow/docs `` ` `` '' '' import contextlib import pathlib import textwrap from typing import namedtuple
__label__0 # run the graph and save scoped checkpoint . with graph1.as_default ( ) , self.session ( graph=graph1 ) as sess : self.evaluate ( variables.global_variables_initializer ( ) ) _ , var_list_1 = meta_graph.export_scoped_meta_graph ( export_scope= '' hidden1 '' ) saver = saver_module.saver ( var_list=var_list_1 , max_to_keep=1 ) saver.save ( sess , saver0_ckpt , write_state=false )
__label__0 parent = self._stack [ -2 ]
__label__0 def is_subtype_of ( self , other ) : # either the value is the same or other has a generalized value that # can represent any specific ones . return ( self.value == other.value ) or ( other.value is none ) `` ` `` '' ''
__label__0 class segmentreductionopstest ( xla_test.xlatestcase ) : `` '' '' test cases for segment reduction ops . '' '' ''
__label__0 @ property def log ( self ) : return self._log
__label__0 def get_current_semver_version ( ) : `` '' '' returns a version object of current version .
__label__0 def test_deprecated_illegal_args ( self ) : instructions = `` this is how you update ... '' date = `` 2016-07-04 '' with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated_args ( `` '' , instructions , `` deprecated '' ) with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated_args ( `` 07-04-2016 '' , instructions , `` deprecated '' ) with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated_args ( date , none , `` deprecated '' ) with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated_args ( date , `` '' , `` deprecated '' ) with self.assertraisesregex ( valueerror , `` argument '' ) : deprecation.deprecated_args ( date , instructions )
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testdefaultglobalstep ( self ) : logdir = self._test_dir ( `` default_global_step '' ) with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( 287 , name= '' global_step '' ) sv = supervisor.supervisor ( logdir=logdir ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertequal ( 287 , sess.run ( sv.global_step ) ) sv.stop ( )
__label__0 # this assertion is expected to fail : the shallow_tree component has # a deeper structure than the input_tree component . with self.assertraisesregex ( # pylint : disable=g-error-prone-assert-raises typeerror , `` if shallow structure is a sequence , input must also be a sequence '' , ) : nest.assert_shallow_structure ( shallow_tree=nmt , input_tree=mt , check_types=false )
__label__0 if isinstance ( obj , dtypes.dtype ) : return obj.name
__label__0 test_obj = callable ( ) self.assertequal ( argspec , tf_inspect.getfullargspec ( test_obj ) )
__label__0 args : sv : a ` supervisor ` . sess : a ` session ` . `` '' '' super ( svsummarythread , self ) .__init__ ( sv.coord , sv.save_summaries_secs ) self._sv = sv self._sess = sess
__label__0 def test_bound_method ( self ) :
__label__0 @ end_compatibility `` '' ''
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( 1 , 2 , true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 examples :
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_hash ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_hash : [ np.zeros ( [ 15 , 1 ] ) ] } , sess )
__label__0 # if building a tpu package , libtpu for cloud tpu vm can be installed via : # $ pip install < tf-tpu project > -f \ # https : //storage.googleapis.com/libtpu-releases/index.html # libtpu is built and uploaded to this link every night ( pst ) . if '_tpu ' in project_name : # for tensorflow-tpu releases , use a set libtpu version ; # for tf-nightly-tpu , use the most recent libtpu-nightly . because of the # timing of these tests , the utc date from eight hours ago is expected to be a # valid version . _libtpu_version = standard_or_nightly ( _version.replace ( '- ' , `` ) , ' 0.1.dev ' + ( datetime.datetime.now ( tz=datetime.timezone.utc ) - datetime.timedelta ( hours=8 ) ) .strftime ( ' % y % m % d ' ) , ) if _libtpu_version.startswith ( ' 0.1 ' ) : required_packages.append ( [ f'libtpu-nightly== { _libtpu_version } ' ] ) else : required_packages.append ( [ f'libtpu== { _libtpu_version } ' ] ) console_scripts.extend ( [ 'start_grpc_tpu_worker = tensorflow.python.tools.grpc_tpu_worker : run ' , ( 'start_grpc_tpu_service = ' 'tensorflow.python.tools.grpc_tpu_worker_service : run ' ) , ] )
__label__0 def get_next ( self ) : `` '' '' returns the next input from the iterator for all replicas .
__label__0 def testlazyloaddict ( self ) : # test that we can override and add fields to the wrapped module . module = mockmodule ( 'test ' ) apis = { 'cmd ' : ( `` , 'cmd ' ) } wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' , public_apis=apis , deprecation=false ) import cmd as _cmd # pylint : disable=g-import-not-at-top # at first cmd key does not exist in __dict__ self.assertnotin ( 'cmd ' , wrapped_module.__dict__ ) # after it is referred ( lazyloaded ) , it gets added to __dict__ wrapped_module.cmd # pylint : disable=pointless-statement self.assertequal ( wrapped_module.__dict__ [ 'cmd ' ] , _cmd ) # when we call setattr , it also gets added to __dict__ setattr ( wrapped_module , 'cmd2 ' , _cmd ) self.assertequal ( wrapped_module.__dict__ [ 'cmd2 ' ] , _cmd )
__label__0 if len ( input_tree ) ! = len ( shallow_tree ) : raise valueerror ( `` the two structures do n't have the same sequence length . input `` f '' structure has length { len ( input_tree ) } , while shallow structure `` f '' has length { len ( shallow_tree ) } . '' )
__label__0 cell [ `` source '' ] = `` \n '' .join ( new_code ) .replace ( `` # # # ! ! ! `` , `` '' ) .replace ( `` # # # === '' , `` \n '' ) code_cell_idx += 1
__label__0 raises : ` typeerror ` : if fn is not a function , or function-like object. `` '' '' if isinstance ( fn , functools.partial ) : fn = fn.func elif _is_callable_object ( fn ) : fn = fn.__call__ elif not callable ( fn ) : raise typeerror ( 'argument ` fn ` should be a callable . ' f'received : fn= { fn } ( of type { type ( fn ) } ) ' ) return tf_inspect.getfullargspec ( fn ) .varkw is not none
__label__0 def keys ( self ) : return self._export ( ) [ 0 ]
__label__0 @ dispatch.dispatch_for_api ( api_without_dispatch_support , { `` x '' : maskedtensor } ) def my_version ( x ) : # pylint : disable=unused-variable del x
__label__0 def testexportmultiplefunctions ( self ) : export_decorator1 = tf_export.tf_export ( 'namea ' , 'nameb ' ) export_decorator2 = tf_export.tf_export ( 'namec ' , 'named ' ) decorated_function1 = export_decorator1 ( self._test_function ) decorated_function2 = export_decorator2 ( self._test_function2 ) self.assertequal ( decorated_function1 , self._test_function ) self.assertequal ( decorated_function2 , self._test_function2 ) self.assertequal ( ( 'namea ' , 'nameb ' ) , decorated_function1._tf_api_names ) self.assertequal ( ( 'namec ' , 'named ' ) , decorated_function2._tf_api_names ) self.assertequal ( tf_export.get_symbol_from_name ( 'nameb ' ) , decorated_function1 ) self.assertequal ( tf_export.get_symbol_from_name ( 'named ' ) , decorated_function2 ) self.assertequal ( tf_export.get_symbol_from_name ( tf_export.get_canonical_name_for_symbol ( decorated_function1 ) ) , decorated_function1 ) self.assertequal ( tf_export.get_symbol_from_name ( tf_export.get_canonical_name_for_symbol ( decorated_function2 ) ) , decorated_function2 )
__label__0 `` ` python flatten_up_to ( 0 , 0 ) # output : [ 0 ] flatten_up_to ( 0 , [ 0 , 1 , 2 ] ) # output : [ [ 0 , 1 , 2 ] ] flatten_up_to ( [ 0 , 1 , 2 ] , 0 ) # output : typeerror flatten_up_to ( [ 0 , 1 , 2 ] , [ 0 , 1 , 2 ] ) # output : [ 0 , 1 , 2 ] `` `
__label__0 this splitter will modify the passed in proto in place. `` '' ''
__label__0 return tf_decorator.make_decorator ( target , wrapper )
__label__0 args : last_checkpoints_with_time : a list of tuples of checkpoint filenames and timestamps .
__label__0 @ property def summary_op ( self ) : `` '' '' return the summary tensor used by the chief supervisor .
__label__0 def testexportclasses ( self ) : export_decorator_a = tf_export.tf_export ( 'testclassa1 ' ) export_decorator_a ( self._test_class_a ) self.assertequal ( ( 'testclassa1 ' , ) , self._test_class_a._tf_api_names ) self.assertnotin ( '_tf_api_names ' , self._test_class_b.__dict__ )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for v1 vs v2 api comparison . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 `` `` ''
__label__0 def is_repeated ( field_desc : descriptor.fielddescriptor ) - > bool : return field_desc.label == descriptor.fielddescriptor.label_repeated
__label__0 @ dataclasses.dataclass class maskedtensor : mask : bool value : tensor.tensor
__label__0 class sequenceexample ( typing.namedtuple ) : context : dict [ str , feature ] feature_lists : dict [ str , list [ feature ] ] `` `
__label__0 def testinitializers ( self ) : initializers = [ `` zeros '' , `` ones '' , `` constant '' , `` random_uniform '' , `` random_normal '' , `` truncated_normal '' , `` variance_scaling '' , `` orthogonal '' , `` glorot_uniform '' , `` glorot_normal '' , `` identity '' , `` lecun_normal '' , `` lecun_uniform '' , `` he_normal '' , `` he_uniform '' , ] self.verify_compat_v1_rename_correctness ( initializers , ns_prefix= '' initializers '' )
__label__0 def testvariablenotfounderrorraised ( self ) : # restore does some tricky exception handling to figure out if it should # load an object-based checkpoint . tests that the exception handling is n't # too broad . checkpoint_directory = self.get_temp_dir ( ) checkpoint_prefix = os.path.join ( checkpoint_directory , `` ckpt '' )
__label__0 myclass ( `` '' ) self.assertequal ( 1 , mock_warning.call_count ) myclass ( `` '' ) self.assertequal ( 1 , mock_warning.call_count ) self.assertin ( `` is deprecated '' , myclass.__doc__ )
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertfalse ( initialized ) sess.run ( v.initializer ) self.assertequal ( 1 , sess.run ( v ) ) saver.save ( sess , os.path.join ( checkpoint_dir , `` recover_session_checkpoint '' ) ) self._test_recovered_variable ( checkpoint_dir=checkpoint_dir ) self._test_recovered_variable ( checkpoint_filename_with_path=checkpoint_management.latest_checkpoint ( checkpoint_dir ) ) # can not set both checkpoint_dir and checkpoint_filename_with_path . with self.assertraises ( valueerror ) : self._test_recovered_variable ( checkpoint_dir=checkpoint_dir , checkpoint_filename_with_path=checkpoint_management.latest_checkpoint ( checkpoint_dir ) )
__label__0 update = opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 this is the inverse of the ` nest.pack_sequence_as ` method that takes in a flattened list and re-packs it into the nested structure .
__label__0 def test_get_field_and_desc ( self ) : proto = test_message_pb2.manyfields ( field_one=test_message_pb2.manyfields ( repeated_field= [ test_message_pb2.manyfields ( ) , test_message_pb2.manyfields ( string_field= '' inner_inner_string '' , map_field_uint32= { 324 : `` map_value_324 '' , 543 : `` map_value_543 '' , } , ) , ] ) , map_field_int64= { -1345 : `` map_value_-1345 '' , } , nested_map_bool= { true : test_message_pb2.manyfields ( string_field= '' string_true '' ) , false : test_message_pb2.manyfields ( string_field= '' string_false '' ) , } , )
__label__0 s3 = save.save ( none , os.path.join ( save_dir , `` s3 '' ) ) self.assertequal ( [ s2 , s3 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertcheckpointstate ( model_checkpoint_path=s3 , all_model_checkpoint_paths= [ s2 , s3 ] , save_dir=save_dir )
__label__0 # save checkpoint from which to warm-start . _ , prev_vocab_val = self._create_prev_run_var ( `` linear_model/sc_vocab/weights '' , shape= [ 4 , 1 ] , initializer=ones ( ) )
__label__0 def transformgraph ( input_graph_def , inputs , outputs , transforms ) : `` '' '' python wrapper for the graph transform tool .
__label__0 args : shallow_tree : a shallow structure , common to all the inputs . func : callable that takes args ( path , inputs_0_value , ... , inputs_n_value ) , where path is a tuple path to an atom in shallow_tree , and inputs_i_value is the corresponding value from inputs [ i ] . * inputs : structures that are all structurally compatible with shallow_tree . * * kwargs : kwargs to feed to func ( ) . special kwarg ` check_types ` is not passed to func , but instead determines whether the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` .
__label__0 def testsegmentprodnumsegmentsmore ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 0 , 1 , 2 , 60 , 1 ] , dtype=dtype ) , self._segmentprodv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 5 ) )
__label__0 class lazyloadertest ( test.testcase ) :
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor , `` y '' : maskedtensor } ) def masked_add ( x , y , name=none ) : with ops.name_scope ( name ) : return maskedtensor ( x.values + y.values , x.mask & y.mask )
__label__0 graph1 = ops_lib.graph ( ) with graph1.as_default ( ) : var_dict1 = meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden1 '' , to_scope= '' new_hidden1 '' , from_graph=graph , to_graph=graph1 ) self.assertequal ( 1 , len ( var_dict1 ) )
__label__0 sess_old.close ( ) sess_new.close ( )
__label__0 > > > s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > s1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] > > > tf.nest.map_structure ( lambda x , y : none , s1 , s1_list ) traceback ( most recent call last ) : ... typeerror : the two structures do n't have the same nested structure
__label__0 the ` saver ` class adds ops to save and restore variables to and from * checkpoints * . it also provides convenience methods to run these ops .
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] structure : an atom or a nested structure . note , numpy arrays are considered atoms and are not flattened . expand_composites : arg valid for modality.core only . if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 args : threads : optional list of threads to join with the coordinator . if ` none ` , defaults to the threads running the standard services , the threads started for ` queuerunners ` , and the threads started by the ` loop ( ) ` method . to wait on additional threads , pass the list in this parameter . close_summary_writer : whether to close the ` summary_writer ` . defaults to ` true ` if the summary writer was created by the supervisor , ` false ` otherwise . ignore_live_threads : if ` true ` ignores threads that remain running after a grace period when joining threads via the coordinator , instead of raising a runtimeerror. `` '' '' self._coord.request_stop ( ) try : # coord.join ( ) re-raises the first reported exception ; the `` finally '' # block ensures that we clean up whether or not an exception was # reported . self._coord.join ( threads , stop_grace_period_secs=self._stop_grace_secs , ignore_live_threads=ignore_live_threads ) finally : # close the writer last , in case one of the running threads was using it . if close_summary_writer and self._summary_writer : # stop messages are not logged with event.step , # since the session may have already terminated . self._summary_writer.add_session_log ( sessionlog ( status=sessionlog.stop ) ) self._summary_writer.close ( ) self._graph_added_to_summary = false
__label__0 @ tf_export ( v1= [ `` train.saver '' ] ) class saver : # pylint : disable=line-too-long `` '' '' saves and restores variables .
__label__0 warmstarted_count = 0
__label__0 # non-equal dicts . inp_val = dict ( a=2 , b=3 ) inp_ops = dict ( a=dict ( add=1 , mul=2 ) , c=dict ( add=2 , mul=3 ) ) with self.assertraiseswithliteralmatch ( valueerror , nest.shallow_tree_has_invalid_keys.format ( [ `` b '' ] ) ) : nest.map_structure_up_to ( inp_val , lambda val , ops : ( val + ops [ `` add '' ] ) * ops [ `` mul '' ] , inp_val , inp_ops )
__label__0 example : `` ` python from tensorflow.python.util import tf_export cls = tf_export.get_symbol_from_name ( 'keras.optimizers.adam ' )
__label__0 lineno = node.func.value.lineno col_offset = node.func.value.col_offset node.func.value = ast_edits.full_name_node ( `` tf.compat.v1.keras.initializers '' ) node.func.value.lineno = lineno node.func.value.col_offset = col_offset node.func.attr = `` variancescaling '' return node
__label__0 x_ref1 = reference ( x ) x_ref2 = reference ( x ) y_ref2 = reference ( y )
__label__0 - if a feature ` k ` exists in one example with data type ` t ` , it must be of type ` t ` in all other examples when present . it may be omitted . - the number of instances of feature ` k ` list data may vary across examples , depending on the requirements of the model . - if a feature ` k ` does n't exist in an example , a ` k ` -specific default will be used , if configured . - if a feature ` k ` exists in an example but contains no items , the intent is considered to be an empty tensor and no default will be used .
__label__0 @ property def session ( self ) : `` '' '' a tensorflow session object which will execute the ` run ` . '' '' '' return self._session
__label__0 def is_generic_list ( tp ) : `` '' '' returns true if ` tp ` is a parameterized typing.list value . '' '' '' return ( tp not in ( list , typing.list ) and getattr ( tp , '__origin__ ' , none ) in ( list , typing.list ) )
__label__0 def __call__ ( self , * * x ) : del x self.asserttrue ( function_utils.has_kwargs ( foohaskwargs ( ) ) )
__label__0 def testregisterdispatchabletype ( self ) : car = collections.namedtuple ( `` car '' , [ `` size '' , `` speed '' ] ) dispatch.register_dispatchable_type ( car )
__label__0 self.generic_visit ( node )
__label__0 def ismethod ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.ismethod . '' '' '' return _inspect.ismethod ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 self.assertis ( override , override_for_test_op )
__label__0 saver_list2 = graph2.get_collection ( ops_lib.graphkeys.savers ) self.assertequal ( 1 , len ( saver_list2 ) )
__label__0 this helps to avoid circular dependencies. `` '' ''
__label__0 # pylint : enable=protected-access
__label__0 @ test_tfdecorator ( 'decorator 1 ' ) @ test_decorator_increment_first_int_arg @ test_tfdecorator ( 'decorator 3 ' , 'decorator 3 documentation ' ) def test_decorated_function ( x ) : `` '' '' test decorated function docstring . '' '' '' return x * 2
__label__0 returns true for two empty lists , two numeric values with the same value , etc .
__label__0 # make sure dicts are correctly flattened , yielding values , not keys . input_tree = { `` a '' : 1 , `` b '' : { `` c '' : 2 } , `` d '' : [ 3 , ( 4 , 5 ) ] } shallow_tree = { `` a '' : 0 , `` b '' : 0 , `` d '' : [ 0 , 0 ] } input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ 1 , { `` c '' : 2 } , 3 , ( 4 , 5 ) ] )
__label__0 so_lib_paths = [ i for i in os.listdir ( ' . ' ) if os.path.isdir ( i ) and fnmatch.fnmatch ( i , '_solib_ * ' ) ]
__label__0 args : obj : typealias object that needs to be documented . doc : docstring of the typealias . it should follow the standard pystyle docstring rules. `` '' '' try : obj.__doc__ = doc except attributeerror : _extra_docs [ id ( obj ) ] = doc
__label__0 def __model_class1_method__ ( self ) : pass
__label__0 return _tf_core_pack_sequence_as ( structure [ 0 ] , [ func ( * x ) for x in entries ] , expand_composites=expand_composites , )
__label__0 for dtype in self.int_types | self.float_types : minval = dtypes.as_dtype ( dtype ) .min maxval = dtypes.as_dtype ( dtype ) .max
__label__0 class element ( object ) : pass
__label__0 @ function.defun ( dtypes.float32 ) def minus_one ( x ) : return x - 1
__label__0 class constantnodedefsplitter ( splitbasedonsize ) : `` '' '' extracts constant value from a ` const ` nodedef . '' '' ''
__label__0 def testeagergraphcompatibility ( self ) : # save from graph mode and restore from eager mode . graph_ckpt_prefix = os.path.join ( self.get_temp_dir ( ) , `` graph_ckpt '' ) with context.graph_mode ( ) : with self.session ( graph=ops_lib.graph ( ) ) as sess : # create a graph model and save the checkpoint . w1 = resource_variable_ops.resourcevariable ( 1.0 , name= '' w1 '' ) w2 = resource_variable_ops.resourcevariable ( 2.0 , name= '' w2 '' ) graph_saver = saver_module.saver ( [ w1 , w2 ] ) self.evaluate ( variables.global_variables_initializer ( ) ) graph_saver.save ( sess , graph_ckpt_prefix )
__label__0 def sate ( self ) : self._sated = true self._type = none self._repr = none self._stack_frame = none self._logging_module = none
__label__0 def _find_file ( base_paths , relative_paths , filepattern ) : for path in _cartesian_product ( base_paths , relative_paths ) : for file in glob.glob ( os.path.join ( path , filepattern ) ) : return file raise _not_found_error ( base_paths , relative_paths , filepattern )
__label__0 zip_ref.extractall ( ) zip_ref.close ( ) old_py_ver = re.search ( r '' ( cp\d\d-cp\d\d ) '' , origin_tag ) .group ( 1 ) new_py_ver = re.search ( r '' ( cp\d\d-cp\d\d ) '' , new_tag ) .group ( 1 )
__label__0 logging.info ( `` waiting for model to be ready. `` `` ready_for_local_init_op : % s , ready : % s '' , not_ready_local_msg , not_ready_msg ) time.sleep ( self._recovery_wait_secs )
__label__0 from google.cloud import datastore
__label__0 def func ( a , b ) : del a , b
__label__0 returns : an integer tensor for the global_step. `` '' '' return self._global_step
__label__0 # # tensorflow 1.x and 2.x apis
__label__0 def __init__ ( self ) : self.hash_value = 8675309
__label__0 in combination with ` export_meta_graph ( ) ` , this function can be used to
__label__0 import traceback
__label__0 entry % a_inference_f_13__.9 ( arg0.1 : f32 [ 10,10 ] ) - > f32 [ 10,10 ] { % arg0.1 = f32 [ 10,10 ] { 1,0 } parameter ( 0 ) , parameter_replication= { false } % reshape.2 = f32 [ 10,10 ] { 1,0 } reshape ( f32 [ 10,10 ] { 1,0 } % arg0.1 ) % constant.3 = f32 [ ] constant ( 1 ) % broadcast.4 = f32 [ 10,10 ] { 1,0 } broadcast ( f32 [ ] % constant.3 ) % add.5 = f32 [ 10,10 ] { 1,0 } add ( f32 [ 10,10 ] { 1,0 } % reshape.2 , f32 [ 10,10 ] { 1,0 } % broadcast.4 ) % reshape.6 = f32 [ 10,10 ] { 1,0 } reshape ( f32 [ 10,10 ] { 1,0 } % add.5 ) % tuple.7 = ( f32 [ 10,10 ] { 1,0 } ) tuple ( f32 [ 10,10 ] { 1,0 } % reshape.6 ) root % get-tuple-element.8 = f32 [ 10,10 ] { 1,0 } get-tuple-element ( ( f32 [ 10,10 ] { 1,0 } ) % tuple.7 ) , index=0 } `` `
__label__0 raises : typeerror : if ` _device_filters ` is not a dictionary mapping strings to a map of task indices and device filters. `` '' '' self._cluster_device_filters = device_filters_pb2.clusterdevicefilters ( )
__label__0 def testparseerror ( self ) : _ , report , unused_errors , unused_new_text = self._upgrade ( `` import tensorflow as tf\na + \n '' ) self.assertnotequal ( report.find ( `` failed to parse '' ) , -1 )
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` apple '' , `` banana '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_output_layer = variable_scope.get_variable ( `` fruit_output_layer '' , shape= [ 4 , 3 ] , initializer= [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) ws_util._warm_start_var_with_vocab ( fruit_output_layer , new_vocab_path , current_vocab_size=3 , prev_ckpt=self.get_temp_dir ( ) , prev_vocab_path=prev_vocab_path , axis=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.asserttrue ( isinstance ( fruit_output_layer , variables.partitionedvariable ) ) fruit_output_layer_vars = fruit_output_layer._get_variable_list ( ) self.assertallclose ( [ [ 0.3 , 0.5 , 0 . ] , [ 0.8 , 1.0 , 0 . ] ] , fruit_output_layer_vars [ 0 ] .eval ( sess ) ) self.assertallclose ( [ [ 1.2 , 1.5 , 0 . ] , [ 2.3 , 2. , 0 . ] ] , fruit_output_layer_vars [ 1 ] .eval ( sess ) )
__label__0 text = `` from foo import baz as a '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 shape1 = fh.get_int_list ( min_length=0 , max_length=8 , min_int=0 , max_int=8 ) shape2 = fh.get_int_list ( min_length=0 , max_length=8 , min_int=0 , max_int=8 ) shape3 = fh.get_int_list ( min_length=0 , max_length=8 , min_int=0 , max_int=8 ) shape4 = fh.get_int_list ( min_length=0 , max_length=8 , min_int=0 , max_int=8 )
__label__0 # s1 should still be here , we are not checking now to reduce time # variance in the test .
__label__0 savespec = saveable_object.savespec saveableobject = saveable_object.saveableobject
__label__0 returns : global step read tensor if there is global_step_tensor else return none. `` '' '' graph = graph or ops.get_default_graph ( ) global_step_read_tensor = _get_global_step_read ( graph ) if global_step_read_tensor is not none : return global_step_read_tensor global_step_tensor = get_global_step ( graph ) if global_step_tensor is none : return none # add 'zero ' so that it will create a copy of variable as tensor . with graph.as_default ( ) as g , g.name_scope ( none ) : with g.name_scope ( global_step_tensor.op.name + '/ ' ) : # must ensure that global_step is initialized before this run . if isinstance ( global_step_tensor , variables.variable ) : global_step_value = cond.cond ( variable_v1.is_variable_initialized ( global_step_tensor ) , global_step_tensor.read_value , lambda : global_step_tensor.initial_value ) else : global_step_value = global_step_tensor
__label__0 def symbol_collector_v1 ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) api_names_v1 = tf_export.get_v1_names ( attr ) for name in api_names_v1 : cls.v1_symbols [ `` tf . '' + name ] = attr
__label__0 self.assertin ( test_module1.moduleclass1 , called ) self.assertin ( test_module2.moduleclass2 , called )
__label__0 this definition passes static type verification for :
__label__0 returned dict is keyed by argument name . each value is a deprecatedargspec with the following fields : position : the zero-based argument position of the argument within the signature . none if the argument is n't found in the signature . ok_values : values of this argument for which warning will be suppressed .
__label__0 if self._variable_averages is not none : with ops.control_dependencies ( [ sync_op ] ) , ops.name_scope ( `` '' ) : sync_op = self._variable_averages.apply ( self._variables_to_average )
__label__0 def testremovedeprecatedkeywordalias ( self ) : `` '' '' test that we get the expected result if a keyword alias is removed . '' '' '' text = `` g ( a , b , kw1=x , c=c ) \n '' acceptable_outputs = [ # not using deprecated alias , so original is ok text , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 def _random_string ( length ) : return bytes ( `` '' .join ( random.choices ( string.ascii_lowercase , k=length ) ) , encoding= '' utf-8 '' , )
__label__0 self.float_size_good = ( self.want_floats.size == self.got_floats.size )
__label__0 a dense tensor has a static data type ( dtype ) , and may have a static rank and shape . tensor objects are immutable . mutable objects may be backed by a tensor which holds the unique handle that identifies the mutable object. `` '' ''
__label__0 class checkpointreaderforv2test ( checkpointreadertest ) : _write_version = saver_pb2.saverdef.v2
__label__0 returns : a ` unicode ` ( python 2 ) or ` str ` ( python 3 ) object .
__label__0 returns : a decorator .
__label__0 5 . ` tf.tensor ` ( will not flatten ) :
__label__0 # remove things that are not visible . for name , child in list ( children ) : if self._is_private ( full_path , name , child ) : children.remove ( ( name , child ) )
__label__0 `` ` python input_tree = [ [ ( ' a ' , 1 ) , [ ( ' b ' , 2 ) , [ ( ' c ' , 3 ) , [ ( 'd ' , 4 ) ] ] ] ] ] shallow_tree = [ [ 'level_1 ' , [ 'level_2 ' , [ 'level_3 ' , [ 'level_4 ' ] ] ] ] ]
__label__0 list : * first
__label__0 this context manager creates and automatically recovers a session . it optionally starts the standard services that handle checkpoints and summaries . it monitors exceptions raised from the ` with ` block or from the services and stops the supervisor as needed .
__label__0 # clean up test_op._tf_fallback_dispatchers = original_handlers
__label__0 class foo ( object ) : foo = tf_decorator.tfdecorator ( 'descr ' , descr ( ) )
__label__0 with open ( report_filename , `` w '' ) as report_file : report_file.write ( report ) report_file.write ( detailed_report_header ) report_file.write ( report_text )
__label__0 # skip over the serialized input , and the names input . fetch_list = parse_example_op.inputs [ 2 : ]
__label__0 file_count = 0 tree_errors = { } report = `` '' report += ( `` = '' * 80 ) + `` \n '' report += `` input tree : % r\n '' % root_directory report += ( `` = '' * 80 ) + `` \n ''
__label__0 def testsetsdecoratornametofunctionthatcallsmakedecoratorifabsent ( self ) :
__label__0 # verify non-duplicate names work . saver_module.saver ( { `` v0 '' : v0 , `` v2 '' : v2.saveable } )
__label__0 text = ( contrib_alias + `` layers.xavier_initializer_conv2d ( `` `` false , 12 , tf.float32 ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution= ( \ '' uniform\ '' if false else \ '' truncated_normal\ '' ) , `` `` seed=12 , `` `` dtype=tf.float32 ) \n '' , )
__label__0 the order of pairs produced matches that of ` nest.flatten ` . this allows you to flatten a nested structure while keeping information about where in the structure each atom was located . see ` nest.yield_flat_paths ` for more information about tuple paths .
__label__0 for other in structure [ 1 : ] : _tf_core_assert_same_structure ( structure [ 0 ] , other , check_types=check_types , expand_composites=expand_composites , )
__label__0 return nest.map_structure ( _replace_resource_variable_with_atom , values )
__label__0 else : # there is no version info available before cuda 10.1 , just find the file . header_version = cuda_version header_path = _find_file ( base_paths , _header_paths ( ) , `` cublas_api.h '' ) # cublas version is the same as cuda version ( x.y ) . cublas_version = required_version
__label__0 unflattened = nest.pack_sequence_as ( structure_of_mess , flattened ) self.assertequal ( unflattened , mess )
__label__0 args : global_step : an integer tensor of size 1 that counts steps . if set to use_default , creates global_step tensor. `` '' '' if global_step is supervisor.use_default : global_step = self._get_first_op_from_collection ( ops.graphkeys.global_step ) if global_step is none : global_step = self._default_global_step_tensor ( ) if global_step is not none : ops.add_to_collection ( ops.graphkeys.global_step , global_step ) self._global_step = global_step
__label__0 with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = resource_variable_ops.resourcevariable ( 10.0 , name= '' v0 '' ) v1 = resource_variable_ops.resourcevariable ( 20.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) # initialize all variables if not context.executing_eagerly ( ) : self.evaluate ( [ variables.global_variables_initializer ( ) ] )
__label__0 returns : a list of checkpoint filenames , sorted from oldest to newest. `` '' '' return list ( self._checkpointfilename ( p ) for p in self._last_checkpoints )
__label__0 * the wrong number of floats are found . * the float values are not within tolerence .
__label__0 `` ` result `` ` `` '' '' ) , ( 'skip-unlabeled ' , [ ] , `` '' '' hello
__label__0 def testgetsourcefile ( self ) : self.assertequal ( __file__ , tf_inspect.getsourcefile ( test_decorated_function_with_defaults ) )
__label__0 from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.framework import tensor from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops.ragged import ragged_tensor from tensorflow.python.platform import test from tensorflow.python.util import nest from tensorflow.python.util.nest_util import customnestprotocol
__label__0 > > > @ add_dispatch_support ... def double ( x ) : ... return x * 2 > > > class maskedtensor ( tf.experimental.extensiontype ) : ... values : tf.tensor ... mask : tf.tensor > > > @ dispatch_for_api ( double , { ' x ' : maskedtensor } ) ... def masked_double ( x ) : ... return maskedtensor ( x.values * 2 , y.mask )
__label__0 # make sure we do not import from tensorflow/lite/__init__.py if name == 'lite ' : if self._tfmw_has_lite : attr = self._tfmw_import_module ( name ) setattr ( self._tfmw_wrapped_module , 'lite ' , attr ) func__fastdict_insert ( name , attr ) return attr # placeholder for google-internal contrib error
__label__0 # create a variable in graph_2 under scope `` my_scope '' . variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' my_scope/my_var '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # restore the checkpoint into a different scope `` subgraph_2 '' . new_saver_2 = saver_module.import_meta_graph ( filename + `` .meta '' , graph=graph_2 , import_scope= '' subgraph_2 '' ) # because the variable does not live in scope `` subgraph_2 '' , # import_meta_graph should not attempt to restore the variable . so , # import_meta_graph still wo n't return a saver instance . self.assertisnone ( new_saver_2 )
__label__0 def is_nested ( modality , structure ) : `` '' '' returns true if its input is a nested structure .
__label__0 # proto.field_one.repeated_field [ 15 ] .map_field_uint32 [ 10 ] ret = util.get_field_tag ( proto , [ `` field_one '' , `` repeated_field '' , 15 , `` map_field_uint32 '' , 10 ] ) self.assertlen ( ret , 5 ) self.assertequal ( 1 , ret [ 0 ] .field ) self.assertequal ( 2 , ret [ 1 ] .field ) self.assertequal ( 15 , ret [ 2 ] .index ) self.assertequal ( 5 , ret [ 3 ] .field ) self.assertequal ( 10 , ret [ 4 ] .map_key.ui32 ) self.assertfalse ( ret [ 4 ] .map_key.hasfield ( `` i32 '' ) )
__label__0 visitor = public_api.publicapivisitor ( arg_test_visitor ) visitor.do_not_descend_map [ `` tf '' ] .append ( `` contrib '' ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v1 , visitor )
__label__0 this splitter writes to the riegeli file format .
__label__0 def _get_duration_microseconds ( start_time_seconds , end_time_seconds ) : if end_time_seconds < start_time_seconds : # avoid returning negative value in case of clock skew . return 0 return round ( ( end_time_seconds - start_time_seconds ) * 1000000 )
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import keyword_args
__label__0 add_contrib_direct_import_support ( addons_symbol_mappings )
__label__0 use case :
__label__0 def testattacheswrappedattr ( self ) : decorated = tf_decorator.make_decorator ( test_function , test_wrapper ) wrapped_attr = getattr ( decorated , '__wrapped__ ' ) self.assertis ( test_function , wrapped_attr )
__label__0 args : in_filename : filename to parse out_filename : output file to write to no_change_to_outfile_on_error : not modify the output file on errors returns : a tuple representing number of files processed , log of actions , errors `` '' ''
__label__0 raises : valueerror : if the two structures do not have the same number of atoms or if the two structures are not nested in the same way . typeerror : if the two structures differ in the type of sequence in any of their substructures . only possible if ` check_types ` is ` true ` . `` '' '' if modality == modality.core : _tf_core_assert_same_structure ( nest1 , nest2 , check_types , expand_composites ) elif modality == modality.data : _tf_data_assert_same_structure ( nest1 , nest2 , check_types ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 def request_stop ( self , ex=none ) : `` '' '' request that the coordinator stop the threads .
__label__0 3. as input into ` run ` :
__label__0 # generates metagraphdef . saver_module.export_meta_graph ( train_filename )
__label__0 elif _is_type_spec ( shallow_tree ) : if not _is_type_spec ( input_tree ) : raise typeerror ( `` if shallow structure is a typespec , input must also `` `` be a typespec . input has type : % s . '' % type ( input_tree ) ) else : if len ( input_tree ) ! = len ( shallow_tree ) : raise valueerror ( structures_have_mismatching_lengths.format ( input_length=len ( input_tree ) , shallow_length=len ( shallow_tree ) ) ) elif len ( input_tree ) < len ( shallow_tree ) : raise valueerror ( input_tree_smaller_than_shallow_tree.format ( input_size=len ( input_tree ) , shallow_size=len ( shallow_tree ) ) )
__label__1 class solution : def longestincreasingpath ( self , matrix : list [ list [ int ] ] ) - > int : if not matrix : return 0 m , n = len ( matrix ) , len ( matrix [ 0 ] ) memo = [ [ none ] * n for _ in range ( m ) ] max_length = 0 def dfs ( i , j ) : if memo [ i ] [ j ] is not none : return memo [ i ] [ j ] directions = [ ( 0 , 1 ) , ( 0 , -1 ) , ( 1 , 0 ) , ( -1 , 0 ) ] max_path = 1 for dx , dy in directions : x , y = i + dx , j + dy if 0 < = x < m and 0 < = y < n and matrix [ x ] [ y ] > matrix [ i ] [ j ] : max_path = max ( max_path , 1 + dfs ( x , y ) ) memo [ i ] [ j ] = max_path return max_path for i in range ( m ) : for j in range ( n ) : max_length = max ( max_length , dfs ( i , j ) ) return max_length
__label__0 contrib_collective_strategy_warning = ( ast_edits.error , `` ( manual edit required ) `` `` tf.contrib.distribute.collectiveallreducestrategy has `` `` been migrated to `` `` tf.distribute.experimental.multiworkermirroredstrategy . note the `` `` changes in constructor. `` + distribute_strategy_api_changes )
__label__0 # all conditions met , insert v1 and log what we did . # we must have a full name , so the func is an attribute . new_name = full_name.replace ( `` tf . `` , `` tf.compat.v1 . `` , 1 ) node.func = ast_edits.full_name_node ( new_name ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` renaming % s to % s because argument % s is present . % s '' % ( full_name , new_name , arg_name , message if message is not none else `` '' ) ) ) return node
__label__0 @ classmethod def _overload_all_operators ( cls ) : # pylint : disable=invalid-name `` '' '' register overloads for all operators . '' '' '' for operator in tensor_lib.tensor.overloadable_operators : cls._overload_operator ( operator )
__label__0 to create a ` tf.compat.v1.session ` that connects to this server , use the following snippet :
__label__0 import os import pathlib import shutil import types from unittest import mock
__label__0 > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( 'bar ' , ' a b ' ) ( 1 , 2 ) , ... collections.namedtuple ( 'foo ' , ' a b ' ) ( 2 , 3 ) ) traceback ( most recent call last ) : ... typeerror : the two structures do n't have the same nested structure
__label__0 # # # dispatch support
__label__0 def testoneinput ( data ) : `` '' '' test randomized fuzzing input for tf.raw_ops.acos . '' '' '' fh = fuzzinghelper ( data )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this pip smoke test verifies dependency files exist in the pip package .
__label__0 # ditto for custom saveables . with self.assertraisesregex ( valueerror , `` the same saveable will be restored with two names : v2 '' ) : saver_module.saver ( { `` v2 '' : v2.saveable , `` v2too '' : v2.saveable } )
__label__0 x = customtensor ( [ 1 , 2 , 3 ] , 0.2 ) y = customtensor ( [ 7 , 8 , 2 ] , 0.4 ) z = customtensor ( [ 0 , 1 , 2 ] , 0.6 )
__label__0 > > > class maskedtensor ( tf.experimental.extensiontype ) : ... values : tf.tensor ... mask : tf.tensor
__label__0 @ parameterized.parameters ( 1 , 2 , 3 , 5 , 10 ) def testgroups ( self , num_groups ) : lock = lock_util.grouplock ( num_groups ) num_threads = 10 finished = set ( )
__label__0 to switch to native tf2 style , use [ ` tf.keras.optimizers.rmsprop ` ] ( https : //www.tensorflow.org/api_docs/python/tf/keras/optimizers/rmsprop ) instead . please notice that due to the implementation differences , ` tf.keras.optimizers.rmsprop ` and ` tf.compat.v1.train.rmspropoptimizer ` may have slight differences in floating point numerics even though the formula used for the variable updates still matches .
__label__0 this requires the function to be called with all named args , so for using this transformer , the function should also be added to renames .
__label__0 # file that specifies what the state of the git repo is spec = { }
__label__0 this function tests if the ` input_tree ` structure can be created from the ` shallow_tree ` structure by replacing its leaf nodes with deeper tree structures .
__label__0 finally : # clean up . dispatch._global_dispatchers = original_global_dispatchers
__label__0 with self.assertraisesregex ( valueerror , `` same nested structure '' ) : nest.map_structure ( lambda x , y : none , ( ( 3 , 4 ) , 5 ) , ( 3 , ( 4 , 5 ) ) )
__label__0 def testrequeststoponexception ( self ) : with self.cached_session ( ) as sess : queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) qr = queue_runner_impl.queuerunner ( queue , [ _mockop ( `` not an op '' ) ] ) coord = coordinator.coordinator ( ) threads = qr.create_threads ( sess , coord ) for t in threads : t.start ( ) # the exception should be re-raised when joining . with self.assertraisesregex ( valueerror , `` operation not in the graph '' ) : coord.join ( )
__label__0 x = array_ops.ones ( shape= ( 3 , 3 ) ) y = tensortracer ( `` y '' ) trace = x [ y ] self.assertequal ( str ( trace ) , `` __operators__.getitem ( % s , y ) '' % x )
__label__0 with self.session ( graph=ops_lib.graph ( ) ) as sess : # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_op ( 10.0 , name= '' v0 '' ) v1 = variable_op ( 20.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) v2_init = v2.insert ( `` k1 '' , 30.0 )
__label__0 for tensorflow , we check that it does not export anything under subpackage names used by components ( keras , etc . ) .
__label__0 def before_run ( self , run_context ) : print ( 'before calling session.run ( ) . ' ) return sessionrunargs ( self.your_tensor )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 # restores into the same number of partitions . restored_full = _restore ( partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=2 ) ) self.assertallequal ( saved_full , restored_full )
__label__0 def testsetstfdecoratornametodecoratornamearg ( self ) : decorated = tf_decorator.make_decorator ( test_function , test_wrapper , 'test decorator name ' ) decorator = getattr ( decorated , '_tf_decorator ' ) self.assertequal ( 'test decorator name ' , decorator.decorator_name )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' tests for tensorflow.python.training.saver.py . '' '' ''
__label__0 yields : tuple of ( field in the proto or ` none ` if none are found , field descriptor , key into this map field ( or none ) , index into this repeated field ( or none ) ) `` '' '' if not isinstance ( fields , list ) : fields = [ fields ]
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( add_car )
__label__0 # mapping from function to the new name of the function # add additional renames not in renames_v2.py to all_renames_v2.py . self.symbol_renames = all_renames_v2.symbol_renames self.import_rename = import_rename if self.import_rename : self.import_renames = { `` tensorflow '' : ast_edits.importrename ( `` tensorflow.compat.v2 '' , excluded_prefixes= [ `` tensorflow.contrib '' , `` tensorflow.flags '' , `` tensorflow.compat.v1 '' , `` tensorflow.compat.v2 '' , `` tensorflow.google '' ] , ) } else : self.import_renames = { }
__label__0 self.assertequal ( wrap1 , wrap1 ) self.assertequal ( wrap1 , wrap2 ) self.assertequal ( o , wrap1.unwrapped ) self.assertequal ( o , wrap2.unwrapped ) with self.assertraises ( typeerror ) : bool ( o == wrap1 ) with self.assertraises ( typeerror ) : bool ( wrap1 ! = o )
__label__0 if you create several savers , you can specify a different filename for the protocol buffer file in the call to ` save ( ) ` . `` '' ''
__label__0 @ test_injectable_decorator_square @ test_injectable_decorator_increment def test_rewrappable_decorated ( x ) : return x * 2
__label__0 return { `` cuda_version '' : cuda_version , `` cuda_include_dir '' : os.path.dirname ( cuda_header_path ) , `` cuda_library_dir '' : os.path.dirname ( cuda_library_path ) , `` cuda_binary_dir '' : cuda_binary_dir , `` nvvm_library_dir '' : nvvm_library_dir , `` cupti_include_dir '' : os.path.dirname ( cupti_header_path ) , `` cupti_library_dir '' : os.path.dirname ( cupti_library_path ) , `` cuda_toolkit_path '' : cuda_toolkit_paths [ 0 ] , `` nvml_header_dir '' : os.path.dirname ( nvml_header_dir ) , }
__label__0 def process_tree_inplace ( self , root_directory ) : `` '' '' process a directory of python files in place . '' '' '' files_to_process = [ ] for dir_name , _ , file_list in os.walk ( root_directory ) : py_files = [ os.path.join ( dir_name , f ) for f in file_list if f.endswith ( `` .py '' ) ] files_to_process += py_files
__label__0 import os import subprocess
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import tf_stack
__label__0 import os
__label__0 def testdispatchfortypes_newargs ( self ) : original_handlers = test_op_with_optional._tf_fallback_dispatchers [ : ]
__label__0 def testimport_nochangeneeded ( self ) : text = `` import bar as b '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import test as test_lib from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_export from tensorflow.python.util import tf_inspect from tensorflow.tools.common import public_api from tensorflow.tools.common import traverse from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import tf_upgrade_v2
__label__0 @ staticmethod def create_local_server ( config=none , start=true ) : `` '' '' creates a new single-process cluster running on the local host .
__label__0 # assert function docs are properly updated . self.assertequal ( `` deprecated function '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' % ( date , instructions ) , getattr ( _object , `` _fn '' ) .__doc__ )
__label__0 # create another saver and recover last checkpoints . save2 = saver_module.saver ( { `` v '' : v } , max_to_keep=10 ) self.assertequal ( [ ] , save2.last_checkpoints ) save2.recover_last_checkpoints ( [ s1 , s2 , s3 ] ) self.assertequal ( [ s1 , s2 , s3 ] , save2.last_checkpoints )
__label__0 def __init__ ( self , sv , sess , step_counter=none ) : `` '' '' create a ` svstepcounterthread ` .
__label__0 > > > dict = { `` key3 '' : { `` c '' : ( 1.0 , 2.0 ) , `` a '' : ( 3.0 ) } , ... `` key1 '' : { `` m '' : `` val1 '' , `` g '' : `` val2 '' } } > > > tf.nest.flatten ( dict ) [ 'val2 ' , 'val1 ' , 3.0 , 1.0 , 2.0 ]
__label__0 def testoneinput ( data ) : `` '' '' test randomized fuzzing input for tf.raw_ops.acosh . '' '' '' fh = fuzzinghelper ( data )
__label__0 def format_bytes ( b : int ) - > str : `` '' '' formats bytes into a human-readable string . '' '' '' for i in range ( 1 , len ( _byte_units ) ) : if b < _byte_units [ i ] [ 0 ] : n = f '' { b / _byte_units [ i-1 ] [ 0 ] : .2f } '' units = _byte_units [ i - 1 ] [ 1 ] break else : n = f '' { b / _byte_units [ -1 ] [ 0 ] : .2f } '' units = _byte_units [ -1 ] [ 1 ] n = n.rstrip ( `` 0 '' ) .rstrip ( `` . '' ) return f '' { n } { units } ''
__label__0 def testmultiplesessions ( self ) : server = self._cached_server with ops.graph ( ) .as_default ( ) : c = constant_op.constant ( [ [ 2 , 1 ] ] ) d = constant_op.constant ( [ [ 1 ] , [ 2 ] ] ) e = math_ops.matmul ( c , d )
__label__0 def restore ( self , sess , save_path ) : `` '' '' restores previously saved variables .
__label__0 class apple ( fruit ) : flavor = tf.constant ( [ 1 , 2 ] )
__label__0 def should_stop ( self ) : `` '' '' check if the coordinator was told to stop .
__label__0 1. created from a ` tf.distribute.distributeddataset ` :
__label__0 if any ( i in file for i in path_to_exclude ) : continue
__label__0 # check large nodes are chunked away . self.assertprotoequals ( graph_def.node [ 1 ] , chunks [ 1 ] ) self.assertprotoequals ( graph_def.node [ 2 ] , chunks [ 2 ] ) self.assertprotoequals ( graph_def.node [ 3 ] , chunks [ 3 ] ) self.assertprotoequals ( graph_def.node [ 5 ] , chunks [ 4 ] )
__label__0 text = `` tf.xla.experimental.compile ( 0 ) '' expected_text = `` tf.xla.experimental.compile ( 0 ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 a structure of type ` t ` is a structure whose atomic items are of type ` t ` . for example , a structure of ` tf.tensor ` only contains ` tf.tensor ` as its atoms .
__label__0 flags.define_string ( 'site_path ' , 'api_docs/java ' , 'path prefix in the _toc.yaml ' )
__label__0 def apis_with_type_based_dispatch ( ) : `` '' '' returns a list of tensorflow apis that support type-based dispatch . '' '' '' return sorted ( _type_based_dispatch_signatures , key=lambda api : f '' { api.__module__ } . { api.__name__ } '' )
__label__0 def _getattribute1 ( self , name ) : # pylint : disable=unused-argument return 2
__label__0 # assert calling new fn issues log warning . self.assertequal ( `` prop_with_doc '' , _object ( ) ._prop ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 assert_global_step ( global_step_tensor ) return global_step_tensor
__label__0 # create a fresh graph . graph_2 = ops_lib.graph ( ) with session.session ( graph=graph_2 ) as sess : # restore the above checkpoint under scope `` subgraph_1 '' . new_saver_1 = saver_module.import_meta_graph ( filename + `` .meta '' , graph=graph_2 , import_scope= '' subgraph_1 '' ) # there are no variables to restore , so import_meta_graph should not # return a saver . self.assertisnone ( new_saver_1 )
__label__0 def _testmultisavercollectionrestore ( self , test_dir ) : filename = os.path.join ( test_dir , `` metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) saver1_ckpt = os.path.join ( test_dir , `` saver1.ckpt '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : # imports from meta_graph . saver_module.import_meta_graph ( filename ) # retrieves savers collection . verifies there are 2 entries . savers = ops_lib.get_collection ( `` savers '' ) self.assertequal ( 2 , len ( savers ) ) # retrieves saver0 . verifies that new_saver0 can restore v0 , but not v1 . new_saver0 = savers [ 0 ] new_saver0.restore ( sess , saver0_ckpt ) v0 = sess.graph.get_tensor_by_name ( `` v0:0 '' ) v1 = sess.graph.get_tensor_by_name ( `` v1:0 '' ) self.assertallequal ( [ [ 1.0 , 2.0 ] , [ 3.0 , 4.0 ] , [ 5.0 , 6.0 ] ] , self.evaluate ( v0 ) ) self.assertequal ( [ 3 , 2 ] , v0.get_shape ( ) ) self.assertequal ( [ ] , v1.get_shape ( ) ) with self.assertraiseswithpredicatematch ( errors_impl.operror , lambda e : `` uninitialized value v1 '' in e.message ) : self.evaluate ( v1 ) # retrieves saver1 . verifies that new_saver1 can restore v1 . new_saver1 = savers [ 1 ] new_saver1.restore ( sess , saver1_ckpt ) v1 = sess.graph.get_tensor_by_name ( `` v1:0 '' ) self.assertequal ( 11.0 , self.evaluate ( v1 ) )
__label__0 def _library_paths ( ) : `` '' '' returns hard-coded set of relative paths to look for library files . '' '' '' return [ `` '' , `` lib64 '' , `` lib '' , `` lib/ * -linux-gnu '' , `` lib/x64 '' , `` extras/cupti/ * '' , `` local/cuda/lib64 '' , `` local/cuda/extras/cupti/lib64 '' , ]
__label__0 args : primary : the primary ` variable ` or ` tensor ` . val : a ` tensor ` specifying the initial value of the slot . name : name to use for the slot variable . colocate_with_primary : boolean . if true the slot is located on the same device as ` primary ` . copy_xla_sharding : boolean . if true also copies xla sharding from primary .
__label__0 if __name__ == '__main__ ' : # use importlib to import python submodule of tensorflow . # we delete python submodule in root __init__.py file . this means # normal import wo n't work for some python versions . for pkg in packages : recursive_import ( importlib.import_module ( pkg [ : -1 ] ) ) absltest.main ( )
__label__0 args : a : first arg b : second arg `` '' '' new_docs = deprecation.rewrite_argument_docstring ( deprecation.rewrite_argument_docstring ( docs , `` a '' , `` left '' ) , `` b '' , `` right '' ) new_docs_ref = `` '' '' add ` left ` and ` right `
__label__0 import_header = ( `` import tensorflow.compat.v1 as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) text = import_header + old_symbol expected_header = ( `` import tensorflow.compat.v2 as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) expected_text = expected_header + new_symbol _ , _ , _ , new_text = self._upgrade ( text , import_rename=true , upgrade_compat_v1_import=true ) self.assertequal ( new_text , expected_text )
__label__0 pointxy = collections.namedtuple ( `` point '' , [ `` x '' , `` y '' ] ) # pylint : disable=invalid-name unsafe_map_pattern = ( `` nest can not guarantee that it is safe to map one to `` `` the other . '' ) bad_pack_pattern = ( `` attempted to pack value : \n .+\ninto a structure , but `` `` found incompatible type ` < ( type|class ) 'str ' > ` instead . '' )
__label__0 the global step tensor must be an integer variable . we first try to find it in the collection ` global_step ` , or by name ` global_step:0 ` .
__label__0 os.chdir ( os.path.abspath ( os.path.join ( os.path.dirname ( __file__ ) , ' .. / .. / .. ' ) ) )
__label__0 # max shape can be 8 in length and randomized from 0-8 without running into an # oom error . _min_size = 0 _max_size = 8
__label__0 def recover_last_checkpoints ( self , checkpoint_paths ) : `` '' '' recovers the internal saver state after a crash .
__label__0 @ end_compatibility `` '' '' graph = graph or ops.get_default_graph ( ) if get_global_step ( graph ) is not none : raise valueerror ( ' '' global_step '' already exists . ' ) if context.executing_eagerly ( ) : with ops.device ( 'cpu:0 ' ) : return variable_scope.get_variable ( ops.graphkeys.global_step , shape= [ ] , dtype=dtypes.int64 , initializer=init_ops.zeros_initializer ( ) , trainable=false , aggregation=variables.variableaggregation.only_first_replica , collections= [ ops.graphkeys.global_variables , ops.graphkeys.global_step ] ) # create in proper graph and base name_scope . with graph.as_default ( ) as g , g.name_scope ( none ) : return variable_scope.get_variable ( ops.graphkeys.global_step , shape= [ ] , dtype=dtypes.int64 , initializer=init_ops.zeros_initializer ( ) , trainable=false , aggregation=variables.variableaggregation.only_first_replica , collections= [ ops.graphkeys.global_variables , ops.graphkeys.global_step ] )
__label__0 returns : a ` session ` . may be none if the operation exceeds the timeout specified by config.operation_timeout_in_ms .
__label__0 def parse_cmd_line ( ) : `` '' '' parse command line options .
__label__0 def make_tf_decorator ( target ) : return tf_decorator.tfdecorator ( decorator_name , target , decorator_doc )
__label__0 if hasattr ( module , tensorflow_constants_attr ) : constants_v2.extend ( getattr ( module , tensorflow_constants_attr ) ) return constants_v2
__label__0 class binarydistribution ( distribution ) :
__label__0 args : api_signature : the ` inspect.signature ` of the api whose signature is being checked . signature : dictionary mapping parameter names to type annotations .
__label__0 return rocm_config
__label__0 # validate updated params if centered : self.assertallcloseaccordingtotype ( mg0_np , self.evaluate ( mg0 ) ) self.assertallcloseaccordingtotype ( mg1_np , self.evaluate ( mg1 ) ) self.assertallcloseaccordingtotype ( rms0_np , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( rms1_np , self.evaluate ( rms1 ) ) self.assertallcloseaccordingtotype ( mom0_np , self.evaluate ( mom0 ) ) self.assertallcloseaccordingtotype ( mom1_np , self.evaluate ( mom1 ) ) self.assertallcloseaccordingtotype ( var0_np , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( var1_np , self.evaluate ( var1 ) )
__label__0 decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , argspec ) self.assertequal ( argspec , tf_inspect.getfullargspec ( decorator ) )
__label__0 from tensorflow.python.util.compat import collections_abc
__label__0 if use_resource : var0 = resource_variable_ops.resourcevariable ( var0_np ) var1 = resource_variable_ops.resourcevariable ( var1_np ) else : var0 = variables.variable ( var0_np ) var1 = variables.variable ( var1_np ) grads0 = constant_op.constant ( grads0_np ) grads1 = constant_op.constant ( grads1_np ) opt = rmsprop.rmspropoptimizer ( learning_rate=learning_rate , decay=decay , momentum=momentum , epsilon=epsilon , centered=centered )
__label__0 global_step : 10 `` `
__label__0 this class is a small wrapper that takes care of session creation and checkpoint recovery . it also provides functions that to facilitate coordination among multiple training threads or processes .
__label__0 class tfdoctestoutputcheckertest ( parameterized.testcase ) :
__label__0 def testduplicatedarg ( self ) : with self.assertraisesregex ( typeerror , `` got multiple values for argument ' b ' '' ) : self._matmul_func.canonicalize ( 2 , 3 , false , b=4 )
__label__0 this decorator raises a ` valueerror ` if the input ` func ` is called with any non-keyword args . this prevents the caller from providing the arguments in wrong order .
__label__0 example usage :
__label__0 table_header = textwrap.dedent ( `` '' ''
__label__0 cuda_binary_dir = os.path.dirname ( nvcc_path ) nvvm_library_dir = os.path.dirname ( nvvm_path )
__label__0 upgrade_dir = os.path.join ( self.get_temp_dir ( ) , `` foo '' ) other_dir = os.path.join ( self.get_temp_dir ( ) , `` bar '' ) os.mkdir ( upgrade_dir ) os.mkdir ( other_dir ) file_c = os.path.join ( other_dir , `` c.py '' ) file_d = os.path.join ( upgrade_dir , `` d.py '' )
__label__0 exporting a function or a class :
__label__0 def _model_ready_for_local_init ( self , sess : session.session ) - > tuple [ bool , optional [ str ] ] : `` '' '' checks if the model is ready to run local_init_op .
__label__0 if ` structure ` is a scalar , ` flat_sequence ` must be a single-element list ; in this case the return value is ` flat_sequence [ 0 ] ` .
__label__0 if _at_least_version ( cuda_version , `` 11.0 '' ) :
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def __init__ ( self , api_change_spec ) : if not isinstance ( api_change_spec , apichangespec ) : raise typeerror ( `` must pass apichangespec to astcodeupgrader , got % s '' % type ( api_change_spec ) ) self._api_change_spec = api_change_spec
__label__0 method = get_symbol_for_name ( tf , method ) arg_spec = tf_inspect.getfullargspec ( method ) for ( arg , pos ) in args : # to deal with the self argument on methods on objects if method_name.startswith ( `` * . `` ) : pos += 1 self.assertequal ( arg_spec [ 0 ] [ pos ] , arg )
__label__0 def get_chief_queue_runner ( self ) : `` '' '' returns the queuerunner for the chief to execute .
__label__0 with self.assertraiseswithpredicatematch ( errors_impl.operror , lambda e : `` uninitialized value v '' in e.message ) : self.evaluate ( v )
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import indexed_slices from tensorflow.python.framework import test_util from tensorflow.python.ops import embedding_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import rmsprop
__label__0 logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing tf.contrib.layers xavier initializer '' `` to a tf.compat.v1.keras.initializers.variancescaling and '' `` converting arguments.\n '' ) )
__label__0 class moduleclass1 ( object ) :
__label__0 * check the types of iterables :
__label__0 args : obj : the class-attribute to hide from the generated docs .
__label__0 def testdispatcherrorbadsignaturetype ( self ) : with self.assertraisesregex ( typeerror , `` signatures must be dictionaries mapping parameter `` `` names to type annotations '' ) :
__label__0 self.assertregex ( repr ( frames [ -1 ] ) , 'func ' ) self.assertregex ( repr ( frames [ -2 ] ) , 'testgetuserframes ' )
__label__0 # pylint : disable=missing-function-docstring def _tf_data_assert_shallow_structure ( shallow_tree , input_tree , check_types=true ) : if _tf_data_is_nested ( shallow_tree ) : if not _tf_data_is_nested ( input_tree ) : raise typeerror ( `` if shallow structure is a sequence , input must also be a sequence. `` f '' input has type : ' { type ( input_tree ) .__name__ } ' . '' )
__label__0 import collections import time
__label__0 def testdispatcherrorunsupportedkeywordonlyannotation ( self ) :
__label__0 self.assertequal ( 3 , nest.map_structure ( lambda x : x - 1 , 4 ) )
__label__0 def testupdatesdictwithmissingentries ( self ) : test_function.foobar = true decorated = tf_decorator.make_decorator ( test_function , test_wrapper ) self.asserttrue ( decorated.foobar ) del test_function.foobar
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for utilities working with arbitrarily nested structures . '' '' ''
__label__0 a second process could wait for the model to be ready by doing the following :
__label__0 t = typevar ( 't ' )
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { 'my_feature ' : tf.io.raggedfeature ( dtype=tf.string ) } ) { 'my_feature ' : < tf.tensor : shape= ( 2 , ) , dtype=string , numpy=array ( [ b'abc ' , b'12345 ' ] , dtype=object ) > }
__label__0 library_path = _find_library ( base_paths , `` cusparse '' , cusparse_version )
__label__0 * multiple python dictionaries :
__label__0 > > > structure1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > structure2 = ( ( ( `` foo1 '' , `` foo2 '' ) , `` foo3 '' ) , `` foo4 '' , ( `` foo5 '' , `` foo6 '' ) ) > > > structure3 = [ ( ( `` a '' , `` b '' ) , `` c '' ) , `` d '' , [ `` e '' , `` f '' ] ] > > > tf.nest.assert_same_structure ( structure1 , structure2 ) > > > tf.nest.assert_same_structure ( structure1 , structure3 , check_types=false )
__label__0 _doc_private = `` _tf_docs_doc_private ''
__label__0 returns : the result of the operation , or ` not_supported ` if no registered dispatcher can handle the given arguments. `` '' '' for dispatcher in getattr ( op , fallback_dispatch_attr ) : result = dispatcher.handle ( args , kwargs ) if result is not opdispatcher.not_supported : return result for dispatcher in _global_dispatchers : result = dispatcher.handle ( op , args , kwargs ) if result is not opdispatcher.not_supported : return result return opdispatcher.not_supported
__label__0 def bound ( self , a , b=2 , c='hello ' ) : return ( a , b , c )
__label__0 with ops.control_dependencies ( [ update_op ] ) : # sync_op needs to insert tokens to the token queue at the end of the # step so the replicas can fetch them to start the next step . tokens = array_ops.fill ( [ self._tokens_per_step ] , global_step ) sync_op = sync_token_queue.enqueue_many ( ( tokens , ) )
__label__0 note that in the dense implementation of this algorithm , variables and their corresponding accumulators ( momentum , gradient moving average , square gradient moving average ) will be updated even if the gradient is zero ( i.e . accumulators will decay , momentum will be applied ) . the sparse implementation ( used when the gradient is an ` indexedslices ` object , typically because of ` tf.gather ` or an embedding lookup in the forward pass ) will not update variable slices or their accumulators unless those slices were used in the forward pass ( nor is there an `` eventual '' correction to account for these omitted updates ) . this leads to more efficient updates for large embedding lookup tables ( where most of the slices are not accessed in a particular graph execution ) , but differs from the published algorithm .
__label__0 def yield_sorted_items ( modality , iterable ) : if modality == modality.core : return _tf_core_yield_sorted_items ( iterable ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 `` `` '' utility functions for writing decorators ( which modify docstrings ) . '' '' '' import sys
__label__0 # create a checkpoint . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 10.0 , name= '' ready_for_local_init_op_restore_v_ '' + str ( uid ) ) summary.scalar ( `` ready_for_local_init_op_restore_v_ '' + str ( uid ) , v ) sv = supervisor.supervisor ( logdir=logdir ) sv.prepare_or_wait_for_session ( server.target ) save_path = sv.save_path self._wait_for_glob ( save_path , 3.0 ) self._wait_for_glob ( os.path.join ( logdir , `` * events * '' ) , 3.0 , for_checkpoint=false ) # wait to make sure everything is written to file before stopping . time.sleep ( 1 ) sv.stop ( )
__label__0 def get_callbacks ( ) : return [ explicit_filter_keep_keras ]
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a deprecated module .
__label__0 tensorrt_version = header_version.split ( `` . `` ) [ 0 ] library_path = _find_library ( base_paths , `` nvinfer '' , tensorrt_version )
__label__0 def __repr__ ( self ) : return self._tfmw_wrapped_module.__repr__ ( )
__label__0 # if obj is a python 'type ' if type ( obj ) .__name__ == type.__name__ : return obj.__name__
__label__0 args : node : ast.call object full_name : the precomputed full name of the callable , if one exists , none otherwise . name : the precomputed name of the callable , if one exists , none otherwise .
__label__0 `` `` '' a lazyloader class . '' '' ''
__label__0 # hide raw_ops from search . for name , obj in tf_inspect.getmembers ( tf.raw_ops ) : if not name.startswith ( `` _ '' ) : doc_controls.hide_from_search ( obj )
__label__0 # todo ( rahulkamat ) : add missing types that are convertible to tensor . tensorlike = union [ tensor , tensorprotocol , int , float , bool , str , bytes , complex , tuple , list , np.ndarray , np.generic ] doc_typealias.document ( obj=tensorlike , doc=textwrap.dedent ( `` '' '' \ union of all types that can be converted to a ` tf.tensor ` by ` tf.convert_to_tensor ` .
__label__0 def testqualnameonboundproperty ( self ) : if hasattr ( testdecoratedclass ( ) .return_params , '__qualname__ ' ) : self.assertequal ( 'testdecoratedclass.return_params ' , testdecoratedclass ( ) .return_params.__qualname__ )
__label__0 @ property def two_prop ( self ) : return 2
__label__0 if ` structure ` is or contains a dict instance , the keys will be sorted to pack the flat sequence in deterministic order . this is true also for ` ordereddict ` instances : their sequence order is ignored , the sorting order of keys is used instead . the same convention is followed in ` flatten ` . this correctly repacks dicts and ` ordereddict ` s after they have been flattened , and also allows flattening an ` ordereddict ` and then repacking it back using a corresponding plain dict , or vice-versa . dictionaries with non-sortable keys can not be flattened .
__label__0 def wrapped_func ( * args , * * kwargs ) : if name_index < len ( args ) : name = args [ name_index ] args = args [ : name_index ] + args [ name_index + 1 : ] else : name = kwargs.pop ( `` name '' , none ) if name is none : return func ( * args , * * kwargs ) else : with ops.name_scope ( name ) : return func ( * args , * * kwargs )
__label__0 # tf.train.featurelists featurelists = dict [ str , featurelist ]
__label__0 `` ` pip install tensorflow `` ` `` '' ''
__label__0 return { `` cusolver_version '' : header_version , `` cusolver_include_dir '' : os.path.dirname ( header_path ) , `` cusolver_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 def make_tf_decorator ( target ) : return tf_decorator.tfdecorator ( decorator_name , target , decorator_doc )
__label__0 # ` target ` is a method or an instance with __call__ return callable ( target ) and not _inspect.isfunction ( target )
__label__0 a training program that saves regularly looks like :
__label__0 # captures the timestamp of the first saver object instantiation or end of a # save operation . can be accessed by multiple saver instances . _end_time_of_last_write = none _end_time_of_last_write_lock = threading.lock ( )
__label__0 if not flags.test_log_output_dir : print ( text_format.messagetostring ( test_results ) ) return
__label__0 import enum import sys
__label__0 # __file__ is the path to this file docs_tools_dir = pathlib.path ( __file__ ) .resolve ( ) .parent tensorflow_root = docs_tools_dir.parents [ 2 ] source_path = tensorflow_root / 'tensorflow/java/src/main/java ' op_source_path = ( tensorflow_root / 'bazel-bin/tensorflow/java/ops/src/main/java/org/tensorflow/op ' )
__label__0 # keep the tensorproto 's dtype , tensor_shape , and version_number fields , # but clear the raw tensor content / `` xxx_val '' attributes . kept_attributes = { key : getattr ( tensor_proto , key ) for key in _keep_tensor_proto_fields } tensor_proto.clear ( ) for field , val in kept_attributes.items ( ) : if isinstance ( val , message.message ) : getattr ( tensor_proto , field ) .mergefrom ( val ) else : setattr ( tensor_proto , field , val )
__label__0 def fn ( a , test_arg ) : if test_arg ! = expected_test_arg : return valueerror ( 'partial fn does not work correctly ' ) return a
__label__0 with self.cached_session ( ) as sess : # initialize all variables self.evaluate ( init_all_op )
__label__0 # remove things that are visible , but which should not be descended into . for name , child in list ( children ) : if self._do_not_descend ( full_path , name ) : children.remove ( ( name , child ) )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add ) def my_add ( x , y , name=none ) : # pylint : disable=unused-variable del x , y , name
__label__0 # fetch params to validate initial values self.assertallclose ( [ 1.0 , 2.0 ] , self.evaluate ( var0 ) ) self.assertallclose ( [ 3.0 , 4.0 ] , self.evaluate ( var1 ) ) # step 1 : the rms accumulators where 1. so we should see a normal # update : v -= grad * learning_rate opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) # check the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) ] ) , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) ] ) , self.evaluate ( var1 ) ) # step 2 : the root mean square accumulators contain the previous update . opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) # check the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) - ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1.0 ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) - ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1.0 ) ) ] ) , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) - ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 1e-5 + 1.0 ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) - ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 1e-5 + 1.0 ) ) ] ) , self.evaluate ( var1 ) )
__label__0 @ property def save_summaries_secs ( self ) : `` '' '' return the delay between summary computations .
__label__0 is_ready , msg = self._model_ready ( sess ) if not is_ready : raise runtimeerror ( `` init operations did not make model ready. `` `` init op : % s , init fn : % s , local_init_op : % s , error : % s '' % ( _maybe_name ( init_op ) , init_fn , self._local_init_op , msg ) ) return sess
__label__0 module1.py : `` ` python class newnameforclass : pass `` `
__label__0 args : server_or_cluster_def : a ` tf.train.serverdef ` or ` tf.train.clusterdef ` protocol buffer , or a ` tf.train.clusterspec ` object , describing the server to be defined and/or the cluster of which it is a member . job_name : ( optional . ) specifies the name of the job of which the server is a member . defaults to the value in ` server_or_cluster_def ` , if specified . task_index : ( optional . ) specifies the task index of the server in its job . defaults to the value in ` server_or_cluster_def ` , if specified . otherwise defaults to 0 if the server 's job has only one task . protocol : ( optional . ) specifies the protocol to be used by the server . acceptable values include ` `` grpc '' , `` grpc+verbs '' ` . defaults to the value in ` server_or_cluster_def ` , if specified . otherwise defaults to ` `` grpc '' ` . config : ( options . ) a ` tf.compat.v1.configproto ` that specifies default configuration options for all sessions that run on this server .
__label__0 args : arg0 : arg 0. arg1 : arg 1 .
__label__0 additionally , optional arguments to the ` saver ( ) ` constructor let you control the proliferation of checkpoint files on disk :
__label__0 feature_lists : { } `` `
__label__0 self.assertequal ( 7 , nest.map_structure ( lambda x , y : x + y , 3 , 4 ) )
__label__0 args : nest1 : an atom or a nested structure . nest2 : an atom or a nested structure . check_types : if ` true ` ( default ) types of structures are checked as well , including the keys of dictionaries . if set to ` false ` , for example a list and a tuple of objects will look the same if they have the same size . note that namedtuples with identical name and fields are always considered to have the same shallow structure . two types will also be considered the same if they are both list subtypes ( which allows `` list '' and `` _listwrapper '' from trackable dependency tracking to compare equal ) . ` check_types=true ` only checks type of sub-structures . the types of atoms are not checked . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 softmax_output_layer_kernel_vocab_info = tf.vocabinfo ( new_vocab='class_vocab ' , new_vocab_size=5 , num_oov_buckets=0 , # no oov for classes . old_vocab='old_class_vocab ' , old_vocab_size=8 , backup_initializer=tf.compat.v1.glorot_uniform_initializer ( ) , axis=1 )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { 0 : maskedtensor , 1 : maskedtensor } ) def masked_add ( x , y , name=none ) : with ops.name_scope ( name ) : return maskedtensor ( x.values + y.values , x.mask & y.mask )
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 yields : pairs of ( path , value ) , where path the tuple path of a leaf node in shallow_tree , and value is the value of the corresponding node in input_tree. `` '' '' if modality == modality.core : yield from _tf_core_yield_flat_up_to ( shallow_tree , input_tree , is_nested_fn , path ) elif modality == modality.data : yield from _tf_data_yield_flat_up_to ( shallow_tree , input_tree ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 # we should be done . with self.assertraises ( stopiteration ) : next ( rr )
__label__0 @ traceback_utils.filter_traceback def op_dispatch_handler ( * args , * * kwargs ) : `` '' '' call ` dispatch_target ` , peforming dispatch when appropriate . '' '' ''
__label__0 with self.assertraises ( errors.notfounderror ) as cs : b_saver.restore ( sess=sess , save_path=save_path )
__label__0 example usage : `` ` python embeddings_vocab_info = tf.vocabinfo ( new_vocab='embeddings_vocab ' , new_vocab_size=100 , num_oov_buckets=1 , old_vocab='pretrained_embeddings_vocab ' , old_vocab_size=10000 , backup_initializer=tf.compat.v1.truncated_normal_initializer ( mean=0.0 , stddev= ( 1 / math.sqrt ( embedding_dim ) ) ) , axis=0 )
__label__0 # we still need all the names that are toplevel on tensorflow_core from tensorflow_core import *
__label__0 filtered_filenames = frozenset ( ( self._filename , ) ) if self.parent is not none : filtered_filenames |= self.parent.get_filtered_filenames ( ) self._cached_set = filtered_filenames return filtered_filenames
__label__0 # the global step should now be 2 and the gradients should have been # applied twice . self.assertallequal ( 2 , sessions [ 1 ] .run ( global_step ) ) self.assertallclose ( 0 - 2 * ( 0.1 + 0.3 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_0_g_1 ) ) self.assertallclose ( 1 - 2 * ( 0.9 + 1.1 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_1_g_1 ) )
__label__0 end_time = time.time ( ) metrics.addcheckpointwriteduration ( api_label=_saver_label , microseconds=_get_duration_microseconds ( start_time , end_time ) ) global _end_time_of_last_write with _end_time_of_last_write_lock : metrics.addtrainingtimesaved ( api_label=_saver_label , microseconds=_get_duration_microseconds ( _end_time_of_last_write , end_time ) ) _end_time_of_last_write = end_time
__label__0 # the next one should have the values from the summary . ev = next ( rr ) self.assertprotoequals ( `` '' '' value { tag : 'c1 ' simple_value : 1.0 } value { tag : 'c2 ' simple_value : 2.0 } value { tag : 'c3 ' simple_value : 3.0 } `` '' '' , ev.summary )
__label__0 # todo ( allenl ) : remove these aliases once all users are migrated off . get_checkpoint_state = checkpoint_management.get_checkpoint_state update_checkpoint_state = checkpoint_management.update_checkpoint_state generate_checkpoint_state_proto = ( checkpoint_management.generate_checkpoint_state_proto ) latest_checkpoint = checkpoint_management.latest_checkpoint checkpoint_exists = checkpoint_management.checkpoint_exists get_checkpoint_mtimes = checkpoint_management.get_checkpoint_mtimes remove_checkpoint = checkpoint_management.remove_checkpoint
__label__0 def _getattr ( self , name ) : # pylint : disable=unused-argument return 3
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' upgrader for python scripts from 1 . * to 2.0 tensorflow using safety mode . '' '' ''
__label__0 returns : global step read tensor .
__label__0 # * * kwargs passed in that we can not inspect , should warn text = `` f ( a , b , kw1=c , * * kwargs ) \n '' ( _ , report , _ ) , _ = self._upgrade ( reorderkeywordspec ( ) , text ) self.assertnotin ( `` manual check required '' , report )
__label__0 @ tf_export ( 'compat.as_str_any ' ) def as_str_any ( value , encoding='utf-8 ' ) : `` '' '' converts input to ` str ` type .
__label__0 args : arg0 : arg 0. arg1 : arg 1 .
__label__0 def __getitem__ ( self , item ) : raise valueerror ( `` can not get item : % s '' % item )
__label__0 def testfromimport ( self ) : # foo should be renamed to bar . text = `` from foo import a '' expected_text = `` from bar import a '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 tensorboard will pick the graph from the file and display it graphically so you can interactively explore the graph you built . you will usually pass the graph from the session in which you launched it :
__label__0 raises : valueerror : when callable 's signature can not be expressed with argspec . typeerror : for objects of unsupported types. `` '' '' if isinstance ( obj , functools.partial ) : return _get_argspec_for_partial ( obj )
__label__0 def group ( self , group_id ) : `` '' '' enter a context where the lock is with group ` group_id ` .
__label__0 upgrader = ast_edits.astcodeupgrader ( renameimports ( ) ) upgrader.process_tree ( upgrade_dir , output_dir , copy_other_files=true )
__label__0 # check that the parameter nodes have been restored . if not context.executing_eagerly ( ) : self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) )
__label__0 var0_np , mg0_np , rms0_np , mom0_np = self._rmsprop_update_numpy ( var0_np , grads0_np , mg0_np , rms0_np , mom0_np , learning_rate , decay , momentum , epsilon , centered ) var1_np , mg1_np , rms1_np , mom1_np = self._rmsprop_update_numpy ( var1_np , grads1_np , mg1_np , rms1_np , mom1_np , learning_rate , decay , momentum , epsilon , centered )
__label__0 config.cpu_info.copyfrom ( gather_cpu_info ( ) ) config.platform_info.copyfrom ( gather_platform_info ( ) )
__label__0 def testflattenwithtuplepathsupto ( self ) : def get_paths_and_values ( shallow_tree , input_tree ) : path_value_pairs = nest.flatten_with_tuple_paths_up_to ( shallow_tree , input_tree ) paths = [ p for p , _ in path_value_pairs ] values = [ v for _ , v in path_value_pairs ] return paths , values
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 args : files : a list of file paths to test . globs : the global namespace the tests are run in . set_up : run before each test , receives the test as argument . tear_down : run after each test , receives the test as argument .
__label__0 names , slices , dtypes = zip ( * restore_specs ) # load all tensors onto cpu 0 for compatibility with existing code . with ops.device ( `` cpu:0 '' ) : return io_ops.restore_v2 ( filename_tensor , names , slices , dtypes )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 finally : dispatch.unregister_dispatch_for ( handler )
__label__0 def api_without_dispatch_support ( x ) : return x + 1
__label__0 input_tree_flattened_as_shallow_tree = flatten_up_to ( shallow_tree , input_tree ) input_tree_flattened = flatten ( input_tree )
__label__0 returns : the user-defined object , with the same class of the current object .
__label__0 after restoring , re-encode your checkpoint using ` tf.train.checkpoint.save ` or ` tf.keras.model.save_weights ` .
__label__0 def register_binary_elementwise_assert_api ( func ) : `` '' '' decorator that registers a tensorflow op as a binary elementwise assert api .
__label__0 def testexportmultipleconstants ( self ) : module1 = self._createmockmodule ( 'module1 ' ) module2 = self._createmockmodule ( 'module2 ' )
__label__0 `` '' ''
__label__0 import time
__label__0 def testdispatchforsignaturefromannotations ( self ) :
__label__0 ` perreplica ` values exist on the worker devices , with a different value for each replica . they are produced by iterating through a distributed dataset returned by ` tf.distribute.strategy.experimental_distribute_dataset ` ( example 1 , below ) and ` tf.distribute.strategy.distribute_datasets_from_function ` . they are also the typical result returned by ` tf.distribute.strategy.run ` ( example 2 ) .
__label__0 expected_indices = [ 0 , 1 , 2 ] # check that the chunk indices and info are correct . for expected_index , expected_data , chunk in zip ( expected_indices , data , proto.message.chunked_fields ) : i = chunk.message.chunk_index self.assertequal ( expected_index , i )
__label__1 def addoperators ( num , target ) : def generateexpressions ( idx , expr , evaluated , prev_val , target ) : if idx == len ( num ) : if evaluated == target : result.append ( expr ) return for i in range ( idx , len ( num ) ) : if i > idx and num [ idx ] == ' 0 ' : # skip leading zeros break curr_str = num [ idx : i+1 ] curr_val = int ( curr_str ) if idx == 0 : generateexpressions ( i + 1 , curr_str , curr_val , curr_val , target ) else : generateexpressions ( i + 1 , expr + '+ ' + curr_str , evaluated + curr_val , curr_val , target ) generateexpressions ( i + 1 , expr + '- ' + curr_str , evaluated - curr_val , -curr_val , target ) generateexpressions ( i + 1 , expr + ' * ' + curr_str , evaluated - prev_val + prev_val * curr_val , prev_val * curr_val , target ) result = [ ] generateexpressions ( 0 , `` '' , 0 , 0 , target ) return result # test cases num1 , target1 = `` 123 '' , 6 print ( addoperators ( num1 , target1 ) ) # output : [ `` 1 * 2 * 3 '' , '' 1+2+3 '' ] num2 , target2 = `` 232 '' , 8 print ( addoperators ( num2 , target2 ) ) # output : [ `` 2 * 3+2 '' , '' 2+3 * 2 '' ] num3 , target3 = `` 3456237490 '' , 9191 print ( addoperators ( num3 , target3 ) ) # output : [ ]
__label__0 import os import shlex import sys import time
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities related to tensorflow exception stack trace prettifying . '' '' ''
__label__0 self.text_good = super ( tfdoctestoutputchecker , self ) .check_output ( want=want_text_wild , got=got , optionflags=optionflags ) if not self.text_good : return false
__label__0 class _object ( object ) :
__label__0 return files_processed , report_text , errors
__label__0 if isinstance ( shallow_tree , _collections_abc.mapping ) : absent_keys = set ( shallow_tree ) - set ( input_tree ) if absent_keys : raise valueerror ( shallow_tree_has_invalid_keys.format ( sorted ( absent_keys ) ) )
__label__0 def _build_internal ( self , names_to_saveables , reshape=false , sharded=false , max_to_keep=5 , keep_checkpoint_every_n_hours=10000.0 , name=none , restore_sequentially=false , filename= '' model '' , build_save=true , build_restore=true ) : `` '' '' build ( ) with option to only perform save and restore . '' '' '' if not context.executing_eagerly ( ) and ( not build_save or not build_restore ) : raise valueerror ( `` save and restore operations need to be built together `` `` when eager execution is not enabled . '' )
__label__0 threads = [ self.checkedthread ( target=thread_fn , args= ( i , ) ) for i in range ( num_threads ) ]
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' type-based dispatch for tensorflow 's python apis .
__label__0 text = ( `` slim.l1_regularizer ( # stuff before\n '' `` scale=.4 , '' `` scope=\ '' foo\ '' ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l1 ( # stuff before\n '' `` l=.4 ) \n '' , ) self.assertin ( `` dropping scope '' , unused_report )
__label__0 def __new__ ( cls , a , b=1 , c='hello ' ) : pass
__label__0 def testunboundfuncwithoneparamdefault ( self ) :
__label__0 class moduleclass2 ( object ) :
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 class testupgradefiles ( test_util.tensorflowtestcase ) :
__label__0 def testrespectcoordshouldstop ( self ) : with self.cached_session ( ) as sess : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) self.evaluate ( variables.global_variables_initializer ( ) ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) # as the coordinator to stop . the queue runner should # finish immediately . coord = coordinator.coordinator ( ) coord.request_stop ( ) threads = qr.create_threads ( sess , coord ) self.assertequal ( sorted ( t.name for t in threads ) , [ `` queuerunnerthread-fifo_queue-countupto:0 '' , `` queuerunnerthread-fifo_queue-close_on_stop '' ] ) for t in threads : t.start ( ) coord.join ( ) self.assertequal ( 0 , len ( qr.exceptions_raised ) ) # the variable should be 0. self.assertequal ( 0 , self.evaluate ( var ) )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 args : graph : a ` graph ` . the graph that the model will use . defaults to the default ` graph ` . the supervisor may add operations to the graph before creating a session , but the graph should not be modified by the caller after passing it to the supervisor . ready_op : 1-d string ` tensor ` . this tensor is evaluated by supervisors in ` prepare_or_wait_for_session ( ) ` to check if the model is ready to use . the model is considered ready if it returns an empty array . defaults to the tensor returned from ` tf.compat.v1.report_uninitialized_variables ( ) ` if ` none ` , the model is not checked for readiness . ready_for_local_init_op : 1-d string ` tensor ` . this tensor is evaluated by supervisors in ` prepare_or_wait_for_session ( ) ` to check if the model is ready to run the local_init_op . the model is considered ready if it returns an empty array . defaults to ` none ` . if ` none ` , the model is not checked for readiness before running local_init_op . is_chief : if true , create a chief supervisor in charge of initializing and restoring the model . if false , create a supervisor that relies on a chief supervisor for inits and restore . init_op : ` operation ` . used by chief supervisors to initialize the model when it can not be recovered . defaults to an ` operation ` that initializes all global variables . if ` none ` , no initialization is done automatically unless you pass a value for ` init_fn ` , see below . init_feed_dict : a dictionary that maps ` tensor ` objects to feed values . this feed dictionary will be used when ` init_op ` is evaluated . local_init_op : ` operation ` . used by all supervisors to run initializations that should run for every new supervisor instance . by default these are table initializers and initializers for local variables . if ` none ` , no further per supervisor-instance initialization is done automatically . logdir : a string . optional path to a directory where to checkpoint the model and log events for the visualizer . used by chief supervisors . the directory will be created if it does not exist . summary_op : an ` operation ` that returns a summary for the event logs . used by chief supervisors if a ` logdir ` was specified . defaults to the operation returned from summary.merge_all ( ) . if ` none ` , summaries are not computed automatically . saver : a saver object . used by chief supervisors if a ` logdir ` was specified . defaults to the saved returned by saver ( ) . if ` none ` , the model is not saved automatically . global_step : an integer tensor of size 1 that counts steps . the value from 'global_step ' is used in summaries and checkpoint filenames . default to the op named 'global_step ' in the graph if it exists , is of rank 1 , size 1 , and of type tf.int32 or tf.int64 . if ` none ` the global step is not recorded in summaries and checkpoint files . used by chief supervisors if a ` logdir ` was specified . save_summaries_secs : number of seconds between the computation of summaries for the event log . defaults to 120 seconds . pass 0 to disable summaries . save_model_secs : number of seconds between the creation of model checkpoints . defaults to 600 seconds . pass 0 to disable checkpoints . recovery_wait_secs : number of seconds between checks that the model is ready . used by supervisors when waiting for a chief supervisor to initialize or restore the model . defaults to 30 seconds . stop_grace_secs : grace period , in seconds , given to running threads to stop when ` stop ( ) ` is called . defaults to 120 seconds . checkpoint_basename : the basename for checkpoint saving . session_manager : ` sessionmanager ` , which manages session creation and recovery . if it is ` none ` , a default ` sessionmanager ` will be created with the set of arguments passed in for backwards compatibility . summary_writer : ` summarywriter ` to use or ` use_default ` . can be ` none ` to indicate that no summaries should be written . init_fn : optional callable used to initialize the model . called after the optional ` init_op ` is called . the callable must accept one argument , the session being initialized . local_init_run_options : runoptions to be passed as the sessionmanager local_init_run_options parameter .
__label__0 this decorator does not print deprecation messages . todo ( annarev ) : eventually start printing deprecation warnings when @ deprecation_endpoints decorator is added .
__label__0 def __init__ ( self , write_version=saver_pb2.saverdef.v2 ) : self._write_version = write_version
__label__0 class tfdecoratorrewraptest ( test.testcase ) :
__label__0 def test_injectable_decorator_increment ( target ) :
__label__0 def testcurrentframe ( self ) : self.assertequal ( inspect.currentframe ( ) , tf_inspect.currentframe ( ) )
__label__0 for example :
__label__0 def testsegmentmin ( self ) : for dtype in self.int_types | self.float_types : maxval = dtypes.as_dtype ( dtype ) .max if dtype == np.float64 and self._finddevice ( `` tpu '' ) : maxval = np.inf self.assertallclose ( np.array ( [ 0 , maxval , 2 , 3 ] , dtype=dtype ) , self._segmentminv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 a ` tf.distribute.server ` instance encapsulates a set of devices and a ` tf.compat.v1.session ` target that can participate in distributed training . a server belongs to a cluster ( specified by a ` tf.train.clusterspec ` ) , and corresponds to a particular task in a named job . the server can communicate with any other server in the same cluster. `` '' ''
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # # ============================================================================== `` '' '' tests for graphdef splitter . '' '' ''
__label__0 # whether to rename to compat.v2 _import_rename_default = false
__label__0 class savertest ( test.testcase ) :
__label__0 def has_ext_modules ( self ) : return true
__label__0 def testsavetouri ( self ) : # parseuri functions do n't work on windows yet . # todo ( jhseu ) : remove this check when it works . if os.name == `` nt '' : self.skiptest ( `` local uri support does n't work on windows '' ) save_path = `` file : // '' + os.path.join ( self.get_temp_dir ( ) , `` uri '' )
__label__0 nest.assert_same_structure ( nesttest.named0ab ( 3 , 4 ) , nesttest.named0ab ( `` a '' , `` b '' ) )
__label__0 class tracingcontext ( metaclass=abc.abcmeta ) : `` '' '' contains information scoped to the tracing of multiple objects .
__label__0 def explicit_filter_keep_keras ( parent_path , parent , children ) : `` '' '' like explicit_package_contents_filter , but keeps keras . '' '' '' new_children = public_api.explicit_package_contents_filter ( parent_path , parent , children )
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) )
__label__0 4. as input into ` reduce ` :
__label__0 field , _ = util.get_field ( proto , [ `` nested_map_bool '' , false , `` string_field '' ] ) self.assertequal ( `` string_false '' , field )
__label__0 elif ( _is_composite_tensor ( shallow_tree ) or _is_type_spec ( shallow_tree ) ) and ( _is_composite_tensor ( input_tree ) or _is_type_spec ( input_tree ) ) : pass # compatibility will be checked below .
__label__0 converts from any python constant representation of a ` pathlike ` object or ` str ` to bytes .
__label__0 class fruit : flavor = tf.constant ( [ 0 , 0 ] )
__label__0 def testinplace ( self ) : `` '' '' check to make sure we do n't have a file system race . '' '' '' temp_file = tempfile.namedtemporaryfile ( `` w '' , delete=false ) original = `` tf.conj ( a ) \n '' upgraded = `` tf.math.conj ( a ) \n '' temp_file.write ( original ) temp_file.close ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2.tfapichangespec ( ) ) upgrader.process_file ( temp_file.name , temp_file.name ) self.assertallequal ( open ( temp_file.name ) .read ( ) , upgraded ) os.unlink ( temp_file.name )
__label__0 @ property def dtype ( self ) : pass
__label__0 __slots__ = ( )
__label__0 def _contrib_layers_xavier_initializer_transformer ( parent , node , full_name , name , logs ) : `` '' '' updates references to contrib.layers.xavier_initializer .
__label__0 for example :
__label__0 with self.cached_session ( ) as sess : v = variable_v1.variablev1 ( 10.0 , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 self.assertlen ( ret , 3 ) self.assertequal ( 7 , ret [ 0 ] .field ) self.assertequal ( false , ret [ 1 ] .map_key.boolean ) self.assertequal ( 6 , ret [ 2 ] .field )
__label__0 # verifies that there are 2 entries in savers collection . savers = getattr ( collection_def , kind ) self.assertequal ( 2 , len ( savers.value ) )
__label__0 raises : runtimeerror : if ./configure needs to be run , runtimeerror will be raised. `` '' ''
__label__0 # this assertion is expected to pass : a variablespec with alias_id and # a variable are considered identical . inp_shallow = resource_variable_ops.variablespec ( none , alias_id=0 ) inp_deep = resource_variable_ops.resourcevariable ( 1 . ) nest.assert_shallow_structure ( inp_shallow , inp_deep , expand_composites=false ) nest.assert_shallow_structure ( inp_shallow , inp_deep , expand_composites=true )
__label__0 kwargs = { } self.assertequal ( tf_inspect.getcallargs ( func , 0 , * * kwargs ) , { 'positional ' : 0 , 'func ' : 1 , 'func_and_positional ' : 2 , 'kwargs ' : 3 } ) kwargs = dict ( func=4 , func_and_positional=5 , kwargs=6 ) self.assertequal ( tf_inspect.getcallargs ( func , 0 , * * kwargs ) , { 'positional ' : 0 , 'func ' : 4 , 'func_and_positional ' : 5 , 'kwargs ' : 6 } )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenwithtuplepathsuptocompatible ( self ) : simple_list = [ 2 ] mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) flat_path_mt = nest.flatten_with_tuple_paths_up_to ( shallow_tree=simple_list , input_tree=mt , check_types=false ) # expected flat_path_mt = [ ( ( 0 , ) , tensor ( [ 1 ] ) ) ] self.assertequal ( flat_path_mt [ 0 ] [ 0 ] , ( 0 , ) ) self.assertallequal ( flat_path_mt [ 0 ] [ 1 ] , [ 1 ] )
__label__0 > > > class maskedtensor ( tf.experimental.extensiontype ) : ... values : tf.tensor ... mask : tf.tensor > > > @ dispatch_for_binary_elementwise_assert_apis ( maskedtensor , maskedtensor ) ... def binary_elementwise_assert_api_handler ( assert_func , x , y ) : ... merged_mask = tf.logical_and ( x.mask , y.mask ) ... selected_x_values = tf.boolean_mask ( x.values , merged_mask ) ... selected_y_values = tf.boolean_mask ( y.values , merged_mask ) ... assert_func ( selected_x_values , selected_y_values ) > > > a = maskedtensor ( [ 1 , 1 , 0 , 1 , 1 ] , [ false , false , true , true , true ] ) > > > b = maskedtensor ( [ 2 , 2 , 0 , 2 , 2 ] , [ true , true , true , false , false ] ) > > > tf.debugging.assert_equal ( a , b ) # assert passed ; no exception was thrown
__label__0 empty_set = frozenset ( )
__label__0 if len ( sparse_types ) ! = num_sparse : raise valueerror ( `` len ( sparse_types ) attribute does not match `` `` nsparse attribute ( % d vs % d ) '' % ( len ( sparse_types ) , num_sparse ) )
__label__0 returns : an op or ` none ` . `` '' '' return self._init_op
__label__0 @ classmethod def setupclass ( cls ) : super ( obsoletesessionmanagertest , cls ) .setupclass ( ) resource_variables_toggle.disable_resource_variables ( )
__label__0 def testupgradeinplacewithsymlinkindifferentdir ( self ) : if os.name == `` nt '' : self.skiptest ( `` os.symlink does n't work uniformly on windows . '' )
__label__0 @ tf_export ( v1= [ 'train.create_global_step ' ] ) def create_global_step ( graph=none ) : `` '' '' create global step tensor in graph .
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import data_flow_ops from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import coordinator from tensorflow.python.training import monitored_session from tensorflow.python.training import queue_runner_impl
__label__0 # verifies round trip from proto- > spec- > proto is correct . cluster_spec = server_lib.clusterspec ( cluster_def ) self.assertprotoequals ( cluster_def , cluster_spec.as_cluster_def ( ) )
__label__0 def __init__ ( self , var_list=none , reshape=false , sharded=false , max_to_keep=5 , keep_checkpoint_every_n_hours=10000.0 , name=none , restore_sequentially=false , saver_def=none , builder=none , defer_build=false , allow_empty=false , write_version=saver_pb2.saverdef.v2 , pad_step_number=false , save_relative_paths=false , filename=none ) : `` '' '' creates a ` saver ` .
__label__0 # copy xla sharding attributes from the primary if the slot variable has the # same rank as the primary . def _has_same_rank ( primary_shape , slot_shape ) : return ( primary_shape.rank is not none and slot_shape.rank is not none and primary_shape.rank == slot_shape.rank )
__label__0 @ property def ready_op ( self ) : `` '' '' return the ready op used by the supervisor .
__label__0 def hipfft_version_numbers ( path ) : possible_version_files = [ `` include/hipfft/hipfft-version.h '' , # rocm 5.2 `` hipfft/include/hipfft-version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` hipfft version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` hipfftversionmajor '' ) minor = _get_header_version ( version_file , `` hipfftversionminor '' ) patch = _get_header_version ( version_file , `` hipfftversionpatch '' ) return major , minor , patch
__label__0 logs.append ( ( ast_edits.warning , node.lineno , node.col_offset , `` changing dataset. % s ( ) to tf.compat.v1.data. % s ( dataset ) . `` `` please check this transformation.\n '' % ( name , name ) ) )
__label__0 @ property def is_tensor_like ( self ) : return true
__label__0 def testdropout ( self ) : text = `` tf.nn.dropout ( x , keep_prob , name=\ '' foo\ '' ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.nn.dropout ( x , rate=1 - ( keep_prob ) , name=\ '' foo\ '' ) \n '' , )
__label__0 # copyright 2019 the openxla authors . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . `` `` '' lit runner configuration . '' '' ''
__label__0 func_signature = tf_inspect.signature ( func ) ok = len ( api_signature.parameters ) == len ( func_signature.parameters ) if ok : for param_1 , param_2 in zip ( api_signature.parameters.values ( ) , func_signature.parameters.values ( ) ) : if ( param_1.name ! = param_2.name ) or ( param_1.kind ! = param_2.kind ) : ok = false if not ok : raise valueerror ( f '' dispatch function 's signature { func_signature } does `` f '' not match api 's signature { api_signature } . '' )
__label__0 def test_is_tensor_upgrade ( self ) : text = `` tf.contrib.framework.is_tensor ( x ) '' expected = `` tf.is_tensor ( x ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 `` ` python with tf.graph ( ) .as_default ( ) : ... add operations to the graph ... # create a sessionmanager that will wait for the model to become ready . sm = sessionmanager ( ) sess = sm.wait_for_session ( master ) # use the session to train the graph . while true : sess.run ( < my_train_op > ) `` `
__label__0 returns : true if the input is a nested structure. `` '' '' if modality == modality.core : return _tf_core_is_nested ( structure ) elif modality == modality.data : return _tf_data_is_nested ( structure ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 args : name : benchmark target identifier . test_name : a unique bazel target , e.g . `` //path/to : test '' test_args : a string containing all arguments to run the target with . benchmark_type : a string representing the benchmarktype enum ; the benchmark type for this target . skip_processing_logs : whether to skip processing test results from log files .
__label__0 if hasattr ( module , tensorflow_constants_attr_v1 ) : constants_v1.extend ( getattr ( module , tensorflow_constants_attr_v1 ) ) return constants_v1
__label__0 # use the most preferred temp directory . config.test_exec_root = ( os.environ.get ( `` test_undeclared_outputs_dir '' ) or os.environ.get ( `` test_tmpdir '' ) or os.path.join ( tempfile.gettempdir ( ) , `` lit '' ) )
__label__0 for _ in range ( burn_iter ) : nest.assert_same_structure ( s1 , s2 )
__label__0 # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) )
__label__0 args : logs : the logs to add . must be a list of tuples ` ( severity , lineno , col_offset , msg ) ` . `` '' '' self._log.extend ( logs ) for log in logs : print ( `` % s line % d : % d : % s '' % log )
__label__0 class initclass ( object ) :
__label__0 def test_prop_wrong_order ( self ) : with self.assertraisesregex ( valueerror , `` make sure @ property appears before @ deprecated in your source code '' ) : # pylint : disable=unused-variable
__label__0 code_cell_idx = 0 for cell in new_notebook [ `` cells '' ] : if not is_python ( cell ) : continue
__label__0 class moduledeprecationspec ( ast_edits.noupdatespec ) : `` '' '' a specification which deprecates ' a.b ' . '' '' ''
__label__0 class compatv1importreplacer ( ast.nodevisitor ) : `` '' '' ast visitor that replaces ` import tensorflow.compat.v1 as tf ` .
__label__0 args : path : an object that can be converted to path representation .
__label__0 any further depth in structure in ` input_tree ` is retained as structures in the partially flatten output .
__label__0 raises : typeerror : if ` var_list ` is invalid . valueerror : if any of the keys or values in ` var_list ` are not unique . runtimeerror : if eager execution is enabled and ` var_list ` does not specify a list of variables to save .
__label__0 if ` date ` is none , 'after < date > ' is replaced with 'in a future version ' . < function > will include the class name if it is a method .
__label__0 import_header = `` import tensorflow as tf\n '' text = import_header + old_symbol expected_text = `` import tensorflow.compat.v2 as tf\n '' + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 the arguments specified by ` args ` and ` kwargs ` follow normal function call rules . the returned ` concretefunction ` has the same set of positional and keyword arguments as ` self ` , but their types are compatible to the types specified by ` args ` and ` kwargs ` ( though not neccessarily equal ) .
__label__0 args : iterable : an iterable .
__label__0 goodbye `` '' '' ) , ( 'list ' , [ ( ' a ' , none ) , ( ' b ' , ' b ' ) , ( ' c ' , ' c ' ) , ( 'd ' , none ) ] , `` '' '' hello
__label__0 exampletuples = list [ tuple [ str , optional [ str ] ] ]
__label__0 del another_handler
__label__0 def __len__ ( self ) : return 1
__label__0 with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises valueerror , nest.structures_have_mismatching_lengths.format ( input_length=len ( input_tree ) , shallow_length=len ( shallow_tree ) ) , ) : get_paths_and_values ( shallow_tree , input_tree )
__label__0 # assert calls without the deprecated argument log nothing . with self.assertraisesregex ( valueerror , `` not present . * \\ [ 'missing'\\ ] '' ) : deprecation.deprecated_args ( date , instructions , `` missing '' ) ( _fn )
__label__0 returns : a new structure with ` resourcevariable ` s in ` values ` converted to atoms. `` '' '' def _replace_resource_variable_with_atom ( x ) : if _pywrap_utils.isresourcevariable ( x ) : return 0 # tf.nest treats 0 or tf.constant ( 0 ) as an atom . else : return x
__label__0 result = { }
__label__0 args : * args : arguments for get_slot ( ) . * * kwargs : keyword arguments for get_slot ( ) .
__label__0 parser.add_argument ( `` -- configure '' , type=str , help= '' path to configure as a git repo dependency tracking sentinel '' )
__label__0 def as_saver_def ( self ) : `` '' '' generates a ` saverdef ` representation of this saver .
__label__0 class apichangespec : `` '' '' this class defines the transformations that need to happen .
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 args : instance : an instance of ` tuple ` , ` list ` , ` namedtuple ` , ` dict ` , ` collections.ordereddict ` , or ` composite_tensor.composite_tensor ` or ` type_spec.typespec ` . args : items to be converted to the ` instance ` type .
__label__0 ` ref_symlink ` is unused in this script but passed , because the build system uses that file to detect when commits happen .
__label__0 # # # usage
__label__0 def testrewrapofdecoratorfunction ( self ) :
__label__0 # ` tf ` has an ` __all__ ` that does n't list important things like ` keras ` . # the doc generator recognizes ` __all__ ` as the list of public symbols . # so patch ` tf.__all__ ` to list everything . tf.__all__ = [ item_name for item_name , value in tf_inspect.getmembers ( tf ) ]
__label__0 ` children ` , a list of ` ( name , object ) ` pairs are determined by ` tf_inspect.getmembers ` . to avoid visiting parts of the tree , ` children ` can be modified in place , using ` del ` or slice assignment .
__label__0 args : value : an input value belonging to this tracetype . cast_context : a context reserved for internal/future usage .
__label__0 server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_spec.as_cluster_def ( ) , job_name=job_name , task_index=task_index , protocol=protocol ) if config is not none : server_def.default_session_config.mergefrom ( config ) return server_def
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== from tensorflow.python.platform import googletest # pylint : disable=g-direct-tensorflow-import from tensorflow.python.util import pywrap_xla_ops
__label__0 from tensorflow.python.util.tf_export import tf_export
__label__0 # when ` check_types=true ` is set , ` flatten_up_to ` would fail when input_tree # and shallow_tree args do n't have the same type with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( simple_list ) , input_type=type ( mt ) ) , ) : nest.flatten_up_to ( shallow_tree=simple_list , input_tree=mt , check_types=true )
__label__0 @ tf_export ( `` __internal__.types.data.dataset '' , v1= [ ] ) class datasetv2 ( abc.abc ) : `` '' '' represents the tensorflow 2 type ` tf.data.dataset ` . '' '' ''
__label__0 def testregisterunaryelementwiseapiafterhandler ( self ) : # test that it 's ok to call register_unary_elementwise_api after # dispatch_for_unary_elementwise_apis .
__label__0 @ tf_export ( `` __internal__.nest.yield_flat_paths '' , v1= [ ] ) def yield_flat_paths ( nest , expand_composites=false ) : `` '' '' yields paths for some nested structure .
__label__0 finder = doctest.doctestfinder ( ) finder.find ( module )
__label__0 # upgrade explicit compat v1 imports if ` upgrade_compat_v1_import ` is # enabled . then preprocess the updated root node . # we only do this upgrading once , because some forms of the import may # still cause errors but are n't trivially upgradeable , and we do n't want # to enter an infinite loop . e.g . ` from tensorflow.compat import v1 , v2 ` . if ( compat_v1_import in detections and self.upgrade_compat_v1_import and not after_compat_v1_upgrade ) : compatv1importreplacer ( ) .visit ( root_node ) return self.preprocess ( root_node , after_compat_v1_upgrade=true )
__label__0 args : allowed_set : an allowlisted set of dtypes to choose from instead of all of them .
__label__0 def testwarmstartembeddingcolumn ( self ) : # create old and new vocabs for embedding column `` sc_vocab '' . prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' )
__label__0 args : object : an object , possibly decorated .
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , ( 0 , 1 ) , nesttest.named0ab ( `` a '' , `` b '' ) )
__label__0 def preprocess ( self , root_node , after_compat_v1_upgrade=false ) : visitor = ast_edits.pastaanalyzevisitor ( tfapiimportanalysisspec ( ) ) visitor.visit ( root_node ) detections = set ( visitor.results )
__label__0 def testsegmentminnumsegmentsmore ( self ) : for dtype in self.int_types | self.float_types : maxval = dtypes.as_dtype ( dtype ) .max if dtype == np.float64 and self._finddevice ( `` tpu '' ) : maxval = np.inf self.assertallclose ( np.array ( [ 0 , maxval , 2 , 3 , maxval ] , dtype=dtype ) , self._segmentminv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 5 ) )
__label__0 def map_structure_up_to ( modality , shallow_tree , func , * inputs , * * kwargs ) : `` '' '' applies a function or op to a number of partially flattened inputs .
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) sparse_apply_ftrl = gen_training_ops.sparse_apply_ftrl ( var , accum , linear , grad , constant_op.constant ( indices , self._totype ( indices.dtype ) ) , lr , l1 , l2 , lr_power=lr_power ) out = self.evaluate ( sparse_apply_ftrl ) self.assertshapeequal ( out , sparse_apply_ftrl )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( arg0 , arg1 , deprecated=true ) : return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 module_wrapper._per_module_warning_limit = 5
__label__0 with ` expand_composites=false ` , we treat raggedtensor as a scalar .
__label__0 new_keywords = [ ] for kw in node.keywords : if kw.arg ! = `` align_corners '' : new_keywords.append ( kw ) node.keywords = new_keywords
__label__0 if ( len ( deprecated_positions ) + is_varargs_deprecated + is_kwargs_deprecated ! = len ( deprecated_arg_names_or_tuples ) ) : known_args = ( arg_spec.args + arg_spec.kwonlyargs + [ arg_spec.varargs , arg_spec.varkw ] ) missing_args = [ arg_name for arg_name in deprecated_arg_names if arg_name not in known_args ] raise valueerror ( 'the following deprecated arguments are not present ' f'in the function signature : { missing_args } . ' 'expected arguments from the following list : ' f ' { known_args } . ' )
__label__0 https : //docs.python.org/3/library/doctest.html # doctestparser-objects
__label__0 returns : the actual path the proto is written to. `` '' ''
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def testcalldeprecatedmodule ( self , mock_warning ) : from tensorflow.python.util import deprecated_module # pylint : disable=g-import-not-at-top self.assertequal ( 0 , mock_warning.call_count ) result = deprecated_module.a ( ) self.assertequal ( 1 , mock_warning.call_count ) self.assertequal ( 1 , result )
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 2 ] ) ) nmt_flat_paths = nest.flatten_with_tuple_paths ( nmt ) self.assertequal ( nmt_flat_paths [ 0 ] [ 0 ] , ( 0 , 0 ) ) self.assertallequal ( nmt_flat_paths [ 0 ] [ 1 ] , [ 2 ] )
__label__0 # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) : # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 20.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) v2_init = v2.insert ( `` k1 '' , 30.0 ) save = saver_module.saver ( var_list= { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } , restore_sequentially=true , save_relative_paths=true ) init_all_op = [ variables.global_variables_initializer ( ) , v2_init ]
__label__0 missing_names = set ( variable_names ) - set ( names_to_keys.keys ( ) ) if missing_names : raise valueerror ( `` attempting to warm-start from an object-based checkpoint , but found `` `` that the checkpoint did not contain values for all variables . the `` `` following variables were missing : { } '' .format ( missing_names ) ) return { name : names_to_keys [ name ] for name in variable_names }
__label__0 def testgetargspeconpartialwithvarkwargs ( self ) : `` '' '' tests getargspec on partial function with variable keyword arguments . '' '' ''
__label__0 def test_function ( self ) : self.assertequal ( `` _test_function '' , decorator_utils.get_qualified_name ( _test_function ) )
__label__0 def _gather_gpu_devices_cudart ( ) : `` '' '' try to gather nvidia gpu device information via libcudart . '' '' '' dev_info = [ ]
__label__0 applies ` func ( tuple_path , x [ 0 ] , x [ 1 ] , ... , * * kwargs ) ` where ` x [ i ] ` is an entry in ` structure [ i ] ` and ` tuple_path ` is a tuple of indices and/or dictionary keys ( as returned by ` nest.yield_flat_paths ` ) , which uniquely specifies the common path to x [ i ] in the structures . all structures in ` structure ` must have the same arity , and the return value will contain the results in the same structure . special kwarg ` check_types ` determines whether the types of iterables within the structure must be the same -- see * * kwargs definition below .
__label__0 def testkerassavemodelformat ( self ) : text = `` tf.keras.models.save_model ( model , path ) '' expected_text = `` tf.keras.models.save_model ( model , path , save_format='h5 ' ) '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertnotin ( `` saves to the tensorflow savedmodel format by default '' , report )
__label__0 # call standard transformers for this node . # make sure warnings come first , since args or names triggering warnings # may be removed by the other transformations . self._maybe_add_call_warning ( node , full_name , name ) # make all args into kwargs self._maybe_add_arg_names ( node , full_name ) # argument name changes or deletions self._maybe_modify_args ( node , full_name , name )
__label__0 returns : a ` sessionrunargs ` object `` '' '' return self._original_args
__label__0 > > > tensor = tf.ragged.constant ( [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] ) > > > tf.nest.flatten ( tensor , expand_composites=false ) [ < tf.raggedtensor [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] > ]
__label__0 if kwargs : raise valueerror ( `` only valid keyword arguments are ` check_types ` and `` `` ` expand_composites ` , not : ` % s ` `` % `` ` , ` `` .join ( kwargs.keys ( ) ) )
__label__0 import collections import itertools import typing # pylint : disable=unused-import ( used in doctests )
__label__0 _contrib_rnn_warning = ( ast_edits.warning , `` ( manual edit required ) tf.contrib.rnn . * has been deprecated , and `` `` widely used cells/functions will be moved to tensorflow/addons `` `` repository . please check it there and file github issues if necessary . '' )
__label__0 * test : ( test , start ) is indexed to fetch recent start times for a given test .
__label__0 # create a second helper , identical to the first . save2 = saver_module.saver ( { `` v '' : v } , max_to_keep=2 ) save2.set_last_checkpoints ( save.last_checkpoints )
__label__0 return { `` curand_version '' : header_version , `` curand_include_dir '' : os.path.dirname ( header_path ) , `` curand_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 def get_func_name ( func ) : `` '' '' returns name of passed callable . '' '' '' _ , func = tf_decorator.unwrap ( func ) if callable ( func ) : if tf_inspect.isfunction ( func ) : return func.__name__ elif tf_inspect.ismethod ( func ) : return ' % s. % s ' % ( func.__self__.__class__.__name__ , func.__func__.__name__ , ) else : # probably a class instance with __call__ return str ( type ( func ) ) else : raise valueerror ( 'argument ` func ` must be a callable . ' f'received func= { func } ( of type { type ( func ) } ) ' )
__label__0 # if there is no v2 canonical name , get v1 canonical name . api_names_attr = api_attrs_v1 [ api_name ] .names api_names = getattr ( undecorated_symbol , api_names_attr ) v1_canonical_name = get_canonical_name ( api_names , deprecated_api_names ) if add_prefix_to_v1_names : return 'compat.v1. % s ' % v1_canonical_name return v1_canonical_name
__label__0 gen_java.gen_java_docs ( package='org.tensorflow ' , source_path=merged_source / 'java ' , output_dir=pathlib.path ( flags.output_dir ) , site_path=pathlib.path ( flags.site_path ) )
__label__0 # make straightforward changes to convert to 2.0. in harder cases , # use compat.v1 . _default_mode = `` default ''
__label__0 the following collection types are recognized by ` tf.nest ` as nested structures :
__label__0 def preprocess ( self , root_node ) : # pylint : disable=unused-argument `` '' '' preprocess a parse tree . return a preprocessed node , logs and errors . '' '' '' return root_node , [ ] , [ ]
__label__0 returns : the ` decorator_func ` argument with new metadata attached. `` '' '' if decorator_name is none : decorator_name = inspect.currentframe ( ) .f_back.f_code.co_name decorator = tfdecorator ( decorator_name , target , decorator_doc , decorator_argspec ) setattr ( decorator_func , '_tf_decorator ' , decorator ) # objects that are callables ( e.g. , a functools.partial object ) may not have # the following attributes . if hasattr ( target , '__name__ ' ) : decorator_func.__name__ = target.__name__ if hasattr ( target , '__qualname__ ' ) : decorator_func.__qualname__ = target.__qualname__ if hasattr ( target , '__module__ ' ) : decorator_func.__module__ = target.__module__ if hasattr ( target , '__dict__ ' ) : # copy dict entries from target which are not overridden by decorator_func . for name in target.__dict__ : if name not in decorator_func.__dict__ : decorator_func.__dict__ [ name ] = target.__dict__ [ name ] if hasattr ( target , '__doc__ ' ) : decorator_func.__doc__ = decorator.__doc__ decorator_func.__wrapped__ = target # keeping a second handle to ` target ` allows callers to detect whether the # decorator was modified using ` rewrap ` . decorator_func.__original_wrapped__ = target if decorator_argspec : decorator_func.__signature__ = fullargspec_to_signature ( decorator_argspec ) elif callable ( target ) : try : signature = inspect.signature ( target ) except ( typeerror , valueerror ) : # certain callables such as builtins can not be inspected for signature . pass else : bound_instance = _get_bound_instance ( target ) # present the decorated func as a method as well if bound_instance and 'self ' in signature.parameters : signature = inspect.signature ( list ( signature.parameters.values ( ) ) [ 1 : ] ) decorator_func.__self__ = bound_instance
__label__0 args : summary_op : an operation that returns a summary for the event logs . if set to use_default , create an op that merges all the summaries. `` '' '' if summary_op is supervisor.use_default : summary_op = self._get_first_op_from_collection ( ops.graphkeys.summary_op ) if summary_op is none : summary_op = _summary.merge_all ( ) if summary_op is not none : ops.add_to_collection ( ops.graphkeys.summary_op , summary_op ) self._summary_op = summary_op
__label__0 `` ` hlomodule a_inference_f_120__.8 , entry_computation_layout= { ( f32 [ 10,20 ] { 1,0 } , f32 [ 10,20 ] { 1,0 } ) - > f32 [ 10,20 ] { 1,0 } }
__label__0 use this method when the ` var ` is backed by vocabulary . this method stitches the given ` var ` such that values corresponding to individual features in the vocabulary remain consistent irrespective of changing order of the features between old and new vocabularies .
__label__0 def testcastpositionalsecondargument ( self ) : for ( name , dtype ) in [ ( `` int32 '' , `` int32 '' ) , ( `` int64 '' , `` int64 '' ) , ( `` float '' , `` float32 '' ) , ( `` double '' , `` float64 '' ) , ( `` complex64 '' , `` complex64 '' ) , ( `` complex128 '' , `` complex128 '' ) , ( `` bfloat16 '' , `` bfloat16 '' ) ] : text = `` tf.to_ % s ( x , 'test ' ) '' % name expected_text = `` tf.cast ( x , name='test ' , dtype=tf. % s ) '' % dtype _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 returns : a ` tf.train.serverdef ` .
__label__0 foo ( x ) # here ` x ` is the actual value `` ` `` '' ''
__label__0 if axis == 0 : new_row_vocab_size = current_vocab_size new_col_vocab_size = v_shape [ 1 ] old_row_vocab_size = previous_vocab_size old_row_vocab_file = prev_vocab_path new_row_vocab_file = current_vocab_path old_col_vocab_file = none new_col_vocab_file = none num_row_oov_buckets = current_oov_buckets num_col_oov_buckets = 0 elif axis == 1 : # note that we must compute this value across all partitions , whereas # in the axis = 0 case , we can simply use v_shape [ 1 ] because we do n't # allow partitioning across axis = 1. new_row_vocab_size = total_v_first_axis new_col_vocab_size = current_vocab_size old_row_vocab_size = -1 old_row_vocab_file = none new_row_vocab_file = none old_col_vocab_file = prev_vocab_path new_col_vocab_file = current_vocab_path num_row_oov_buckets = 0 num_col_oov_buckets = current_oov_buckets else : raise valueerror ( `` the only supported values for the axis argument are 0 `` `` and 1. provided axis : { } '' .format ( axis ) )
__label__0 if need_to_bind_api_args : tensor_api = lambda v1 , v2 : api ( v1 , v2 , * args , * * kwargs ) else : tensor_api = api
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenuptocompatible ( self ) : simple_list = [ 2 ] mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) flattened_mt = nest.flatten_up_to ( shallow_tree=simple_list , input_tree=mt , check_types=false ) # expected flat_path_mt = [ tensor ( [ 1 ] ) ] self.assertallequal ( flattened_mt [ 0 ] , [ 1 ] ) flattened_list = nest.flatten_up_to ( shallow_tree=mt , input_tree=simple_list , check_types=false ) self.assertequal ( flattened_list , [ 2 ] )
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 args : saver_def : a ` saverdef ` protocol buffer . import_scope : optional ` string ` . name scope to use .
__label__0 args : sess : a ` session ` .
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.function_warnings = { `` * .foo '' : ( ast_edits.warning , `` not good '' ) }
__label__0 > > > a = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ true , true , true , true , true ] ) > > > b = maskedtensor ( [ 0 , 0 , 0 , 0 , 2 ] , [ true , true , true , true , true ] ) > > > tf.debugging.assert_greater ( a , b ) traceback ( most recent call last ) : ... invalidargumenterror : condition x > y did not hold .
__label__0 def __init__ ( self , major , minor , patch , identifier_string , version_type ) : `` '' '' constructor .
__label__0 def testdispatchfortypes_providingmissingargs ( self ) : original_handlers = test_op_with_optional._tf_fallback_dispatchers [ : ]
__label__0 def two ( self ) : return 2
__label__0 ` getcallargs ` will use the argspec from the outermost decorator that provides it . if no attached decorators modify argspec , the final unwrapped target 's argspec will be used. `` '' '' func = func_and_positional [ 0 ] positional = func_and_positional [ 1 : ] argspec = getfullargspec ( func ) call_args = named.copy ( ) this = getattr ( func , 'im_self ' , none ) or getattr ( func , '__self__ ' , none ) if ismethod ( func ) and this : positional = ( this , ) + positional remaining_positionals = [ arg for arg in argspec.args if arg not in call_args ] call_args.update ( dict ( zip ( remaining_positionals , positional ) ) ) default_count = 0 if not argspec.defaults else len ( argspec.defaults ) if default_count : for arg , value in zip ( argspec.args [ -default_count : ] , argspec.defaults ) : if arg not in call_args : call_args [ arg ] = value if argspec.kwonlydefaults is not none : for k , v in argspec.kwonlydefaults.items ( ) : if k not in call_args : call_args [ k ] = v return call_args
__label__0 < function > ( from < module > ) is deprecated and will be removed after < date > . instructions for updating : < instructions >
__label__0 rocrand_config = { `` rocrand_version_number '' : rocrand_version_number ( rocm_install_path ) }
__label__0 def _adamupdatenumpy ( self , param , g_t , t , m , v , alpha , beta1 , beta2 , epsilon ) : alpha_t = alpha * np.sqrt ( 1 - beta2 * * t ) / ( 1 - beta1 * * t )
__label__0 if save._write_version is saver_pb2.saverdef.v1 : # restore different ops from shard 0 of the saved files . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : v0 = variable_v1.variablev1 ( 111 , name= '' v0 '' ) t0 = saver_test_utils.checkpointedop ( name= '' t0 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` t0 '' : t0.saveable } , write_version=self._write_version , sharded=true ) self.evaluate ( variables.global_variables_initializer ( ) ) t0.insert ( `` k11 '' , 33.0 ) .run ( ) self.assertequal ( 111 , self.evaluate ( v0 ) ) self.assertequal ( b '' k11 '' , self.evaluate ( t0.keys ( ) ) ) self.assertequal ( 33.0 , self.evaluate ( t0.values ( ) ) ) save.restore ( sess , save_path + `` -00000-of-00002 '' ) self.assertequal ( 10 , self.evaluate ( v0 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( t0.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( t0.values ( ) ) )
__label__0 this helps to avoid circular dependencies. `` '' ''
__label__0 @ parameterized.named_parameters ( [ dict ( testcase_name= '' tuples '' , s1= ( 1 , 2 , 3 ) , s2= ( 4 , 5 ) , error_type=valueerror ) , dict ( testcase_name= '' dicts '' , s1= { `` a '' : 1 } , s2= { `` b '' : 2 } , error_type=valueerror ) , dict ( testcase_name= '' mixed '' , s1= ( 1 , 2 ) , s2= [ 3 , 4 ] , error_type=typeerror ) , dict ( testcase_name= '' nested '' , s1= { `` a '' : [ 2 , 3 , 4 ] , `` b '' : [ 1 , 3 ] } , s2= { `` b '' : [ 5 , 6 ] , `` a '' : [ 8 , 9 ] } , error_type=valueerror ) ] ) def testmapwithtuplepathsincompatiblestructures ( self , s1 , s2 , error_type ) : with self.assertraises ( error_type ) : nest.map_structure_with_tuple_paths ( lambda path , * s : 0 , s1 , s2 )
__label__0 this will produce the following docs :
__label__0 returns : a structure of the same form as the input structures whose leaves are the result of evaluating func on corresponding leaves of the input structures .
__label__0 def after_create_session ( self , session , coord ) : `` '' '' runs syncreplicasoptimizer initialization ops . '' '' '' local_init_success , msg = session_manager._ready ( # pylint : disable=protected-access self._ready_for_local_init_op , session , `` model is not ready for syncreplicasoptimizer local init . '' ) if not local_init_success : raise runtimeerror ( `` init operations did not make model ready for syncreplicasoptimizer `` `` local_init . init op : % s , error : % s '' % ( self._local_init_op.name , msg ) ) session.run ( self._local_init_op ) if self._init_tokens_op is not none : session.run ( self._init_tokens_op ) if self._q_runner is not none : self._q_runner.create_threads ( session , coord=coord , daemon=true , start=true )
__label__0 class customsaveable ( saver_module.basesaverbuilder.saveableobject ) : `` '' '' a custom saveable for checkpointedop . '' '' ''
__label__0 def _process_traceback_frames ( tb ) : new_tb = none tb_list = list ( traceback.walk_tb ( tb ) ) for f , line_no in reversed ( tb_list ) : if include_frame ( f.f_code.co_filename ) : new_tb = types.tracebacktype ( new_tb , f , f.f_lasti , line_no ) if new_tb is none and tb_list : f , line_no = tb_list [ -1 ] new_tb = types.tracebacktype ( new_tb , f , f.f_lasti , line_no ) return new_tb
__label__0 def _validate_symbol_names ( self ) - > none : `` '' '' validate you are exporting symbols under an allowed package .
__label__0 the tracing type is used to build the signature of a tf.function when traced , and to match arguments with existing signatures . when a function object is called , tf.function looks at the tracing type of the call arguments . if an existing signature of matching type exists , it will be used . otherwise , a new function is traced , and its signature will use the tracing type of the call arguments .
__label__0 @ runtime_checkable class supportstracingprotocol ( protocol ) : `` '' '' a protocol allowing custom classes to control tf.function retracing . '' '' ''
__label__0 return texts , np.array ( floats )
__label__0 `` '' '' print ( `` rename and copy binaries with % s to % s . '' % ( origin_tag , new_tag ) ) origin_binary = binary_string_template % ( package , version , origin_tag ) new_binary = binary_string_template % ( package , version , new_tag ) zip_ref = zipfile.zipfile ( os.path.join ( directory , origin_binary ) , `` r '' )
__label__0 def testdispatchforunaryelementwiseapis ( self ) :
__label__0 def test_flags_flags ( self ) : _ , _ , errors , _ = self._upgrade ( `` tf.flags.flags '' ) self.assertin ( `` tf.flags and tf.app.flags have been removed '' , errors [ 0 ] )
__label__0 class getfuncnametest ( test.testcase ) :
__label__0 todo ( b/268078256 ) : check if this comment is valid , and if so , ensure it 's handled in the function below . the list will be sorted by name .
__label__0 def _restore_checkpoint ( self , master : str , saver : saver_lib.saver = none , checkpoint_dir : str = none , checkpoint_filename_with_path : str = none , wait_for_checkpoint=false , max_wait_secs=7200 , config=none , ) - > tuple [ session.session , bool ] : `` '' '' creates a ` session ` , and tries to restore a checkpoint .
__label__0 def _csv_data ( self , logdir ) : # create a small data file with 3 csv records . data_path = os.path.join ( logdir , `` data.csv '' ) with open ( data_path , `` w '' ) as f : f.write ( `` 1,2,3\n '' ) f.write ( `` 4,5,6\n '' ) f.write ( `` 7,8,9\n '' ) return data_path
__label__0 is_line_split = false for line_idx , code_line in enumerate ( cell_lines ) :
__label__0 text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=foo ( ) ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( foo ( ) ) ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 from google.protobuf import message from tensorflow.core.framework import graph_pb2 from tensorflow.core.framework import node_def_pb2 from tensorflow.python.framework import tensor_util from tensorflow.tools.proto_splitter import constants from tensorflow.tools.proto_splitter import split from tensorflow.tools.proto_splitter import util
__label__0 class unaliasedtfimport ( ast_edits.analysisresult ) :
__label__0 > > > ragged_tensor1 = tf.raggedtensor.from_row_splits ( ... values= [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ] , ... row_splits= [ 0 , 4 , 4 , 7 , 8 , 8 ] ) > > > ragged_tensor2 = tf.raggedtensor.from_row_splits ( ... values= [ 3 , 1 , 4 ] , ... row_splits= [ 0 , 3 ] ) > > > tf.nest.assert_same_structure ( ... ragged_tensor1 , ... ragged_tensor2 , ... expand_composites=true )
__label__0 for trace in trace_map.values ( ) : self.assertregex ( repr ( trace ) , 'tf_stack_test.py ' , trace )
__label__0 def gather_build_configuration ( ) : build_config = test_log_pb2.buildconfiguration ( ) build_config.mode = flags.compilation_mode # include all flags except includes cc_flags = [ flag for flag in shlex.split ( flags.cc_flags ) if not flag.startswith ( `` -i '' ) ] build_config.cc_flags.extend ( cc_flags ) return build_config
__label__0 if not self._tfmw_public_apis : raise if name not in self._tfmw_public_apis : raise attr = self._tfmw_import_module ( name )
__label__0 def sum_tensors ( * tensors ) : return sum ( tensors )
__label__0 def __exit__ ( self , unused_type , unused_value , unused_traceback ) : top = self._stack_dict [ self._thread_key ] .pop ( ) assert top is self , 'concurrent access ? '
__label__0 def __call__ ( self ) : pass
__label__0 def find_cuda_config ( ) : `` '' '' returns a dictionary of cuda library and header file paths . '' '' '' libraries = [ argv.lower ( ) for argv in sys.argv [ 1 : ] ] cuda_version = os.environ.get ( `` tf_cuda_version '' , `` '' ) base_paths = _list_from_env ( `` tf_cuda_paths '' , _get_default_cuda_paths ( cuda_version ) ) base_paths = [ path for path in base_paths if os.path.exists ( path ) ]
__label__0 num_dense = len ( dense_types ) num_ragged = len ( ragged_value_types ) assert len ( ragged_value_types ) == len ( ragged_split_types ) assert len ( parse_example_op.inputs ) == 5 + num_dense
__label__0 # there are more classes descended into , at least __class__ and # __class__.__base__ , neither of which are interesting to us , and which may # change as part of python version etc. , so we do n't test for them .
__label__0 return '\n'.join ( lines )
__label__0 _address_re = re.compile ( r'\bat 0x [ 0-9a-f ] * ? > ' ) # todo ( yashkatariya ) : add other tensor 's string substitutions too . # tf.raggedtensor does n't need one . _numpy_output_re = re.compile ( r ' < tf.tensor. * ? numpy= ( . * ? ) > ' , re.dotall )
__label__0 class tfapichangespec ( ast_edits.noupdatespec ) : `` '' '' list of maps that describe what changed in the api . '' '' ''
__label__0 if self.want_floats.size == 0 : # if there are no floats in the `` want '' string , ignore all the floats in # the result . `` np.array ( [ ... ] ) '' matches `` np.array ( [ 1.0 , 2.0 ] ) '' return true
__label__0 _contrib_seq2seq_warning = ( ast_edits.warning , `` ( manual edit required ) tf.contrib.seq2seq . * have been migrated to `` `` ` tfa.seq2seq . * ` in tensorflow addons . please see `` `` https : //github.com/tensorflow/addons for more info . '' )
__label__0 from absl.testing import absltest from absl.testing import parameterized
__label__0 def hipruntime_version_number ( path ) : possible_version_files = [ `` include/hip/hip_version.h '' , # rocm 5.2 `` hip/include/hip/hip_version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` hip runtime version file not found in { } '' .format ( possible_version_files ) )
__label__0 with self.assertraisesregex ( valueerror , ( `` do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\nsecond structure : `` ) ) : nest.assert_same_structure ( structure1 , structure_different_nesting )
__label__0 def g ( a , b , kw1 , c ) : ... def g2 ( a , b , kw1 , c , d ) : ...
__label__0 def func ( m , n ) : return 2 * m + n
__label__0 # gather the rest info = cpuinfo.get_cpu_info ( ) cpu_info.cpu_info = info [ 'brand ' ] cpu_info.num_cores = info [ 'count ' ] cpu_info.mhz_per_cpu = info [ 'hz_advertised_raw ' ] [ 0 ] / 1.0e6 l2_cache_size = re.match ( r ' ( \d+ ) ' , str ( info.get ( 'l2_cache_size ' , `` ) ) ) if l2_cache_size : # if a value is returned , it 's in kb cpu_info.cache_size [ 'l2 ' ] = int ( l2_cache_size.group ( 0 ) ) * 1024
__label__0 tf.compat.v1.enable_v2_behavior ( )
__label__0 def generate_raw_ops_doc_lt_214 ( self ) : `` '' '' generates docs for ` tf.raw_ops ` . '' '' '' del self
__label__0 # we still need all the names that are toplevel on tensorflow_core from tensorflow_core import *
__label__0 an output is marked as used if any of its attributes are read , modified , or updated . examples when the output is a ` tensor ` include :
__label__0 note : to use locking , the file is first opened , then its descriptor is used to lock and read it . the lock is released when the file is closed . do not open that same file a 2nd time while the lock is already held , because when that 2nd file descriptor is closed , the lock will be released prematurely. `` '' '' client = datastore.client ( )
__label__0 def __reduce__ ( self ) : return importlib.import_module , ( self.__name__ , )
__label__0 with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) self.asserttrue ( isinstance ( fruit_weights , variables.partitionedvariable ) ) prev_tensor_name , var = ws_util._get_var_info ( fruit_weights ) checkpoint_utils.init_from_checkpoint ( self.get_temp_dir ( ) , { prev_tensor_name : var } ) self.evaluate ( variables.global_variables_initializer ( ) ) fruit_weights = fruit_weights._get_variable_list ( ) new_val = np.concatenate ( [ fruit_weights [ 0 ] .eval ( sess ) , fruit_weights [ 1 ] .eval ( sess ) ] , axis=0 ) self.assertallclose ( prev_val , new_val )
__label__0 > > > int_feature = tf.train.feature ( ... int64_list=tf.train.int64list ( value= [ 1 , 2 , 3 , 4 ] ) ) > > > float_feature = tf.train.feature ( ... float_list=tf.train.floatlist ( value= [ 1. , 2. , 3. , 4 . ] ) ) > > > bytes_feature = tf.train.feature ( ... bytes_list=tf.train.byteslist ( value= [ b '' abc '' , b '' 1234 '' ] ) ) > > > > > > example = tf.train.example ( ... features=tf.train.features ( feature= { ... 'my_ints ' : int_feature , ... 'my_floats ' : float_feature , ... 'my_bytes ' : bytes_feature , ... } ) )
__label__0 with self.assertraisesregex ( typeerror , `` did n't return a depth=1 structure of bools '' ) : nest.get_traverse_shallow_structure ( lambda _ : [ 1 ] , [ 1 ] )
__label__0 finally : dispatch.unregister_dispatch_for ( binary_elementwise_api_handler )
__label__0 parser = argparse.argumentparser ( description= '' '' '' git hash injection into bazel . if used with -- configure < path > will search for git directory and put symlinks into source so that a bazel genrule can call -- generate '' '' '' )
__label__0 class testclass ( object ) :
__label__0 def test_private_child_removal ( self ) : visitor = self.testvisitor ( ) children = [ ( 'name1 ' , 'thing1 ' ) , ( '_name2 ' , 'thing2 ' ) ] public_api.publicapivisitor ( visitor ) ( 'test ' , 'dummy ' , children ) # make sure the private symbols are removed before the visitor is called . self.assertequal ( [ ( 'name1 ' , 'thing1 ' ) ] , visitor.last_children ) self.assertequal ( [ ( 'name1 ' , 'thing1 ' ) ] , children )
__label__0 1. python dict :
__label__0 # suppress normal variable inits to make sure the local one is # initialized via local_init_op . sv = supervisor.supervisor ( logdir=logdir , init_op=none ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) self.assertallclose ( [ 4 , 5 , 6 ] , sess.run ( w ) ) sv.stop ( )
__label__0 # as shown in the previous test , the local_step for all workers should be # still 0 so their next computation will also be dropped . sessions [ 0 ] .run ( train_ops [ 0 ] ) sessions [ 1 ] .run ( train_ops [ 1 ] ) sessions [ 2 ] .run ( train_ops [ 2 ] )
__label__0 def testwithcallableclass ( self ) : callable_instance = sillycallableclass ( ) code = function_utils.get_func_code ( callable_instance ) self.assertisnotnone ( code ) self.assertregex ( code.co_filename , 'function_utils_test.py ' )
__label__0 contrib_one_device_strategy_warning = ( ast_edits.error , `` ( manual edit required ) tf.contrib.distribute.onedevicestrategy has `` `` been migrated to tf.distribute.onedevicestrategy. `` + distribute_strategy_api_changes )
__label__0 try : import attr # pylint : disable=g-import-not-at-top except importerror : attr = none
__label__0 returns : decorated function or method .
__label__0 returns : a shallow structure containing python bools , which can be passed to ` map_structure_up_to ` and ` flatten_up_to ` .
__label__0 def test_eager_argmax ( self ) : def fn ( ) : _ = math_ops.argmax ( [ 0 , 1 ] , axis=2 )
__label__0 save2 = saver_module.saver ( [ v ] ) save2.restore ( sess , save_path ) self.assertequal ( self.evaluate ( v ) , [ 1 ] )
__label__0 if had_keras and not has_keras : new_children.append ( ( `` keras '' , parent.keras ) )
__label__0 def fn ( self , * * x ) : del x self.asserttrue ( function_utils.has_kwargs ( foohaskwargs ( ) .fn ) )
__label__0 not_supported = opdispatcher.not_supported
__label__0 def _userpcconfig ( self ) : `` '' '' return a ` tf.compat.v1.configproto ` that ensures we use the rpc stack for tests .
__label__0 def test_function ( x ) : `` '' '' test function docstring . '' '' '' return x + 1
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import function_utils
__label__0 example usage :
__label__0 shallow_tree = { ' a ' : none , ' b ' : { 1 : none } } map_structure_with_tuple_paths_up_to ( shallow_tree , print_path_and_values , lowercase , uppercase , check_types=false ) path : ( ' a ' , ) , values : ( ' a ' , ' a ' ) path : ( ' b ' , 1 ) , values : ( 'b1 ' , b1 ' ) `` `
__label__0 def getfile ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getfile . '' '' '' unwrapped_object = tf_decorator.unwrap ( object ) [ 1 ]
__label__0 class indexedslices ( object ) : `` '' '' interface for internal isinstance checks to framework/indexed_slices.py .
__label__0 def testkwargnotinterned ( self ) : func = ( _function_parameter_canonicalizer_binding_for_test .functionparametercanonicalizer ( [ 'long_parameter_name ' ] , ( ) ) ) kwargs = dict ( [ ( ' _'.join ( [ 'long ' , 'parameter ' , 'name ' ] ) , 5 ) ] ) func.canonicalize ( * * kwargs )
__label__0 if isinstance ( shallow_tree , _wrapt.objectproxy ) : shallow_type = type ( shallow_tree.__wrapped__ ) else : shallow_type = type ( shallow_tree )
__label__0 from tensorflow.python.util import deprecated_module_new from tensorflow.python.util import deprecation
__label__0 returns : a bool. `` '' '' return self._is_chief
__label__0 updates docstrings for ` dispatch_for_api ` , ` dispatch_for_unary_elementwise_apis ` , and ` dispatch_for_binary_elementwise_apis ` , by replacing the string ' < < api_list > > ' with a list of apis that have been registered for that decorator. `` '' '' _update_docstring_with_api_list ( dispatch_for_unary_elementwise_apis , _unary_elementwise_apis ) _update_docstring_with_api_list ( dispatch_for_binary_elementwise_apis , _binary_elementwise_apis ) _update_docstring_with_api_list ( dispatch_for_binary_elementwise_assert_apis , _binary_elementwise_assert_apis ) _update_docstring_with_api_list ( dispatch_for_api , _type_based_dispatch_signatures )
__label__1 def generate_prime_numbers ( n ) : primes = [ ] for num in range ( 2 , n + 1 ) : if all ( num % i ! = 0 for i in range ( 2 , int ( num * * 0.5 ) + 1 ) ) : primes.append ( num ) return primes
__label__0 output_checker.check_output ( want=want , got=got , optionflags=doctest.ellipsis )
__label__0 furthermore , if a symbol previously added with ` add_to_global_allowlist ` , then it will always be allowed . this is useful for internal tests .
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , [ maskedtensor ] ) def my_add ( x , y , name=none ) : # pylint : disable=unused-variable del x , y , name
__label__0 def __setattr__ ( self , name , value ) : if name.startswith ( _tensorflow_lazy_loader_prefix ) : super ( ) .__setattr__ ( name , value ) else : module = self._load ( ) setattr ( module , name , value ) self.__dict__ [ name ] = value try : # check if the module has __all__ if name not in self.__all__ and name ! = `` __all__ '' : self.__all__.append ( name ) except attributeerror : pass
__label__0 ` recovery_wait_secs ` is the number of seconds between checks that the model is ready . it is used by processes to wait for a model to be initialized or restored . defaults to 30 seconds .
__label__0 def testisanytargetmethod ( self ) : class mymodule :
__label__0 # we should be done . self.assertraises ( stopiteration , lambda : next ( rr ) )
__label__0 # split ` graphdef.node ` node_splitter = repeatedmessagesplitter ( proto , `` node '' , [ constantnodedefsplitter , largemessagesplitter ] , parent_splitter=self , fields_in_parent= [ ] , )
__label__0 args : root_directory : directory to walk and process . output_root_directory : directory to use as base . copy_other_files : copy files that are not touched by this converter .
__label__0 the specific implementation uses 0 as the tf.nest atom , but other tf.nest atoms could also serve the purpose . note , the ` typespec ` of none is not a tf.nest atom .
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 def testposandkwd ( self ) : self.assertequal ( self._matmul_func.canonicalize ( 2 , 3 , transpose_a=true , name='my_matmul ' ) , [ 2 , 3 , true , false , false , false , false , false , 'my_matmul ' ] )
__label__0 def object_graph_key_mapping ( checkpoint_path ) : `` '' '' return name to key mappings from the checkpoint .
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 returns : decorated function or method .
__label__0 conditionally conformant pair of ` sequenceexample ` s , the parser configuration determines if the second ` feature_lists ` is consistent ( zero-length ) or invalid ( missing `` movie_ratings '' ) :
__label__0 def f ( self , a ) : pass
__label__0 # tf.raw_ops.acos takes tf.bfloat16 , tf.half , tf.float32 , tf.float64 , # tf.complex64 , tf.complex128 , but get_random_numeric_tensor only generates # tf.float16 , tf.float32 , tf.float64 , tf.int32 , tf.int64 dtype = fh.get_tf_dtype ( allowed_set= [ tf.float16 , tf.float32 , tf.float64 ] ) input_tensor = fh.get_random_numeric_tensor ( dtype=dtype ) _ = tf.raw_ops.acosh ( x=input_tensor )
__label__0 def testgetgpucompilablekernelnames ( self ) : `` '' '' tests retrieving compilable op names for gpu . '' '' '' op_names = pywrap_xla_ops.get_gpu_kernel_names ( ) self.assertgreater ( op_names.__len__ ( ) , 0 ) self.assertequal ( op_names.count ( 'max ' ) , 1 ) self.assertequal ( op_names.count ( 'min ' ) , 1 ) self.assertequal ( op_names.count ( 'matmul ' ) , 1 )
__label__0 if the model can not be recovered successfully then it is initialized by running the ` init_op ` and calling ` init_fn ` if they are provided . the ` local_init_op ` is also run after init_op and init_fn , regardless of whether the model was recovered successfully , but only if ` ready_for_local_init_op ` passes .
__label__0 class cyclist ( object ) : pass cyclist.cycle = cyclist
__label__0 # match anything , except if the look-behind sees a closing fence . no_fence = ' ( . ( ? < ! `` ` ) ) * ? ' self.fence_cell_re = re.compile ( rf '' '' '' ^ ( # after a newline \s * `` ` \s * ( { fence_label } ) \n # open a labeled `` ` fence ( ? p < doctest > { no_fence } ) # match anything except a closing fence \n\s * `` ` \s * ( \n| $ ) # close the fence . ) ( # optional ! [ \s\n ] * # any number of blank lines . `` ` \s * \n # open `` ` ( ? p < output > { no_fence } ) # anything except a closing fence \n\s * `` ` # close the fence . ) ? `` `` '' , # multiline so ^ matches after a newline re.multiline | # dotall so ` . ` matches newlines . re.dotall | # verbose to allow comments/ignore-whitespace . re.verbose )
__label__0 @ tf_contextlib.contextmanager def test_params_and_defaults ( a , b=2 , c=true , d='hello ' ) : return [ a , b , c , d ]
__label__0 cached_classproperty.__doc__ = _cachedclassproperty.__doc__
__label__0 def _load ( self ) : `` '' '' import the target module and insert it into the parent 's namespace . '' '' '' module = _importlib.import_module ( self.__name__ ) self._parent_module_globals [ self._local_name ] = module self.__dict__.update ( module.__dict__ ) return module
__label__0 `` ` python with tf.graph ( ) .as_default ( ) : ... add operations to the graph ... # create a sessionmanager that will checkpoint the model in '/tmp/mydir ' . sm = sessionmanager ( ) sess = sm.prepare_session ( master , init_op , saver , checkpoint_dir ) # use the session to train the graph . while true : sess.run ( < my_train_op > ) `` `
__label__0 try : x = car ( constant_op.constant ( 1 ) , constant_op.constant ( 3 ) ) y = car ( constant_op.constant ( 10 ) , constant_op.constant ( 20 ) ) z = math_ops.add ( x , y ) self.assertallequal ( z.size , 11 ) self.assertallequal ( z.speed , 23 )
__label__0 def yield_flat_up_to ( modality , shallow_tree , input_tree , is_nested_fn , path= ( ) ) : `` '' '' yields ( path , value ) pairs of input_tree flattened up to shallow_tree .
__label__0 return os.getenv ( `` git_commit '' )
__label__0 def testnameonboundproperty ( self ) : self.assertequal ( 'return_params ' , testdecoratedclass ( ) .return_params.__name__ )
__label__0 if os.name == 'nt ' : extension_name = 'python/_pywrap_tensorflow_internal.pyd ' else : extension_name = 'python/_pywrap_tensorflow_internal.so '
__label__0 returns : consumed integer based on input bytes and constraints. `` '' '' return self.fdp.consumeintinrange ( min_int , max_int )
__label__0 # fetch params to validate initial values self.assertallclose ( [ 1.0 , 2.0 ] , self.evaluate ( var0 ) ) self.assertallclose ( [ 3.0 , 4.0 ] , self.evaluate ( var1 ) ) # step 1 : the rms accumulators where 1. so we should see a normal # update : v -= grad * learning_rate self.evaluate ( update ) # check the root mean square accumulators . self.assertallcloseaccordingtotype ( np.array ( [ 0.901 , 0.901 ] ) , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.90001 , 0.90001 ] ) , self.evaluate ( rms1 ) ) # check the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) ] ) , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) ] ) , self.evaluate ( var1 ) ) # step 2 : the root mean square accumulators contain the previous update . self.evaluate ( update ) # check the rms accumulators . self.assertallcloseaccordingtotype ( np.array ( [ 0.901 * 0.9 + 0.001 , 0.901 * 0.9 + 0.001 ] ) , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.90001 * 0.9 + 1e-5 , 0.90001 * 0.9 + 1e-5 ] ) , self.evaluate ( rms1 ) ) # check the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) - ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1.0 ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1.0 ) ) - ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1.0 ) ) ] ) , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) - ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 1e-5 + 1.0 ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1.0 ) ) - ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 1e-5 + 1.0 ) ) ] ) , self.evaluate ( var1 ) )
__label__0 s1 = save.save ( none , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s1 ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s1 ] , save_dir=save_dir )
__label__0 s2 = save.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s1 , s2 ] , save.last_checkpoints ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( 2 , len ( gfile.glob ( s1 ) ) ) else : self.assertequal ( 4 , len ( gfile.glob ( s1 + `` * '' ) ) ) self.asserttrue ( gfile.exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( 2 , len ( gfile.glob ( s2 ) ) ) else : self.assertequal ( 4 , len ( gfile.glob ( s2 + `` * '' ) ) ) self.asserttrue ( gfile.exists ( checkpoint_management.meta_graph_filename ( s2 ) ) )
__label__0 flags.define_string ( `` output_dir '' , `` /tmp/out '' , `` a directory , where the docs will be output to . '' )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 returns : a tuple ( args , kwargs ) , where any positional or keyword parameters in ` iterable_params ` have their value converted to a ` list ` . `` '' '' args = list ( args ) for name , index in iterable_params : if index < len ( args ) : args [ index ] = list ( args [ index ] ) elif name in kwargs : kwargs [ name ] = list ( kwargs [ name ] ) return tuple ( args ) , kwargs
__label__0 def stop_on_exception ( self ) : `` '' '' context handler to stop the supervisor when an exception is raised .
__label__0 def __repr__ ( self ) : if self._tfll_initialized : return ( f '' < keraslazyloader ( { self._tfll_keras_version } ) `` f '' { self.__name__ } as { self._tfll_local_name } mode= { self._tfll_mode } > '' ) return `` < keraslazyloader > ''
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_prop_no_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 text = `` tf.argmin ( input , 0 , n ) '' expected_text = `` tf.argmin ( input , 0 , name=n ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def _find_roctracer_config ( rocm_install_path ) :
__label__0 def build ( self ) : debug_info_str = super ( ) .build ( ) debug_info = graph_debug_info_pb2.graphdebuginfo ( ) debug_info.parsefromstring ( debug_info_str ) return debug_info
__label__0 def build ( self ) : if context.executing_eagerly ( ) : raise runtimeerror ( `` use save/restore instead of build in eager mode . '' ) self._build ( self._filename , build_save=true , build_restore=true )
__label__0 def __tf_tensor__ ( self , dtype=none , name=none ) : `` '' '' converts this object to a tensor .
__label__0 def testunboundfuncwithtwoparamsdefaultonepositional ( self ) :
__label__0 # only traverse modules and classes if not tf_inspect.isclass ( root ) and not tf_inspect.ismodule ( root ) : return
__label__0 args : deprecated_name : name of old module . new_module : module to replace the old module . deletion_version : version of tensorflow in which the old module will be removed .
__label__0 def testtypebaseddispatchtargetsfor ( self ) : maskedtensorlist = typing.list [ typing.union [ maskedtensor , tensor_lib.tensor ] ] try :
__label__0 @ tf_export ( `` __internal__.nest.is_mapping '' , v1= [ ] ) def is_mapping ( obj ) : `` '' '' returns a true if its input is a collections.mapping . '' '' '' return _is_mapping ( obj )
__label__0 class samevariablescleartest ( test.testcase ) :
__label__0 graph_saver = saver_module.saver ( [ w1 , w2 ] ) graph_saver.restore ( none , graph_ckpt_prefix )
__label__0 the compatibility module also provides the following aliases for common sets of python types :
__label__0 returns : a function that takes symbol as an argument and adds _tf_deprecated_api_names to that symbol . _tf_deprecated_api_names would be set to a list of deprecated endpoint names for the symbol. `` '' ''
__label__0 _ , biases = while_loop.while_loop ( loop_cond , loop_body , [ constant_op.constant ( 0 ) , variable_v1.variablev1 ( array_ops.zeros ( [ 32 ] ) ) ] ) hidden2 = nn_ops.relu ( math_ops.matmul ( hidden1 , weights ) + biases ) # linear with ops_lib.name_scope ( `` softmax_linear '' ) : weights = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 32 , 10 ] , stddev=1.0 / math.sqrt ( float ( 32 ) ) ) , name= '' weights '' ) biases = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' biases '' ) logits = math_ops.matmul ( hidden2 , weights ) + biases ops_lib.add_to_collection ( `` logits '' , logits ) init_all_op = variables.global_variables_initializer ( )
__label__0 # the next one should be a stop message if we closed cleanly . ev = next ( rr ) self.assertequal ( event_pb2.sessionlog.stop , ev.session_log.status )
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) ws_util._warm_start_var_with_vocab ( fruit_weights , new_vocab_path , 5 , self.get_temp_dir ( ) , prev_vocab_path , previous_vocab_size=2 ) self.evaluate ( variables.global_variables_initializer ( ) ) # old vocabulary limited to [ 'apple ' , 'banana ' ] . self.assertallclose ( [ [ 0 . ] , [ 0 . ] , [ 1 . ] , [ 0.5 ] , [ 0 . ] ] , fruit_weights.eval ( sess ) )
__label__0 this does not create any symlinks . it requires the build system to build unconditionally .
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 def _add_deprecated_arg_notice_to_docstring ( doc , date , instructions , deprecated_names ) : `` '' '' adds a deprecation notice to a docstring for deprecated arguments . '' '' ''
__label__0 if collab == `` true '' : env [ `` collaborator_build '' ] = true
__label__1 def findmediansortedarrays ( nums1 , nums2 ) : m , n = len ( nums1 ) , len ( nums2 ) # nums1의 길이가 항상 더 길도록 스왑 if m > n : nums1 , nums2 , m , n = nums2 , nums1 , n , m imin , imax , half_len = 0 , m , ( m + n + 1 ) // 2 while imin < = imax : i = ( imin + imax ) // 2 j = half_len - i if i < m and nums2 [ j-1 ] > nums1 [ i ] : # i가 작은 경우 , i를 증가시켜서 j를 줄입니다 . imin = i + 1 elif i > 0 and nums1 [ i-1 ] > nums2 [ j ] : # i가 큰 경우 , i를 감소시켜서 j를 늘립니다 . imax = i - 1 else : # 적절한 i를 찾았습니다 . if i == 0 : max_of_left = nums2 [ j-1 ] elif j == 0 : max_of_left = nums1 [ i-1 ] else : max_of_left = max ( nums1 [ i-1 ] , nums2 [ j-1 ] ) if ( m + n ) % 2 == 1 : return max_of_left if i == m : min_of_right = nums2 [ j ] elif j == n : min_of_right = nums1 [ i ] else : min_of_right = min ( nums1 [ i ] , nums2 [ j ] ) return ( max_of_left + min_of_right ) / 2.0 # test cases nums1_1 , nums2_1 = [ 1 , 3 ] , [ 2 ] nums1_2 , nums2_2 = [ 1 , 2 ] , [ 3 , 4 ] print ( findmediansortedarrays ( nums1_1 , nums2_1 ) ) # output : 2.00000 print ( findmediansortedarrays ( nums1_2 , nums2_2 ) ) # output : 2.50000
__label__0 with self.cached_session ( ) as sess : v = variable_v1.variablev1 ( [ 10.0 ] , name= '' v '' ) # run the initializer now to avoid the 0.5s overhead of the first run ( ) # call , which throws the test timing off in fastbuild mode . self.evaluate ( variables.global_variables_initializer ( ) ) # create a saver that will keep the last 2 checkpoints plus one every 0.7 # seconds . start_time = time.time ( ) mock_time.time.return_value = start_time save = saver_module.saver ( { `` v '' : v } , max_to_keep=2 , keep_checkpoint_every_n_hours=0.7 / 3600 ) self.assertequal ( [ ] , save.last_checkpoints )
__label__0 # transform from x.f ( y ) to tf.compat.v1.data.f ( x , y ) # fortunately , node.func.value should already have valid position info node.args = [ node.func.value ] + node.args node.func.value = ast_edits.full_name_node ( `` tf.compat.v1.data '' )
__label__0 like ` tf_inspect.getfullargspec ` and python ` inspect.getfullargspec ` , it does not unwrap python decorators .
__label__0 def test_contrib_to_addons_move ( self ) : small_mapping = { `` tf.contrib.layers.poincare_normalize '' : `` tfa.layers.poincarenormalize '' , `` tf.contrib.layers.maxout '' : `` tfa.layers.maxout '' , `` tf.contrib.layers.group_norm '' : `` tfa.layers.groupnormalization '' , `` tf.contrib.layers.instance_norm '' : `` tfa.layers.instancenormalization '' , } for symbol , replacement in small_mapping.items ( ) : text = `` { } ( 'stuff ' , * args , * * kwargs ) '' .format ( symbol ) _ , report , _ , _ = self._upgrade ( text ) self.assertin ( replacement , report )
__label__0 text = ( `` tf.sparse_split ( sp_input=sp_input , num_split=num_split , `` `` name=name , split_dim=axis ) '' ) expected_text = ( `` tf.sparse.split ( sp_input=sp_input , num_split=num_split , `` `` name=name , axis=axis ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def _write_graph ( self ) : `` '' '' writes graph_def to ` logdir ` and adds it to summary if applicable . '' '' '' assert self._is_chief if self._logdir : training_util.write_graph ( self._graph.as_graph_def ( add_shapes=true ) , self._logdir , `` graph.pbtxt '' ) if self._summary_writer and not self._graph_added_to_summary : self._summary_writer.add_graph ( self._graph ) self._summary_writer.add_meta_graph ( self._meta_graph_def ) self._graph_added_to_summary = true
__label__0 `` ` c `` `
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated arguments ) '' `` \n '' `` \ndeprecated : some arguments are deprecated : ` ( deprecated ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : \n % s '' `` \n '' `` \nargs : '' `` \n arg0 : arg 0 . '' `` \n arg1 : arg 1 . '' `` \n deprecated : deprecated ! '' `` \n '' `` \nreturns : '' `` \n sum of args . '' % ( date , instructions ) , _fn.__doc__ )
__label__0 raises : valueerror : if ` flat_sequence ` and ` structure ` have different atom counts . typeerror : ` structure ` is or contains a dict with non-sortable keys. `` '' '' return nest_util.pack_sequence_as ( nest_util.modality.core , structure , flat_sequence , expand_composites )
__label__0 with self.assertraisesregex ( valueerror , `` same nested structure '' ) : nest.map_structure ( lambda x , y : none , 3 , ( 3 , ) )
__label__0 else : header_version = cuda_version header_path = _find_file ( base_paths , _header_paths ( ) , `` cusolver_common.h '' ) cusolver_version = required_version
__label__0 # will just use session 1 to verify all the variables later . var_0_g_1 = graphs [ 1 ] .get_tensor_by_name ( `` v0:0 '' ) var_1_g_1 = graphs [ 1 ] .get_tensor_by_name ( `` v1:0 '' ) var_sparse_g_1 = graphs [ 1 ] .get_tensor_by_name ( `` v_sparse:0 '' ) local_step_1 = graphs [ 1 ] .get_tensor_by_name ( `` sync_rep_local_step:0 '' ) global_step = graphs [ 1 ] .get_tensor_by_name ( `` global_step:0 '' )
__label__0 * these nested structure vs. nested structure comparisons will pass :
__label__0 self._tfll_keras_version = keras_version if keras_version is not none : if self._tfll_submodule is not none : package_name += `` . '' + self._tfll_submodule super ( ) .__init__ ( self._tfll_name , self._tfll_parent_module_globals , package_name ) else : raise importerror ( # pylint : disable=raise-missing-from `` keras can not be imported . check that it is installed . '' )
__label__0 if ` shallow_tree ` and ` input_tree ` are not sequences , this returns a single-item list : ` [ ( ( ) , input_tree ) ] ` .
__label__0 def testpacksequenceas_wronglengthserror ( self ) : with self.assertraisesregex ( valueerror , `` structure had 2 atoms , but flat_sequence had 3 items . `` ) : nest.pack_sequence_as ( [ `` hello '' , `` world '' ] , [ `` and '' , `` goodbye '' , `` again '' ] )
__label__0 # check the sep parameter : if it 's definitely an empty string , use # tf.strings.bytes_split ( ) . if we ca n't tell , then use compat.v1 . found_sep = false for i , kw in enumerate ( node.keywords ) : if kw.arg == `` sep '' : found_sep = true if isinstance ( kw.value , ast.str ) : if kw.value.s == `` '' : node = _rename_func ( node , full_name , `` tf.strings.bytes_split '' , logs , `` splitting bytes is not handled by tf.strings.bytes_split ( ) . '' ) node.keywords.pop ( i ) else : return _rename_to_compat_v1 ( node , full_name , logs , `` the semantics for tf.string_split 's sep parameter have changed `` `` when sep is the empty string ; but sep is not a string literal , `` `` so we ca n't tell if it 's an empty string . '' ) if not found_sep : return _rename_to_compat_v1 ( node , full_name , logs , `` the semantics for tf.string_split 's sep parameter have changed `` `` when sep unspecified : it now splits on all whitespace , not just `` `` the space character . '' ) # check the result_type parameter return _string_split_rtype_transformer ( parent , node , full_name , name , logs )
__label__0 tf.ones ( [ 4 , 5 ] ) except attributeerror : pass `` '' ''
__label__0 def testunboundfuncwithoneparamdefaultonepositional ( self ) :
__label__0 def __init__ ( self , * args ) : self._storage = set ( self._wrap_key ( obj ) for obj in list ( * args ) )
__label__0 @ tf_export ( `` nest.assert_same_structure '' ) def assert_same_structure ( nest1 , nest2 , check_types=true , expand_composites=false ) : `` '' '' asserts that two structures are nested in the same way .
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' unit tests for tf_should_use . '' '' ''
__label__0 def testdispatchfortensorlike ( self ) : maskedortensorlike = typing.union [ maskedtensor , core_tf_types.tensorlike ]
__label__0 def testname ( self ) : with ops.name_scope ( `` scope '' ) : queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 , name= '' queue '' ) qr = queue_runner_impl.queuerunner ( queue , [ control_flow_ops.no_op ( ) ] ) self.assertequal ( `` scope/queue '' , qr.name ) queue_runner_impl.add_queue_runner ( qr ) self.assertequal ( 1 , len ( ops.get_collection ( ops.graphkeys.queue_runners , `` scope '' ) ) )
__label__0 `` ` python lowercase = { ' a ' : ' a ' , ' b ' : ( 'b0 ' , 'b1 ' ) } uppercase = { ' a ' : ' a ' , ' b ' : ( 'b0 ' , 'b1 ' ) }
__label__0 there are two main usages of a ` distributeddataset ` object :
__label__0 args : args : the arguments to the operation . kwargs : they keyword arguments to the operation .
__label__0 if not is_nested_fn ( flat_sequence ) : raise typeerror ( `` attempted to pack value : \n { } \ninto a structure , but found `` `` incompatible type ` { } ` instead . `` .format ( truncate ( flat_sequence , 100 ) , type ( flat_sequence ) ) )
__label__0 returns : the first op found in a collection , or ` none ` if the collection is empty. `` '' '' try : op_list = ops.get_collection ( key ) if len ( op_list ) > 1 : logging.info ( `` found % d % s operations . returning the first one . `` , len ( op_list ) , key ) if op_list : return op_list [ 0 ] except lookuperror : pass
__label__0 def _testwhileloopandgradientserdes ( self , outer_body_fn ) : # build a while loop with ` outer_body_fn ` , export it , and verify that it can # be imported and the gradient can be built and run correctly . # pylint : disable=g-long-lambda return self._testgradientserdes ( lambda x : while_loop.while_loop ( lambda i , y : i < 5 , outer_body_fn , [ 0 , x ] ) [ 1 ] ) # pylint : enable=g-long-lambda
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' compat tests . '' '' ''
__label__0 class rmspropoptimizertest ( test.testcase ) :
__label__0 @ test_util.run_v1_only ( `` train.saver is v1 only api . '' ) def testreshape ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` variables_reshape '' ) with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : var = variable_v1.variablev1 ( [ [ 1.0 , 2.0 , 3.0 ] , [ 4.0 , 5.0 , 6.0 ] ] ) init = variables.global_variables_initializer ( ) save = saver_module.saver ( ) init.run ( ) save.save ( sess , save_path )
__label__0 # check if build files load py_test . files_missing_load = [ ] for build_file in build_files : updated_build_file = subprocess.check_output ( [ 'buildozer ' , '-stdout ' , 'new_load //tensorflow : tensorflow.bzl py_test ' , build_file ] ) with open ( build_file , ' r ' ) as f : if f.read ( ) ! = updated_build_file : files_missing_load.append ( build_file )
__label__0 raises : tf.errors.operror : or one of its subclasses if an error occurs while joining the tensorflow server. `` '' '' c_api.tf_serverjoin ( self._server )
__label__0 from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect
__label__0 # pylint : disable=line-too-long console_scripts = [ 'toco_from_protos = tensorflow.lite.toco.python.toco_from_protos : main ' , 'tflite_convert = tensorflow.lite.python.tflite_convert : main ' , 'toco = tensorflow.lite.python.tflite_convert : main ' , 'saved_model_cli = tensorflow.python.tools.saved_model_cli : main ' , ( 'import_pb_to_tensorboard = ' ' tensorflow.python.tools.import_pb_to_tensorboard : main ' ) , # we need to keep the tensorboard command , even though the console script # is now declared by the tensorboard pip package . if we remove the # tensorboard command , pip will inappropriately remove it during install , # even though the command is not removed , just moved to a different wheel . # we exclude it anyway if building tf_nightly . standard_or_nightly ( 'tensorboard = tensorboard.main : run_main ' , none ) , 'tf_upgrade_v2 = tensorflow.tools.compatibility.tf_upgrade_v2_main : main ' , ] console_scripts = [ s for s in console_scripts if s is not none ] # pylint : enable=line-too-long
__label__0 import bar as f
__label__0 class currentmodulefilter ( stacktracefilter ) : `` '' '' filters stack frames from the module where this is used ( best effort ) . '' '' ''
__label__0 expected_message = chunk_pb2.chunkedmessage ( ) text_format.parse ( `` '' '' chunk_index : 0 chunked_fields { field_tag { field : 2 } field_tag { field : 1 } field_tag { index : 0 } message { chunk_index : 1 } } '' '' '' , expected_message , ) self.assertprotoequals ( expected_message , chunked_message )
__label__0 * empty structures :
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) )
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] ) z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y.values ) self.assertallequal ( z.mask , x.mask & y.mask )
__label__0 # this is an object-based checkpoint . we 'll print a warning and then do # the restore . logging.warning ( `` restoring an object-based checkpoint using a name-based saver . this `` `` may be somewhat fragile , and will re-build the saver . instead , `` `` consider loading object-based checkpoints using `` `` tf.train.checkpoint ( ) . '' ) self._object_restore_saver = saver_from_object_based_checkpoint ( checkpoint_path=save_path , var_list=self._var_list , builder=self._builder , names_to_keys=names_to_keys , cached_saver=self._object_restore_saver ) self._object_restore_saver.restore ( sess=sess , save_path=save_path ) except errors.invalidargumenterror as err : # there is a mismatch between the graph and the checkpoint being loaded . # we add a more reasonable error message here to help users ( b/110263146 ) raise _wrap_restore_error_with_msg ( err , `` a mismatch between the current graph and the graph '' ) metrics.addcheckpointreadduration ( api_label=_saver_label , microseconds=_get_duration_microseconds ( start_time , time.time ( ) ) )
__label__0 args : op : python function : the operation to dispatch for . args : the arguments to the operation . kwargs : they keyword arguments to the operation .
__label__0 _names : sequence [ str ] _names_v1 : sequence [ str ] _api_name : str
__label__0 # pylint : disable=g-import-not-at-top try : from tensorflow.python.types import doc_typealias _extra_docs = getattr ( doc_typealias , `` _extra_docs '' , { } ) del doc_typealias except importerror : _extra_docs = { } # pylint : enable=g-import-not-at-top
__label__0 the following code will raise an exception : `` ` python shallow_tree = { `` a '' : `` a '' , `` b '' : `` b '' } input_tree = { `` a '' : 1 , `` c '' : 2 } assert_shallow_structure ( shallow_tree , input_tree ) `` `
__label__0 sequenceexample.__doc__ = `` '' '' \ a ` sequenceexample ` represents a sequence of features and some context .
__label__0 returns : consumed a bool based on input bytes and constraints. `` '' '' return self.fdp.consumebool ( )
__label__0 required_version actual_version result -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 1 1.1 true 1.2 1 false 1.2 1.3 false 1 true
__label__0 from tensorflow.tools.docs import tf_doctest_lib
__label__0 * ` collections.abc.sequence ` ( except ` string ` and ` bytes ` ) . this includes ` list ` , ` tuple ` , and ` namedtuple ` . * ` collections.abc.mapping ` ( with sortable keys ) . this includes ` dict ` and ` collections.ordereddict ` . * ` collections.abc.mappingview ` ( with sortable keys ) . * [ ` attr.s ` classes ] ( https : //www.attrs.org/ ) .
__label__0 returns : canonical name for the api symbol ( for e.g . initializers.zeros ) if canonical name could be determined . otherwise , returns none. `` '' '' if not hasattr ( symbol , '__dict__ ' ) : return none api_names_attr = api_attrs [ api_name ] .names _ , undecorated_symbol = tf_decorator.unwrap ( symbol ) if api_names_attr not in undecorated_symbol.__dict__ : return none api_names = getattr ( undecorated_symbol , api_names_attr ) deprecated_api_names = undecorated_symbol.__dict__.get ( '_tf_deprecated_api_names ' , [ ] )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=old_vocab_path , old_vocab_size=old_vocab_size ) ws_util.warm_start ( ckpt_to_initialize_from=self.get_temp_dir ( ) , vars_to_warm_start= '' . * sc_vocab . * '' , var_name_to_vocab_info= { `` linear_model/sc_vocab/weights '' : vocab_info } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . 'banana ' is n't in the # first two entries of the old vocabulary , so it 's newly initialized . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ [ [ 1 ] , [ 0 ] ] ] } , sess )
__label__0 the ` inputs ` are flattened up to ` shallow_tree ` before being mapped .
__label__0 def test_tfdecorator ( decorator_name , decorator_doc=none ) :
__label__0 # copyright 2024 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' prints sycl library and header directories and versions found on the system .
__label__0 if __name__ == '__main__ ' : flags.mark_flags_as_required ( [ 'output_dir ' ] ) app.run ( main )
__label__0 def testwarmstart_bucketizedcolumn ( self ) : # create feature column . real = fc.numeric_column ( `` real '' ) real_bucket = fc.bucketized_column ( real , boundaries= [ 0. , 1. , 2. , 3 . ] )
__label__0 args : obj : the object to hide from the generated docs .
__label__0 import keras from packaging import version import tensorboard import tensorflow as tf from tensorflow_docs.api_generator import public_api
__label__0 `` ` sdkfjgsd skip `` `
__label__0 # the first event should list the file_version . ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version )
__label__0 def main ( ) : `` '' '' this script updates all instances of version in the tensorflow directory .
__label__0 self.assertequal ( `` a '' , nest.pack_sequence_as ( 5 , [ `` a '' ] ) ) self.assertequal ( np.array ( [ 5 ] ) , nest.pack_sequence_as ( `` scalar '' , [ np.array ( [ 5 ] ) ] ) )
__label__0 @ dispatch.dispatch_for_binary_elementwise_apis ( maskedtensor , maskedtensor ) def another_handler ( api_func , x , y ) : return maskedtensor ( api_func ( x.values , y.values ) , x.mask )
__label__0 seed = fh.get_int ( ) indices = tf.random.uniform ( shape=shape1 , minval=0 , maxval=1000 , dtype=tf.int64 , seed=seed ) values = tf.random.uniform ( shape=shape2 , minval=0 , maxval=1000 , dtype=tf.int64 , seed=seed ) dense_shape = tf.random.uniform ( shape=shape3 , minval=0 , maxval=1000 , dtype=tf.int64 , seed=seed ) weights = tf.random.uniform ( shape=shape4 , minval=0 , maxval=1000 , dtype=tf.int64 , seed=seed )
__label__0 @ staticmethod def from_proto ( saver_def , import_scope=none ) : `` '' '' returns a ` saver ` object created from ` saver_def ` .
__label__0 for modality.data , nested structures are treated differently than modality.core . please refer to class modality 's documentation above to read up on these differences .
__label__0 args : sess : a ` session ` . queue_runners : a list of ` queuerunners ` . if not specified , we 'll use the list of queue runners gathered in the graph under the key ` graphkeys.queue_runners ` .
__label__0 def add_logs ( self , logs ) : `` '' '' record a log and print it .
__label__0 def test_callable ( self ) :
__label__0 if want == got : return true
__label__0 @ dispatch.dispatch_for_api ( array_ops.concat , { `` values '' : typing.list [ maskedtensor ] } ) def masked_concat ( values , axis , name=none ) : with ops.name_scope ( name ) : return maskedtensor ( array_ops.concat ( [ v.values for v in values ] , axis ) , array_ops.concat ( [ v.mask for v in values ] , axis ) )
__label__0 if args.nightly : if args.version : new_version = version.parse_from_string ( args.version , nightly_version ) new_version.set_identifier_string ( `` -dev '' + time.strftime ( `` % y % m % d '' ) ) else : new_version = version ( old_version.major , str ( old_version.minor ) , old_version.patch , `` -dev '' + time.strftime ( `` % y % m % d '' ) , nightly_version ) else : new_version = version.parse_from_string ( args.version , regular_version ) # update apple silicon release ci files for release builds only update_m1_builds ( old_version , new_version )
__label__0 @ abc.abstractmethod def __eq__ ( self , other ) - > bool : pass
__label__0 with ops.graph ( ) .as_default ( ) : w_scalar = variable_v1.variablev1 ( 37 , name= '' w '' ) with session.session ( server.target , config=isolate_config ) as sess : with self.assertraises ( errors_impl.failedpreconditionerror ) : sess.run ( w_scalar ) sess.run ( w_scalar.initializer ) self.assertallequal ( 37 , sess.run ( w_scalar ) )
__label__0 def register_binary_elementwise_api ( func ) : `` '' '' decorator that registers a tensorflow op as a binary elementwise api . '' '' '' _binary_elementwise_apis.append ( func ) for args , handler in _elementwise_api_handlers.items ( ) : if len ( args ) == 2 : _add_dispatch_for_binary_elementwise_api ( func , args [ 0 ] , args [ 1 ] , handler ) return func
__label__0 see ` core.polymorphicfunction ` and ` core.concretefunction ` .
__label__0 partial_func = functools.partial ( func , 7 , 10 ) argspec = tf_inspect.argspec ( args= [ ] , varargs=none , keywords=none , defaults=none )
__label__0 try :
__label__0 def testgetfullargsspecforpartial ( self ) :
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 def func ( a=0 ) : return a
__label__0 export_decorator = tf_export.tf_export ( 'name_a ' , 'name_b ' ) export_decorator.export_constant ( 'module1 ' , 'test_constant ' ) self.assertequal ( [ ( ( 'name_a ' , 'name_b ' ) , 'test_constant ' ) ] , module1._tf_api_constants ) self.assertequal ( [ ( ( 'name_a ' , 'name_b ' ) , 'test_constant ' ) ] , tf_export.get_v1_constants ( module1 ) ) self.assertequal ( [ ( ( 'name_a ' , 'name_b ' ) , 'test_constant ' ) ] , tf_export.get_v2_constants ( module1 ) )
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 # create a checkpoint . checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` recover_session_ready_for_local_init_fails_to_ready_local '' ) try : gfile.deleterecursively ( checkpoint_dir ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 for ( i , index ) in enumerate ( indices ) : self.assertallcloseaccordingtotype ( x [ index ] - lr * grad [ i ] * ( y [ index ] + grad [ i ] * grad [ i ] ) * * ( lr_power ) , self.evaluate ( var ) [ index ] ) self.assertallcloseaccordingtotype ( y [ index ] + grad [ i ] * grad [ i ] , self.evaluate ( accum ) [ index ] )
__label__0 `` ` import deprecation import new_module
__label__0 # handles # cmakedefine01 lines match = _cmake_define01_regex.match ( line ) if match : name = match.group ( 1 ) value = cmake_vars.get ( name , `` 0 '' ) return `` # define { } { } \n '' .format ( name , value )
__label__0 args : path : path to checkpoint directory or file . variable_names : list of variable names to load from the checkpoint .
__label__0 total_v_first_axis = sum ( v.get_shape ( ) .as_list ( ) [ 0 ] for v in var ) for v in var : v_shape = v.get_shape ( ) .as_list ( ) slice_info = v._get_save_slice_info ( ) partition_info = none if slice_info : partition_info = variable_scope._partitioninfo ( full_shape=slice_info.full_shape , var_offset=slice_info.var_offset )
__label__0 _deprecated = `` _tf_docs_deprecated ''
__label__0 # output is : [ 2 , 4 ] `` `
__label__0 def wait_for_stop ( self ) : `` '' '' block waiting for the coordinator to stop . '' '' '' self._coord.wait_for_stop ( )
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] ) z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y.values ) self.assertallequal ( z.mask , x.mask & y.mask )
__label__0 although saver works in some cases when executing eagerly , it is fragile . please switch to ` tf.train.checkpoint ` or ` tf.keras.model.save_weights ` , which perform a more robust object-based saving . these apis will load checkpoints written by ` saver ` . @ end_compatibility `` '' '' global _end_time_of_last_write with _end_time_of_last_write_lock : if _end_time_of_last_write is none : _end_time_of_last_write = time.time ( )
__label__0 # test max_to_keep being 0. save2 = saver_module.saver ( { `` v '' : v } , max_to_keep=0 ) self.assertequal ( [ ] , save2.last_checkpoints ) s1 = save2.save ( sess , os.path.join ( save_dir2 , `` s1 '' ) ) self.assertequal ( [ ] , save2.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) s2 = save2.save ( sess , os.path.join ( save_dir2 , `` s2 '' ) ) self.assertequal ( [ ] , save2.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) )
__label__0 using as a context manager with ` .group ( group_id ) ` is the easiest way . you can also use the ` acquire ` and ` release ` method directly. `` '' ''
__label__0 # alias for backwards-compatibility . add_dispatch_list = add_fallback_dispatch_list
__label__0 def testwarmstart_listofregexes ( self ) : # save checkpoint from which to warm-start . [ prev_v1_val , prev_v1_momentum_val , prev_v2_val , _ ] = self._create_prev_run_vars ( var_names= [ `` v1 '' , `` v1/momentum '' , `` v2 '' , `` v2/momentum '' ] , shapes= [ [ 10 , 1 ] ] * 4 , initializers= [ ones ( ) ] * 4 )
__label__0 # waits up until max_wait_secs for checkpoint to become available . wait_time = 0 ckpt = checkpoint_management.get_checkpoint_state ( checkpoint_dir ) while not ckpt or not ckpt.model_checkpoint_path : if wait_for_checkpoint and wait_time < max_wait_secs : logging.info ( `` waiting for checkpoint to be available . '' ) time.sleep ( self._recovery_wait_secs ) wait_time += self._recovery_wait_secs ckpt = checkpoint_management.get_checkpoint_state ( checkpoint_dir ) else : return sess , false
__label__0 # 1. the checkpoint would not be loaded successfully as is . try to parse # it as an object-based checkpoint . try : names_to_keys = object_graph_key_mapping ( save_path ) except errors.notfounderror : # 2. this is not an object-based checkpoint , which likely means there # is a graph mismatch . re-raise the original error with # a helpful message ( b/110263146 ) raise _wrap_restore_error_with_msg ( err , `` a variable name or other graph key that is missing '' )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 def testextendafterqueuerunners ( self ) : server = self._cached_server with session.session ( server.target ) as sess : input_queue = input_ops.input_producer ( constant_op.constant ( [ 0 . ] , dtype=dtypes.float32 ) ) self.assertisnotnone ( input_queue )
__label__0 and the changes to the api consist of renaming , reordering , and/or removing arguments . thus , we want to be able to generate changes to produce each of the following new apis :
__label__0 # comparing tf.debugging.assert_integer with tf.compat.v1.assert_integer . x_supported_dtypes = [ tf.float16 , tf.float32 , tf.float64 , tf.int32 , tf.int64 , tf.string ] random_dtype_index = fh.get_int ( min_int=0 , max_int=5 ) x_dtype = x_supported_dtypes [ random_dtype_index ] x_shape = fh.get_int_list ( min_length=0 , max_length=6 , min_int=0 , max_int=10 ) seed = fh.get_int ( ) try : x = tf.random.uniform ( shape=x_shape , dtype=x_dtype , seed=seed , maxval=10 ) except valueerror : x = tf.constant ( [ `` test_string '' ] ) message = fh.get_string ( 128 ) name = fh.get_string ( 128 ) try : v2_output = tf.debugging.assert_integer ( x=x , message=message , name=name ) except exception as e : # pylint : disable=broad-except v2_output = e try : v1_output = tf.compat.v1.assert_integer ( x=x , message=message , name=name ) except exception as e : # pylint : disable=broad-except v1_output = e
__label__0 def build_chunks ( self ) : `` '' '' splits a graphdef proto into smaller chunks . '' '' '' proto = self._proto if not isinstance ( proto , graph_pb2.graphdef ) : raise typeerror ( `` can only split graphdef type protos . '' )
__label__0 the ` session ` argument can be used in case the hook wants to run final ops , such as saving a last checkpoint .
__label__0 def __call__ ( self , * args , * * kwargs ) : self.call_count += 1 return super ( callcounter , self ) .decorated_target ( * args , * * kwargs )
__label__0 @ parameterized.parameters ( # add to_sparse unless result_type is raggedtensor : [ `` tf.strings.split ( x , sep ) '' , `` tf.strings.split ( x , sep ) .to_sparse ( ) '' ] , [ `` tf.strings.split ( x , sep , result_type='sparsetensor ' ) '' , `` tf.strings.split ( x , sep ) .to_sparse ( ) '' ] , [ `` tf.strings.split ( x , sep , result_type='raggedtensor ' ) '' , `` tf.strings.split ( x , sep ) '' ] , [ `` tf.strings.split ( x , sep , result_type=x ) '' , `` tf.compat.v1.strings.split ( x , sep , result_type=x ) '' ] , ) # pyformat : disable def test_strings_split ( self , text , expected_text ) : `` '' '' tests for transforming from tf.strings.split . '' '' '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def func ( m=1 , n=2 ) : return 2 * m + n
__label__0 note : this function does not look into ` compositetensor ` s . replacing ` resourcevariable ` s in a ` compositetensor ` with atoms will change the ` typespec ` of the ` compositetensor ` , which violates the semantics of ` compositetensor ` and tf.nest . so ` resourcevariable ` s in ` compositetensor ` s will be returned as they are .
__label__0 args : test_dir : name of the test directory .
__label__0 def get_effective_source_map ( self ) : `` '' '' returns a map ( filename , lineno ) - > ( filename , lineno , function_name ) . '' '' '' raise notimplementederror ( 'subclasses need to override this ' )
__label__0 # # # # advanced use
__label__0 * ` function_keyword_renames ` : maps function names to a map of old - > new argument names * ` symbol_renames ` : maps function names to new function names * ` change_to_function ` : a set of function names that have changed ( for notifications ) * ` function_reorders ` : maps functions whose argument order has changed to the list of arguments in the new order * ` function_warnings ` : maps full names of functions to warnings that will be printed out if the function is used . ( e.g . tf.nn.convolution ( ) ) * ` function_transformers ` : maps function names to custom handlers * ` module_deprecations ` : maps module names to warnings that will be printed if the module is still used after all other transformations have run * ` import_renames ` : maps import name ( must be a short name without ' . ' ) to importrename instance .
__label__0 def check_session_devices ( sess ) : # make sure we have the correct set of cluster devices devices = sess.list_devices ( ) device_names = set ( d.name for d in devices ) self.assertin ( `` /job : master/replica:0/task:0/device : cpu:0 '' , device_names ) self.assertin ( `` /job : worker/replica:0/task:0/device : cpu:0 '' , device_names )
__label__0 this function works for classes and functions .
__label__0 checks the types of the arguments and keyword arguments ( including elements of lists or tuples ) , and if any argument values have the indicated type ( s ) , then delegates to an override function. `` '' ''
__label__0 def _name_scope_transformer ( parent , node , full_name , name , logs ) : `` '' '' fix name scope invocation to use 'default_name ' and omit 'values ' args . '' '' ''
__label__0 def check_for_old_version ( old_version , new_version ) : `` '' '' check for old version references . '' '' '' for old_ver in [ old_version.string , old_version.pep_440_str ] : check_for_lingering_string ( old_ver )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testflattenandpack ( self ) : structure = ( ( 3 , 4 ) , 5 , ( 6 , 7 , ( 9 , 10 ) , 8 ) ) flat = [ `` a '' , `` b '' , `` c '' , `` d '' , `` e '' , `` f '' , `` g '' , `` h '' ] self.assertequal ( nest.flatten ( structure ) , [ 3 , 4 , 5 , 6 , 7 , 9 , 10 , 8 ] ) self.assertequal ( nest.pack_sequence_as ( structure , flat ) , ( ( `` a '' , `` b '' ) , `` c '' , ( `` d '' , `` e '' , ( `` f '' , `` g '' ) , `` h '' ) ) ) structure = ( nesttest.pointxy ( x=4 , y=2 ) , ( ( nesttest.pointxy ( x=1 , y=0 ) , ) , ) ) flat = [ 4 , 2 , 1 , 0 ] self.assertequal ( nest.flatten ( structure ) , flat ) restructured_from_flat = nest.pack_sequence_as ( structure , flat ) self.assertequal ( restructured_from_flat , structure ) self.assertequal ( restructured_from_flat [ 0 ] .x , 4 ) self.assertequal ( restructured_from_flat [ 0 ] .y , 2 ) self.assertequal ( restructured_from_flat [ 1 ] [ 0 ] [ 0 ] .x , 1 ) self.assertequal ( restructured_from_flat [ 1 ] [ 0 ] [ 0 ] .y , 0 )
__label__0 @ property def jobs ( self ) : `` '' '' returns a list of job names in this cluster .
__label__0 # assert calling new fn with non-deprecated value logs nothing . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=false ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 def test_tfn_div ( self ) : @ def_function.function def wrapped_fn ( x ) : return x / 0 .
__label__0 this simply wraps the get_slot_names ( ) from the actual optimizer .
__label__0 byteslist.__doc__ = `` '' '' \ used in ` tf.train.example ` protos . holds a list of byte-strings .
__label__0 # registries for elementwise apis and api handlers . # # _ * _elementwise_apis : a list of tensorflow apis that have been registered # as elementwise operations using the ` register_ * _elementwise_api ` # decorators . # # _elementwise_api_handlers : dicts mapping from argument type ( s ) to api # handlers that have been registered with the ` dispatch_for_ * _elementwise_apis ` # decorators . # # _elementwise_api_targets : dict mapping from argument type ( s ) to lists of # ` ( api , dispatch_target ) ` pairs . used to impelement # ` unregister_elementwise_api_handler ` . _unary_elementwise_apis = [ ] _binary_elementwise_apis = [ ] _binary_elementwise_assert_apis = [ ] _elementwise_api_handlers = { } _elementwise_api_targets = { }
__label__0 def excluded_from_module_rename ( module , import_rename_spec ) : `` '' '' check if this module import should not be renamed .
__label__0 def testsharedserverongpu ( self ) : if not test.is_gpu_available ( ) : return save_path = os.path.join ( self.get_temp_dir ( ) , `` gpu '' ) with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : with sess.graph.device ( test.gpu_device_name ( ) ) : v0_1 = variable_v1.variablev1 ( 123.45 ) save = saver_module.saver ( { `` v0 '' : v0_1 } , sharded=true , allow_empty=true ) self.evaluate ( variables.global_variables_initializer ( ) ) save.save ( sess , save_path )
__label__0 see the [ training ] ( https : //tensorflow.org/api_guides/python/train ) guide. `` '' ''
__label__0 # some models have startup_delays to help stabilize the model but when using # sync_replicas training , set it to 0 .
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 @ compatibility ( eager ) exporting/importing meta graphs is not supported unless both ` graph_def ` and ` graph ` are provided . no graph exists when eager execution is enabled . @ end_compatibility `` '' '' # pylint : enable=line-too-long if context.executing_eagerly ( ) and not ( graph_def is not none and graph is not none ) : raise runtimeerror ( `` exporting/importing meta graphs is not supported when `` `` eager execution is enabled . no graph exists when eager `` `` execution is enabled . '' ) meta_graph_def , _ = meta_graph.export_scoped_meta_graph ( filename=filename , meta_info_def=meta_info_def , graph_def=graph_def , saver_def=saver_def , collection_list=collection_list , as_text=as_text , graph=graph , export_scope=export_scope , clear_devices=clear_devices , clear_extraneous_savers=clear_extraneous_savers , strip_default_attrs=strip_default_attrs , save_debug_info=save_debug_info , * * kwargs ) return meta_graph_def
__label__0 @ tf_export ( '__internal__.register_get_session_function ' , v1= [ ] ) def register_get_session_function ( func ) : global _keras_get_session_function _keras_get_session_function = func
__label__0 it contains a key-value store ` example.features ` where each key ( string ) maps to a ` tf.train.feature ` message which contains a fixed-type list . this flexible and compact format allows the storage of large amounts of typed data , but requires that the data shape and use be determined by the configuration files and parsers that are used to read and write this format ( refer to ` tf.io.parse_example ` for details ) .
__label__0 def _find_versioned_file ( base_paths , relative_paths , filepatterns , required_version , get_version ) : `` '' '' returns first valid path to a file that matches the requested version . '' '' '' if type ( filepatterns ) not in [ list , tuple ] : filepatterns = [ filepatterns ] for path in _cartesian_product ( base_paths , relative_paths ) : for filepattern in filepatterns : for file in glob.glob ( os.path.join ( path , filepattern ) ) : actual_version = get_version ( file ) if _matches_version ( actual_version , required_version ) : return file , actual_version raise _not_found_error ( base_paths , relative_paths , `` , `` .join ( filepatterns ) + `` matching version ' % s ' '' % required_version )
__label__0 def __init__ ( self , * args , * * kwargs ) : self._wrapped = dict ( * args , * * kwargs )
__label__0 returns : a context manager which will acquire the lock for ` group_id ` . `` '' '' self._validate_group_id ( group_id ) return self._context ( self , group_id )
__label__0 return dist
__label__0 _contrib_cudnn_rnn_warning = ( ast_edits.warning , `` ( manual edit required ) tf.contrib.cudnn_rnn . * has been deprecated , `` `` and the cudnn kernel has been integrated with `` `` tf.keras.layers.lstm/gru in tensorflow 2.0. please check the new api `` `` and use that instead . '' )
__label__0 class child ( parent ) : @ do_not_generate_docs def method1 ( self ) : pass def method2 ( self ) : pass `` `
__label__0 returns : a list of tensors resulting from reading 'saveable ' from 'filename'. `` '' '' # pylint : disable=protected-access tensors = [ ] for spec in saveable.specs : tensors.append ( io_ops.restore_v2 ( filename_tensor , [ spec.name ] , [ spec.slice_spec ] , [ spec.dtype ] ) [ 0 ] )
__label__0 def _maybe_add_call_warning ( self , node , full_name , name ) : `` '' '' print a warning when specific functions are called with selected args .
__label__0 5. tf.tensor ( considered a scalar ) :
__label__0 # pylint : disable=unused-import import functools import inspect
__label__0 # todo ( mdan , anjalisridhar ) : decide the location of this file .
__label__0 args : symbol : symbol to get api names for .
__label__0 # output is : [ 'first_4_evens ' , [ 'first_5_odds ' , 'first_3_primes ' ] ] `` `
__label__0 logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` multiplying scale arg of tf.contrib.layers.l2_regularizer '' `` by half to what tf.keras.regularizers.l2 expects.\n '' ) )
__label__0 > > > structure = { `` foo '' : tf.ragged.constant ( [ [ 1 , 2 ] , [ 3 ] ] ) , ... `` bar '' : tf.constant ( [ [ 5 ] ] ) } > > > flat_sequence = [ `` one '' , `` two '' ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence , ... expand_composites=false ) { 'foo ' : 'two ' , 'bar ' : 'one ' }
__label__0 raises : valueerror : if num_gpus is larger than zero but no gpu is available. `` '' '' if required_gpus == 0 : return available_gpus = tf.config.experimental.list_physical_devices ( 'gpu ' ) if not available_gpus : raise valueerror ( 'requires at least one physical gpu ' ) if len ( available_gpus ) > = required_gpus : tf.config.set_visible_devices ( available_gpus [ : required_gpus ] ) else : # create logical gpus out of one physical gpu for simplicity . note that the # other physical gpus are still available and corresponds to one logical gpu # each . num_logical_gpus = required_gpus - len ( available_gpus ) + 1 logical_gpus = [ tf.config.logicaldeviceconfiguration ( memory_limit=256 ) for _ in range ( num_logical_gpus ) ] tf.config.set_logical_device_configuration ( available_gpus [ 0 ] , logical_gpus )
__label__0 if new_keywords : self.add_log ( info , node.lineno , node.col_offset , `` added keywords to args of function % r '' % full_name ) node.args = new_args node.keywords = new_keywords + ( node.keywords or [ ] ) return true return false
__label__0 this function runs ` git describe ... ` in the path given as ` git_base_path ` . this will return a string of the form : < base-tag > - < number of commits since tag > - < shortened sha hash >
__label__0 migrating to a keras optimizer :
__label__0 with open ( file_a , `` a '' ) as f : f.write ( `` import foo as f '' ) os.symlink ( file_a , file_b )
__label__0 @ parameterized.parameters ( # do n't match ints . [ 'result = 1 ' , [ ] ] , # match floats . [ ' 0.0 ' , [ 0 . ] ] , [ 'text 1.0 text ' , [ 1 . ] ] , [ 'text 1. text ' , [ 1 . ] ] , [ 'text .1 text ' , [ .1 ] ] , [ 'text 1e3 text ' , [ 1000 . ] ] , [ 'text 1.e3 text ' , [ 1000 . ] ] , [ 'text +1 . text ' , [ 1 . ] ] , [ 'text -1. text ' , [ -1 . ] ] , [ 'text 1e+3 text ' , [ 1000 . ] ] , [ 'text 1e-3 text ' , [ 0.001 ] ] , [ 'text +1e3 text ' , [ 1000 . ] ] , [ 'text -1e3 text ' , [ -1000 . ] ] , [ 'text +1e-3 text ' , [ 0.001 ] ] , [ 'text -1e+3 text ' , [ -1000 . ] ] , # match at the start and end of a string . [ '.1 ' , [ .1 ] ] , [ '.1 text ' , [ .1 ] ] , [ 'text .1 ' , [ .1 ] ] , [ ' 0.1 text ' , [ .1 ] ] , [ 'text 0.1 ' , [ .1 ] ] , [ ' 0 . text ' , [ 0 . ] ] , [ 'text 0 . ' , [ 0 . ] ] , [ '1e-1 text ' , [ .1 ] ] , [ 'text 1e-1 ' , [ .1 ] ] , # do n't match floats mixed into text [ 'text1.0 text ' , [ ] ] , [ 'text 1.0text ' , [ ] ] , [ 'text1.0text ' , [ ] ] , [ '0x12e4 ' , [ ] ] , # not 12000 [ 'tensorboard : http : //128.0.0.1:8888 ' , [ ] ] , # with a newline [ ' 1.0 text\n 2.0 3.0 text ' , [ 1. , 2. , 3 . ] ] , # with ints and a float . [ 'shape ( 1,2,3 ) value -1e9 ' , [ -1e9 ] ] , # `` . '' after a float . [ 'no floats at end of sentence : 1.0 . ' , [ ] ] , [ 'no floats with ellipsis : 1.0 ... ' , [ ] ] , # a numpy array [ `` '' '' array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] ] , dtype=float32 ) '' '' '' , [ 1 , 2 , 3 , 4 , 5 , 6 ] ] , # match both parts of a complex number # python style [ ' ( 0.0002+30000j ) ' , [ 0.0002 , 30000 ] ] , [ ' ( 2.3e-10-3.34e+9j ) ' , [ 2.3e-10 , -3.34e+9 ] ] , # numpy style [ 'array ( [ 1.27+5.j ] ) ' , [ 1.27 , 5 ] ] , [ ' ( 2.3e-10+3.34e+9j ) ' , [ 2.3e-10 , 3.34e+9 ] ] , [ `` '' '' array ( [ 1.27e-09+5.e+00j , 2.30e+01-1.e-03j ] ) '' '' '' , [ 1.27e-09 , 5.e+00 , 2.30e+01 , -1.e-03 ] ] , # check examples in tolerence . [ '1e-6 ' , [ 0 ] ] , [ ' 0.0 ' , [ 1e-6 ] ] , [ ' 1.000001e9 ' , [ 1e9 ] ] , [ '1e9 ' , [ 1.000001e9 ] ] , ) def test_extract_floats ( self , text , expected_floats ) : extract_floats = tf_doctest_lib._floatextractor ( ) output_checker = tf_doctest_lib.tfdoctestoutputchecker ( )
__label__0 s = repeatedstringsplitter ( test_message_pb2.repeatedstring ( strings= [ `` a '' , `` b '' , `` c '' ] ) ) chunks , chunked_message = s.split ( ) self.assertlistequal ( [ b '' a '' , b '' b '' , b '' c '' ] , chunks ) self.assertlen ( chunked_message.chunked_fields , 3 )
__label__0 # check if we have a uniform_noise keyword arg for uniform_noise in node.keywords : if uniform_noise.arg == `` uniform_noise '' : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing uniform_noise arg of tf.image.extract_glimpse `` `` to noise , and recomputing value . please check this `` `` transformation.\n '' ) ) uniform_noise.arg = `` noise '' value = `` uniform '' if uniform_noise.value else `` gaussian '' _replace_uniform_noise_node ( uniform_noise , uniform_noise.value ) return node
__label__0 def fn_has_kwargs ( test_arg1 , test_arg2 , * * x ) : if test_arg1 ! = expected_test_arg1 or test_arg2 ! = expected_test_arg2 : return valueerror ( 'partial does not work correctly ' ) return x
__label__0 class composablesplitter ( splitter ) : `` '' '' a splitter that can be composed with other splitters .
__label__0 returns : name of the ` var ` `` '' '' name_to_var_dict = saveable_object_util.op_list_to_dict ( var ) if len ( name_to_var_dict ) > 1 : raise typeerror ( `` ` var ` = % s passed as arg violates the constraints. `` `` name_to_var_dict = % s '' % ( var , name_to_var_dict ) ) return list ( name_to_var_dict.keys ( ) ) [ 0 ]
__label__0 def has_deprecation_decorator ( symbol ) : `` '' '' checks if given object has a deprecation decorator .
__label__0 returns : a function that logs a warning and returns the symbol from the new module . set this function as the module 's ` __getattr__ ` . `` '' ''
__label__0 def test_uniform_unit_scaling_initializer ( self ) : text = `` tf.uniform_unit_scaling_initializer ( 0.5 ) '' expected_text = ( `` tf.compat.v1.keras.initializers.variancescaling ( `` `` scale=0.5 , distribution=\ '' uniform\ '' ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def __iter__ ( self ) : pass
__label__0 ` wait_for_session ( ) ` waits for a model to be initialized by other processes .
__label__0 text = `` tf.nn.space_to_batch ( input , paddings , block_size , name ) '' expected_text = ( `` tf.space_to_batch ( input , paddings=paddings , block_shape=block_size , `` `` name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 with self.session ( graph=graph1 ) as sess : saver_list1 [ 0 ] .restore ( sess , saver1_ckpt ) self.assertequal ( 1.0 , self.evaluate ( var_dict1 [ `` variable1:0 '' ] ) )
__label__0 # lists . data_list = [ [ 2 , 4 , 6 , 8 ] , [ [ 1 , 3 , 5 , 7 , 9 ] , [ 3 , 5 , 7 ] ] ] name_list = [ `` evens '' , [ `` odds '' , `` primes '' ] ] out = nest.map_structure_up_to ( name_list , lambda name , sec : `` first_ { } _ { } '' .format ( len ( sec ) , name ) , name_list , data_list ) self.assertequal ( out , [ `` first_4_evens '' , [ `` first_5_odds '' , `` first_3_primes '' ] ] )
__label__0 the flat list will be in the corresponding order as if you called ` nest.flatten ` on the structure . this is handy for naming tensors such the tf scope structure matches the tuple structure .
__label__0 self.assertnotin ( o , set ( [ wrap1 ] ) ) o.hash_value = id ( o ) # since there is now a hash collision we raise an exception with self.assertraises ( typeerror ) : bool ( o in set ( [ wrap1 ] ) )
__label__0 # ` $ yes | configure ` yes = subprocess.popen ( [ 'yes ' , `` ] , stdout=subprocess.pipe ) configure = subprocess.popen ( [ tensorflow_root / 'configure ' ] , stdin=yes.stdout , cwd=tensorflow_root ) configure.communicate ( )
__label__0 # change outermost ctx to the one given to us ( inner ones should be load ) . node.ctx = ctx return node
__label__0 # creates monitoredsession session = training.monitoredtrainingsession ( master=workers [ worker_id ] .target , is_chief=is_chief , hooks= [ sync_replicas_hook ] )
__label__0 structures_have_mismatching_types = ( `` the two structures do n't have the same sequence type . input structure has `` `` type { input_type } , while shallow structure has type { shallow_type } . '' )
__label__0 2012 ) .
__label__0 args : instance : an instance of a python object . strict : if true , ` instance ` is considered to be a ` namedtuple ` only if it is a `` plain '' namedtuple . for instance , a class inheriting from a ` namedtuple ` will be considered to be a ` namedtuple ` iff ` strict=false ` .
__label__0 returns : a list of ( gradient , variable ) pairs. `` '' '' return self._opt.compute_gradients ( * args , * * kwargs )
__label__0 def teststructure ( self ) : for ( text , expected ) in [ ( `` tf.data.experimental.datasetstructure '' , `` tf.data.datasetspec '' ) , ( `` tf.data.experimental.optionalstructure '' , `` tf.optionalspec '' ) , ( `` tf.data.experimental.raggedtensorstructure '' , `` tf.raggedtensorspec '' ) , ( `` tf.data.experimental.sparsetensorstructure '' , `` tf.sparsetensorspec '' ) , ( `` tf.data.experimental.structure '' , `` tf.typespec '' ) , ( `` tf.data.experimental.tensorarraystructure '' , `` tf.tensorarrayspec '' ) , ( `` tf.data.experimental.tensorstructure '' , `` tf.tensorspec '' ) , ] : _ , unused_report , unused_errors , actual = self._upgrade ( text ) self.assertequal ( actual , expected )
__label__0 for transformer in transformers : logs = [ ] new_node = transformer ( parent , node , full_name , name , logs ) self.add_logs ( logs ) if new_node and new_node is not node : pasta.ast_utils.replace_child ( parent , node , new_node ) node = new_node self._stack [ -1 ] = node
__label__0 returns : ` args ` with the type of ` instance ` . `` '' '' return nest_util.sequence_like ( instance , args )
__label__0 def output_difference ( self , example , got , optionflags ) : got = [ got ]
__label__0 @ classmethod def setupclass ( cls ) : super ( sessionmanagertest , cls ) .setupclass ( ) resource_variables_toggle.disable_resource_variables ( )
__label__0 # fill in default values provided by partial function in all_defaults . for kw , default in iter ( partial_keywords.items ( ) ) : if kw in args : idx = args.index ( kw ) all_defaults [ idx ] = default elif not keywords : raise valueerror ( f ' { obj } does not have a * * kwargs parameter , but ' f'contains an unknown partial keyword { kw } . ' )
__label__0 `` ` python server = tf.distribute.server ( ... ) with tf.compat.v1.session ( server.target ) : # ... `` `
__label__0 @ tf_export ( `` experimental.unregister_dispatch_for '' ) def unregister_dispatch_for ( dispatch_target ) : `` '' '' unregisters a function that was registered with ` @ dispatch_for_ * ` .
__label__0 @ abc.abstractmethod def __hash__ ( self ) - > int : pass
__label__0 tf_export ( 'compat.as_text ' ) ( as_text ) tf_export ( 'compat.as_bytes ' ) ( as_bytes ) tf_export ( 'compat.as_str ' ) ( as_str )
__label__0 any other values are considered * * atoms * * . not all collection types are considered nested structures . for example , the following types are considered atoms :
__label__0 binary_output = fh.get_bool ( ) minlength = fh.get_int ( ) maxlength = fh.get_int ( ) name = fh.get_string ( ) try : _ , _ , _ , = tf.raw_ops.sparsecountsparseoutput ( indices=indices , values=values , dense_shape=dense_shape , weights=weights , binary_output=binary_output , minlength=minlength , maxlength=maxlength , name=name ) except tf.errors.invalidargumenterror : pass
__label__0 saver_list1 = graph1.get_collection ( ops_lib.graphkeys.savers ) self.assertequal ( 1 , len ( saver_list1 ) )
__label__0 the ` ready_for_local_init_op ` is an ` operation ` used to check if the model is ready to run local_init_op . the model is considered ready if that operation returns an empty 1d string tensor . if the operation returns a non empty 1d string tensor , the elements are concatenated and used to indicate to the user why the model is not ready .
__label__0 from tensorflow.core.protobuf import config_pb2 from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect
__label__0 this map can be edited , but it should not be edited once traversal has begun .
__label__0 @ abc.abstractmethod def is_subtype_of ( self , other : `` tracetype '' ) - > bool : `` '' '' returns true if ` self ` is a subtype of ` other ` .
__label__0 def testbatchtospace ( self ) : text = `` tf.batch_to_space_nd ( input , block_shape , crops , name ) '' expected_text = `` tf.batch_to_space ( input , block_shape , crops , name ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 @ dispatch.dispatch_for_types ( test_op_with_optional , customtensor ) def override_for_test_op ( x , y , z , u=none ) : # pylint : disable=unused-variable del u return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 , )
__label__0 # once we did a modification , node is invalid and not worth inspecting # further . also , we only perform modifications for simple nodes , so # there 'd be no point in descending further . if self._maybe_rename ( parent , node , full_name ) : return if self._maybe_change_to_function_call ( parent , node , full_name ) : return
__label__0 field , _ = util.get_field ( proto , [ `` field_one '' , 2 , 1 , `` map_field_uint32 '' , 324 ] ) self.assertequal ( `` map_value_324 '' , field )
__label__0 def __new__ ( cls , fetches , feed_dict=none , options=none ) : return super ( sessionrunargs , cls ) .__new__ ( cls , fetches , feed_dict , options )
__label__0 _above_max_size = lambda x : x > constants.max_size ( ) _greedy_split = lambda x : x > constants.max_size ( ) // 3 _always_split = lambda x : true
__label__0 args : required_gpus : an integer . the number of gpus required .
__label__0 returns : a list of fieldindex protos with the same length as ` fields ` . `` '' '' field_tags = [ ] for _ , field_desc , map_key , list_index in _walk_fields ( proto , fields ) : field_tags.append ( chunk_pb2.fieldindex ( field=field_desc.number ) ) if map_key is not none : key_type = field_desc.message_type.fields_by_name [ `` key '' ] .type field_tags.append ( chunk_pb2.fieldindex ( map_key=_map_key_proto ( key_type , map_key ) ) ) elif list_index is not none : field_tags.append ( chunk_pb2.fieldindex ( index=list_index ) ) return field_tags
__label__0 sessionrunhooks are useful to track training , report progress , request early stopping and more . sessionrunhooks use the observer pattern and notify at the following points : - when a session starts being used - before a call to the ` session.run ( ) ` - after a call to the ` session.run ( ) ` - when the session closed
__label__0 # instead of this : def simple_parametrized_wrapper ( * args , * * kwds ) : return wrapped_fn ( * args , * * kwds )
__label__1 class solution : def finditinerary ( self , tickets : list [ list [ str ] ] ) - > list [ str ] : graph = defaultdict ( list ) # step 1 : create the graph for ticket in sorted ( tickets , reverse=true ) : graph [ ticket [ 0 ] ] .append ( ticket [ 1 ] ) # step 2 : implement dfs def dfs ( curr ) : while graph [ curr ] : dfs ( graph [ curr ] .pop ( ) ) route.append ( curr ) # initialize the route with the starting airport `` jfk '' route = [ ] dfs ( `` jfk '' ) # reverse the route to get the correct order return route [ : :-1 ]
__label__0 # pylint : disable=unused-import import functools import inspect
__label__0 tf_nightly_regex = ( r '' ( .+ ) ( tf_nightly. * ) - ( \d\ . [ \d ] { 1,2 } '' r '' \.\d.dev [ \d ] { 0,8 } ) - ( .+ ) \.whl '' ) binary_string_template = `` % s- % s- % s.whl ''
__label__0 def _apply_dense ( self , grad , var ) : rms = self.get_slot ( var , `` rms '' ) mom = self.get_slot ( var , `` momentum '' ) if self._centered : mg = self.get_slot ( var , `` mg '' ) return gen_training_ops.apply_centered_rms_prop ( var , mg , rms , mom , math_ops.cast ( self._learning_rate_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , var.dtype.base_dtype ) , grad , use_locking=self._use_locking ) .op else : return gen_training_ops.apply_rms_prop ( var , rms , mom , math_ops.cast ( self._learning_rate_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , var.dtype.base_dtype ) , grad , use_locking=self._use_locking ) .op
__label__0 # these should not be visible in the main tf module . try : del core except nameerror : pass
__label__0 self.assertequal ( 10 , sess0.run ( w0 ) ) self.assertequal ( 11 , sess1.run ( vadd1 ) ) self.assertequal ( 10 , sess1.run ( w1 ) ) self.assertequal ( 11 , sess0.run ( v0 ) )
__label__0 def testlocalinitop ( self ) : logdir = self._test_dir ( `` default_local_init_op '' ) with ops.graph ( ) .as_default ( ) : # a local variable . v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , trainable=false , collections= [ ops.graphkeys.local_variables ] )
__label__0 any further depth in structure in ` input_tree ` is retained as structures in the partially flatten output .
__label__0 def run_and_report ( self , s1 , s2 , name ) : burn_iter , test_iter = 100 , 30000
__label__0 decay_function_comment = ( ast_edits.info , `` to use learning rate decay schedules with tensorflow 2.0 , switch to `` `` the schedules in ` tf.keras.optimizers.schedules ` .\n '' )
__label__0 edge cases for atoms :
__label__0 if not hasattr ( module , api_constants_attr ) : setattr ( module , api_constants_attr , [ ] ) # pylint : disable=protected-access getattr ( module , api_constants_attr ) .append ( ( self._names , name ) )
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 6 , _fn ( 1 , 2 , kw1=3 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 def saver_from_object_based_checkpoint ( checkpoint_path , var_list=none , builder=none , names_to_keys=none , cached_saver=none ) : `` '' '' return a ` saver ` which reads from an object-based checkpoint .
__label__0 def _testaddshouldusewarningwhenused ( self , fn , name ) : c = constant_op.constant ( 0 , name=name ) with reroute_error ( ) as error : h = tf_should_use._add_should_use_warning ( c , warn_in_eager=true ) fn ( h ) del h error.assert_not_called ( )
__label__0 `` ` python flatten_with_tuple_paths_up_to ( 0 , 0 ) # output : [ ( ) , 0 ]
__label__0 sv0.stop ( ) sv1.stop ( )
__label__0 # assert function docs are properly updated . self.assertequal ( `` deprecated function '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' % ( date , instructions ) , getattr ( _object , `` _prop '' ) .__doc__ )
__label__0 def gather_available_device_info ( ) : `` '' '' gather list of devices available to tensorflow .
__label__0 def process_file ( in_filename , out_filename , upgrader ) : `` '' '' the function where we inject the support for ipynb upgrade . '' '' '' print ( `` extracting code lines from original notebook '' ) raw_code , notebook = _get_code ( in_filename ) raw_lines = [ cl.code for cl in raw_code ]
__label__0 `` ` python b `` `
__label__0 # verifies behavior of tf.session.reset ( ) . # todo ( b/34465411 ) : starting multiple servers with different configurations # in the same test is flaky . move this test case back into # `` server_lib_test.py '' when this is no longer the case . @ test_util.run_deprecated_v1 def testsamevariablesclear ( self ) : server = server_lib.server.create_local_server ( )
__label__0 note that if sess.run ( ) raises outofrangeerror or stopiteration then hooks.after_run ( ) will not be called but hooks.end ( ) will still be called . if sess.run ( ) raises any other exception then neither hooks.after_run ( ) nor hooks.end ( ) will be called. `` '' ''
__label__0 > > > mt = tf.add ( maskedtensor ( [ 1 , 2 ] , [ true , false ] ) , maskedtensor ( 10 , true ) ) > > > print ( f '' values= { mt.values.numpy ( ) } , mask= { mt.mask.numpy ( ) } '' ) values= [ 11 12 ] , mask= [ true false ]
__label__0 for example , ` tf.function ` uses subtyping for dispatch : if ` a.is_subtype_of ( b ) ` is true , then an argument of ` tracetype ` ` a ` can be used as argument to a ` concretefunction ` traced with an a ` tracetype ` ` b ` .
__label__0 @ decorated_target.setter def decorated_target ( self , decorated_target ) : self._decorated_target = decorated_target
__label__0 repeatedstringsplitter ( proto.rs [ 1 ] , parent_splitter=splitter , fields_in_parent= [ `` rs '' , 1 ] ) .build_chunks ( )
__label__0 as a special case , exceptions used for control flow , such as ` outofrangeerror ` which reports that input queues are exhausted , are not raised again from the ` with ` block : they indicate a clean termination of the training loop and are considered normal termination .
__label__0 api docstring : tensorflow.compat `` '' ''
__label__0 got.append ( self.message ) got = '\n'.join ( got ) return ( super ( tfdoctestoutputchecker , self ) .output_difference ( example , got , optionflags ) )
__label__0 self.assertequal ( { ' a ' : 1 } , tf_inspect.getcallargs ( func , 1 ) )
__label__0 `` ` python code `` ` `` '' '' ) , ( 'last_output ' , [ ( 'code ' , 'result ' ) ] , `` '' '' hello
__label__0 def testwarmstart_explicitcheckpointfile ( self ) : # create vocab for sparse column `` sc_vocab '' . vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` vocab '' ) # create feature column . sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=vocab_path , vocabulary_size=4 )
__label__0 the warm-starting utility expects variable names to match with the variable names in the checkpoint . for object-based checkpoints , the variable names and names in the checkpoint are different . thus , for object-based checkpoints , this function is used to obtain the map from variable names to checkpoint keys .
__label__0 text = `` tf.debugging. % s ( a ) '' % name expected_text = `` tf.compat.v1.debugging. % s ( a ) '' % name _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` % s has been '' % name , report )
__label__0 def uses_star_kwargs_in_call ( node ) : `` '' '' check if an ast.call node uses arbitrary-length * * kwargs .
__label__0 mg0_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) mg1_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) rms0_np = np.array ( [ 1.0 , 1.0 ] , dtype=dtype.as_numpy_dtype ) rms1_np = np.array ( [ 1.0 , 1.0 ] , dtype=dtype.as_numpy_dtype ) mom0_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) mom1_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype )
__label__0 raises : typeerror : if ` func ` is not callable or if the structures do not match each other by depth tree . valueerror : if no structure is provided or if the structures do not match each other by type . valueerror : if wrong keyword arguments are provided. `` '' '' if modality == modality.core : return _tf_core_map_structure ( func , * structure , * * kwargs ) elif modality == modality.data : return _tf_data_map_structure ( func , * structure , * * kwargs ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 # return and cache dunders and our own members . # this is necessary to guarantee successful construction . # in addition , all the accessed attributes used during the construction must # begin with `` __ '' or `` _tfmw '' or `` _fastdict_ '' . if name.startswith ( '__ ' ) or name.startswith ( '_tfmw_ ' ) or name.startswith ( '_fastdict_ ' ) : func__fastdict_insert ( name , attr ) return attr
__label__0 def get_func_code ( func ) : `` '' '' returns func_code of passed callable , or none if not available . '' '' '' _ , func = tf_decorator.unwrap ( func ) if callable ( func ) : if tf_inspect.isfunction ( func ) or tf_inspect.ismethod ( func ) : return func.__code__ # since the object is not a function or method , but is a callable , we will # try to access the __call__method as a function . this works with callable # classes but fails with functool.partial objects despite their __call__ # attribute . try : return func.__call__.__code__ except attributeerror : return none else : raise valueerror ( 'argument ` func ` must be a callable . ' f'received func= { func } ( of type { type ( func ) } ) ' )
__label__0 _t = typevar ( '_t ' )
__label__0 example : start a thread to print losses . we want this thread to run every 60 seconds , so we launch it with ` sv.loop ( ) ` .
__label__0 exporting a constant `` ` python foo = 1 tf_export ( 'consts.foo ' ) .export_constant ( __name__ , 'foo ' ) `` ` `` '' '' from collections.abc import sequence import functools import sys from typing import any , namedtuple , optional , protocol , typevar
__label__0 `` ` python # restoring variables and running operations . saver = tf.train.import_meta_graph ( `` ./model_ex1.meta '' ) sess = tf.session ( ) saver.restore ( sess , `` ./model_ex1 '' ) result = sess.run ( `` v4:0 '' , feed_dict= { `` v1:0 '' : 12.0 , `` v2:0 '' : 3.3 } ) print ( result ) `` `
__label__0 x = maskedtensor ( [ 1 , 2 , 3 ] , [ true , false , true ] ) y = maskedtensor ( [ 10 , 20 , 30 ] , [ true , true , false ] ) z = some_op ( x , y ) self.assertallequal ( z.values , [ 12 , 24 , 36 ] ) self.assertallequal ( z.mask , [ true , false , false ] )
__label__0 args : group_id : the group for which to acquire and release the lock .
__label__0 field_proto = proto parent_desc = proto.descriptor i = 0 while i < len ( fields ) : field = fields [ i ] field_desc = none map_key = none index = none
__label__0 setattr ( obj , _doc_private , none ) return obj
__label__0 def testgetargspeconpartialinvalidargspec ( self ) : `` '' '' tests getargspec on partial function that does n't have valid argspec . '' '' ''
__label__0 assign_ops = [ ] idx = 0 # load and optionally reshape on the cpu , as string tensors are not # available on the gpu . # todo ( touts ) : re-enable restore on gpu when we can support annotating # string tensors as `` hostmemory '' inputs . for saveable in saveables : shapes = none if reshape : # compute the shapes , let the restore op decide if and how to do # the reshape . shapes = [ ] for spec in saveable.specs : v = spec.tensor shape = v.get_shape ( ) if not shape.is_fully_defined ( ) : shape = array_ops.shape ( v ) shapes.append ( shape ) saveable_tensors = all_tensors [ idx : idx + len ( saveable.specs ) ] idx += len ( saveable.specs ) assign_ops.append ( saveable.restore ( saveable_tensors , shapes ) )
__label__0 def __getitem__ ( self , key ) : return self._storage [ self._wrap_key ( key ) ]
__label__0 if `` cudnn '' in libraries : cudnn_paths = _get_legacy_path ( `` cudnn_install_path '' , base_paths ) cudnn_version = os.environ.get ( `` tf_cudnn_version '' , `` '' ) result.update ( _find_cudnn_config ( cudnn_paths , cudnn_version ) )
__label__0 two representative types of ` tf.distribute.distributedvalues ` are ` tf.types.experimental.perreplica ` and ` tf.types.experimental.mirrored ` values .
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 header = inheritable_header get_header = get_inheritable_header
__label__0 sv = supervisor.supervisor ( logdir=logdir , init_op=none , init_fn=_init_fn ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) sv.stop ( )
__label__1 def remove_whitespace ( text ) : return text.replace ( `` `` , `` '' )
__label__0 def _replace_uniform_noise_node ( parent , old_value ) : `` '' '' replaces old_value with 'uniform ' or 'gaussian ' . '' '' '' uniform = ast.str ( s= '' uniform '' ) gaussian = ast.str ( s= '' gaussian '' ) new_value = ast.ifexp ( body=uniform , test=old_value , orelse=gaussian ) # this copies the prefix and suffix on old_value to new_value . pasta.ast_utils.replace_child ( parent , old_value , new_value ) ast.copy_location ( new_value , old_value ) # put parentheses around noise.value.test ( and remove the old prefix/ # suffix , they should only be around new_value.test ) , so that : # `` uniform '' if ( a if b else c ) else `` gaussian '' is valid . pasta.base.formatting.set ( new_value.test , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( new_value.test , `` suffix '' , `` ) '' )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' training helper that checkpoints models and creates session . '' '' ''
__label__0 myclass ( `` test '' ) self.assertequal ( 0 , mock_warning.call_count ) deprecated_cls ( `` deprecated '' ) self.assertequal ( 1 , mock_warning.call_count ) # make sure the error points to the right file . self.assertregex ( mock_warning.call_args [ 0 ] [ 1 ] , r '' deprecation_test\.py : '' ) deprecated_cls ( `` deprecated again '' ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 def testsparsesplit ( self ) : text = ( `` tf.sparse_split ( sp_input=sp_input , num_split=num_split , axis=axis , `` `` name=name ) '' ) expected_text = ( `` tf.sparse.split ( sp_input=sp_input , num_split=num_split , axis=axis , `` `` name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 class functionparametercanonicalizertest ( test.testcase ) :
__label__0 # emit a warning if one was specified if self._tfll_warning : logging.warning ( self._tfll_warning ) # make sure to only warn once . self._tfll_warning = none
__label__0 @ deprecation.deprecated_arg_values ( date , instructions , warn_once=false , deprecated=true ) def _fn ( arg0 , arg1 , deprecated=true ) : `` '' '' fn doc .
__label__0 this helps to avoid circular dependencies. `` '' ''
__label__0 class fuzzinghelper ( object ) : `` '' '' fuzzinghelper makes handling fuzzeddataprovider easier with tensorflow python fuzzing . '' '' ''
__label__0 try : if not gfile.exists ( test_executable ) : test_executable_py3 = test_executable + `` .python3 '' if not gfile.exists ( test_executable_py3 ) : raise valueerror ( `` executable does not exist : % s '' % test_executable ) test_executable = test_executable_py3 test_args = shlex.split ( test_args )
__label__0 # # both non-list edge-case . # using iterable elements . input_tree = `` input_tree '' shallow_tree = `` shallow_tree '' ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 self.assertequal ( `` var/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float32 , slot.dtype.base_dtype ) self.assertallequal ( [ 1.0 , 2.5 ] , self.evaluate ( slot ) )
__label__0 # verifies that collection where item type does not match expected # type will not be added . ops_lib.add_to_collection ( `` int_collection '' , 3 ) ops_lib.add_to_collection ( `` int_collection '' , 3.5 ) save._add_collection_def ( meta_graph_def , `` int_collection '' ) self.assertequal ( len ( meta_graph_def.collection_def ) , 0 )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 > > > tensor = tf.ragged.constant ( [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] ) > > > tf.nest.flatten ( tensor , expand_composites=false ) [ < tf.raggedtensor [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] > ]
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a sessionrunhook extends ` session.run ( ) ` calls for the ` monitoredsession ` .
__label__0 return prev_tensor_name , var
__label__0 def bulk_restore ( self , filename_tensor , saveables , preferred_shard , restore_sequentially ) :
__label__0 returns : a list of test_log_pb2.availabledeviceinfo messages. `` '' '' device_info_list = [ ] devices = device_lib.list_local_devices ( )
__label__0 def _add_uniform_scaling_initializer_transformer ( parent , node , full_name , name , logs ) : `` '' '' updates references to uniform_unit_scaling_initializer .
__label__0 # the floats should match according to allclose try : self.asserttrue ( output_checker._allclose ( expected_floats , extracted_floats ) ) except assertionerror as e : msg = '\n\nexpected : { } \nfound : { } '.format ( expected_floats , extracted_floats ) e.args = ( e.args [ 0 ] + msg , ) raise e
__label__0 see also ` tf.debugging.enable_traceback_filtering ( ) ` and ` tf.debugging.disable_traceback_filtering ( ) ` . note that filtering out internal frames from the tracebacks of exceptions raised by tensorflow code is the default behavior .
__label__0 def get_current_join_version ( ) - > int : return _join_version
__label__0 with open ( args.symbols , ' r ' ) as f : funs = [ s.strip ( ) for s in f.readlines ( ) ]
__label__0 returns : a name or attribute node. `` '' '' names = name.split ( `` . '' ) names.reverse ( ) node = ast.name ( id=names.pop ( ) , ctx=ast.load ( ) ) while names : node = ast.attribute ( value=node , attr=names.pop ( ) , ctx=ast.load ( ) )
__label__0 creates a new session on 'master ' . if the session is not initialized and can be recovered from a checkpoint , recover it .
__label__0 def __init ( self ) : pass
__label__0 empty_dict = { }
__label__0 def __init__ ( self , component ) : self.component = component
__label__0 the device filters can be partically specified . for remote tasks that do not have device filters specified , all devices will be visible to them. `` '' ''
__label__0 text = `` from tensorflow import * '' expected_text = `` from tensorflow.compat.v1 import * '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 returns : a list of ` ( tuple_path , atom ) ` tuples . each ` tuple_path ` is a tuple of indices and/or dictionary keys that uniquely specify the path to ` atom ` within ` structure ` . `` '' '' return list ( zip ( yield_flat_paths ( structure , expand_composites=expand_composites ) , flatten ( structure , expand_composites=expand_composites ) ) )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' public tensorflow type definitions .
__label__0 def testignoremultistarts ( self ) : with self.cached_session ( ) as sess : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) self.evaluate ( variables.global_variables_initializer ( ) ) coord = coordinator.coordinator ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) threads = [ ] # note that this test does not actually start the threads . threads.extend ( qr.create_threads ( sess , coord=coord ) ) new_threads = qr.create_threads ( sess , coord=coord ) self.assertequal ( [ ] , new_threads )
__label__0 generated tensors are capped at dimension sizes of 8 , as 2^32 bytes of requested memory crashes the fuzzer ( see b/34190148 ) . returns only type that tf.random.uniform can generate . if you need a different type , consider using tf.cast .
__label__0 import argparse import configparser import os import string
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import extension_type from tensorflow.python.framework import ops from tensorflow.python.framework import tensor as tensor_lib from tensorflow.python.framework import tensor_conversion from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import array_ops_stack from tensorflow.python.ops import bitwise_ops from tensorflow.python.ops import gen_math_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import variables from tensorflow.python.ops.linalg import linear_operator_diag from tensorflow.python.ops.proto_ops import decode_proto from tensorflow.python.platform import googletest from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging from tensorflow.python.types import core as core_tf_types from tensorflow.python.util import deprecation from tensorflow.python.util import dispatch from tensorflow.python.util import nest from tensorflow.python.util.tf_export import get_canonical_name_for_symbol from tensorflow.python.util.tf_export import tf_export
__label__0 def get_mtime ( dirpath , fname ) : fpath = os.path.join ( dirpath , fname ) return os.stat ( fpath ) .st_mtime
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a module target for traversetest.test_module . '' '' ''
__label__0 # assert calls without the deprecated arguments log nothing . self.assertequal ( 2 , _fn ( 1 , arg1=2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 * type check is set to false :
__label__0 text = `` tf.initializers.uniform_unit_scaling ( 0.5 ) '' expected_text = ( `` tf.compat.v1.keras.initializers.variancescaling ( `` `` scale=0.5 , distribution=\ '' uniform\ '' ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 simple usage : tf_upgrade_v2.py -- infile foo.py -- outfile bar.py tf_upgrade_v2.py -- infile foo.ipynb -- outfile bar.ipynb tf_upgrade_v2.py -- intree ~/code/old -- outtree ~/code/new `` '' '' ) parser.add_argument ( `` -- infile '' , dest= '' input_file '' , help= '' if converting a single file , the name of the file `` `` to convert '' ) parser.add_argument ( `` -- outfile '' , dest= '' output_file '' , help= '' if converting a single file , the output filename . '' ) parser.add_argument ( `` -- intree '' , dest= '' input_tree '' , help= '' if converting a whole tree of files , the directory `` `` to read from ( relative or absolute ) . '' ) parser.add_argument ( `` -- outtree '' , dest= '' output_tree '' , help= '' if converting a whole tree of files , the output `` `` directory ( relative or absolute ) . '' ) parser.add_argument ( `` -- copyotherfiles '' , dest= '' copy_other_files '' , help= ( `` if converting a whole tree of files , whether to `` `` copy the other files . `` ) , type=bool , default=true ) parser.add_argument ( `` -- inplace '' , dest= '' in_place '' , help= ( `` if converting a set of files , whether to `` `` allow the conversion to be performed on the `` `` input files . `` ) , action= '' store_true '' ) parser.add_argument ( `` -- no_import_rename '' , dest= '' no_import_rename '' , help= ( `` not to rename import to compat.v2 explicitly . `` ) , action= '' store_true '' ) parser.add_argument ( `` -- no_upgrade_compat_v1_import '' , dest= '' no_upgrade_compat_v1_import '' , help= ( `` if specified , do n't upgrade explicit imports of `` `` ` tensorflow.compat.v1 as tf ` to the v2 apis . otherwise , `` `` explicit imports of the form ` tensorflow.compat.v1 as tf ` will `` `` be upgraded . `` ) , action= '' store_true '' ) parser.add_argument ( `` -- reportfile '' , dest= '' report_filename '' , help= ( `` the name of the file where the report log is `` `` stored . '' `` ( default : % ( default ) s ) '' ) , default= '' report.txt '' ) parser.add_argument ( `` -- mode '' , dest= '' mode '' , choices= [ _default_mode , _safety_mode ] , help= ( `` upgrade script mode . supported modes : \n '' `` % s : perform only straightforward conversions to upgrade to `` `` 2.0. in more difficult cases , switch to use compat.v1.\n '' `` % s : keep 1 . * code intact and import compat.v1 `` `` module . '' % ( _default_mode , _safety_mode ) ) , default=_default_mode ) parser.add_argument ( `` -- print_all '' , dest= '' print_all '' , help= '' print full log to stdout instead of just printing errors '' , action= '' store_true '' ) args = parser.parse_args ( )
__label__0 > > > structure = { `` foo '' : tf.ragged.constant ( [ [ 1. , 2 . ] , [ 3 . ] ] ) , ... `` bar '' : tf.constant ( [ [ 5 . ] ] ) } > > > tensors = tf.nest.flatten ( structure , expand_composites=true ) > > > print ( tensors ) [ < tf.tensor : shape= ( 1 , 1 ) , dtype=float32 , numpy=array ( [ [ 5 . ] ] , dtype=float32 ) > , < tf.tensor : shape= ( 3 , ) , dtype=float32 , numpy=array ( [ 1. , 2. , 3 . ] , dtype=float32 ) > , < tf.tensor : shape= ( 3 , ) , dtype=int64 , numpy=array ( [ 0 , 2 , 3 ] ) > ] > > > verified_tensors = [ tf.debugging.check_numerics ( t , 'invalid tensor : ' ) ... if t.dtype==tf.float32 else t ... for t in tensors ] > > > tf.nest.pack_sequence_as ( structure , verified_tensors , ... expand_composites=true ) { 'foo ' : < tf.raggedtensor [ [ 1.0 , 2.0 ] , [ 3.0 ] ] > , 'bar ' : < tf.tensor : shape= ( 1 , 1 ) , dtype=float32 , numpy=array ( [ [ 5 . ] ] , dtype=float32 ) > }
__label__0 returns : decorated function or method .
__label__0 returns : a new structure with the same arity as ` structure [ 0 ] ` , whose atoms correspond to ` func ( x [ 0 ] , x [ 1 ] , ... ) ` where ` x [ i ] ` is the atom in the corresponding location in ` structure [ i ] ` . if there are different structure types and ` check_types ` is ` false ` the structure types of the first structure will be used .
__label__0 `` '' ''
__label__0 before migrating :
__label__0 a = element ( ) b = element ( ) c = element ( ) set1 = object_identity.objectidentityset ( [ a , b ] ) set2 = object_identity.objectidentityset ( [ b , c ] ) diff_set = set1.difference ( set2 ) self.assertin ( a , diff_set ) self.assertnotin ( b , diff_set ) self.assertnotin ( c , diff_set )
__label__0 wrapped_fn = functools.partial ( fn_has_kwargs , test_arg=123 ) self.asserttrue ( function_utils.has_kwargs ( wrapped_fn ) ) some_kwargs = dict ( x=1 , y=2 , z=3 ) self.assertequal ( wrapped_fn ( * * some_kwargs ) , some_kwargs )
__label__0 def testpreparesessionafterstopfornonchief ( self ) : logdir = self._test_dir ( `` prepare_after_stop_nonchief '' ) with ops.graph ( ) .as_default ( ) : sv = supervisor.supervisor ( logdir=logdir , is_chief=false )
__label__0 def testcomplexexpression ( self ) : text = `` ( foo + bar ) [ a ] .word ( ) '' _ = self._upgrade ( text )
__label__0 `` ` hlomodule a_inference_f_13__.9
__label__0 self.assertequal ( { ' a ' : 6 , ' b ' : 7 } , tf_inspect.getcallargs ( func , a=6 , b=7 ) )
__label__0 if not context.executing_eagerly ( ) : self.evaluate ( [ variables.global_variables_initializer ( ) ] )
__label__0 import argparse
__label__0 def testpreparesessionwithreadyforlocalinitop ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) x = variable_v1.variablev1 ( 3 * v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' x '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( x ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( variables.global_variables ( ) ) , local_init_op= [ w.initializer , x.initializer ] ) sess = sm2.prepare_session ( `` '' , init_op=v.initializer ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` x:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) ) self.assertequal ( 1 , sess.run ( w ) ) self.assertequal ( 3 , sess.run ( x ) )
__label__0 __slots__ = ( )
__label__0 @ property def decorated_target ( self ) : return self._decorated_target
__label__0 class svtimercheckpointthread ( coordinator.looperthread ) : `` '' '' a thread to checkpoint on a timer . '' '' ''
__label__0 args : func : a callable with the signature func ( path , * values , * * kwargs ) that is evaluated on the leaves of the structure . * structure : a variable number of compatible structures to process . * * kwargs : optional kwargs to be passed through to func . special kwarg ` check_types ` is not passed to func , but instead determines whether the types of iterables within the structures have to be same ( e.g. , ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . by default , the types must match . to allow iteration over structures of different types ( but common arity ) , set this kwarg to ` false ` .
__label__0 def is_sequence_or_composite ( seq ) : return _is_nested_or_composite ( seq )
__label__0 def testisroutine ( self ) : self.asserttrue ( tf_inspect.isroutine ( len ) ) self.assertfalse ( tf_inspect.isroutine ( testdecoratedclass ) )
__label__0 def testdispatchsignaturewithunspecifiedparameter ( self ) :
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' provides wrapper for tensorflow modules . '' '' ''
__label__0 def is_generic_mapping ( tp ) : `` '' '' returns true if ` tp ` is a parameterized typing.mapping value . '' '' '' return ( tp not in ( collections.abc.mapping , typing.mapping ) and getattr ( tp , '__origin__ ' , none ) in ( collections.abc.mapping , typing.mapping ) )
__label__0 # note : typing.get_args was added in python 3.8. if hasattr ( typing , 'get_args ' ) : get_generic_type_args = typing.get_args else : get_generic_type_args = lambda tp : tp.__args__
__label__0 def build ( self , names_to_saveables , reshape=false , sharded=false , max_to_keep=5 , keep_checkpoint_every_n_hours=10000.0 , name=none , restore_sequentially=false , filename= '' model '' ) : `` '' '' builds save/restore graph nodes or runs save/restore in eager mode .
__label__0 raises : valueerror : if both checkpoint_dir and checkpoint_filename_with_path are set. `` '' ''
__label__0 @ dispatch.dispatch_for_types ( test_op_with_optional , customtensor ) def override_for_test_op ( x , y , z ) : # pylint : disable=unused-variable return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 , )
__label__0 args : string : version string version_type : version parameter
__label__0 args : chunk : proto message or bytes . field_tags : field information about the placement of the chunked data within self._proto . index : optional index at which to insert the chunk . the chunk ordering is important for merging. `` '' '' if self._parent_splitter is not none : self._parent_splitter.add_chunk ( chunk , self._fields_in_parent + field_tags , index ) else : assert self._chunks is not none assert self._chunked_message is not none field = self._chunked_message.chunked_fields.add ( field_tag=util.get_field_tag ( self._proto , field_tags ) ) new_chunk_index = len ( self._chunks ) field.message.chunk_index = new_chunk_index self._add_chunk_order.append ( id ( chunk ) )
__label__0 returns : the global step variable , or ` none ` if none was found .
__label__0 def __call__ ( self , * args , * * kwargs ) : return self._decorated_target ( * args , * * kwargs )
__label__0 _doc_in_current_and_subclasses = `` _tf_docs_doc_in_current_and_subclasses ''
__label__0 v = variable_v1.variablev1 ( 10.0 , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } , max_to_keep=10 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertequal ( [ ] , save.last_checkpoints )
__label__0 # copy the binary with the info we have copy_binary ( directory , origin_tag , new_tag , version , package )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 def testrunstep ( self ) : server = self._cached_server with ops.graph ( ) .as_default ( ) : with session.session ( server.target ) as sess : c = constant_op.constant ( [ [ 2 , 1 ] ] ) d = constant_op.constant ( [ [ 1 ] , [ 2 ] ] ) e = math_ops.matmul ( c , d ) self.assertallequal ( [ [ 4 ] ] , sess.run ( e ) ) # todo ( mrry ) : add ` server.stop ( ) ` and ` server.join ( ) ` when these work .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_deprecated_multiple_args_once_each ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 the canonical use case for this is ` tf.keras.layers.layer.call ` : it 's a public method , essential for anyone implementing a subclass , but it should never be called directly .
__label__0 above user code leads to following execution : call hooks.begin ( ) sess = tf.compat.v1.session ( ) call hooks.after_create_session ( ) while not stop is requested : call hooks.before_run ( ) try : results = sess.run ( merged_fetches , feed_dict=merged_feeds ) except ( errors.outofrangeerror , stopiteration ) : break call hooks.after_run ( ) call hooks.end ( ) sess.close ( )
__label__0 def __init__ ( self , proto , * * kwargs ) : if not isinstance ( proto , test_message_pb2.repeatedstring ) : raise typeerror ( `` can only split repeatedstring type protos '' )
__label__0 argspec = tf_inspect.fullargspec ( args= [ 'self ' , ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 if in_filename.endswith ( `` .py '' ) : files_processed , report_text , errors = \ upgrader.process_file ( in_filename , out_filename ) elif in_filename.endswith ( `` .ipynb '' ) : files_processed , report_text , errors = \ ipynb.process_file ( in_filename , out_filename , upgrader ) else : raise notimplementederror ( `` currently converter only supports python or ipynb '' )
__label__0 def experimental_get_compiler_ir ( self , * args , * * kwargs ) : `` '' '' returns compiler ir for the compiled function .
__label__0 local_init_success , msg = self._try_run_local_init_op ( sess ) if not local_init_success : raise runtimeerror ( `` init operations did not make model ready for local_init. `` `` init op : % s , init fn : % s , error : % s '' % ( _maybe_name ( init_op ) , init_fn , msg ) )
__label__0 import numpy as np
__label__0 `` ` python # create any optimizer to update the variables , say a simple sgd : opt = gradientdescentoptimizer ( learning_rate=0.1 )
__label__0 # runs a simple conv graph to check if vlog crashes . def test_simple_conv ( self ) : height , width = 7 , 9 images = random_ops.random_uniform ( ( 5 , height , width , 3 ) ) w = random_ops.random_normal ( [ 5 , 5 , 3 , 32 ] , mean=0 , stddev=1 ) nn_ops.conv2d ( images , w , strides= [ 1 , 1 , 1 , 1 ] , padding= '' same '' )
__label__0 def _test_dir ( self , test_name ) : test_dir = os.path.join ( self.get_temp_dir ( ) , test_name ) if os.path.exists ( test_dir ) : shutil.rmtree ( test_dir ) return test_dir
__label__0 @ property def last_checkpoints ( self ) : `` '' '' list of not-yet-deleted checkpoint filenames .
__label__0 @ compatibility ( tf2 ) tf.compat.v1.train.rmspropoptimizer is compatible with eager mode and ` tf.function ` . when eager execution is enabled , ` learning_rate ` , ` decay ` , ` momentum ` , and ` epsilon ` can each be a callable that takes no arguments and returns the actual value to use . this can be useful for changing these values across different invocations of optimizer functions .
__label__0 see [ variables ] ( https : //tensorflow.org/guide/variables ) for an overview of variables , saving and restoring .
__label__0 self.assertraises ( runtimeerror , _summary_computed ) self.assertraises ( runtimeerror , _start_standard_services )
__label__0 model_checkpoint_path = compat.as_str ( model_checkpoint_path ) if write_state : self._recordlastcheckpoint ( model_checkpoint_path ) checkpoint_management.update_checkpoint_state_internal ( save_dir=save_path_parent , model_checkpoint_path=model_checkpoint_path , all_model_checkpoint_paths=self.last_checkpoints , latest_filename=latest_filename , save_relative_paths=self._save_relative_paths ) self._maybedeleteoldcheckpoints ( meta_graph_suffix=meta_graph_suffix ) except ( errors.failedpreconditionerror , errors.notfounderror ) as exc : if not gfile.isdirectory ( save_path_parent ) : exc = valueerror ( `` parent directory of { } does n't exist , ca n't save . `` .format ( save_path ) ) raise exc
__label__0 globs [ '_print_if_not_none ' ] = _print_if_not_none # ref : https : //docs.python.org/3/library/doctest.html # doctest.docfilesuite return doctest.docfilesuite ( * files , module_relative=false , parser=fencedcellparser ( fence_label='python ' ) , globs=globs , setup=set_up , teardown=tear_down , checker=fencedcelloutputchecker ( ) , optionflags= ( doctest.ellipsis | doctest.normalize_whitespace | doctest.ignore_exception_detail | doctest.dont_accept_blankline ) , )
__label__0 def __init__ ( self ) : self.function_keyword_renames = { } self.symbol_renames = { } self.change_to_function = { } self.function_reorders = { } self.function_warnings = { } self.function_transformers = { } self.module_deprecations = module_deprecations_v2.module_deprecations
__label__0 pasta.base.formatting.set ( new_value , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( new_value , `` suffix '' , `` ) '' )
__label__0 import ast import collections import os import re import shutil import sys import tempfile import traceback
__label__0 class objectidentitysettest ( test.testcase ) :
__label__0 class testupgrade ( test_util.tensorflowtestcase , parameterized.testcase ) : `` '' '' test various apis that have been changed in 2.0 .
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.abs . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 args : source : can either be a normal string , a byte string , or an ast object . filename : argument should give the file from which the code was read ; pass some recognizable value if it wasn ’ t read from a file ( ' < string > ' is commonly used ) . mode : [ ignored ] always use exec . flags : compiler options . dont_inherit : compiler options . optimize : compiler options .
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=true ) def _fn ( ) : pass
__label__0 > > > def compute_loss ( x ) : ... v = tf.variable ( 3.0 ) ... y = x * v ... loss = x * 5 - x * v ... return loss , [ v ]
__label__0 def testcompatiblewithnamelesscallables ( self ) :
__label__0 create_local_config_python ( os.path.join ( srcs_dir , `` external/local_config_python '' ) )
__label__0 def buildpytestdependencies ( ) : python_targets = getbuild ( `` tensorflow/python '' ) tensorflow_targets = getbuild ( `` tensorflow '' ) # build list of test targets , # python - attr ( manual|pno_pip ) targets = `` + `` .join ( python_targets ) targets += ' - attr ( tags , `` manual|no_pip '' , % s ) ' % `` + `` .join ( tensorflow_targets ) query_kind = `` kind ( py_test , % s ) '' % targets # skip benchmarks etc . query_filter = 'filter ( `` ^ ( ( ? ! benchmark ) . ) * $ '' , % s ) ' % query_kind # get the dependencies query_deps = `` deps ( % s , 1 ) '' % query_filter
__label__0 # non-equal dict/mapping . inp_val = dict ( a=2 , b=3 ) inp_ops = _custommapping ( a=dict ( add=1 , mul=2 ) , c=dict ( add=2 , mul=3 ) ) with self.assertraiseswithliteralmatch ( valueerror , nest.shallow_tree_has_invalid_keys.format ( [ `` b '' ] ) ) : nest.map_structure_up_to ( inp_val , lambda val , ops : ( val + ops [ `` add '' ] ) * ops [ `` mul '' ] , inp_val , inp_ops )
__label__0 def sycl_version_numbers ( path ) : possible_version_files = [ `` compiler/latest/linux/include/sycl/version.hpp '' , `` compiler/latest/include/sycl/version.hpp '' , ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` sycl version file not found in { } '' .format ( possible_version_files ) )
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 def upload_benchmark_data ( client , data ) : `` '' '' parse benchmark data and use the client to upload it to the datastore .
__label__0 this function therefore will return something with the same base structure as ` shallow_tree ` .
__label__0 # # input non-list edge-case . # using iterable elements . input_tree = `` input_tree '' shallow_tree = [ `` shallow_tree '' ] with self.assertraiseswithliteralmatch ( typeerror , nest.if_shallow_is_seq_input_must_be_seq.format ( type ( input_tree ) ) , ) : ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree_paths , [ ( 0 , ) ] ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 @ parameterized.parameters ( [ dict ( inputs= [ ] , expected= [ ] ) , dict ( inputs= [ 23 , `` 42 '' ] , expected= [ ( ( 0 , ) , 23 ) , ( ( 1 , ) , `` 42 '' ) ] ) , dict ( inputs= [ [ [ [ 108 ] ] ] ] , expected= [ ( ( 0 , 0 , 0 , 0 ) , 108 ) ] ) , dict ( inputs=foo ( a=3 , b=bar ( c=23 , d=42 ) ) , expected= [ ( ( `` a '' , ) , 3 ) , ( ( `` b '' , `` c '' ) , 23 ) , ( ( `` b '' , `` d '' ) , 42 ) ] ) , dict ( inputs=foo ( a=bar ( c=23 , d=42 ) , b=bar ( c=0 , d= '' thing '' ) ) , expected= [ ( ( `` a '' , `` c '' ) , 23 ) , ( ( `` a '' , `` d '' ) , 42 ) , ( ( `` b '' , `` c '' ) , 0 ) , ( ( `` b '' , `` d '' ) , `` thing '' ) ] ) , dict ( inputs=bar ( c=42 , d=43 ) , expected= [ ( ( `` c '' , ) , 42 ) , ( ( `` d '' , ) , 43 ) ] ) , dict ( inputs=bar ( c= [ 42 ] , d=43 ) , expected= [ ( ( `` c '' , 0 ) , 42 ) , ( ( `` d '' , ) , 43 ) ] ) , ] ) def testflattenwithtuplepaths ( self , inputs , expected ) : self.assertequal ( nest.flatten_with_tuple_paths ( inputs ) , expected )
__label__0 _ , biases = while_loop.while_loop ( loop_cond , loop_body , [ constant_op.constant ( 0 ) , variable_v1.variablev1 ( array_ops.zeros ( [ 32 ] ) ) ] ) hidden2 = nn_ops.relu ( math_ops.matmul ( hidden1 , weights ) + biases ) # linear with ops_lib.name_scope ( `` softmax_linear '' ) : weights = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 32 , 10 ] , stddev=1.0 / math.sqrt ( float ( 32 ) ) ) , name= '' weights '' ) biases = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' biases '' ) logits = math_ops.matmul ( hidden2 , weights ) + biases ops_lib.add_to_collection ( `` logits '' , logits )
__label__0 # # utilities for writing compatible code
__label__0 # hidden 2 with ops_lib.name_scope ( `` hidden2 '' ) : weights2 = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 128 , 32 ] , stddev=1.0 / math.sqrt ( float ( 128 ) ) ) , name= '' weights '' )
__label__0 x = customtensor ( [ 1 , 2 , 3 ] , 0.2 ) y = customtensor ( [ 7 , 8 , 2 ] , 0.4 ) z = customtensor ( [ 0 , 1 , 2 ] , 0.6 )
__label__0 def testqueuerunnerserializationroundtrip ( self ) : graph = ops.graph ( ) with graph.as_default ( ) : queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 , name= '' queue '' ) enqueue_op = control_flow_ops.no_op ( name= '' enqueue '' ) close_op = control_flow_ops.no_op ( name= '' close '' ) cancel_op = control_flow_ops.no_op ( name= '' cancel '' ) qr0 = queue_runner_impl.queuerunner ( queue , [ enqueue_op ] , close_op , cancel_op , queue_closed_exception_types= ( errors_impl.outofrangeerror , errors_impl.cancellederror ) ) qr0_proto = queue_runner_impl.queuerunner.to_proto ( qr0 ) qr0_recon = queue_runner_impl.queuerunner.from_proto ( qr0_proto ) self.assertequal ( `` queue '' , qr0_recon.queue.name ) self.assertequal ( 1 , len ( qr0_recon.enqueue_ops ) ) self.assertequal ( enqueue_op , qr0_recon.enqueue_ops [ 0 ] ) self.assertequal ( close_op , qr0_recon.close_op ) self.assertequal ( cancel_op , qr0_recon.cancel_op ) self.assertequal ( ( errors_impl.outofrangeerror , errors_impl.cancellederror ) , qr0_recon.queue_closed_exception_types )
__label__0 # todo ( fmuham ) : remove the export as genericfunction in future release . @ tf_export ( `` types.experimental.polymorphicfunction '' , `` types.experimental.genericfunction '' , # deprecated v1= [ ] , ) class polymorphicfunction ( callable , metaclass=abc.abcmeta ) : `` '' '' base class for polymorphic graph functions .
__label__0 cycles ( determined by reference equality , ` is ` ) stop the traversal . a stack of objects is kept to find cycles . objects forming cycles may appear in ` children ` , but ` visit ` will not be called with any object as ` parent ` which is already in the stack .
__label__0 return config
__label__0 returns : a dictionary mapping job names to lists or dictionaries describing the tasks in those jobs. `` '' '' ret = { } for job in self.jobs : task_indices = self.task_indices ( job ) if len ( task_indices ) == 0 : ret [ job ] = { } continue if max ( task_indices ) + 1 == len ( task_indices ) : # return a list because the task indices are dense . this # matches the behavior of ` as_dict ( ) ` before support for # sparse jobs was added . ret [ job ] = self.job_tasks ( job ) else : ret [ job ] = { i : self.task_address ( job , i ) for i in task_indices } return ret
__label__0 the most notable example of nativeobject is tensor. `` '' ''
__label__0 args : proto : parent proto of any message type . fields : list of string/int/map key fields , e.g . [ `` nodes '' , `` attr '' , `` value '' ] can represent ` proto.nodes.attr [ `` value '' ] ` .
__label__0 def get_rename_v2 ( name ) : if name not in all_renames_v2.symbol_renames : return none return all_renames_v2.symbol_renames [ name ]
__label__0 args : run_context : a ` sessionruncontext ` object . run_values : a sessionrunvalues object. `` '' '' pass
__label__0 import os from tensorflow.python.checkpoint import checkpoint_management from tensorflow.python.client import session as session_lib from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import resource_variables_toggle from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.ops import while_loop from tensorflow.python.platform import gfile from tensorflow.python.platform import test from tensorflow.python.training import saver as saver_lib from tensorflow.python.training import server_lib from tensorflow.python.training import session_manager
__label__0 def _getattribute ( self , name ) : # pylint : disable=g-doc-return-or-yield , g-doc-args `` '' '' imports and caches pre-defined api .
__label__0 raises : valueerror if the checkpoint provided is not an object-based checkpoint . notfounderror : if one of the variables in ` var_list ` can not be found in the checkpoint . this could mean the checkpoint or ` names_to_keys ` mapping is missing the variable. `` '' '' if names_to_keys is none : try : names_to_keys = object_graph_key_mapping ( checkpoint_path ) except errors.notfounderror : raise valueerror ( `` checkpoint in % s not an object-based checkpoint . '' % checkpoint_path ) if var_list is none : var_list = variables._all_saveable_objects ( ) # pylint : disable=protected-access if builder is none : builder = bulksaverbuilder ( )
__label__0 return files_processed , report_text , errors
__label__0 def _get_first_op_from_collection ( self , key ) : `` '' '' returns the first ` operation ` from a collection .
__label__0 returns : a session object that can be used to drive the model. `` '' '' # for users who recreate the session with prepare_or_wait_for_session ( ) , we # need to clear the coordinator 's stop_event so that threads managed by the # coordinator can run . self._coord.clear_stop ( ) if self._summary_writer : self._summary_writer.reopen ( )
__label__0 return tf_decorator.make_decorator ( fn , error_handler )
__label__0 self.assertprotoequals ( `` '' '' cluster { job { name : 'local ' tasks { key : 0 value : 'localhost:2222 ' } } } job_name : 'local ' task_index : 0 protocol : 'grpc ' `` '' '' , server_def )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : car , `` y '' : car } ) def add_car ( x , y , name=none ) : with ops.name_scope ( name ) : return car ( x.size + y.size , x.speed + y.speed )
__label__0 args : code_line : a line of python code magic_list : a list of jupyter `` magic '' exceptions
__label__0 historically a _nested structure_ was called a _nested sequence_ in tensorflow . a nested structure is sometimes called a _nest_ or a _tree_ , but the formal name _nested structure_ is preferred .
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 returns : a list of task addresses , where the index in the list corresponds to the task index of each task . the list may contain ` none ` if the job was defined with a sparse set of task indices .
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 ` context ` :
__label__0 `` ` class example ( object ) : @ property @ do_not_generate_docs def x ( self ) : return self._x `` `
__label__0 the motivation for this change is twofold :
__label__0 this method should be called once per splitter to create the chunks . users should call the methods ` split ` or ` write ` instead. `` '' ''
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' documentation control decorators . '' '' ''
__label__0 def _add_should_use_warning ( x , error_in_function=false , warn_in_eager=false ) : `` '' '' wraps object x so that if it is never used , a warning is logged .
__label__0 args : vars_to_warm_start : one of the following :
__label__0 def chunk_constant_value ( node : node_def_pb2.nodedef , size : int ) : `` '' '' extracts and clears the constant value from a nodedef .
__label__0 self.assert_trace_line_count ( fn , count=15 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=25 , filtering_enabled=false )
__label__0 returns : the parsed arguments object. `` '' '' desc = `` upload benchmark results to datastore . '' opts = [ ( `` -a '' , `` -- archivedir '' , str , none , true , `` directory where benchmark files are archived . `` ) , ( `` -d '' , `` -- datadir '' , str , none , true , `` directory of benchmark files to upload . `` ) , ]
__label__0 this decorator logs a deprecation warning whenever the decorated function is called . it has the following format :
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` z '' : maskedtensor } ) def my_add ( x , y , name=none ) : # pylint : disable=unused-variable del x , y , name
__label__0 for example , to ensure that ` method1 ` is * * never documented * * use this decorator on the base-class :
__label__0 if context.executing_eagerly ( ) and not warn_in_eager : return x
__label__1 def greet ( name ) : return f '' hello , { name } ! ''
__label__0 if ` func ` already expects a `` name '' arg , or if ` api_signature ` does not expect a `` name '' arg , then returns ` func ` as-is .
__label__0 # sparse features . for i in range ( num_sparse ) : key = fetched [ sparse_keys_start + i ] feature_config = config.feature_map [ key ] var_len_feature = feature_config.var_len_feature var_len_feature.dtype = sparse_types [ i ] .as_datatype_enum var_len_feature.indices_output_tensor_name = parse_example_op.outputs [ sparse_indices_start + i ] .name var_len_feature.values_output_tensor_name = parse_example_op.outputs [ sparse_values_start + i ] .name var_len_feature.shapes_output_tensor_name = parse_example_op.outputs [ sparse_shapes_start + i ] .name
__label__0 below is an example of migrating away from using a global step to using a keras optimizer :
__label__0 goodbye `` '' '' ) , ( 'not-output ' , [ ( 'code ' , none ) ] , `` '' '' hello
__label__0 class graphdefsplitter ( split.composablesplitter ) : `` '' '' implements proto splitter for graphdef .
__label__0 def get_inheritable_header ( obj ) - > optional [ str ] : return getattr ( obj , _inheritable_header , none )
__label__0 returns : if the line was split with ` \ `
__label__0 def testgetargspeconpartialkeywordargumentwithdefaultvalue ( self ) : `` '' '' tests getargspec on partial function that prunes argument by keyword . '' '' ''
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' graphdef splitter . '' '' ''
__label__0 class tensortraceropdispatcher ( dispatch.globalopdispatcher ) : `` '' '' global op dispatcher for tensortracer . '' '' ''
__label__0 @ test_decorator ( 'decorator ' ) class testdecoratedclass ( object ) : `` '' '' test decorated class . '' '' ''
__label__0 2. returned by ` run ` :
__label__0 see ` coordinator.request_stop ( ) ` .
__label__0 class analysisresult : `` '' '' this class represents an analysis result and how it should be logged .
__label__0 self.assertallcloseaccordingtotype ( var , self.evaluate ( var_t ) ) new_var , _ , _ = self._adamupdatenumpy ( var , grad , t , m , v , lr , beta1 , beta2 , epsilon ) apply_adam = gen_training_ops.apply_adam ( var_t , m_t , v_t , beta1_power_t , beta2_power_t , lr_t , beta1_t , beta2_t , epsilon_t , grad ) out = self.evaluate ( apply_adam ) self.assertshapeequal ( out , apply_adam ) self.assertallcloseaccordingtotype ( new_var , out )
__label__0 class fnargstest ( test.testcase ) :
__label__0 self.assert_trace_line_count ( fn , count=15 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=30 , filtering_enabled=false )
__label__0 ` tf.nest.is_nested ` checks whether an object is a nested structure or an atom . for example :
__label__0 if you are using a tf.estimator.estimator , this will automatically be called during training .
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testnesteddataclassflattenandpack ( self ) : nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) ) leaves = nest.flatten ( nmt ) reconstructed_mt = nest.pack_sequence_as ( nmt , leaves ) self.assertisinstance ( reconstructed_mt , nestedmaskedtensor ) self.assertequal ( reconstructed_mt , nmt )
__label__0 # this should never happen , since we 're only called for attribute nodes . if not isinstance ( node.func , ast.attribute ) : return
__label__0 # arg information parser.add_argument ( `` -- version '' , help= '' < new_major_ver > . < new_minor_ver > . < new_patch_ver > '' , default= '' '' ) parser.add_argument ( `` -- nightly '' , help= '' disable the service provisioning step '' , action= '' store_true '' )
__label__0 # sometimes a line would start with ` \n ` and content after # that 's the hack for this raw_code.append ( codeline ( cell_index , code_line.rstrip ( ) .replace ( `` \n '' , `` # # # === '' ) ) )
__label__0 @ functools.wraps ( func ) def new_func ( * args , * * kwargs ) : `` '' '' deprecation wrapper . '' '' '' if _print_deprecation_warnings : named_args = tf_inspect.getcallargs ( func , * args , * * kwargs ) for arg_name , arg_value in deprecated_kwargs.items ( ) : if arg_name in named_args and _safe_eq ( named_args [ arg_name ] , arg_value ) : if ( func , arg_name ) not in _printed_warning : if warn_once : _printed_warning [ ( func , arg_name ) ] = true _log_deprecation ( 'from % s : calling % s ( from % s ) with % s= % s is deprecated and ' 'will be removed % s.\ninstructions for updating : \n % s ' , _call_location ( ) , decorator_utils.get_qualified_name ( func ) , func.__module__ , arg_name , arg_value , 'in a future version ' if date is none else ( 'after % s ' % date ) , instructions ) return func ( * args , * * kwargs )
__label__0 args : master : ` string ` representation of the tensorflow master to use . init_op : optional ` operation ` used to initialize the model . saver : a ` saver ` object used to restore a model . checkpoint_dir : path to the checkpoint files . the latest checkpoint in the dir will be used to restore . checkpoint_filename_with_path : full file name path to the checkpoint file . wait_for_checkpoint : whether to wait for checkpoint to become available . max_wait_secs : maximum time to wait for checkpoints to become available . config : optional ` configproto ` proto used to configure the session . init_feed_dict : optional dictionary that maps ` tensor ` objects to feed values . this feed dictionary is passed to the session ` run ( ) ` call when running the init op . init_fn : optional callable used to initialize the model . called after the optional ` init_op ` is called . the callable must accept one argument , the session being initialized .
__label__0 def _find_tensorrt_config ( base_paths , required_version ) :
__label__0 total_size_diff = 0
__label__0 # strip spaces from the versions . actual_version = actual_version.strip ( ) required_version = required_version.strip ( ) return actual_version.startswith ( required_version )
__label__0 > > > structure = ( ( ' a ' , ' b ' ) , ( ' c ' , 'd ' , ' e ' ) , ' f ' ) > > > flat_sequence = [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) ( ( 1.0 , 2.0 ) , ( 3.0 , 4.0 , 5.0 ) , 6.0 )
__label__0 if _above_max_size ( self.proto_size ) : # split functiondeflibrary.function.node_def size_diff += repeatedmessagesplitter ( self._proto , `` node_def '' , [ constantnodedefsplitter , largemessagesplitter ] , parent_splitter=self , fields_in_parent= [ ] , ) .build_chunks ( ) return size_diff
__label__0 # namedtuples . ab_tuple = collections.namedtuple ( `` ab_tuple '' , `` a , b '' ) input_tree = ab_tuple ( a= [ 0 , 1 ] , b=2 ) shallow_tree = ab_tuple ( a=0 , b=1 ) ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( `` a '' , ) , ( `` b '' , ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ [ 0 , 1 ] , 2 ] )
__label__0 to understand how hooks interact with calls to ` monitoredsession.run ( ) ` , look at following code : with monitoredtrainingsession ( hooks=your_hooks , ... ) as sess : while not sess.should_stop ( ) : sess.run ( your_fetches )
__label__0 def _is_linux ( ) : return platform.system ( ) == `` linux ''
__label__0 filter out headers by their path and replace paths for some of them .
__label__0 * use ` __iter__ ` to create an explicit iterator , which is of type ` tf.distribute.distributediterator `
__label__0 major = _get_header_version ( version_file , `` rocm_version_major '' ) minor = _get_header_version ( version_file , `` rocm_version_minor '' ) patch = _get_header_version ( version_file , `` rocm_version_patch '' ) return major , minor , patch
__label__0 structure3 = collections.defaultdict ( list ) structure3 [ `` a '' ] = [ 1 , 2 , 3 , 4 ] structure3 [ `` b '' ] = [ 2 , 3 , 4 , 5 ]
__label__0 def __setitem__ ( self , key , value ) : self._storage [ self._wrap_key ( key ) ] = value
__label__0 `` ` class parent ( object ) : @ for_subclass_implementers def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 def export_meta_graph ( self , filename=none , collection_list=none , as_text=false , export_scope=none , clear_devices=false , clear_extraneous_savers=false , strip_default_attrs=false , save_debug_info=false ) : # pylint : disable=line-too-long `` '' '' writes ` metagraphdef ` to save_path/filename .
__label__0 # wrap the optimizer with sync_replicas_optimizer with 50 replicas : at each # step the optimizer collects 50 gradients before applying to variables . # note that if you want to have 2 backup replicas , you can change # total_num_replicas=52 and make sure this number matches how many physical # replicas you started in your job . opt = tf.compat.v1.train.syncreplicasoptimizer ( opt , replicas_to_aggregate=50 , total_num_replicas=50 )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def testlazyloadermock ( self , mock_warning ) : name = lazyloadertest.__module__ lazy_loader_module = lazy_loader.lazyloader ( `` lazy_loader_module '' , globals ( ) , name , warning= '' test warning . '' )
__label__0 mytuple ( 1 , 2 ) self.assertequal ( 1 , mock_warning.call_count ) mytuple ( 3 , 4 ) self.assertequal ( 1 , mock_warning.call_count ) self.assertin ( `` is deprecated '' , mytuple.__doc__ )
__label__0 if isinstance ( obj , tensor_shape.dimension ) : return obj.value
__label__0 def update_readme ( old_version , new_version ) : `` '' '' update readme . '' '' '' pep_440_str = new_version.pep_440_str replace_string_in_line ( r '' % s\. % s\ . ( [ [ : alnum : ] ] + ) - '' % ( old_version.major , old_version.minor ) , `` % s- '' % pep_440_str , readme_md )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow.tools.docs.generate2 . '' '' ''
__label__0 x = tensortracer ( `` x '' ) y = tensortracer ( `` y '' ) trace = math_ops.add ( math_ops.abs ( tensor_conversion.convert_to_tensor_v2_with_dispatch ( x ) ) , y , ) self.assertequal ( str ( trace ) , `` math.add ( math.abs ( convert_to_tensor ( x ) ) , y ) '' )
__label__0 > > > tensor = tf.ragged.constant ( [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] ) > > > tf.nest.flatten ( tensor , expand_composites=true ) [ < tf.tensor : shape= ( 7 , ) , dtype=int32 , numpy=array ( [ 3 , 1 , 4 , 1 , 5 , 9 , 2 ] , dtype=int32 ) > , < tf.tensor : shape= ( 4 , ) , dtype=int64 , numpy=array ( [ 0 , 4 , 4 , 7 ] ) > ]
__label__0 for each job , if the task index space is dense , the corresponding value will be a list of network addresses ; otherwise it will be a dictionary mapping ( sparse ) task indices to the corresponding addresses .
__label__0 class testastedits ( test_util.tensorflowtestcase ) :
__label__0 def _testtypesforftrl ( self , x , y , z , lr , grad , use_gpu=none , l1=0.0 , l2=0.0 , lr_power=-0.5 ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) linear = variable_v1.variablev1 ( z ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } }
__label__0 notice [ 0 ] = f ' { notice_type } : { notice [ 0 ] } ' notice = [ `` ] + notice + ( [ instructions ] if instructions else [ ] )
__label__0 def testconvolutionopupdate ( self ) : text = ( `` tf.nn.convolution ( input , filter , padding , strides , dilation_rate , `` `` name , data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) expected_text = ( `` tf.nn.convolution ( input , filters=filter , padding=padding , `` `` strides=strides , dilations=dilation_rate , name=name , `` `` data_format=data_format ) '' ) self.assertequal ( new_text , expected_text )
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( partial_func ) )
__label__0 def isfunction ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isfunction . '' '' '' return _inspect.isfunction ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 mt_combined_with_path = nest.map_structure_with_tuple_paths_up_to ( mt_out_template , tuple_path_sum , mt , mt2 , mt3 ) self.assertisinstance ( mt_combined_with_path , maskedtensor ) # metadata uses the one from the first arg ( mt_out_template ) . self.assertequal ( mt_combined_with_path.mask , false ) # tesnor index is 0 for the only compoenent in maskedtensor . self.assertallequal ( mt_combined_with_path.value [ 0 ] , ( 0 , ) ) # sum of all input tensors . self.assertallequal ( mt_combined_with_path.value [ 1 ] , [ 6 ] )
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=true ) class myclass ( ) : `` '' '' a test class . '' '' ''
__label__0 # lint.thenchange ( //tensorflow/virtual_root_template_v1.__init__.py.oss )
__label__0 def _build_eager ( self , checkpoint_path , build_save , build_restore ) : self._build ( checkpoint_path , build_save=build_save , build_restore=build_restore )
__label__0 def testdiscard ( self ) : a = object ( ) b = object ( ) set1 = object_identity.objectidentityset ( [ a , b ] ) set1.discard ( a ) self.assertin ( b , set1 ) self.assertnotin ( a , set1 )
__label__0 if isinstance ( obj , wrapt.objectproxy ) : return obj.__wrapped__
__label__0 def testcompatvalidencoding ( self ) : self.assertequal ( compat.as_bytes ( `` hello '' , `` utf8 '' ) , b '' hello '' ) self.assertequal ( compat.as_text ( b '' hello '' , `` utf-8 '' ) , `` hello '' )
__label__0 the contained list can be one of three types :
__label__0 def testexportsingleconstant ( self ) : module1 = self._createmockmodule ( 'module1 ' )
__label__0 def func ( a ) : return a
__label__0 class deprecatedaliastest ( test.testcase ) :
__label__0 def generate ( arglist , git_tag_override=none ) : `` '' '' generate version_info.cc as given ` destination_file ` .
__label__0 def testv2keywordargnames ( self ) : # this test converts a call of the form : # tf.foo ( arg1=0 , arg2=1 , ... ) # to 2.0. then , checks that converted function has valid argument names . if not hasattr ( tf.compat , `` v2 '' ) : return v2_arg_exceptions = { `` verify_shape_is_now_always_true '' , # these arguments should not be used , they just specify # that a function takes named arguments . `` keyword_required '' , `` _sentinel '' , } v1_name_exceptions = { `` tf.print '' , # requires print_function import } function_warnings = ( tf_upgrade_v2.tfapichangespec ( ) .function_warnings ) function_transformers = ( tf_upgrade_v2.tfapichangespec ( ) .function_transformers ) keyword_renames = ( tf_upgrade_v2.tfapichangespec ( ) .function_keyword_renames )
__label__0 def get_int_or_float_list ( self , min_length=_min_length , max_length=_max_length ) : `` '' '' consume a signed integer or float list with given constraints based on a consumed bool .
__label__0 # all arg warnings are handled here , since only we have the args arg_warnings = self._get_applicable_dict ( `` function_arg_warnings '' , full_name , name )
__label__0 return raw_code , notebook
__label__0 def testupgradecopywithsymlink ( self ) : if os.name == `` nt '' : self.skiptest ( `` os.symlink does n't work uniformly on windows . '' )
__label__0 requirements : version : the version tag or nightly : create a nightly tag with current date
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.update_reorders ( )
__label__0 @ test_util.run_deprecated_v1 def testdense ( self ) : # todo ( yori ) : use parameterizedtest when available for ( dtype , learning_rate , decay , momentum , epsilon , centered , use_resource ) in _testparams : with test_util.use_gpu ( ) : # initialize variables for numpy implementation . var0_np = np.array ( [ 1.0 , 2.0 ] , dtype=dtype.as_numpy_dtype ) grads0_np = np.array ( [ 0.1 , 0.2 ] , dtype=dtype.as_numpy_dtype ) var1_np = np.array ( [ 3.0 , 4.0 ] , dtype=dtype.as_numpy_dtype ) grads1_np = np.array ( [ 0.01 , 0.2 ] , dtype=dtype.as_numpy_dtype )
__label__0 @ dispatch.dispatch_for_binary_elementwise_apis ( maskedtensor , maskedtensor ) def handler ( api_func , x , y ) : return maskedtensor ( api_func ( x.values , y.values ) , x.mask & y.mask )
__label__0 flags.define_bool ( `` search_hints '' , true , `` include meta-data search hints at the top of each file . '' )
__label__0 if num_ragged ! = 0 : del ragged_values_start # unused del ragged_row_splits_start # unused raise valueerror ( `` ragged features are not yet supported by `` `` example_parser_configuration.proto '' )
__label__0 @ dispatch.dispatch_for_binary_elementwise_apis ( maskedtensor , maskedtensor ) def binary_elementwise_api_handler ( api_func , x , y ) : return maskedtensor ( api_func ( x.values , y.values ) , x.mask & y.mask )
__label__0 class tfdecorator ( object ) : `` '' '' base class for all tensorflow decorators .
__label__0 self._test_function = _test_function self._test_function2 = _test_function2 self._test_class_a = testclassa self._test_class_b = testclassb
__label__0 # pylint : disable=line-too-long # specially handled functions # each transformer is a callable which will be called with the arguments # transformer ( parent , node , full_name , name , logs ) # where logs is a list to which ( level , line , col , msg ) tuples can be # appended , full_name is the fqn of the function called ( or none if that is # unknown ) , name is the name of the function called ( or none is that is # unknown ) . node is an ast.call node representing this function call , and # parent is its parent in the ast . # the function may modify node ( but not parent ) , and must return # - none , if nothing was modified # - node , if node was modified in place ( make sure to use # pasta.ast_utils.replace_child to swap out children , otherwise formatting # may get messy ) # - a replacement for node , if the whole call node was replaced . the caller # will take care of changing parent . # after modifying this dict , run the following to update reorders_v2.py : # bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map # pylint : enable=line-too-long self.function_transformers = { `` * .make_initializable_iterator '' : _iterator_transformer , `` * .make_one_shot_iterator '' : _iterator_transformer , `` tf.nn.dropout '' : _dropout_transformer , `` tf.to_bfloat16 '' : _cast_transformer , `` tf.to_complex128 '' : _cast_transformer , `` tf.to_complex64 '' : _cast_transformer , `` tf.to_double '' : _cast_transformer , `` tf.to_float '' : _cast_transformer , `` tf.to_int32 '' : _cast_transformer , `` tf.to_int64 '' : _cast_transformer , `` tf.nn.softmax_cross_entropy_with_logits '' : _softmax_cross_entropy_with_logits_transformer , `` tf.image.extract_glimpse '' : _extract_glimpse_transformer , `` tf.image.resize_area '' : _image_resize_transformer , `` tf.image.resize_bicubic '' : _image_resize_transformer , `` tf.image.resize_bilinear '' : _image_resize_transformer , `` tf.image.resize_nearest_neighbor '' : _image_resize_transformer , `` tf.nn.fractional_avg_pool '' : _pool_seed_transformer , `` tf.nn.fractional_max_pool '' : _pool_seed_transformer , `` tf.name_scope '' : _name_scope_transformer , # todo ( b/129398290 ) # `` tf.string_split '' : _string_split_transformer , `` tf.strings.split '' : _string_split_rtype_transformer , `` tf.device '' : functools.partial ( _rename_if_arg_found_transformer , arg_name= '' device_name '' , arg_ok_predicate=_is_ast_str , remove_if_ok=false , message= '' tf.device no longer takes functions as an argument. `` `` we could not determine that the argument value is a string , so `` `` the call was converted to compat.v1 . `` ) , `` tf.zeros_like '' : functools.partial ( _rename_if_arg_found_transformer , arg_name= '' optimize '' , arg_ok_predicate=_is_ast_true , remove_if_ok=true , message= '' tf.zeros_like no longer takes an optimize argument , and `` `` behaves as if optimize=true . this call site specifies something `` `` other than optimize=true , so it was converted to compat.v1 . `` ) , `` tf.ones_like '' : functools.partial ( _rename_if_arg_found_transformer , arg_name= '' optimize '' , arg_ok_predicate=_is_ast_true , remove_if_ok=true , message= '' tf.ones_like no longer takes an optimize argument , and `` `` behaves as if optimize=true . this call site specifies something `` `` other than optimize=true , so it was converted to compat.v1 . `` ) , `` tf.while_loop '' : functools.partial ( _rename_if_arg_found_transformer , arg_name= '' return_same_structure '' , arg_ok_predicate=_is_ast_true , remove_if_ok=true , message= '' tf.while_loop no longer takes 'return_same_structure ' `` `` argument and behaves as if return_same_structure=true . this call `` `` site specifies something other than return_same_structure=true , `` `` so it was converted to compat.v1 . `` ) , `` tf.nn.ctc_beam_search_decoder '' : functools.partial ( _rename_if_arg_found_transformer , arg_name= '' merge_repeated '' , arg_ok_predicate=_is_ast_false , remove_if_ok=true , message= '' tf.nn.ctc_beam_search_decoder no longer takes the `` `` 'merge_repeated ' argument and behaves as if merge_repeated=false. `` `` this call site specifies something other than `` `` merge_repeated=false , so it was converted to compat.v1 . `` ) , `` tf.nn.dilation2d '' : functools.partial ( _add_argument_transformer , arg_name= '' data_format '' , arg_value_ast=ast.str ( `` nhwc '' ) ) , `` tf.nn.erosion2d '' : functools.partial ( _add_argument_transformer , arg_name= '' data_format '' , arg_value_ast=ast.str ( `` nhwc '' ) ) , `` tf.contrib.summary.always_record_summaries '' : functools.partial ( _add_summary_recording_cond_transformer , cond= '' true '' ) , `` tf.contrib.summary.audio '' : _add_summary_step_transformer , `` tf.contrib.summary.generic '' : _add_summary_step_transformer , `` tf.contrib.summary.histogram '' : _add_summary_step_transformer , `` tf.contrib.summary.image '' : _add_summary_step_transformer , `` tf.contrib.summary.never_record_summaries '' : functools.partial ( _add_summary_recording_cond_transformer , cond= '' false '' ) , `` tf.contrib.summary.scalar '' : _add_summary_step_transformer , `` tf.contrib.layers.l1_regularizer '' : _contrib_layers_l1_regularizer_transformer , `` tf.contrib.layers.l2_regularizer '' : _contrib_layers_l2_regularizer_transformer , `` tf.contrib.layers.xavier_initializer '' : _contrib_layers_xavier_initializer_transformer , `` tf.contrib.layers.xavier_initializer_conv2d '' : _contrib_layers_xavier_initializer_transformer , `` tf.contrib.layers.variance_scaling_initializer '' : _contrib_layers_variance_scaling_initializer_transformer , `` tf.initializers.uniform_unit_scaling '' : _add_uniform_scaling_initializer_transformer , `` tf.uniform_unit_scaling_initializer '' : _add_uniform_scaling_initializer_transformer , `` slim.l1_regularizer '' : _contrib_layers_l1_regularizer_transformer , `` slim.l2_regularizer '' : _contrib_layers_l2_regularizer_transformer , `` slim.xavier_initializer '' : _contrib_layers_xavier_initializer_transformer , `` slim.xavier_initializer_conv2d '' : _contrib_layers_xavier_initializer_transformer , `` slim.variance_scaling_initializer '' : _contrib_layers_variance_scaling_initializer_transformer , `` tf.keras.models.save_model '' : functools.partial ( _add_argument_transformer , arg_name= '' save_format '' , arg_value_ast=ast.str ( `` h5 '' ) ) , } all_renames_v2.add_contrib_direct_import_support ( self.function_transformers )
__label__0 def testposandkwd2 ( self ) : self.assertequal ( self._matmul_func.canonicalize ( 2 , b=3 ) , [ 2 , 3 , false , false , false , false , false , false , none ] )
__label__1 def calculate_factorial ( n ) : result = 1 for i in range ( 1 , n + 1 ) : result * = i return result
__label__0 class pickletest ( test.testcase ) :
__label__0 def h ( a , kw1 , kw2 ) : ...
__label__0 # this is checking actual equality of types , empty list ! = empty tuple self.assertnotequal ( ( ) , nest.map_structure ( lambda x : x + 1 , [ ] ) )
__label__0 self.assertequal ( set ( range ( num_threads ) ) , finished )
__label__0 returns : node , if it was modified , else none. `` '' '' for arg_name in arg_names : rename_node = _rename_if_arg_found_transformer ( parent , node , full_name , name , logs , arg_name , arg_ok_predicate , remove_if_ok , message ) node = rename_node if rename_node else node
__label__0 also see ` tf.function ` . `` '' ''
__label__0 @ tf_export ( v1= [ `` train.supervisor '' ] ) class supervisor : `` '' '' a training helper that checkpoints models and computes summaries .
__label__0 def testdocstringonboundproperty ( self ) : self.assertequal ( 'return parameters . ' , testdecoratedclass ( ) .return_params.__doc__ )
__label__0 add_fallback_dispatch_list ( op_dispatch_handler ) op_dispatch_handler = tf_decorator.make_decorator ( dispatch_target , op_dispatch_handler ) add_type_based_api_dispatcher ( op_dispatch_handler ) api_dispatcher = getattr ( op_dispatch_handler , type_based_dispatch_attr , none ) return op_dispatch_handler
__label__0 @ test_util.run_deprecated_v1 def testminimizesparseresourcevariable ( self ) : for dtype in [ dtypes.float32 , dtypes.float64 ] : with self.cached_session ( ) : var0 = resource_variable_ops.resourcevariable ( [ [ 1.0 , 2.0 ] ] , dtype=dtype ) x = constant_op.constant ( [ [ 4.0 ] , [ 5.0 ] ] , dtype=dtype ) pred = math_ops.matmul ( embedding_ops.embedding_lookup ( [ var0 ] , [ 0 ] ) , x ) loss = pred * pred sgd_op = rmsprop.rmspropoptimizer ( learning_rate=1.0 , decay=0.0 , momentum=0.0 , epsilon=0.0 , centered=false ) .minimize ( loss ) self.evaluate ( variables.global_variables_initializer ( ) ) # fetch params to validate initial values self.assertallcloseaccordingtotype ( [ [ 1.0 , 2.0 ] ] , self.evaluate ( var0 ) ) # run 1 step of sgd self.evaluate ( sgd_op ) # validate updated params self.assertallcloseaccordingtotype ( [ [ 0. , 1 . ] ] , self.evaluate ( var0 ) , atol=0.01 )
__label__0 losses_comment = ( ast_edits.info , `` tf.losses have been replaced with object oriented versions in '' `` tf 2.0 and after . the loss function calls have been converted to `` `` compat.v1 for backward compatibility . please update these calls to `` `` the tf 2.0 versions . '' )
__label__0 # since ` noise ` / ` uniform_noise ` is optional arg , nothing needs to be # done if len ( node.args ) < 5. if len ( node.args ) > = 5 : _replace_uniform_noise_node ( node , node.args [ 5 ] ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing uniform_noise arg of tf.image.extract_glimpse to `` `` noise , and recomputing value.\n '' ) ) return node
__label__0 while the list of keys , and the contents of each key _could_ be different for every ` example ` , tensorflow expects a fixed list of keys , each with a fixed ` tf.dtype ` . a conformant ` example ` dataset obeys the following conventions :
__label__0 decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , argspec ) self.assertequal ( argspec , tf_inspect.getfullargspec ( decorator ) )
__label__0 `` ` # tf.train.feature feature = union [ list [ bytes ] , list [ int64 ] , list [ float ] ]
__label__0 returns : a ` bytes ` object .
__label__0 text = `` tf.contrib.layers.variance_scaling_initializer ( factor=1.0 ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 ) \n '' , )
__label__0 db = dict ( ) models = set ( ) batch_sizes = set ( ) state = enum.enum ( `` state '' , `` find_config_or_model find_running_time '' )
__label__0 `` ` python b `` `
__label__0 args : min_float : minimum allowed float . max_float : maximum allowed float .
__label__0 def _find_cublas_config ( base_paths , required_version , cuda_version ) :
__label__0 just like sess.close ( ) but ignores exceptions .
__label__0 def testpreparesessionwithreadynotreadyforlocal ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( variables.global_variables ( ) ) , local_init_op=w.initializer ) with self.assertraisesregex ( runtimeerror , `` init operations did not make model ready for local_init '' ) : sm2.prepare_session ( `` '' , init_op=none )
__label__0 def testcreateslotfromvariablerespectsscope ( self ) : # see discussion on # 2740 . # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : with variable_scope.variable_scope ( `` scope '' ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) slot = slot_creator.create_slot ( v , initialized_value ( v ) , name= '' slot '' ) self.assertequal ( `` scope/scope/var/slot '' , slot.op.name )
__label__0 returns : list of all api constants under the given module. `` '' '' constants_v1 = [ ] tensorflow_constants_attr_v1 = api_attrs_v1 [ tensorflow_api_name ] .constants
__label__0 create a session on 'master ' , recovering or initializing the model as needed , or wait for a session to be ready . if running as the chief and ` start_standard_service ` is set to true , also call the session manager to start the standard services .
__label__0 def testinitsetsdecoratordoctotargetdoc ( self ) : self.assertequal ( 'test function docstring . ' , tf_decorator.tfdecorator ( `` , test_function ) .__doc__ )
__label__0 def testupgradecopywithsymlinkindifferentdir ( self ) : if os.name == `` nt '' : self.skiptest ( `` os.symlink does n't work uniformly on windows . '' )
__label__0 this contains most of the synchronization implementation and also wraps the apply_gradients ( ) from the real optimizer .
__label__0 from tensorflow.python.types import doc_typealias
__label__0 import inspect import io import os import tempfile
__label__0 if check_types_dict : if `` check_types '' not in check_types_dict or len ( check_types_dict ) > 1 : raise valueerror ( `` only valid keyword argument for ` check_types_dict ` is `` f '' 'check_types ' . got { check_types_dict } . '' ) check_types = check_types_dict [ `` check_types '' ] else : check_types = true
__label__0 builds a graph , exports it , and verifies that it can be imported and the gradient can be built and run correctly .
__label__0 * scalars :
__label__0 > > > optimizer = tf.keras.optimizers.sgd ( .01 ) > > > print ( `` before training : '' , optimizer.iterations.numpy ( ) ) before training : 0 > > > with tf.gradienttape ( ) as tape : ... loss , var_list = compute_loss ( 3 ) ... grads = tape.gradient ( loss , var_list ) ... optimizer.apply_gradients ( zip ( grads , var_list ) ) > > > print ( `` after training : '' , optimizer.iterations.numpy ( ) ) after training : 1
__label__0 > > > structure = { `` key3 '' : { `` c '' : ( 'alpha ' , 'beta ' ) , `` a '' : ( 'gamma ' ) } , ... `` key1 '' : { `` e '' : `` val1 '' , `` d '' : `` val2 '' } } > > > flat_sequence = [ 'val2 ' , 'val1 ' , 3.0 , 1.0 , 2.0 ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) { 'key3 ' : { ' c ' : ( 1.0 , 2.0 ) , ' a ' : 3.0 } , 'key1 ' : { ' e ' : 'val1 ' , 'd ' : 'val2 ' } }
__label__0 def testwrite ( self ) : path = os.path.join ( self.create_tempdir ( ) , `` split-repeat '' ) data = [ _random_string ( 5 ) , _random_string ( 10 ) , _random_string ( 15 ) ] returned_path = repeatedstringsplitter ( test_message_pb2.repeatedstring ( strings=data ) ) .write ( path ) self.assertequal ( returned_path , f '' { path } .cpb '' )
__label__0 `` ` python doctest : +skip `` `
__label__0 raises : runtimeerror : if the script is not being run from tf source dir `` '' ''
__label__0 the ` ready_op ` is an ` operation ` used to check if the model is ready . the model is considered ready if that operation returns an empty 1d string tensor . if the operation returns a non empty 1d string tensor , the elements are concatenated and used to indicate to the user why the model is not ready .
__label__0 visitor = public_api.publicapivisitor ( conversion_visitor ) visitor.do_not_descend_map [ `` tf '' ] .append ( `` contrib '' ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v1 , visitor )
__label__0 should * not * be used with subclasses of ` compositetensor ` or ` extensiontype ` ( which are automatically registered ) .
__label__0 `` `` '' generate __all__ from a module docstring . '' '' '' import re as _re import sys as _sys
__label__0 def testpreparesessionfails ( self ) : checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` prepare_session '' ) checkpoint_dir2 = os.path.join ( self.get_temp_dir ( ) , `` prepare_session2 '' ) try : gfile.deleterecursively ( checkpoint_dir ) gfile.deleterecursively ( checkpoint_dir2 ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 init = checkpoint_ops._load_and_remap_matrix_initializer ( ckpt_path=checkpoint_utils._get_checkpoint_filename ( prev_ckpt ) , old_tensor_name=prev_tensor_name , new_row_vocab_size=new_row_vocab_size , new_col_vocab_size=new_col_vocab_size , old_row_vocab_size=old_row_vocab_size , old_row_vocab_file=old_row_vocab_file , new_row_vocab_file=new_row_vocab_file , old_col_vocab_file=old_col_vocab_file , new_col_vocab_file=new_col_vocab_file , num_row_oov_buckets=num_row_oov_buckets , num_col_oov_buckets=num_col_oov_buckets , initializer=initializer ) new_init_val = ops.convert_to_tensor ( init ( shape=v_shape , partition_info=partition_info ) ) v._initializer_op = state_ops.assign ( v , new_init_val )
__label__0 rms0 = opt.get_slot ( var0 , `` rms '' ) self.asserttrue ( rms0 is not none ) rms1 = opt.get_slot ( var1 , `` rms '' ) self.asserttrue ( rms1 is not none ) mom0 = opt.get_slot ( var0 , `` momentum '' ) self.asserttrue ( mom0 is not none ) mom1 = opt.get_slot ( var1 , `` momentum '' ) self.asserttrue ( mom1 is not none )
__label__0 # if an actual name was given ... if name_found and pasta.dump ( name ) ! = `` none '' : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` ` name ` passed to ` name_scope ` . because you may be re-entering '' `` an existing scope , it is not safe to convert automatically , `` `` the v2 name_scope does not support re-entering scopes by '' `` name.\n '' ) ) # rename to compat.v1 new_name = `` tf.compat.v1.name_scope '' logs.append ( ( ast_edits.info , node.func.lineno , node.func.col_offset , `` renamed % r to % r '' % ( full_name , new_name ) ) ) new_name_node = ast_edits.full_name_node ( new_name , node.func.ctx ) ast.copy_location ( new_name_node , node.func ) pasta.ast_utils.replace_child ( node , node.func , new_name_node ) return node
__label__0 # build a graph with 2 parameter nodes on different devices . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : v0 = variable_v1.variablev1 ( 10 , name= '' v0 '' ) t0 = saver_test_utils.checkpointedop ( name= '' t0 '' ) with sess.graph.device ( `` /cpu:1 '' ) : v1 = variable_v1.variablev1 ( 20 , name= '' v1 '' ) t1 = saver_test_utils.checkpointedop ( name= '' t1 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` t0 '' : t0.saveable , `` t1 '' : t1.saveable } , write_version=self._write_version , sharded=true ) self.evaluate ( variables.global_variables_initializer ( ) ) t0.insert ( `` k1 '' , 30.0 ) .run ( ) t1.insert ( `` k2 '' , 40.0 ) .run ( ) val = save.save ( sess , save_path ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( save_path + `` - ? ? ? ? ? -of-00002 '' , val ) else : self.assertequal ( save_path , val ) meta_graph_filename = checkpoint_management.meta_graph_filename ( val ) self.assertequal ( save_path + `` .meta '' , meta_graph_filename )
__label__0 for meta_graph_def in [ meta_graph_def_simple , meta_graph_def_devices_cleared , meta_graph_def_from_graph_def ] : with session.session ( graph=ops_lib.graph ( ) ) as sess : saver_module.import_meta_graph ( meta_graph_def , import_scope= '' new_model '' ) self.evaluate ( variables.global_variables_initializer ( ) ) for i in range ( 10 ) : self.assertequal ( i * i , sess.run ( `` new_model/output:0 '' ) ) with self.assertraises ( errors.outofrangeerror ) : sess.run ( `` new_model/output:0 '' )
__label__0 from tensorflow.python.util import tf_inspect as _tf_inspect
__label__0 def find_rocm_config ( ) : `` '' '' returns a dictionary of rocm components config info . '' '' '' rocm_install_path = _get_rocm_install_path ( ) if not os.path.exists ( rocm_install_path ) : raise configerror ( 'specified rocm_path `` { } '' does not exist'.format ( rocm_install_path ) )
__label__0 feature_lists : { feature_list : { key : `` movie_ratings '' value : { } } } `` `
__label__0 import_header = `` import tensorflow as tf , other_import as y\n '' text = import_header + old_symbol new_import_header = `` import tensorflow.compat.v2 as tf , other_import as y\n '' expected_text = new_import_header + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 # pylint : disable=missing-function-docstring def _tf_core_assert_same_structure ( nest1 , nest2 , check_types=true , expand_composites=false ) : # convert to bool explicitly as otherwise pybind will not be able # to handle # type mismatch message correctly . see github issue 42329 for details . check_types = bool ( check_types ) expand_composites = bool ( expand_composites ) try : _pywrap_utils.assertsamestructure ( nest1 , nest2 , check_types , expand_composites ) except ( valueerror , typeerror ) as e : str1 = str ( _tf_core_map_structure ( lambda _ : _dot , nest1 ) ) str2 = str ( _tf_core_map_structure ( lambda _ : _dot , nest2 ) ) raise type ( e ) ( `` % s\nentire first structure : \n % s\nentire second structure : \n % s '' % ( str ( e ) , str1 , str2 ) )
__label__0 1. python dict ( ordered by key ) :
__label__0 return a list of files in the given directory , sorted from older to newer file according to their modification times . only return actual files , skipping directories , symbolic links , pipes , etc .
__label__0 return copy_tx ( x , tf_should_use_helper )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : # initialize with zeros . var = variable_scope.get_variable ( `` v1 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= [ var ] ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started ( init overridden to ones ) . self.assertallequal ( var , prev_int_val )
__label__0 # run two tf.functions simultaneously to make sure there is no race # condition between the two ops that caused deadlock before ( b/270712679 ) . thread1 = threading.thread ( target=lambda : self.evaluate ( fn_disable_copy_on_read ( ) ) ) thread2 = threading.thread ( target=lambda : self.evaluate ( fn_resource_sparse_apply_adagrad_v2 ( ) ) ) thread1.start ( ) thread2.start ( ) thread1.join ( ) thread2.join ( )
__label__0 def _maybe_add_arg_names ( self , node , full_name ) : `` '' '' make args into keyword args if function called full_name requires it . '' '' '' function_reorders = self._api_change_spec.function_reorders
__label__0 warns if necessary .
__label__0 @ doc_controls.doc_private @ abc.abstractmethod def __tf_tracing_type__ ( self , context : tracingcontext ) - > tracetype : `` '' '' returns the tracing type of this object .
__label__0 - a summary thread computing summaries every save_summaries_secs . - a checkpoint thread saving the model every save_model_secs . - a stepcounter thread measure step time .
__label__0 # append new path to list self._last_checkpoints.append ( ( latest_save_path , time.time ( ) ) )
__label__0 # if i break the loop comprehension , then the test times out in ` small ` # size . if any ( module.__name__.startswith ( package + prefix ) # pylint : disable=g-complex-comprehension for prefix in flags.module_prefix_skip for package in packages ) : continue testcase = tftestcase ( ) tests.addtests ( doctest.doctestsuite ( module , test_finder=doctest.doctestfinder ( exclude_empty=false ) , extraglobs= { 'tf ' : tf , 'np ' : np , 'os ' : os } , setup=testcase.set_up , teardown=testcase.tear_down , checker=tf_doctest_lib.tfdoctestoutputchecker ( ) , optionflags= ( doctest.ellipsis | doctest.normalize_whitespace | doctest.ignore_exception_detail | doctest.dont_accept_blankline ) , ) ) return tests
__label__0 def _check ( self , doc , expected ) : self.assertequal ( decorator_utils.add_notice_to_docstring ( doc=doc , instructions= '' instructions '' , no_doc_str= '' nothing here '' , suffix_str= '' ( suffix ) '' , notice= [ `` go away '' ] ) , expected )
__label__0 def testgraphextension ( self ) : test_dir = self._get_test_dir ( `` graph_extension '' ) # train.saver and train.import_meta_graph are v1 only apis . with ops_lib.graph ( ) .as_default ( ) : self._testgraphextensionsave ( test_dir ) self._testgraphextensionrestore ( test_dir ) self._testrestorefromtraingraphwithcontrolcontext ( test_dir )
__label__0 # resets target . sessions abort . use sess_2 to verify . session.session.reset ( server.target ) with self.assertraises ( errors_impl.abortederror ) : self.assertallequal ( [ [ 4 ] ] , sess_2.run ( v2 ) )
__label__0 parser.add_argument ( `` -- raw_generate '' , type=str , help= '' generate build_info.py '' )
__label__0 `` `` '' interface that provides access to keras dependencies .
__label__0 def empty ( ) : pass
__label__0 # check that credentials are specified to access the datastore . if not os.environ.get ( `` google_application_credentials '' ) : raise valueerror ( `` google_application_credentials env . var . is not set . '' )
__label__0 return nest_util.pack_sequence_as ( nest_util.modality.core , structure , flatten ( structure ) , false , sequence_fn=sequence_fn , )
__label__0 def create_zeros_slot ( primary , name , dtype=none , colocate_with_primary=true , * , copy_xla_sharding=false ) : `` '' '' create a slot initialized to 0 with same shape as the primary object .
__label__0 if not chunked_message.chunked_fields : path = f '' { file_prefix } .pb '' file_io.atomic_write_string_to_file ( path , self._proto.serializetostring ( deterministic=true ) ) logging.info ( `` unchunked file exported to % s '' , path ) return path
__label__0 # create another saver and recover last checkpoints . the removed # checkpoint would be correctly omitted . save3 = saver_module.saver ( { `` v '' : v } , max_to_keep=10 ) self.assertequal ( [ ] , save3.last_checkpoints ) save3.recover_last_checkpoints ( [ s1 , s2 , s3 ] ) self.assertequal ( [ s2 , s3 ] , save3.last_checkpoints ) s4 = save3.save ( none , os.path.join ( save_dir , `` ckpt-4 '' ) ) self.assertcheckpointstate ( model_checkpoint_path=s4 , all_model_checkpoint_paths= [ s2 , s3 , s4 ] , save_dir=save_dir )
__label__0 the context manager is typically used as follows :
__label__0 class graphdebuginfobuilder ( _tf_stack.graphdebuginfobuilder ) :
__label__0 if not updated_aliases : self.generic_visit ( node ) return
__label__0 this helps to avoid circular dependencies. `` '' ''
__label__0 self.assertequal ( graph_def.node [ 4 ] .attr [ `` value '' ] .tensor.tensor_content , chunks [ 2 ] ) self.assertequal ( graph_def.node [ 2 ] .attr [ `` value '' ] .tensor.tensor_content , chunks [ 1 ] )
__label__0 proto_size = proto.bytesize ( ) if proto_size < constants.max_size ( ) : return
__label__0 def testeq ( self ) : self.assertequal ( server_lib.clusterspec ( { } ) , server_lib.clusterspec ( { } ) ) self.assertequal ( server_lib.clusterspec ( { `` job '' : [ `` host:2222 '' ] } ) , server_lib.clusterspec ( { `` job '' : [ `` host:2222 '' ] } ) , ) self.assertequal ( server_lib.clusterspec ( { `` job '' : { 0 : `` host:2222 '' } } ) , server_lib.clusterspec ( { `` job '' : [ `` host:2222 '' ] } ) )
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 def testglobaldispatcherlinearoperators ( self ) : original_global_dispatchers = dispatch._global_dispatchers try : tensortraceropdispatcher ( ) .register ( )
__label__0 # keywords are reordered , so we should reorder arguments too text = `` f ( a , b , c , d ) \n '' acceptable_outputs = [ `` f ( a , b , d , c ) \n '' , `` f ( a=a , b=b , kw1=c , kw2=d ) \n '' , `` f ( a=a , b=b , kw2=d , kw1=c ) \n '' , ] ( _ , report , _ ) , new_text = self._upgrade ( reorderkeywordspec ( ) , text ) self.assertin ( new_text , acceptable_outputs ) self.assertnotin ( `` manual check required '' , report )
__label__0 def filter_traceback ( fn ) : `` '' '' decorator to filter out tf-internal stack trace frames in exceptions .
__label__0 from tensorflow.compiler.tests import xla_test from tensorflow.python.client import device_lib from tensorflow.python.framework import dtypes from tensorflow.python.ops import array_ops from tensorflow.python.ops import math_ops from tensorflow.python.platform import googletest
__label__0 # # inform about the addons mappings for symbol , replacement in all_renames_v2.addons_symbol_mappings.items ( ) : warning = ( ast_edits.warning , ( `` ( manual edit required ) ` { } ` has been migrated to ` { } ` in `` `` tensorflow addons . the api spec may have changed during the `` `` migration . please see https : //github.com/tensorflow/addons `` `` for more info . `` ) .format ( symbol , replacement ) ) self.function_warnings [ symbol ] = warning
__label__0 if the ` tf.distribute.distributediterator ` has reached the end of the sequence , the returned ` tf.experimental.optional ` will have no value .
__label__0 import argparse import os import re import shutil import tempfile import zipfile
__label__0 def main ( ) : try : for key , value in sorted ( find_sycl_config ( ) .items ( ) ) : print ( `` % s : % s '' % ( key , value ) ) except configerror as e : sys.stderr.write ( `` \nerror : { } \n\n '' .format ( str ( e ) ) ) sys.exit ( 1 )
__label__0 # list module renames . if changed , please update max_submodule_depth . self.import_renames = { `` tensorflow '' : ast_edits.importrename ( `` tensorflow.compat.v1 '' , excluded_prefixes= [ `` tensorflow.contrib '' , `` tensorflow.flags '' , `` tensorflow.compat '' , `` tensorflow.compat.v1 '' , `` tensorflow.compat.v2 '' , `` tensorflow.google '' ] , ) } # needs to be updated if self.import_renames is changed . self.max_submodule_depth = 2
__label__0 def bar ( * args ) : # pylint : disable=no-method-argument return args [ 1 ] + args [ 2 ]
__label__0 def flatten ( self ) - > list [ `` tracetype '' ] : `` '' '' returns a list of tensorspecs corresponding to ` to_tensors ` values . '' '' '' return [ ]
__label__0 example :
__label__0 tf_fn_in_this_function = def_function.function ( in_this_function ) with self.assertraisesregex ( runtimeerror , r'object was never used . * blah0:0 ' ) : tf_fn_in_this_function ( ) self.assertfalse ( gc.garbage )
__label__0 this decorator is similar to ` functools.cached_property ` , but it adds a property to the class , not to individual instances. `` '' ''
__label__0 def _assert_subset ( self , expected_subset , actual_set ) : self.asserttrue ( actual_set.issuperset ( expected_subset ) , msg= '' % s is not a superset of % s . '' % ( actual_set , expected_subset ) )
__label__0 # note : non-capturing groups `` ( ? '' are not returned in matched groups , or by # re.split . _float_re = re.compile ( r '' '' '' ( # captures the float value . ( ? : [ -+ ] | # start with a sign is okay anywhere . ( ? : # otherwise : ^| # start after the start of string ( ? < = [ ^\w . ] ) # not after a word char , or a . ) ) ( ? : # digits and exponent - something like : { digits_dot_maybe_digits } { exponent } ? | # `` 1.0 '' `` 1 . '' `` 1.0e3 '' , `` 1.e3 '' { dot_digits } { exponent } ? | # `` .1 '' `` .1e3 '' { digits } { exponent } | # `` 1e3 '' { digits } ( ? =j ) # `` 300j '' ) ) j ? # optional j for cplx numbers , not captured . ( ? = # only accept the match if $ | # * at the end of the string , or [ ^\w . ] # * next char is not a word char or `` . '' ) `` '' '' .format ( # digits , a `` . '' and optional more digits : `` 1.1 '' . digits_dot_maybe_digits=r ' ( ? : [ 0-9 ] +\. ( ? : [ 0-9 ] * ) ) ' , # a `` . '' with trailing digits `` .23 '' dot_digits=r ' ( ? : \ . [ 0-9 ] + ) ' , # digits : `` 12 '' digits=r ' ( ? : [ 0-9 ] + ) ' , # the exponent : an `` e '' or `` e '' , optional sign , and at least one digit . # `` e-123 '' , `` e+12 '' , `` e12 '' exponent=r ' ( ? : [ ee ] [ -+ ] ? [ 0-9 ] + ) ' ) , re.verbose )
__label__0 new_name = ( import_rename_spec.new_name + import_alias.name [ len ( import_component ) : ] )
__label__0 if not is_loaded_from_checkpoint : # do not need to run checks for readiness return sess , false
__label__0 def process_benchmarks ( log_files ) : benchmarks = test_log_pb2.benchmarkentries ( ) for f in log_files : content = gfile.gfile ( f , `` rb '' ) .read ( ) if benchmarks.mergefromstring ( content ) ! = len ( content ) : raise exception ( `` failed parsing benchmark entry from % s '' % f ) return benchmarks
__label__0 1. call ` tf_decorator.make_decorator ` on your wrapper function . if your decorator is stateless , or can capture all of the variables it needs to work with through lexical closure , this is the simplest option . create your wrapper function as usual , but instead of returning it , return ` tf_decorator.make_decorator ( target , your_wrapper ) ` . this will attach some decorator introspection metadata onto your wrapper and return it .
__label__0 yield doctest.example ( lineno=string [ : match.start ( ) ] .count ( '\n ' ) + 1 , source=source , want=want )
__label__0 return tf_decorator.make_decorator ( f , wrapper , decorator_argspec=f_argspec )
__label__0 > > > structure = { `` key3 '' : `` '' , `` key1 '' : `` '' , `` key2 '' : `` '' } > > > flat_sequence = [ `` value1 '' , `` value2 '' , `` value3 '' ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) { 'key3 ' : 'value3 ' , 'key1 ' : 'value1 ' , 'key2 ' : 'value2 ' }
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' types specific to tf.distribute . '' '' ''
__label__0 class mango ( fruit ) : flavor = tf.constant ( [ 3 , 4 ] ) `` `
__label__0 # gives ` < class 'keras.optimizer_v2.adam.adam ' > ` print ( cls )
__label__0 self.assertequal ( { 'cls ' : test , ' a ' : 5 , ' b ' : 3 , ' c ' : 'goodbye ' } , tf_inspect.getcallargs ( test.test , 5 , c='goodbye ' ) )
__label__0 flags.define_list ( 'module ' , [ ] , ' a list of specific module to run doctest on . ' ) flags.define_list ( 'module_prefix_skip ' , [ ] , ' a list of modules to ignore when resolving modules . ' ) flags.define_boolean ( 'list ' , none , 'list all the modules in the core package imported . ' ) flags.define_integer ( 'required_gpus ' , 0 , 'the number of gpus required for the tests . ' )
__label__0 emptynt = collections.namedtuple ( `` empty_nt '' , `` '' ) # pylint : disable=invalid-name
__label__0 class globalsteptest ( test.testcase ) :
__label__0 contrib_summary_graph_comment = ( ast_edits.error , `` ( manual edit required ) tf.contrib.summary.graph ( ) has no direct `` `` equivalent in tf 2.0 because manual graph construction has been `` `` superseded by use of tf.function . to log tf.function execution graphs `` `` to the summary writer , use the new tf.compat.v2.summary.trace_ * `` `` functions instead . '' )
__label__0 __slots__ = ( `` repeated_field '' , `` message_splitters '' )
__label__0 - for modality.data : applies ` func ( x [ 0 ] , x [ 1 ] , ... ) ` where x [ i ] is an entry in ` structure [ i ] ` . all structures in ` structure ` must have the same arity , and the return value will contain the results in the same structure .
__label__0 return saver ( saver_def=meta_graph_def.saver_def , name=scope ) else : if variables._all_saveable_objects ( scope=import_scope ) : # pylint : disable=protected-access # return the default saver instance for all graph variables . return saver ( ) else : # if no graph variables exist , then a saver can not be constructed . logging.info ( `` saver not created because there are no variables in the '' `` graph to restore '' ) return none
__label__0 self.assertequal ( expected_example_tuples , example_tuples )
__label__0 args : sess : a tensorflow ` session ` object . global_step_tensor : ` tensor ` or the ` name ` of the operation that contains the global step .
__label__0 import datetime import fnmatch import os import platform import re import sys
__label__0 raises : typeerror : if ` traverse_fn ` returns a nested structure for an atom input . or a structure with depth higher than 1 for a nested structure input , or if any leaf values in the returned structure or scalar are not type ` bool ` . `` '' '' is_nested_fn = _is_nested_or_composite if expand_composites else is_nested to_traverse = traverse_fn ( structure ) if not is_nested_fn ( structure ) : if not isinstance ( to_traverse , bool ) : raise typeerror ( `` traverse_fn returned structure : % s for non-structure : % s '' % ( to_traverse , structure ) ) return to_traverse level_traverse = [ ] if isinstance ( to_traverse , bool ) : if not to_traverse : # do not traverse this substructure at all . exit early . return false else : # traverse the entire substructure . for branch in nest_util.yield_value ( nest_util.modality.core , structure ) : level_traverse.append ( get_traverse_shallow_structure ( traverse_fn , branch , expand_composites=expand_composites ) ) elif not is_nested_fn ( to_traverse ) : raise typeerror ( `` traverse_fn returned a non-bool scalar : % s for input : % s '' % ( to_traverse , structure ) ) else : # traverse some subset of this substructure . assert_shallow_structure ( to_traverse , structure , expand_composites=expand_composites ) for t , branch in zip ( nest_util.yield_value ( nest_util.modality.core , to_traverse ) , nest_util.yield_value ( nest_util.modality.core , structure ) , ) : if not isinstance ( t , bool ) : raise typeerror ( `` traverse_fn did n't return a depth=1 structure of bools . saw : % s `` `` for structure : % s '' % ( to_traverse , structure ) ) if t : level_traverse.append ( get_traverse_shallow_structure ( traverse_fn , branch ) ) else : level_traverse.append ( false ) return nest_util.sequence_like ( structure , level_traverse )
__label__0 def __exit__ ( self , type_arg , value_arg , traceback_arg ) : del type_arg , value_arg , traceback_arg self._lock.release ( self._group_id )
__label__0 def restore ( self , restored_tensors , restored_shapes ) : `` '' '' restore the same value into both variables . '' '' '' pass
__label__0 def get_v1_names ( symbol : any ) - > sequence [ str ] : `` '' '' get a list of tf 1 . * names for this symbol .
__label__0 before :
__label__0 # the lint error here is incorrect . def __init__ ( self , local_name , parent_module_globals , name ) : self._local_name = local_name self._parent_module_globals = parent_module_globals super ( _lazyloader , self ) .__init__ ( name )
__label__0 * specifying ` 'grpc : //hostname : port ' ` requests a session that uses the rpc interface to a specific host , and also allows the in-process master to access remote tensorflow workers . often , it is appropriate to pass ` server.target ` ( for some ` tf.distribute.server ` named ` server ) .
__label__0 def testwrapperisamodule ( self ) : module = mockmodule ( 'test ' ) wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' ) self.asserttrue ( tf_inspect.ismodule ( wrapped_module ) )
__label__0 returns : ( function_name , list of arg names ) tuple. `` '' '' open_paren_index = call_str.find ( `` ( `` ) close_paren_index = call_str.rfind ( `` ) '' )
__label__0 result = { } if `` cuda '' in libraries : cuda_paths = _list_from_env ( `` cuda_toolkit_path '' , base_paths ) res = _find_cuda_config ( cuda_paths , cuda_version )
__label__0 applies ` func ( x [ 0 ] , x [ 1 ] , ... ) ` where x [ i ] enumerates all atoms in ` structure [ i ] ` . all items in ` structure ` must have the same arity , and the return value will contain results with the same structure layout .
__label__0 args : master : ` string ` representation of the tensorflow master to use . saver : a ` saver ` object used to restore a model . checkpoint_dir : path to the checkpoint files . the latest checkpoint in the dir will be used to restore . checkpoint_filename_with_path : full file name path to the checkpoint file . wait_for_checkpoint : whether to wait for checkpoint to become available . max_wait_secs : maximum time to wait for checkpoints to become available . config : optional ` configproto ` proto used to configure the session .
__label__0 you can create a ` tf.distribute.distributediterator ` by calling ` iter ` on a ` tf.distribute.distributeddataset ` or creating a python loop over a ` tf.distribute.distributeddataset ` .
__label__0 `` `` ''
__label__0 args : structure : a nested structure to be remapped .
__label__0 tests on partial function with variable keyword arguments. `` '' ''
__label__0 from google.protobuf.any_pb2 import any
__label__0 def max_size ( ) - > int : `` '' '' returns the maximum size each proto chunk . '' '' '' return _max_size
__label__0 def _make_cluster_device_filters ( self ) : `` '' '' creates ` clusterdevicefilters ` proto based on the ` _device_filters ` .
__label__0 refer to [ nesting data structures ] ( https : //en.wikipedia.org/wiki/nesting_ ( computing ) # data_structures ) .
__label__0 self.assertnotequal ( xla_sharding.get_tensor_sharding ( v ) , xla_sharding.get_tensor_sharding ( slot ) )
__label__0 # this copies the prefix and suffix on old_value to new_value . pasta.ast_utils.replace_child ( parent , old_value , new_value )
__label__0 ev = next ( rr ) self.assertprotoequals ( `` value { tag : ' v ' simple_value : 1.0 } '' , ev.summary )
__label__0 ` tf.distribute.distributedvalues ` can be reduced via ` strategy.reduce ` to obtain a single value across replicas ( example 4 ) , used as input into ` tf.distribute.strategy.run ` ( example 3 ) , or collected to inspect the per-replica values using ` tf.distribute.strategy.experimental_local_results ` ( example 5 ) .
__label__0 def testsharded ( self ) : save_dir = self._get_test_dir ( `` max_to_keep_sharded '' )
__label__0 try : del tools except nameerror : pass
__label__0 returns : a structure of the same form as the input structures whose leaves are the result of evaluating func on corresponding leaves of the input structures .
__label__0 def testunboundfuncwithoneparampositional ( self ) :
__label__0 # tf.raw_ops.acos takes tf.bfloat16 , tf.half , tf.float32 , tf.float64 , tf.int8 , # tf.int16 , tf.int32 , tf.int64 , tf.complex64 , tf.complex128 , but # get_random_numeric_tensor only generates tf.float16 , tf.float32 , tf.float64 , # tf.int32 , tf.int64 input_tensor = fh.get_random_numeric_tensor ( ) _ = tf.raw_ops.acos ( x=input_tensor )
__label__0 ` destination_file ` is the filename where version_info.cc will be written
__label__0 self.assertprotoequals ( expected_proto , cluster_spec.as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_cluster_def ( ) ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_dict ( ) ) .as_cluster_def ( ) )
__label__0 # tensors for learning rate and momentum . created in _prepare . self._learning_rate_tensor = none self._decay_tensor = none self._momentum_tensor = none self._epsilon_tensor = none
__label__0 @ property def element_spec ( self ) : `` '' '' the type specification of an element of this ` tf.distribute.distributeddataset ` .
__label__0 self.assertlen ( splitter.split ( ) [ 0 ] , 1 ) splitter.add_chunk ( `` '' , [ ] ) self.assertlen ( splitter.split ( ) [ 0 ] , 2 )
__label__0 import time from typing import optional , tuple
__label__0 def decorator ( func ) :
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] shallow_tree : nested structure . traverse no further than its leaf nodes . input_tree : nested structure . return the paths and values from this tree . must have the same upper structure as shallow_tree . is_nested_fn : arg valid for modality.core only . function used to test if a value should be treated as a nested structure . path : arg valid for modality.core only . tuple . optional argument , only used when recursing . the path from the root of the original shallow_tree , down to the root of the shallow_tree arg of this recursive call .
__label__0 raises : valueerror : if ` job_name ` does not name a job in this cluster. `` '' '' try : job = self._cluster_spec [ job_name ] except keyerror : raise valueerror ( `` no such job in cluster : % r '' % job_name ) ret = [ none for _ in range ( max ( job.keys ( ) ) + 1 ) ] for i , task in job.items ( ) : ret [ i ] = task return ret
__label__0 @ tf_export ( `` __internal__.dispatch.globalopdispatcher '' , v1= [ ] ) class globalopdispatcher ( object ) : `` '' '' abstract base class for tensorflow global operator dispatchers . '' '' ''
__label__0 argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } , ) decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , argspec ) signature = inspect.signature ( [ inspect.parameter ( ' a ' , inspect.parameter.keyword_only , default=2 ) , inspect.parameter ( ' b ' , inspect.parameter.keyword_only , default=1 ) , inspect.parameter ( ' c ' , inspect.parameter.keyword_only , default='hello ' ) , ] ) partial_with_decorator = functools.partial ( decorator , a=2 ) self.assertequal ( argspec , tf_inspect.getfullargspec ( decorator ) ) self.assertequal ( signature , inspect.signature ( partial_with_decorator ) )
__label__0 def split ( self , ) - > tuple [ sequence [ union [ message.message , bytes ] ] , chunk_pb2.chunkedmessage ] : `` '' '' splits a proto message into a sequence of protos/bytes . '' '' '' if self._parent_splitter : raise valueerror ( `` a child composablesplitter 's ` split ` method should not be called `` `` directly , since it inherit chunks from a parent object . please call `` `` the parent 's ` split ( ) ` method instead . '' )
__label__0 def testdispatchwithkwargs ( self ) :
__label__0 if platform.system ( ) == 'linux ' and platform.machine ( ) == 'x86_64 ' : required_packages.append ( fake_required_packages )
__label__0 this class must provide the following fields :
__label__0 def add_log ( self , severity , lineno , col , msg ) : self._log.append ( ( severity , lineno , col , msg ) ) print ( `` % s line % d : % d : % s '' % ( severity , lineno , col , msg ) )
__label__0 returns : a bool , indicating if the check was successful or not. `` '' ''
__label__0 goodbye `` '' '' ) , ( 'two-outputs ' , [ ( ' a ' , ' a ' ) , ( ' b ' , ' b ' ) ] , `` '' '' hello
__label__0 hipruntime_config = { `` hipruntime_version_number '' : hipruntime_version_number ( rocm_install_path ) }
__label__0 sessions.append ( session ) graphs.append ( graph ) train_ops.append ( train_op )
__label__0 field , field_desc = util.get_field ( proto , [ ] ) self.assertis ( proto , field ) self.assertisnone ( field_desc )
__label__0 import argparse import importlib
__label__0 builder = tf_stack.graphdebuginfobuilder ( ) builder.accumulatestacktrace ( 'func1 ' , 'node1 ' , stack1 ) builder.accumulatestacktrace ( 'func2 ' , 'node2 ' , stack2 ) builder.accumulatestacktrace ( 'func3 ' , 'node3 ' , stack3 ) debug_info = builder.build ( )
__label__0 raises : typeerror : if ` func ` is not callable or if the structures do not match each other by depth tree . typeerror : if ` check_types ` is not ` false ` and the two structures differ in the type of sequence in any of their substructures . valueerror : if no structures are provided. `` '' '' def wrapper_func ( tuple_path , * inputs , * * kwargs ) : string_path = `` / '' .join ( str ( s ) for s in tuple_path ) return func ( string_path , * inputs , * * kwargs )
__label__0 def get_outputs ( self ) : return self.outfiles
__label__0 boolean_options = [ 'force ' ]
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # tf.train.featurelist featurelist = list [ feature ]
__label__0 the module flag has to be relative to the core package imported . for example , if ` module=keras.layers ` then , this function will return all the modules in the submodule .
__label__0 if partitioner : vs = [ variable_scope.get_variable ( var_name , shape=var_full_shape , initializer=rnd , partitioner=partitioner , use_resource=use_resource ) ] else : if use_resource : vs = [ resource_variable_ops.resourcevariable ( rnd , name=var_name ) ] else : vs = [ variable_v1.variablev1 ( rnd , name=var_name ) ]
__label__0 for d in devices : device_info = test_log_pb2.availabledeviceinfo ( ) device_info.name = d.name device_info.type = d.device_type device_info.memory_limit = d.memory_limit device_info.physical_description = d.physical_device_desc device_info_list.append ( device_info )
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 for location in external_header_locations : if location in install_dir : extra_dir = install_dir.replace ( location , external_header_locations [ location ] ) if not os.path.exists ( extra_dir ) : self.mkpath ( extra_dir ) self.copy_file ( header , extra_dir )
__label__0 return result
__label__0 def test_contrib_rnn_deprecation ( self ) : _ , report , _ , _ = self._upgrade ( `` tf.contrib.rnn '' ) self.assertin ( `` tf.contrib.rnn . * has been deprecated '' , report )
__label__0 returns : the new node `` '' '' def _get_distribution ( old_value ) : `` '' '' returns an ast matching the following : ( `` uniform '' if ( old_value ) else `` truncated_normal '' ) `` '' '' dist = pasta.parse ( `` \ '' uniform\ '' if old_value else \ '' truncated_normal\ '' '' ) ifexpr = dist.body [ 0 ] .value pasta.ast_utils.replace_child ( ifexpr , ifexpr.test , old_value )
__label__0 super ( ) .__init__ ( name )
__label__0 # set all attributes related to checkpointing and writing events to none . # afterwards , set them appropriately for chief supervisors , as these are # the only supervisors that can write checkpoints and events . self._logdir = none self._save_summaries_secs = none self._save_model_secs = none self._save_path = none self._summary_writer = none
__label__0 returns : a list of tuples : ( device_name , basesaverbuilder.saveableobject ) tuples . the list is sorted by ascending device_name .
__label__0 # partially providing keyword args should fail . with self.assertraisesregex ( valueerror , `` only accepts keyword arguments '' ) : self.assertequal ( 3 , func_with_decorator ( 1 , b=2 ) )
__label__0 def _getargspec ( target ) : `` '' '' a python3 version of getargspec .
__label__0 def main ( _ ) : `` '' '' run an interactive console . '' '' '' code.interact ( ) return 0
__label__0 def check_existence ( filename ) : `` '' '' check the existence of file or dir . '' '' '' if not os.path.exists ( filename ) : raise runtimeerror ( `` % s not found . '' % filename )
__label__0 any further depth in structure in ` input_tree ` is retained as structures in the partially flattened output .
__label__0 def test_contrib_summary_generic_nostep ( self ) : text = `` tf.contrib.summary.generic ( 'foo ' , myval ) '' expected = ( `` tf.compat.v2.summary.write ( tag='foo ' , data=myval , `` `` step=tf.compat.v1.train.get_or_create_global_step ( ) ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'name ' argument '' , errors [ 0 ] ) self.assertin ( `` 'step ' argument '' , errors [ 1 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 2 ] )
__label__0 returns : a list suitable for use as ` __all__ ` . `` '' '' if doc_string_modules is none : doc_string_modules = [ _sys.modules [ module_name ] ] cur_members = set ( name for name , _ in _tf_inspect.getmembers ( _sys.modules [ module_name ] ) )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for python module traversal . '' '' ''
__label__0 structures_have_mismatching_types = nest_util.structures_have_mismatching_types
__label__0 returns : an op to save the variables , which , when evaluated , returns the prefix `` < user-fed prefix > '' only and does not include the sharded spec suffix. `` '' '' # implementation details : most clients should skip . # # suffix for any well-formed `` checkpoint_prefix '' , when sharded . # transformations : # * users pass in `` save_path '' in save ( ) and restore ( ) . say `` myckpt '' . # * checkpoint_prefix gets fed < save_path > < _sharded_suffix > . # * if checkpoint_prefix is a s3 bucket path `` .part '' is appended to it # * otherwise _temp/part is appended which is normalized relative to the os # example : # during runtime , a temporary directory is first created , which contains # files # # < train dir > /myckpt_temp/ # part- ? ? ? ? ? -of- ? ? ? ? ? { .index , .data-00000-of-00001 } # # before .save ( ) finishes , they will be ( hopefully , atomically ) renamed to # # < train dir > / # myckpt { .index , .data- ? ? ? ? ? -of- ? ? ? ? ? } # # filesystems with eventual consistency ( such as s3 ) , do n't need a # temporary location . using a temporary directory in those cases might # cause situations where files are not available during copy . # # users only need to interact with the user-specified prefix , which is # `` < train dir > /myckpt '' in this case . save ( ) and restore ( ) work with the # prefix directly , instead of any physical pathname . ( on failure and # subsequent restore , an outdated and orphaned temporary directory can be # safely removed . ) with ops.device ( `` cpu '' ) : _sharded_suffix = array_ops.where ( string_ops.regex_full_match ( checkpoint_prefix , `` ^s3 : // . * '' ) , constant_op.constant ( `` .part '' ) , constant_op.constant ( os.path.normpath ( `` _temp/part '' ) ) ) tmp_checkpoint_prefix = string_ops.string_join ( [ checkpoint_prefix , _sharded_suffix ] )
__label__0 # suppress normal variable inits to make sure the local one is # initialized via local_init_op . sv = supervisor.supervisor ( logdir=logdir , init_op=none , is_chief=false ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) sv.stop ( )
__label__0 def get_nvcc_version ( path ) : pattern = r '' cuda compilation tools , release \d+\.\d+ , v ( \d+\.\d+\.\d+ ) '' for line in subprocess.check_output ( [ path , `` -- version '' ] ) .splitlines ( ) : match = re.match ( pattern , line.decode ( `` ascii '' ) ) if match : return match.group ( 1 ) return none
__label__0 config.test_format = lit.formats.shtest ( execute_external=true )
__label__0 args : job_name : the string name of a job in this cluster . task_index : a non-negative integer .
__label__0 export information is stored in the module where constants/string literals are defined .
__label__0 # check the chunkedmessage proto . self.assertlen ( chunked_message.chunked_fields , 2 ) self.assertequal ( 1 , chunked_message.chunked_fields [ 0 ] .message.chunk_index ) self.assertequal ( 2 , chunked_message.chunked_fields [ 1 ] .message.chunk_index ) self.assertempty ( chunked_message.chunked_fields [ 0 ] .field_tag ) self.assertempty ( chunked_message.chunked_fields [ 1 ] .field_tag )
__label__0 # we now have 2 'last_checkpoints ' : [ s2 , s3 ] , and s1 on disk . the next # call to save ( ) , will delete s2 , because max_to_keep is 2 , and because # we already kept the old s1 . s2 is very close in time to s1 so it gets # deleted . s4 = save.save ( sess , os.path.join ( save_dir , `` s4 '' ) ) self.assertequal ( [ s3 , s4 ] , save.last_checkpoints )
__label__0 raises : valueerror : if num_groups is less than 1. `` '' '' if num_groups < 1 : raise valueerror ( `` argument ` num_groups ` must be a positive integer. `` f '' received : num_groups= { num_groups } '' ) self._ready = threading.condition ( threading.lock ( ) ) self._num_groups = num_groups self._group_member_counts = [ 0 ] * self._num_groups
__label__0 raises : typeerror : if the global step tensor has a non-integer type , or if it is not a ` variable ` .
__label__0 def get_float_list ( self , min_length=_min_length , max_length=_max_length ) : `` '' '' consume a float list with given constraints .
__label__0 def __eq__ ( self , other ) : return self.mask == other.mask and math_ops.reduce_all ( self.value == other.value )
__label__0 # check __init__ signature matches for doc generation . self.assertequal ( tf_inspect.getfullargspec ( myclass.__init__ ) , tf_inspect.getfullargspec ( deprecated_cls.__init__ ) )
__label__0 args : deprecated_name : the name of the symbol that is being deprecated , to be used in the warning message . this should be its fully qualified name to avoid confusion . name : the name of the symbol that is to be used instead of the deprecated name . this should be a fully qualified name to avoid confusion . func_or_class : the ( non-deprecated ) class or function for which a deprecated alias should be created . warn_once : if true ( the default ) , only print a deprecation warning the first time this function is used , or the class is instantiated .
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 # check that the parameter exists , and has an appropriate kind . param = api_signature.parameters.get ( param_name , none ) if param is none : raise valueerror ( `` signature includes annotation for unknown `` f '' parameter { param_name ! r } . '' ) if param.kind not in ( tf_inspect.parameter.positional_only , tf_inspect.parameter.positional_or_keyword ) : raise valueerror ( `` dispatch currently only supports type annotations `` `` for positional parameters ; ca n't handle annotation `` f '' for { param.kind ! r } parameter { param_name } . '' )
__label__0 these types should not be exported . external code should not rely on these. `` '' ''
__label__0 def testattachesatfdecoratorattr ( self ) : decorated = tf_decorator.make_decorator ( test_function , test_wrapper ) decorator = getattr ( decorated , '_tf_decorator ' ) self.assertisinstance ( decorator , tf_decorator.tfdecorator )
__label__0 def testdelattr ( self ) : module = mockmodule ( 'test ' ) wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' ) setattr ( wrapped_module , 'foo ' , 1 ) self.assertequal ( wrapped_module.foo , 1 ) delattr ( wrapped_module , 'foo ' ) self.assertfalse ( hasattr ( wrapped_module , 'foo ' ) ) # try setting the attr again setattr ( wrapped_module , 'foo ' , 1 ) self.assertequal ( wrapped_module.foo , 1 ) delattr ( wrapped_module , 'foo ' ) self.assertfalse ( hasattr ( wrapped_module , 'foo ' ) )
__label__0 class metagraphtest ( test.testcase ) :
__label__0 def assertcheckpointstate ( self , model_checkpoint_path , all_model_checkpoint_paths , save_dir ) : checkpoint_state = checkpoint_management.get_checkpoint_state ( save_dir ) self.assertequal ( checkpoint_state.model_checkpoint_path , model_checkpoint_path ) self.assertequal ( checkpoint_state.all_model_checkpoint_paths , all_model_checkpoint_paths )
__label__0 # to grab the eigenvalues the diag operator just calls convert_to_tensor # ( twice ) in this case . trace = linear_operator_diag.linearoperatordiag ( x ) .eigvals ( ) self.assertequal ( str ( trace ) , `` convert_to_tensor ( convert_to_tensor ( x , dtype=none , dtype_hint=none , `` `` name=diag ) ) '' )
__label__0 returns : a nested structure of ` tf.typespec ` objects matching the structure of an element of this ` tf.distribute.distributediterator ` . this returned value is typically a ` tf.distribute.distributedvalues ` object and specifies the ` tf.tensorspec ` of individual components. `` '' '' raise notimplementederror ( `` distributediterator.element_spec ( ) must be implemented in descendants '' )
__label__0 locate benchmark files in the data directory , process them , and upload their data to the datastore . after processing each file , move it to the archive directory for safe-keeping . each file is locked for processing , which allows multiple uploader instances to run concurrently if needed , each one handling different benchmark files , skipping those already locked by another .
__label__0 # use only the old names , in order text = `` h ( a , kw1_alias=x , kw2_alias=y ) \n '' acceptable_outputs = [ `` h ( a , x , y ) \n '' , `` h ( a , kw1=x , kw2=y ) \n '' , `` h ( a=a , kw1=x , kw2=y ) \n '' , `` h ( a , kw2=y , kw1=x ) \n '' , `` h ( a=a , kw2=y , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removemultiplekeywordarguments ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utility functions for training . '' '' '' from tensorflow.python.eager import context from tensorflow.python.framework import dtypes from tensorflow.python.framework import graph_io from tensorflow.python.framework import ops from tensorflow.python.framework import tensor from tensorflow.python.ops import cond from tensorflow.python.ops import init_ops from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops import state_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util.tf_export import tf_export
__label__0 if _at_least_version ( cuda_version , `` 10.1 '' ) :
__label__0 def _addshardedrestoreops ( self , filename_tensor , per_device , restore_sequentially , reshape ) : `` '' '' add ops to restore variables from multiple devices .
__label__0 def get_compatible_func ( op , func ) : `` '' '' returns a compatible function .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_kwonlyargs ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def testupdatedocstringswithapilists ( self ) : dispatch.update_docstrings_with_api_lists ( ) self.assertregex ( dispatch.dispatch_for_api.__doc__ , r '' ( ? s ) the tensorflow apis that may be overridden `` r '' by ` @ dispatch_for_api ` are : \n\n . * '' r '' \ * ` tf\.concat\ ( values , axis , name\ ) ` \n . * '' r '' \ * ` tf\.math\.add\ ( x , y , name\ ) ` \n . * '' ) self.assertregex ( dispatch.dispatch_for_unary_elementwise_apis.__doc__ , r '' ( ? s ) the unary elementwise apis are : \n\n . * '' r '' \ * ` tf\.math\.abs\ ( x , name\ ) ` \n . * '' r '' \ * ` tf\.math\.cos\ ( x , name\ ) ` \n . * '' ) self.assertregex ( dispatch.dispatch_for_binary_elementwise_apis.__doc__ , r '' ( ? s ) the binary elementwise apis are : \n\n . * '' r '' \ * ` tf\.math\.add\ ( x , y , name\ ) ` \n . * '' r '' \ * ` tf\.math\.multiply\ ( x , y , name\ ) ` \n . * '' )
__label__0 # get functions def get_call_context_function ( ) : global _keras_call_context_function return _keras_call_context_function
__label__0 from tensorflow.python.framework import tensor_shape from tensorflow.python.platform import test from tensorflow.python.util import serialization
__label__0 2. inspect the ` tf.typespec ` of the data generated by ` distributeddataset ` .
__label__0 self.assertallequal ( self.evaluate ( w1 ) , 1.0 ) self.assertallequal ( self.evaluate ( w2 ) , 2.0 )
__label__0 example :
__label__0 class exporttype ( protocol ) :
__label__0 __getattr__ = deprecation.deprecate_moved_module ( __name__ , deprecated_module_new , `` 2.9 '' )
__label__1 def is_prime ( number ) : if number < = 1 : return false for i in range ( 2 , int ( number * * 0.5 ) + 1 ) : if number % i == 0 : return false return true
__label__0 return attr
__label__0 `` ` /child.md # method2 `` `
__label__0 elif not ( isinstance ( shallow_tree , _collections_abc.mapping ) and isinstance ( input_tree , _collections_abc.mapping ) ) : raise typeerror ( structures_have_mismatching_types.format ( input_type=type ( input_tree ) , shallow_type=type ( shallow_tree ) ) )
__label__0 def as_bytes ( bytes_or_text , encoding='utf-8 ' ) : `` '' '' converts ` bytearray ` , ` bytes ` , or unicode python input types to ` bytes ` .
__label__0 `` `` ''
__label__0 a ` tf.distribute.distributeddataset ` could be thought of as a `` distributed '' dataset . when you use ` tf.distribute ` api to scale training to multiple devices or machines , you also need to distribute the input data , which leads to a ` tf.distribute.distributeddataset ` instance , instead of a ` tf.data.dataset ` instance in the non-distributed case . in tf 2.x , ` tf.distribute.distributeddataset ` objects are python iterables .
__label__0 def testlargefeed ( self ) : server = self._cached_server with session.session ( server.target , config=self._userpcconfig ( ) ) as sess : feed_val = np.empty ( [ 10000 , 3000 ] , dtype=np.float32 ) feed_val.fill ( 0.5 ) p = array_ops.placeholder ( dtypes.float32 , shape= [ 10000 , 3000 ] ) min_t = math_ops.reduce_min ( p ) max_t = math_ops.reduce_max ( p ) min_val , max_val = sess.run ( [ min_t , max_t ] , feed_dict= { p : feed_val } ) self.assertequal ( 0.5 , min_val ) self.assertequal ( 0.5 , max_val )
__label__0 @ tf_export ( 'debugging.is_traceback_filtering_enabled ' ) def is_traceback_filtering_enabled ( ) : `` '' '' check whether traceback filtering is currently enabled .
__label__0 expected_str = ( `` clusterspec ( { 'ps ' : [ 'ps0:1111 ' ] , 'worker ' : [ 'worker0:3333 ' , `` `` 'worker1:4444 ' ] } ) '' ) self.assertequal ( expected_str , str ( cluster_spec ) )
__label__0 def __new__ ( cls , new_vocab , new_vocab_size , num_oov_buckets , old_vocab , old_vocab_size=-1 , backup_initializer=none , axis=0 ) : if axis ! = 0 and axis ! = 1 : raise valueerror ( `` the only supported values for the axis argument are 0 `` `` and 1. provided axis : { } '' .format ( axis ) )
__label__0 @ attr.s class sampleattr ( object ) : field1 = attr.ib ( ) field2 = attr.ib ( )
__label__0 # isolate patch and identifier string if identifier string exists . extension_split = extension.split ( `` - '' , 1 ) patch = extension_split [ 0 ] if len ( extension_split ) == 2 : identifier_string = `` - '' + extension_split [ 1 ] else : identifier_string = `` ''
__label__0 parts = [ warning , table_header ] xla_compiled_ops = get_gpu_kernel_names ( ) for op_name in sorted ( dir ( tf.raw_ops ) ) : try : ops._gradient_registry.lookup ( op_name ) # pylint : disable=protected-access has_gradient = `` \n { heavy check mark } \n { variation selector-16 } '' except lookuperror : has_gradient = `` \n { cross mark } '' is_xla_compilable = `` \n { cross mark } '' if op_name in xla_compiled_ops : is_xla_compilable = `` \n { heavy check mark } \n { variation selector-16 } ''
__label__0 use case :
__label__0 # value file names will be mapped to the keys link_map = { `` head '' : none , `` branch_ref '' : none }
__label__0 the following example shows how this decorator can be used to update all binary elementwise operations to handle a ` maskedtensor ` type :
__label__0 def testrecursivecreate ( self ) : test_dir = self._get_test_dir ( `` deep_dir '' ) variable_v1.variablev1 ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] , dtype=dtypes.float32 , name= '' v0 '' ) path = graph_io.write_graph ( ops_lib.get_default_graph ( ) .as_graph_def ( ) , os.path.join ( test_dir , `` l1 '' , `` l2 '' , `` l3 '' ) , `` graph.pbtxt '' ) truth = os.path.join ( test_dir , `` l1 '' , `` l2 '' , `` l3 '' , `` graph.pbtxt '' ) self.assertequal ( path , truth ) self.asserttrue ( os.path.exists ( path ) )
__label__0 def _header_paths ( ) : `` '' '' returns hard-coded set of relative paths to look for header files . '' '' '' return [ `` '' , `` include '' , `` include/cuda '' , `` include/ * -linux-gnu '' , `` extras/cupti/include '' , `` include/cuda/cupti '' , `` local/cuda/extras/cupti/include '' , `` targets/x86_64-linux/include '' , ]
__label__0 this proto implements the ` list [ bytes ] ` portion .
__label__0 returns : sum of args. `` '' '' return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 from tensorflow.python.checkpoint import checkpoint_management from tensorflow.python.client import session from tensorflow.python.distribute import distribute_lib from tensorflow.python.framework import errors from tensorflow.python.framework import ops from tensorflow.python.platform import tf_logging as logging from tensorflow.python.training import saver as saver_lib from tensorflow.python.util.tf_export import tf_export
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.module_deprecations.update ( { `` a.b '' : ( ast_edits.error , `` a.b is evil . '' ) } )
__label__0 do_not_document = [ `` tf.__internal__ '' , `` tf.keras.__internal__ '' , `` tf.keras.wrappers '' , `` tf.__operators__ '' , `` tf.tools '' , `` tf.compat.v1.pywrap_tensorflow '' , `` tf.pywrap_tensorflow '' , `` tf.flags '' , `` tf.batch_mat_mul_v3 '' , `` tf.sparse_segment_sum_grad '' ] for path in do_not_document : item = tf for part in path.split ( `` . `` ) [ 1 : ] : item = getattr ( item , part , none ) if item is none : continue doc_controls.do_not_generate_docs ( item )
__label__0 note : this is implemented by adding a hidden attribute on the object , so it can not be used on objects which do not allow new attributes to be added . so this decorator must go * below * ` @ property ` , ` @ classmethod ` , or ` @ staticmethod ` :
__label__0 visitor = public_api.publicapivisitor ( conversion_visitor ) visitor.do_not_descend_map [ `` tf '' ] .append ( `` contrib '' ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v1 , visitor ) collect = false traverse.traverse ( tf.compat.v1 , visitor )
__label__0 if ` date ` is none , 'after < date > ' is replaced with 'in a future version ' . < function > will include the class name if it is a method .
__label__0 text = `` optimizer.compute_gradients ( a , colocate_gradients_with_ops=false ) \n '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` optimizer.compute_gradients ( a ) \n '' , new_text ) self.assertin ( `` optimizer.compute_gradients no longer takes '' , report )
__label__0 def __iter__ ( self ) : `` '' '' creates an iterator for the ` tf.distribute.distributeddataset ` .
__label__0 # starts worker 1. thread_1.start ( ) thread_1.join ( ) thread_0.join ( )
__label__0 class maxtokeeptest ( test.testcase ) :
__label__0 def __init__ ( self , two_attr=2 ) : self.two_attr = two_attr
__label__0 # if more than max_to_keep , remove oldest . if len ( self._last_checkpoints ) > self.saver_def.max_to_keep : self._checkpoints_to_be_deleted.append ( self._last_checkpoints.pop ( 0 ) )
__label__0 def __init__ ( self , import_rename=false , upgrade_compat_v1_import=false ) : self.upgrade_compat_v1_import = upgrade_compat_v1_import
__label__0 6 . ` tf.raggedtensor ` : this is a composite tensor thats representation consists of a flattened list of 'values ' and a list of 'row_splits ' which indicate how to chop up the flattened list into different rows . for more details on ` tf.raggedtensor ` , please visit https : //www.tensorflow.org/api_docs/python/tf/raggedtensor .
__label__0 canonical = none for api_name in all_exports : try : canonical = tf_export.get_canonical_name_for_symbol ( self._index [ name ] , api_name=api_name ) except attributeerror : canonical = none if canonical is not none : break
__label__0 def testdispatchformultiplesignatures ( self ) :
__label__0 def testunsortedsegmentops1dindices1ddatanegativeindices ( self ) : `` '' '' tests for min , max , and prod ops .
__label__0 def _score_name ( self , path : doc_generator_visitor.apipath ) - > tfnamescore : name = `` . `` .join ( path ) all_exports = [ tf_export.tensorflow_api_name , tf_export.keras_api_name , ]
__label__0 ev = next ( rr ) self.asserttrue ( ev.meta_graph_def )
__label__0 class pickletest ( test.testcase ) :
__label__0 def _tfmw_import_module ( self , name ) : `` '' '' lazily loading the modules . '' '' '' # we ignore 'app ' because it is accessed in __init__.py of tf.compat.v1 . # that way , if a user only imports tensorflow.compat.v1 , it is not # considered v1 api usage . if ( self._tfmw_is_compat_v1 and name ! = 'app ' and not tfmodulewrapper.compat_v1_usage_recorded ) : tfmodulewrapper.compat_v1_usage_recorded = true compat_v1_usage_gauge.get_cell ( ) .set ( true )
__label__0 def test_contrib_framework_argsort ( self ) : text = `` tf.contrib.framework.argsort '' expected = `` tf.argsort '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 e.g . `` ` python foo = 1 bar = 2 tf_export ( `` consts.foo '' ) .export_constant ( __name__ , 'foo ' ) tf_export ( `` consts.bar '' ) .export_constant ( __name__ , 'bar ' ) `` `
__label__0 from tensorflow.core.framework import graph_pb2 from tensorflow.core.protobuf import config_pb2 from tensorflow.core.protobuf import meta_graph_pb2 from tensorflow.core.util import event_pb2 from tensorflow.python.checkpoint import checkpoint_management from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import meta_graph from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import io_ops from tensorflow.python.ops import parsing_ops from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import gfile from tensorflow.python.platform import test from tensorflow.python.summary import summary from tensorflow.python.summary import summary_iterator from tensorflow.python.summary.writer import writer from tensorflow.python.training import input as input_lib from tensorflow.python.training import saver as saver_lib from tensorflow.python.training import server_lib from tensorflow.python.training import session_manager as session_manager_lib from tensorflow.python.training import supervisor
__label__0 def setup_gpu ( required_gpus ) : `` '' '' sets up the gpu devices .
__label__0 note that this process modifies decorator_func .
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] nest1 : an atom or a nested structure . nest2 : an atom or a nested structure . check_types : - for modality.core : if ` true ` ( default ) types of structures are checked as well , including the keys of dictionaries . if set to ` false ` , for example a list and a tuple of objects will look the same if they have the same size . note that namedtuples with identical name and fields are always considered to have the same shallow structure . two types will also be considered the same if they are both list subtypes ( which allows `` list '' and `` _listwrapper '' from trackable dependency tracking to compare equal ) . ` check_types=true ` only checks type of sub-structures . the types of atoms are not checked . - for modality.data : if ` true ` ( default ) types of sequences should be same as well . for dictionary , `` type '' of dictionary is considered to include its keys . in other words , two dictionaries with different keys are considered to have a different `` type '' . if set to ` false ` , two iterables are considered same as long as they yield the elements that have same structures . expand_composites : arg only valid for modality.core . if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 import pickle import types
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ np.zeros ( [ 4 , 1 ] ) ] } , sess )
__label__0 self.assertequal ( _get_write_histogram_proto ( ) .num , num_writes_start + 1 ) self.assertequal ( _get_read_histogram_proto ( ) .num , num_reads_start + 1 ) # check that training time saved has not increased . self.assertequal ( metrics.gettrainingtimesaved ( api_label=api_label ) , time_after_one_save ) save.save ( sess , save_path )
__label__0 class installcommand ( installcommandbase ) : `` '' '' override the dir where the headers go . '' '' ''
__label__0 # positional * args passed in that we can not inspect , should warn text = `` f ( a , b , * args ) \n '' ( _ , report , _ ) , _ = self._upgrade ( reorderkeywordspec ( ) , text ) self.assertin ( `` manual check required '' , report )
__label__0 def test_contrib_summary_histogram_nostep ( self ) : text = `` tf.contrib.summary.histogram ( 'foo ' , myval ) '' expected = ( `` tf.compat.v2.summary.histogram ( name='foo ' , data=myval , `` `` step=tf.compat.v1.train.get_or_create_global_step ( ) ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'step ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 the following example shows how this decorator can be used to update all binary elementwise assert operations to handle a ` maskedtensor ` type :
__label__0 def isanytargetmethod ( object ) : # pylint : disable=redefined-builtin # pylint : disable=g-doc-args , g-doc-return-or-yield `` '' '' checks if ` object ` or a tf decorator wrapped target contains self or cls .
__label__0 def test_module ( self ) : visitor = testvisitor ( ) traverse.traverse ( test_module1 , visitor )
__label__0 def test_eager_add ( self ) :
__label__0 # generally such lookups should be done using ` threading.local ( ) ` . see # https : //blogs.gnome.org/jamesh/2008/06/11/tls-python/ for a detailed # explanation of why . however the transform stacks are expected to be empty # when a thread is joined , so reusing the key does not introduce a correctness # issue . moreover , get_ident is faster than storing and retrieving a unique # key in a thread local store . _get_thread_key = threading.get_ident
__label__0 print ( x_ref1 == y ) == > false `` ` `` '' ''
__label__0 globs for the checkpoints pointed to by ` checkpoint_paths ` . if the files exist , use their mtime as the checkpoint timestamp .
__label__0 @ tf_export ( v1= [ 'train.get_or_create_global_step ' ] ) def get_or_create_global_step ( graph=none ) : `` '' '' returns and create ( if necessary ) the global step tensor .
__label__0 parser = argparse.argumentparser ( description= '' cherry picking automation . '' )
__label__0 goodbye `` '' '' ) , ( 'two ' , [ ( ' a ' , none ) , ( ' b ' , none ) ] , `` '' '' hello
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import module_wrapper from tensorflow.python.util import tf_inspect from tensorflow.tools.compatibility import all_renames_v2
__label__0 this method is a replacement for __getattribute__ ( ) . it will be added into the extended python module as a callback to reduce api overhead. `` '' '' # avoid infinite recursions func__fastdict_insert = object.__getattribute__ ( self , '_fastdict_insert ' )
__label__0 args : value : a value belonging to this tracetype
__label__0 * these atom vs. atom comparisons will pass :
__label__0 import_header = `` from tensorflow import * \n '' text = import_header + old_symbol expected_text = `` from tensorflow.compat.v2 import * \n '' + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 def create_local_config_python ( dst_dir : str ) - > none : `` '' '' copy python and numpy header files to the destination directory . '' '' '' shutil.copytree ( `` external/pypi_numpy/site-packages/numpy/core/include '' , os.path.join ( dst_dir , `` numpy_include '' ) , ) if is_windows ( ) : path = `` external/python_ * /include '' else : path = `` external/python_ * /include/python * '' shutil.copytree ( glob.glob ( path ) [ 0 ] , os.path.join ( dst_dir , `` python_include '' ) )
__label__0 in the case of dict instances , the sequence consists of the values , sorted by key to ensure deterministic behavior . this is true also for ordereddict instances : their sequence order is ignored , the sorting order of keys is used instead . the same convention is followed in ` nest.pack_sequence_as ` . this correctly repacks dicts and ordereddicts after they have been flattened , and also allows flattening an ordereddict and then repacking it back using a corresponding plain dict , or vice-versa . dictionaries with non-sortable keys can not be flattened .
__label__0 returns : list of all api constants under the given module. `` '' '' constants_v2 = [ ] tensorflow_constants_attr = api_attrs [ tensorflow_api_name ] .constants
__label__0 def testoneinput ( data ) : `` '' '' test randomized fuzzing input for tf.raw_ops.abs . '' '' '' fh = fuzzinghelper ( data )
__label__0 dtype = fh.get_tf_dtype ( ) shape = fh.get_int_list ( ) try : with open ( _default_filename , ' w ' ) as f : f.write ( fh.get_string ( ) ) _ = tf.raw_ops.immutableconst ( dtype=dtype , shape=shape , memory_region_name=_default_filename ) except ( tf.errors.invalidargumenterror , tf.errors.internalerror , unicodeencodeerror , unicodedecodeerror ) : pass
__label__0 args : op : an op , either _ready_op or _ready_for_local_init_op , which defines the readiness of the model . sess : a ` session ` . msg : a message to log to warning if not ready
__label__0 def testgetmodule ( self ) : self.assertequal ( inspect.getmodule ( testdecoratedclass ) , tf_inspect.getmodule ( testdecoratedclass ) ) self.assertequal ( inspect.getmodule ( test_decorated_function ) , tf_inspect.getmodule ( test_decorated_function ) ) self.assertequal ( inspect.getmodule ( test_undecorated_function ) , tf_inspect.getmodule ( test_undecorated_function ) )
__label__0 returns : a coordinator object. `` '' '' return self._coord
__label__0 returns : new graphdef with transforms applied. `` '' ''
__label__0 if there 're more available gpus than needed , it hides the additional ones . if there 're less , it creates logical devices . this is to make sure the tests see a fixed number of gpus regardless of the environment .
__label__0 self.assertequal ( { ' a ' : 5 , ' b ' : 2 } , tf_inspect.getcallargs ( func , 5 ) )
__label__0 def __init__ ( self , server_or_cluster_def , job_name=none , task_index=none , protocol=none , config=none , start=true ) : `` '' '' creates a new server with the given definition .
__label__0 > > > global_batch_size = 16 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensors ( ( [ 1 . ] , [ 2 ] ) ) .repeat ( 100 ) .batch ( global_batch_size ) > > > dist_dataset = strategy.experimental_distribute_dataset ( dataset ) > > > dist_dataset.element_spec ( perreplicaspec ( tensorspec ( shape= ( none , 1 ) , dtype=tf.float32 , name=none ) , tensorspec ( shape= ( none , 1 ) , dtype=tf.float32 , name=none ) ) , perreplicaspec ( tensorspec ( shape= ( none , 1 ) , dtype=tf.int32 , name=none ) , tensorspec ( shape= ( none , 1 ) , dtype=tf.int32 , name=none ) ) )
__label__0 returns false for ( pd.dataframe , none ) , and other pairs which should not be considered equivalent .
__label__0 @ parameterized.named_parameters ( * examples ) def test_parser ( self , expected_example_tuples : exampletuples , string : str ) : self._do_test ( expected_example_tuples , string )
__label__0 with context.eager_mode ( ) : v = _countingsaveable ( `` foo '' ) saver = saver_module.saver ( var_list= [ v ] ) test_dir = self.get_temp_dir ( ) prefix = os.path.join ( test_dir , `` ckpt '' ) with self.cached_session ( ) as sess : save_path = saver.save ( sess , prefix ) self.assertequal ( 1 , v.eval_count ) saver.restore ( sess , save_path ) self.assertequal ( 1 , v.eval_count )
__label__0 # now try a restore with the sharded filename . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : v0 = variable_v1.variablev1 ( 111 , name= '' v0 '' ) t0 = saver_test_utils.checkpointedop ( name= '' t0 '' ) with sess.graph.device ( `` /cpu:1 '' ) : v1 = variable_v1.variablev1 ( 222 , name= '' v1 '' ) t1 = saver_test_utils.checkpointedop ( name= '' t1 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` t0 '' : t0.saveable , `` t1 '' : t1.saveable } , write_version=self._write_version , sharded=true ) self.evaluate ( variables.global_variables_initializer ( ) ) t0.insert ( `` k11 '' , 33.0 ) .run ( ) t1.insert ( `` k22 '' , 44.0 ) .run ( ) self.assertequal ( 111 , self.evaluate ( v0 ) ) self.assertequal ( 222 , self.evaluate ( v1 ) ) self.assertequal ( b '' k11 '' , self.evaluate ( t0.keys ( ) ) ) self.assertequal ( 33.0 , self.evaluate ( t0.values ( ) ) ) self.assertequal ( b '' k22 '' , self.evaluate ( t1.keys ( ) ) ) self.assertequal ( 44.0 , self.evaluate ( t1.values ( ) ) ) save_path = os.path.join ( self.get_temp_dir ( ) , `` sharded_basics '' ) if save._write_version is saver_pb2.saverdef.v1 : save.restore ( sess , save_path + `` - ? ? ? ? ? -of- ? ? ? ? ? '' ) else : save.restore ( sess , save_path ) self.assertequal ( 10 , self.evaluate ( v0 ) ) self.assertequal ( 20 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( t0.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( t0.values ( ) ) ) self.assertequal ( b '' k2 '' , self.evaluate ( t1.keys ( ) ) ) self.assertequal ( 40.0 , self.evaluate ( t1.values ( ) ) )
__label__0 learning_rate = lambda : 2.0 decay = lambda : 0.9 momentum = lambda : 0.0 epsilon = lambda : 1.0 opt = rmsprop.rmspropoptimizer ( learning_rate , decay , momentum , epsilon )
__label__0 args : * args : arguments for compute_gradients ( ) . * * kwargs : keyword arguments for compute_gradients ( ) .
__label__0 def _get_object_checkpoint_renames ( path , variable_names ) : `` '' '' returns a dictionary mapping variable names to checkpoint keys .
__label__0 with self.assertraisesregex ( valueerror , `` . * does not support dispatch . `` ) :
__label__0 `` ` b `` `
__label__0 def __eq__ ( self , other ) : return self._cluster_spec == other
__label__0 # adding s2 again ( old s2 is removed first , then new s2 appended ) s2 = save.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s3 , s2 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s1 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s3 , s2 ] , save_dir=save_dir )
__label__0 # todo ( amitpatankar ) : clean up denylist . # list of dependencies that should not included in the pip package . dependency_denylist = [ `` //tensorflow/cc/saved_model : saved_model_test_files '' , `` //tensorflow/cc/saved_model : saved_model_half_plus_two '' , `` //tensorflow : no_tensorflow_py_deps '' , `` //tensorflow/tools/pip_package : win_pip_package_marker '' , `` //tensorflow/core : image_testdata '' , `` //tensorflow/core/kernels/cloud : bigquery_reader_ops '' , `` //tensorflow/python : extra_py_tests_deps '' , `` //tensorflow/python : mixed_precision '' , `` //tensorflow/python : tf_optimizer '' , `` //tensorflow/python/framework : test_file_system.so '' , `` //tensorflow/python/debug : grpc_tensorflow_server.par '' , `` //tensorflow/python/feature_column : vocabulary_testdata '' , `` //tensorflow/python/util : nest_test_main_lib '' , # lite `` //tensorflow/lite/experimental/examples/lstm : rnn_cell '' , `` //tensorflow/lite/experimental/examples/lstm : rnn_cell.py '' , `` //tensorflow/lite/experimental/examples/lstm : unidirectional_sequence_lstm_test '' , # pylint : disable=line-too-long `` //tensorflow/lite/experimental/examples/lstm : unidirectional_sequence_lstm_test.py '' , # pylint : disable=line-too-long `` //tensorflow/lite/python : interpreter '' , `` //tensorflow/lite/python : interpreter_test '' , `` //tensorflow/lite/python : interpreter.py '' , `` //tensorflow/lite/python : interpreter_test.py '' , ]
__label__0 def testgetargspeconpartialwithvarargs ( self ) : `` '' '' tests getargspec on partial function with variable arguments . '' '' ''
__label__0 class globalstepreadtest ( test.testcase ) :
__label__0 def _resource_apply_sparse ( self , grad , var , indices ) : rms = self.get_slot ( var , `` rms '' ) mom = self.get_slot ( var , `` momentum '' ) if self._centered : mg = self.get_slot ( var , `` mg '' ) return gen_training_ops.resource_sparse_apply_centered_rms_prop ( var.handle , mg.handle , rms.handle , mom.handle , math_ops.cast ( self._learning_rate_tensor , grad.dtype ) , math_ops.cast ( self._decay_tensor , grad.dtype ) , math_ops.cast ( self._momentum_tensor , grad.dtype ) , math_ops.cast ( self._epsilon_tensor , grad.dtype ) , grad , indices , use_locking=self._use_locking ) else : return gen_training_ops.resource_sparse_apply_rms_prop ( var.handle , rms.handle , mom.handle , math_ops.cast ( self._learning_rate_tensor , grad.dtype ) , math_ops.cast ( self._decay_tensor , grad.dtype ) , math_ops.cast ( self._momentum_tensor , grad.dtype ) , math_ops.cast ( self._epsilon_tensor , grad.dtype ) , grad , indices , use_locking=self._use_locking )
__label__0 the `` testresults '' object contains test metadata and multiple benchmark entries . the datastore schema splits this information into two kinds ( like tables ) , one holding the test metadata in a single `` test '' entity ( like rows ) , and one holding each related benchmark entry in a separate `` entry '' entity . datastore create a unique id ( retrieval key ) for each entity , and this id is always returned along with the data when an entity is fetched .
__label__0 def prepare_headers ( headers : list [ str ] , srcs_dir : str ) - > none : `` '' '' copy and rearrange header files in the target directory .
__label__0 def __init__ ( self ) : pass
__label__0 self.assertequal ( ( 1 * 2 + 1 ) * * 2 , test_rewrappable_decorated ( 1 ) ) prev_target , _ = tf_decorator.unwrap ( test_rewrappable_decorated ) tf_decorator.rewrap ( test_rewrappable_decorated , prev_target , new_target ) self.assertequal ( ( 1 * 3 + 1 ) * * 2 , test_rewrappable_decorated ( 1 ) )
__label__0 the ` job_name ` , ` task_index ` , and ` protocol ` arguments are optional , and override any information provided in ` server_or_cluster_def ` .
__label__0 returns : a summarywriter. `` '' '' return self._summary_writer
__label__0 @ property def decorator_doc ( self ) : return self._decorator_doc
__label__0 # __file__ is the path to this file docs_tools_dir = pathlib.path ( __file__ ) .resolve ( ) .parent tensorflow_root = docs_tools_dir.parents [ 2 ]
__label__0 `` ` class parent : @ do_not_doc_in_subclasses def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 @ test_decorator ( 'decorator ' ) def test_decorated_function_with_defaults ( a , b=2 , c='hello ' ) : `` '' '' test decorated function with defaults docstring . '' '' '' return [ a , b , c ]
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` deprecated function argument values '' `` \n '' `` \ndeprecated : some argument values are deprecated : ` ( deprecated=true ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 def _totype ( self , dtype ) : if dtype == np.float16 : return dtypes.float16 elif dtype == np.float32 : return dtypes.float32 elif dtype == np.float64 : return dtypes.float64 elif dtype == np.int32 : return dtypes.int32 elif dtype == np.int64 : return dtypes.int64 else : assert false , ( dtype )
__label__0 doc = _add_deprecated_arg_value_notice_to_docstring ( func.__doc__ , date , instructions , deprecated_kwargs ) return tf_decorator.make_decorator ( func , new_func , 'deprecated ' , doc )
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import fast_module_type fastmoduletype = fast_module_type.get_fast_module_type_class ( )
__label__0 result = { }
__label__0 args : client : datastore client connection data : json-encoded benchmark data `` '' '' test_result = json.loads ( data )
__label__0 header_path , header_version = _find_header ( base_paths , `` cufft.h '' , required_version , get_header_version ) cufft_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 # list of subpackage names used by tensorflow components . have to check that # tensorflow core repo does not export any symbols under these names . subpackage_namespaces = [ ]
__label__0 finally : dispatch.unregister_dispatch_for ( handler )
__label__0 def testcontribl2expr ( self ) : text = `` tf.contrib.layers.l2_regularizer ( 1 - func ( 3 + 4 . ) , scope=\ '' foo\ '' ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l2 ( 0.5 * ( 1 - func ( 3 + 4 . ) ) ) \n '' , )
__label__0 class reorderkeywordspec ( ast_edits.noupdatespec ) : `` '' '' a specification where kw2 gets moved in front of kw1 .
__label__0 # todo ( aselle ) : explicitly not testing command line interface and process_tree # for now , since this is a one off utility .
__label__0 def testwarmstart_sparsecolumnvocabularyconstrainedvocabsizes ( self ) : # create old vocabulary , and use a size smaller than the total number of # entries . old_vocab_path = self._write_vocab ( [ `` apple '' , `` guava '' , `` banana '' ] , `` old_vocab '' ) old_vocab_size = 2 # [ 'apple ' , 'guava ' ]
__label__0 raises : valueerror : if required args are not provided. `` '' '' if not ( current_vocab_path and current_vocab_size and prev_ckpt and prev_vocab_path ) : raise valueerror ( `` invalid args : must provide all of [ current_vocab_path , `` `` current_vocab_size , prev_ckpt , prev_vocab_path } . '' ) if checkpoint_utils._is_variable ( var ) : var = [ var ] elif ( isinstance ( var , list ) and all ( checkpoint_utils._is_variable ( v ) for v in var ) ) : var = var elif isinstance ( var , variables_lib.partitionedvariable ) : var = var._get_variable_list ( ) else : raise typeerror ( `` var must be one of the following : a variable , list of variable or `` `` partitionedvariable , but is { } '' .format ( type ( var ) ) )
__label__0 def _get_composite_version_number ( major , minor , patch ) : return 10000 * major + 100 * minor + patch
__label__0 for save_path in paths : # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 20.0 , name= '' v1 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 } , restore_sequentially=true ) init_all_op = variables.global_variables_initializer ( )
__label__0 returns : a list of threads that are running the standard services . you can use the supervisor 's coordinator to join these threads with : sv.coord.join ( < list of threads > )
__label__0 dtype = fh.get_tf_dtype ( ) # max shape can be 8 in length and randomized from 0-8 without running into # a oom error . shape = fh.get_int_list ( min_length=0 , max_length=8 , min_int=0 , max_int=8 ) seed = fh.get_int ( ) try : x = tf.random.uniform ( shape=shape , dtype=dtype , seed=seed ) src_format_digits = str ( fh.get_int ( min_int=0 , max_int=999999999 ) ) dest_format_digits = str ( fh.get_int ( min_int=0 , max_int=999999999 ) ) _ = tf.raw_ops.dataformatvecpermute ( x , src_format=src_format_digits , dst_format=dest_format_digits , name=fh.get_string ( ) ) except ( tf.errors.invalidargumenterror , valueerror , typeerror ) : pass
__label__0 args : date : string or none . the date the function is scheduled to be removed . must be iso 8601 ( yyyy-mm-dd ) , or none . instructions : string . instructions on how to update code using the deprecated function . warn_once : boolean . set to ` true ` to warn only the first time the decorated function is called . otherwise , every call will log a warning .
__label__0 # todo ( drpng ) : remove this after legacy uses are resolved . write_graph = graph_io.write_graph
__label__0 # check if we have a keep_prob keyword arg for keep_prob in node.keywords : if keep_prob.arg == `` keep_prob '' : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing keep_prob arg of tf.nn.dropout to rate\n '' ) ) keep_prob.arg = `` rate '' _replace_keep_prob_node ( keep_prob , keep_prob.value ) return node
__label__0 # assert function docs are properly updated . self.assertequal ( `` fn doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : \n % s '' % ( date , instructions ) , getattr ( _object , `` _fn '' ) .__doc__ )
__label__0 @ test_util.run_v1_only ( `` sparseapplyftrlmultiplylinearbylr op returns a ref , `` `` so it is not supported in eager mode . '' ) def testsparseapplyftrlmultiplylinearbylrdim1 ( self ) : for ( dtype , index_type ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ np.int32 , np.int64 ] ) : x_val = [ [ 0.0 ] , [ 0.0 ] , [ 0.0 ] ] y_val = [ [ 4.0 ] , [ 5.0 ] , [ 6.0 ] ] z_val = [ [ 0.0 ] , [ 0.0 ] , [ 0.0 ] ] x = np.array ( x_val ) .astype ( dtype ) y = np.array ( y_val ) .astype ( dtype ) z = np.array ( z_val ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) grad_val = [ [ 1.5 ] , [ 2.5 ] ] grad = np.array ( grad_val ) .astype ( dtype ) indices = np.array ( [ 0 , 2 ] ) .astype ( index_type ) self._testtypesforsparseftrlmultiplylinearbylr ( x , y , z , lr , grad , indices )
__label__0 def check_cuda_lib ( path , check_soname=true ) : `` '' '' tests if a library exists on disk and whether its soname matches the filename .
__label__0 # create new vocab for sparse column `` sc_vocab '' . current_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` current_vocab '' ) # create feature column . only use 2 of the actual entries , resulting in # [ 'apple ' , 'banana ' ] for the new vocabulary . sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=current_vocab_path , vocabulary_size=2 )
__label__0 return `` \n '' .join ( [ content , raw_ops_doc ] )
__label__0 `` `` '' expands cmake variables in a text file . '' '' ''
__label__0 def testsingletensorevaluation ( self ) :
__label__0 self.assertequal ( 3 , double_wrapped_fn ( 3 ) ) # pylint : disable=no-value-for-parameter self.assertequal ( 3 , double_wrapped_fn ( a=3 ) ) # pylint : disable=no-value-for-parameter
__label__0 def testsetstfdecoratordoctodecoratordocarg ( self ) : decorated = tf_decorator.make_decorator ( test_function , test_wrapper , decorator_doc='test decorator doc ' ) decorator = getattr ( decorated , '_tf_decorator ' ) self.assertequal ( 'test decorator doc ' , decorator.decorator_doc )
__label__0 def _contrib_layers_variance_scaling_initializer_transformer ( parent , node , full_name , name , logs ) : `` '' '' updates references to contrib.layers.variance_scaling_initializer .
__label__0 @ property @ abc.abstractmethod def inference_fn ( self ) - > atomicfunction : `` '' '' returns the original ` atomicfunction ` owned by this concretefunction . '' '' ''
__label__0 source_ast = ast.parse ( source )
__label__0 `` ` /parent.md # method1 # method2 /child1.md # method2 /child2.md # method2 `` `
__label__0 @ staticmethod def _add_collection_def ( meta_graph_def , key , export_scope=none ) : `` '' '' adds a collection to metagraphdef protocol buffer .
__label__0 the variables can be provided explicitly through vars_to_warm_start , or they are retrieved from collections ( see below ) .
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # lint.ifchange `` '' '' tensorflow root package '' '' ''
__label__0 `` ` python # optimizers can create a slot for each variable to track accumulators accumulators = { var : create_zeros_slot ( var , `` momentum '' ) for var in vs } for var in vs : apply_momentum ( var , accumulators [ var ] , lr , grad , momentum_tensor )
__label__0 parser = argparse.argumentparser ( description= '' '' '' build info injection into the pip package . '' '' '' )
__label__0 this class must provide the following fields :
__label__0 yields : the iterable elements in a deterministic order. `` '' '' # pylint : disable=protected-access if isinstance ( iterable , _collections_abc.mapping ) : # iterate through dictionaries in a deterministic order by sorting the # keys . notice this means that we ignore the original order of ` ordereddict ` # instances . this is intentional , to avoid potential bugs caused by mixing # ordered and plain dicts ( e.g. , flattening a dict but using a # corresponding ` ordereddict ` to pack it back ) . for key in _tf_data_sorted ( iterable ) : yield iterable [ key ] # to avoid circular imports . sparse_tensor # depends on tensorflow/python/util/nest.py transitively , and if we try to # import sparse_tensor again , it results in a circular import . instead , here # we check the class name instead of using ` isinstance ` . elif iterable.__class__.__name__ == `` sparsetensorvalue '' : yield iterable elif _is_attrs ( iterable ) : for _ , attr in _get_attrs_items ( iterable ) : yield attr elif isinstance ( iterable , customnestprotocol ) : flat_component = iterable.__tf_flatten__ ( ) [ 1 ] assert isinstance ( flat_component , tuple ) yield from flat_component else : for value in iterable : yield value
__label__0 non-conformant ` featurelists ` ( mismatched types ) :
__label__0 # configure a server using the default local server options . server = server_lib.server.create_local_server ( config=config , start=false ) self.assertequal ( 0.1 , server.server_def.default_session_config.gpu_options . per_process_gpu_memory_fraction )
__label__0 def testcreateslotfromfirstmdimensionvariable ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.test_session ( ) : s = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) p_v = variable_scope.get_variable ( `` var '' , shape= [ 2 , 2 ] , partitioner=partitioned_variables.fixed_size_partitioner ( 2 ) ) for i , v in enumerate ( p_v ) : slot = slot_creator.create_slot ( v , initialized_value ( s ) , name= '' slot '' ) si = slot._save_slice_info
__label__0 def testrandommultinomialtorandomcategorical ( self ) : text = ( `` tf.random.multinomial ( logits , samples , seed , name , output_dtype ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) expected_text = ( `` tf.random.categorical ( logits , samples , seed=seed , name=name , `` `` dtype=output_dtype ) \n '' ) self.assertequal ( new_text , expected_text )
__label__0 def testresourcevariablereadopsaddeddeterministically ( self ) : graph_defs = [ ] num_graphs = 10 for _ in range ( num_graphs ) : with ops_lib.graph ( ) .as_default ( ) as g : for i in range ( 20 ) : resource_variable_ops.resourcevariable ( i , name= '' var % s '' % i ) saver_module.saver ( ) graph_defs.append ( g.as_graph_def ( ) ) for i in range ( num_graphs - 1 ) : self.assertequal ( graph_defs [ i ] , graph_defs [ i + 1 ] )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( arg0 , arg1 , * * deprecated ) : return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 `` `` ''
__label__0 self.local_step_init_op = state_ops.assign ( self._local_step , global_step ) chief_init_ops = [ self.local_step_init_op ] self.ready_for_local_init_op = variables.report_uninitialized_variables ( variables.global_variables ( ) )
__label__0 graph_def = self._make_graph_def_with_constant_nodes ( sizes ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , _ = s.split ( ) self.assertlen ( chunks , 1 ) self.assertprotoequals ( graph_def , chunks [ 0 ] )
__label__0 `` ` /parent.md # method1 # method2 /child1.md # method1 # method2 /child2.md # method2 /child11.md # method1 # method2 `` `
__label__0 def test_regular ( self ) : expected = ( `` brief ( suffix ) \n\nwarning : go away\ninstructions\n\ndocstring\n\n '' `` args : \n arg1 : desc '' ) # no indent for main docstring self._check ( `` brief\n\ndocstring\n\nargs : \n arg1 : desc '' , expected ) # 2 space indent for main docstring , blank lines not indented self._check ( `` brief\n\n docstring\n\n args : \n arg1 : desc '' , expected ) # 2 space indent for main docstring , blank lines indented as well . self._check ( `` brief\n \n docstring\n \n args : \n arg1 : desc '' , expected ) # no indent for main docstring , first line blank . self._check ( `` \n brief\n \n docstring\n \n args : \n arg1 : desc '' , expected ) # 2 space indent , first line blank . self._check ( `` \n brief\n \n docstring\n \n args : \n arg1 : desc '' , expected )
__label__0 @ test_util.run_v1_only ( `` requires tf v1 variable behavior . '' ) def testwaitforsessioninsufficientreadyforlocalinitcheck ( self ) : with ops.graph ( ) .as_default ( ) as graph : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) sm = session_manager.sessionmanager ( graph=graph , ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=none , local_init_op=w.initializer ) with self.assertraisesregex ( errors_impl.deadlineexceedederror , `` session was not ready after waiting . * '' ) : sm.wait_for_session ( `` '' , max_wait_secs=3 )
__label__0 argspec = tf_inspect.fullargspec ( args= [ 'cls ' , ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 cell_index = 0 for cell in notebook [ `` cells '' ] : if is_python ( cell ) : cell_lines = cell [ `` source '' ]
__label__0 text = `` import foo.baz as a '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 def _make_server_def ( server_or_cluster_def , job_name , task_index , protocol , config ) : `` '' '' creates a ` tf.train.serverdef ` protocol buffer .
__label__0 this function is intended for documentation-generation purposes .
__label__0 # error when restoring with default reshape=false with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : var = variable_v1.variablev1 ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] ) save = saver_module.saver ( ) with self.assertraisesregex ( errors_impl.invalidargumenterror , `` assign requires shapes of both tensors to match . `` ) : save.restore ( sess , save_path )
__label__0 raises : runtimeerror : if the model can not be initialized or recovered . valueerror : if both checkpoint_dir and checkpoint_filename_with_path are set. `` '' ''
__label__0 def _is_windows ( ) : return platform.system ( ) == `` windows ''
__label__0 # python3 ast requires the args for the attribute , but codegen will mess up # the arg order if we just set them to 0. new_arg.value.lineno = node.lineno new_arg.value.col_offset = node.col_offset+100
__label__0 def testbasegetattribute ( self ) : # tests that the default attribute lookup works . module = childfastmodule ( `` test '' ) module.foo = 1 self.assertequal ( 1 , module.foo )
__label__0 `` ` @ do_not_generate_docs class parent ( object ) : def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 # # # # registered apis
__label__0 input_tree = `` input_tree '' shallow_tree = [ `` shallow_tree_9 '' , `` shallow_tree_8 '' ] with self.assertraiseswithliteralmatch ( typeerror , nest.if_shallow_is_seq_input_must_be_seq.format ( type ( input_tree ) ) , ) : ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree_paths , [ ( 0 , ) , ( 1 , ) ] ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 def testdispatcherrorsignaturemismatchextraparam ( self ) : with self.assertraisesregex ( valueerror , r '' dispatch function 's signature \ ( x , y , name=none , extra_ '' r '' arg=none\ ) does not match api 's signature \ ( x , y , name=none\ ) . `` ) :
__label__0 def __init__ ( self , a , b=1 , c='hello ' ) : pass
__label__0 major , minor , patch = hipfft_version_numbers ( rocm_install_path )
__label__0 this function is meant to be used when defining a backwards-compatibility alias for a symbol which has been moved . for example :
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_positional_and_named ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 self.assertlen ( splitter.split ( ) [ 0 ] , 7 ) # adds 2 chunks .
__label__0 @ parameterized.named_parameters ( * examples ) def test_parser_no_blanks ( self , expected_example_tuples : exampletuples , string : str ) : string = string.replace ( '\n\n ' , '\n ' ) self._do_test ( expected_example_tuples , string )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 self.assertequal ( 0 , mock_warning.call_count ) lazy_loader_module.foo = 0 self.assertequal ( 1 , mock_warning.call_count ) foo = lazy_loader_module.foo self.assertequal ( 1 , mock_warning.call_count )
__label__0 https : //docs.python.org/3/library/functions.html # compile
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf_doctest . '' '' ''
__label__0 class child1 ( parent ) : def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 raises : valueerror : if date is not none or in iso 8601 format , instructions are empty , the deprecated arguments are not present in the function signature , the second element of a deprecated_tuple is not a list , or if a kwarg other than ` warn_once ` is passed. `` '' '' _validate_deprecation_args ( date , instructions ) if not deprecated_arg_names_or_tuples : raise valueerror ( 'specify which argument is deprecated . ' ) if kwargs and list ( kwargs.keys ( ) ) ! = [ 'warn_once ' ] : kwargs.pop ( 'warn_once ' , none ) raise valueerror ( f'illegal argument passed to deprecated_args : { kwargs } ' ) warn_once = kwargs.get ( 'warn_once ' , true )
__label__0 return cpu_info
__label__0 tf_should_use_helper = _tfshouldusehelper ( type_=type ( x ) , repr_=repr ( x ) , stack_frame=stack_frame , error_in_function=error_in_function , warn_in_eager=warn_in_eager )
__label__0 self.assertequal ( `` ps0:2222 '' , cluster_spec.task_address ( `` ps '' , 0 ) ) self.assertequal ( `` sparse0:2222 '' , cluster_spec.task_address ( `` sparse '' , 0 ) ) with self.assertraises ( valueerror ) : cluster_spec.task_address ( `` unknown '' , 0 ) with self.assertraises ( valueerror ) : cluster_spec.task_address ( `` sparse '' , 2 )
__label__0 raises : valueerror : if ` structure ` contains more elements than ` flat ` ( assuming indexing starts from ` index ` ) . `` '' '' packed = [ ] for s in _tf_data_yield_value ( structure ) : if _tf_data_is_nested ( s ) : new_index , child = _tf_data_packed_nest_with_indices ( s , flat , index ) packed.append ( sequence_like ( s , child ) ) # pylint : disable=protected-access index = new_index else : packed.append ( flat [ index ] ) index += 1 return index , packed
__label__0 # save checkpoint from which to warm-start . _ , prev_hash_val = self._create_prev_run_var ( `` linear_model/sc_hash/weights '' , shape= [ 15 , 1 ] , initializer=norms ( ) )
__label__0 returns : a ` bytes ` object .
__label__0 - a regular expression ( string ) that captures which variables to warm-start ( see tf.compat.v1.get_collection ) . this expression will only consider variables in the trainable_variables collection -- if you need to warm-start non_trainable vars ( such as optimizer accumulators or batch norm statistics ) , please use the below option . - a list of strings , each a regex scope provided to tf.compat.v1.get_collection with global_variables ( please see tf.compat.v1.get_collection ) . for backwards compatibility reasons , this is separate from the single-string argument type . - a list of variables to warm-start . if you do not have access to the ` variable ` objects at the call site , please use the above option . - ` none ` , in which case only trainable variables specified in ` var_name_to_vocab_info ` will be warm-started .
__label__0 args : code_line : a line of python code
__label__0 def bulk_restore ( self , filename_tensor , saveables , preferred_shard , restore_sequentially ) : `` '' '' restore all tensors contained in saveables .
__label__0 q = data_flow_ops.fifoqueue ( 10 , [ dtypes.float32 ] ) enqueue_op = q.enqueue ( 37.0 ) dequeue_t = q.dequeue ( )
__label__0 1. create a keras optimizer , which generates an ` iterations ` variable . this variable is automatically incremented when calling ` apply_gradients ` . 2. manually create and increment a ` tf.variable ` .
__label__0 if args.raw_generate : write_build_info ( args.raw_generate , args.key_value ) else : raise runtimeerror ( `` -- raw_generate must be used . '' )
__label__0 args : ready_op : ` tensor ` to check if the model is initialized . if it 's set to use_default , creates an op that checks all the variables are initialized . ready_for_local_init_op : ` tensor ` to check if the model is ready to run local_init_op . if it 's set to use_default , creates an op that checks all the global variables are initialized. `` '' '' if ready_op is supervisor.use_default : ready_op = self._get_first_op_from_collection ( ops.graphkeys.ready_op ) if ready_op is none : ready_op = variables.report_uninitialized_variables ( ) ops.add_to_collection ( ops.graphkeys.ready_op , ready_op ) self._ready_op = ready_op
__label__0 > > > @ tf.function ... def f ( x ) : ... return x > > > f_concrete = f.get_concrete_function ( tf.tensorspec ( [ ] , tf.float64 ) )
__label__0 # verifies that there is no saver_def in meta_graph_def . self.assertfalse ( meta_graph_def.hasfield ( `` saver_def '' ) ) # verifies that there is saver_def in meta_graph_def0 and 1. self.asserttrue ( meta_graph_def0.hasfield ( `` saver_def '' ) ) self.asserttrue ( meta_graph_def1.hasfield ( `` saver_def '' ) )
__label__0 # loads the checkpoint . _restore_checkpoint_and_maybe_run_saved_model_initializers ( sess , saver , ckpt.model_checkpoint_path ) saver.recover_last_checkpoints ( ckpt.all_model_checkpoint_paths ) return sess , true
__label__0 # output is : ab_tuple ( a=6 , b=15 ) `` `
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { ... 'my_ints ' : tf.io.raggedfeature ( dtype=tf.int64 ) , ... 'my_floats ' : tf.io.raggedfeature ( dtype=tf.float32 ) , ... 'my_bytes ' : tf.io.raggedfeature ( dtype=tf.string ) } ) { 'my_bytes ' : < tf.tensor : shape= ( 2 , ) , dtype=string , numpy=array ( [ b'abc ' , b'1234 ' ] , dtype=object ) > , 'my_floats ' : < tf.tensor : shape= ( 4 , ) , dtype=float32 , numpy=array ( [ 1. , 2. , 3. , 4 . ] , dtype=float32 ) > , 'my_ints ' : < tf.tensor : shape= ( 4 , ) , dtype=int64 , numpy=array ( [ 1 , 2 , 3 , 4 ] ) > }
__label__0 partial_func = functools.partial ( func , 7 ) argspec = tf_inspect.argspec ( args= [ 'n ' ] , varargs=none , keywords=none , defaults=none )
__label__0 args : names_to_ok_vals : dict from string arg_name to a list of values , possibly empty , which should not elicit a warning . arg_spec : output from tf_inspect.getfullargspec on the called function .
__label__0 def fn_has_no_kwargs ( x , test_arg ) : if test_arg ! = expected_test_arg : return valueerror ( 'partial fn does not work correctly ' ) return x
__label__0 var = variable_v1.variablev1 ( 1. , dtype=dtypes.float32 , trainable=false , name= '' var '' )
__label__0 ` tf.distribute.distributediterator ` is the primary mechanism for enumerating elements of a ` tf.distribute.distributeddataset ` . it supports the python iterator protocol , which means it can be iterated over using a for-loop or by fetching individual elements explicitly via ` get_next ( ) ` .
__label__0 class stacktracetransform ( object ) : `` '' '' base class for stack trace transformation functions . '' '' ''
__label__0 def _find_hipsparse_config ( rocm_install_path ) :
__label__0 # pylint : enable=line-too-long
__label__0 def deprecated_arg_values ( date , instructions , warn_once=true , * * deprecated_kwargs ) : `` '' '' decorator for marking specific function argument values as deprecated .
__label__0 placing the loop inside a ` tf.function ` will give a performance boost . however ` break ` and ` return ` are currently not supported if the loop is placed inside a ` tf.function ` . we also do n't support placing the loop inside a ` tf.function ` when using ` tf.distribute.experimental.multiworkermirroredstrategy ` or ` tf.distribute.experimental.tpustrategy ` with multiple workers .
__label__0 returns : the map marking symbols to not explore. `` '' '' return self._do_not_descend_map
__label__0 # cached values are used on subsequent accesses . self.assertequal ( myclass.value , `` myclass '' ) self.assertequal ( myclass.value , `` myclass '' ) self.assertequal ( log , [ myclass ] )
__label__0 def __call__ ( self , * args , * * kwargs ) : `` '' '' executes this callable .
__label__0 symbol_name = get_canonical_name_for_symbol ( op ) return tensortracer ( symbol_name , args , kwargs )
__label__0 def test_contrib_summary_always_record_summaries ( self ) : text = `` tf.contrib.summary.always_record_summaries ( ) '' expected = `` tf.compat.v2.summary.record_if ( true ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 class nesttest ( parameterized.testcase , test.testcase ) :
__label__0 returns : an op or ` none ` . `` '' '' return self._ready_op
__label__0 args : major : major string eg . ( 1 ) minor : minor string eg . ( 3 ) patch : patch string eg . ( 1 ) identifier_string : extension string eg . ( -rc0 ) version_type : version parameter ( ( regular|nightly ) _version ) `` '' '' self.major = major self.minor = minor self.patch = patch self.identifier_string = identifier_string self.version_type = version_type self._update_string ( )
__label__0 # pylint : disable=unused-import import functools
__label__0 def testgetargspeconpartialwithdecorator ( self ) : `` '' '' tests getargspec on decorated partial function . '' '' ''
__label__0 def visit_import ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting an import node in the ast .
__label__0 # need to do this in a separate test because of the amount of memory needed # to run this test . def testlargepartitionedvariables ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` large_variable '' ) var_name = `` my_var '' # saving large partition variable . with session.session ( `` '' , graph=ops.graph ( ) ) as sess : with ops.device ( `` /cpu:0 '' ) : # create a partitioned variable which is larger than int32 size but # split into smaller sized variables . init = lambda shape , dtype , partition_info : constant_op.constant ( true , dtype , shape ) partitioned_var = list ( variable_scope.get_variable ( var_name , shape= [ 1 < < 31 ] , partitioner=partitioned_variables.fixed_size_partitioner ( 4 ) , initializer=init , dtype=dtypes.bool ) ) self.evaluate ( variables.global_variables_initializer ( ) ) save = saver.saver ( partitioned_var ) val = save.save ( sess , save_path ) self.assertequal ( save_path , val )
__label__0 returns : a dictionary mapping ` func ` 's named arguments to the values they would receive if ` func ( * positional , * * named ) ` were called .
__label__0 # slots can also be used for moving averages mavg = create_slot ( var , var.initialized_value ( ) , `` exponential_moving_avg '' ) update_mavg = mavg.assign_sub ( ( mavg - var ) * ( 1 - decay ) ) `` ` `` '' '' # pylint : disable=g-bad-name
__label__0 returns : true if the input is a nested structure. `` '' '' return nest_util.is_nested ( nest_util.modality.core , seq )
__label__0 def set_last_checkpoints ( self , last_checkpoints ) : `` '' '' deprecated : use set_last_checkpoints_with_time .
__label__0 return result
__label__0 logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing tf.contrib.layers.variance_scaling_initializer '' `` to a tf.compat.v1.keras.initializers.variancescaling and '' `` converting arguments.\n '' ) )
__label__0 def testcompatinvalidencoding ( self ) : with self.assertraises ( lookuperror ) : compat.as_bytes ( `` hello '' , `` invalid '' )
__label__1 def is_leap_year ( year ) : return year % 4 == 0 and ( year % 100 ! = 0 or year % 400 == 0 )
__label__0 def testwarmstartvarwithcolumnvocabcurrentvarpartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_output_layer '' , initializer= [ [ 0.5 , 0.3 ] , [ 1. , 0.8 ] , [ 1.5 , 1.2 ] , [ 2. , 2.3 ] ] )
__label__0 self._accumulator_list.append ( ( grad_accum , var.device ) )
__label__0 @ test_util.run_v1_only ( `` exporting/importing meta graphs is only supported in v1 . '' ) def testclearextraneoussavers ( self ) : test_dir = self._get_test_dir ( `` clear_extraneous_savers '' ) filename = os.path.join ( test_dir , `` metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) saver1_ckpt = os.path.join ( test_dir , `` saver1.ckpt '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : # creates a graph . v0 = variable_v1.variablev1 ( [ [ 1.0 , 2.0 ] , [ 3.0 , 4.0 ] , [ 5.0 , 6.0 ] ] , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 11.0 , name= '' v1 '' )
__label__0 visitor = testvisitor ( ) traverse.traverse ( cyclist , visitor ) # we simply want to make sure we terminate .
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` deprecated function '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 * specifying ` 'local ' ` requests a session that uses the rpc-based `` master interface '' to run tensorflow programs . see ` tf.train.server.create_local_server ` for details .
__label__0 def testreorderfileneedsupdate ( self ) : reordered_function_names = ( tf_upgrade_v2.tfapichangespec ( ) .reordered_function_names ) function_reorders = ( tf_upgrade_v2.tfapichangespec ( ) .function_reorders ) manual_function_reorders = ( tf_upgrade_v2.tfapichangespec ( ) .manual_function_reorders )
__label__0 in terms of support level , classes implementing this protocol - are supported by tf.nest and tf.data functions . - have limited support from tf.function , which requires writing a custom tracetype subclass to be used as the input or output of a tf.function . - are not supported by savedmodel .
__label__0 > > > tensor = tf.ragged.constant ( [ [ 3 , 1 , 4 , 1 ] , [ ] , [ 5 , 9 , 2 ] ] ) > > > tf.nest.flatten ( tensor , expand_composites=true ) [ < tf.tensor : shape= ( 7 , ) , dtype=int32 , numpy=array ( [ 3 , 1 , 4 , 1 , 5 , 9 , 2 ] , dtype=int32 ) > , < tf.tensor : shape= ( 4 , ) , dtype=int64 , numpy=array ( [ 0 , 4 , 4 , 7 ] ) > ]
__label__0 from tensorflow.python.types import core from tensorflow.python.util.tf_export import tf_export from tensorflow.tools.docs import doc_controls
__label__0 from tensorflow.core.framework import summary_pb2 from tensorflow.core.protobuf import config_pb2 from tensorflow.core.protobuf import meta_graph_pb2 from tensorflow.core.protobuf import queue_runner_pb2 from tensorflow.core.protobuf import rewriter_config_pb2 from tensorflow.core.protobuf import saver_pb2 from tensorflow.python.checkpoint import checkpoint_management from tensorflow.python.client import session from tensorflow.python.data.ops import dataset_ops from tensorflow.python.data.ops import iterator_ops from tensorflow.python.eager import context from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors from tensorflow.python.framework import errors_impl from tensorflow.python.framework import function from tensorflow.python.framework import graph_io from tensorflow.python.framework import meta_graph from tensorflow.python.framework import ops as ops_lib from tensorflow.python.framework import test_util from tensorflow.python.lib.io import file_io from tensorflow.python.ops import array_ops from tensorflow.python.ops import array_ops_stack from tensorflow.python.ops import cond from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import data_flow_ops from tensorflow.python.ops import gradients_impl from tensorflow.python.ops import math_ops from tensorflow.python.ops import nn_ops from tensorflow.python.ops import partitioned_variables from tensorflow.python.ops import random_ops from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops import sparse_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.ops import while_loop import tensorflow.python.ops.nn_grad # pylint : disable=unused-import from tensorflow.python.platform import gfile from tensorflow.python.platform import test from tensorflow.python.saved_model.pywrap_saved_model import metrics from tensorflow.python.summary import summary from tensorflow.python.trackable import base as trackable_base from tensorflow.python.training import adam from tensorflow.python.training import gradient_descent from tensorflow.python.training import py_checkpoint_reader from tensorflow.python.training import queue_runner_impl from tensorflow.python.training import saver as saver_module from tensorflow.python.training import saver_test_utils from tensorflow.python.util import compat
__label__0 self.asserttrue ( nest.is_namedtuple ( subfoo ( 1 , 2 ) ) )
__label__0 nest.assert_same_structure ( nesttest.samenameab ( 0 , 1 ) , nesttest.samenameab2 ( 2 , 3 ) )
__label__0 def _default_global_step_tensor ( self ) : `` '' '' returns the global_step from the default graph .
__label__0 # check that values stayed the same self.assertequal ( module.foo , foo ) self.assertequal ( module.bar , bar )
__label__0 raises : runtimeerror : if the savers collection already has more than one items. `` '' '' collection_key = ops.graphkeys.savers savers = ops.get_collection ( collection_key ) if savers : if len ( savers ) > 1 : raise runtimeerror ( `` more than one item in collection { } . `` `` please indicate which one to use by passing it to the constructor . '' .format ( collection_key ) ) return savers [ 0 ] saver = saver ( sharded=true , allow_empty=true ) if saver is not none : ops.add_to_collection ( collection_key , saver ) return saver
__label__0 if obj is ellipsis : return { 'class_name ' : '__ellipsis__ ' }
__label__0 raises : typeerror : the nest is or contains a dict with non-sortable keys. `` '' '' if modality == modality.core : return _tf_core_flatten ( structure , expand_composites ) elif modality == modality.data : return _tf_data_flatten ( structure ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 if defaults : all_defaults [ -len ( defaults ) : ] = defaults
__label__0 raises : valueerror : if ` func ` is not callable. `` '' ''
__label__0 stores results in a global dict .
__label__0 return wrapper
__label__0 `` ` python # choose a task as the chief . this could be based on server_def.task_index , # or job_def.name , or job_def.tasks . it 's entirely up to the end user . # but there can be only one * chief * . is_chief = ( server_def.task_index == 0 ) server = tf.distribute.server ( server_def )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests to check that py_test are properly loaded in build files . '' '' ''
__label__0 def __init__ ( self ) : self.internal_set = _tf_stack.pybindfileset ( )
__label__0 def testpartitionedvariable ( self ) : self._testpartitionedvariables ( use_resource=false )
__label__0 * a single python dict :
__label__0 def load_from_files ( files , globs : optional [ dict [ str , any ] ] = none , set_up : optional [ callable [ [ any ] , none ] ] = none , tear_down : optional [ callable [ [ any ] , none ] ] = none ) - > doctest.docfilesuite : `` '' '' creates a doctest suite from the files list .
__label__0 if not hasattr ( symbol , '__dict__ ' ) : return names_v2 if tensorflow_api_attr in symbol.__dict__ : names_v2.extend ( getattr ( symbol , tensorflow_api_attr ) ) if keras_api_attr in symbol.__dict__ : names_v2.extend ( getattr ( symbol , keras_api_attr ) ) return names_v2
__label__0 major = _get_header_version ( version_file , `` __libsycl_major_version '' ) minor = _get_header_version ( version_file , `` __libsycl_minor_version '' ) patch = _get_header_version ( version_file , `` __libsycl_patch_version '' ) return major , minor , patch
__label__0 # version of the join implementation . # this should be incremented almost whenever the joining implementation is # updated . the only time this number should not be incremented is if the change # is extremely trivial . _join_version = 0
__label__0 def testismethod ( self ) : self.asserttrue ( tf_inspect.ismethod ( testdecoratedclass ( ) .two ) ) self.assertfalse ( tf_inspect.ismethod ( test_decorated_function ) )
__label__0 def fn ( test_arg1 , test_arg2 , a ) : if test_arg1 ! = expected_test_arg1 or test_arg2 ! = expected_test_arg2 : return valueerror ( 'partial fn does not work correctly ' ) return a
__label__0 def func ( m , n , l , k=4 ) : return 2 * m + l + n * k
__label__0 > > > # define a type and register a dispatcher to override ` tf.abs ` : > > > class mytensor ( tf.experimental.extensiontype ) : ... value : tf.tensor > > > @ tf.experimental.dispatch_for_api ( tf.abs ) ... def my_abs ( x : mytensor ) : ... return mytensor ( tf.abs ( x.value ) ) > > > tf.abs ( mytensor ( 5 ) ) mytensor ( value= < tf.tensor : shape= ( ) , dtype=int32 , numpy=5 > )
__label__0 @ parameterized.parameters ( [ 'hello . 2.0 ' , 'hello . 2.0000001 ' ] , [ 'hello ... 2.0 ' , 'hello 2.0000001 ' ] ) def test_extra_dots ( self , want , got ) : output_checker = tf_doctest_lib.tfdoctestoutputchecker ( ) self.asserttrue ( output_checker.check_output ( want=want , got=got , optionflags=doctest.ellipsis ) )
__label__0 cufft_paths = base_paths if tuple ( int ( v ) for v in cuda_version.split ( `` . '' ) ) < ( 11 , 0 ) : cufft_paths = cuda_paths cufft_version = os.environ.get ( `` tf_cufft_version '' , `` '' ) result.update ( _find_cufft_config ( cufft_paths , cufft_version , cuda_version ) )
__label__0 @ decorator_utils.cached_classproperty def value ( cls ) : # pylint : disable=no-self-argument log.append ( cls ) return cls.__name__
__label__0 this class must provide the following fields :
__label__0 @ property def errors ( self ) : return [ log for log in self._log if log [ 0 ] == error ]
__label__0 another_bad_dictionary = mapping_type ( { ( 4 , 5 , ( 6 , 8 ) ) : ( `` a '' , `` b '' , ( `` c '' , ( `` d '' , `` e '' ) ) ) } ) with self.assertraisesregex ( valueerror , `` key had [ 0-9 ] * elements , but value had [ 0-9 ] * elements '' ) : nest.flatten_dict_items ( another_bad_dictionary )
__label__0 nested_list2 = [ [ [ 2 ] ] ] nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 3 ] ) )
__label__0 example usage : ./check_cuda_libs.py /path/to/lib1.so true /path/to/lib2.so false `` '' '' import os import os.path import platform import subprocess import sys
__label__0 @ test_util.run_in_graph_and_eager_modes def testnonreshaperesourcevariable ( self ) : self._testnonreshape ( resource_variable_ops.resourcevariable )
__label__0 def testgetargspeconpartialpositionalargumentonly ( self ) : `` '' '' tests getargspec on partial function with only positional arguments . '' '' ''
__label__0 def testdispatchwithvarargs ( self ) :
__label__0 if not os.path.exists ( install_dir ) : self.mkpath ( install_dir ) return self.copy_file ( header , install_dir )
__label__0 # this assertion is expected to pass : two namedtuples with the same # name and field names are considered to be identical . nest.assert_same_structure ( nesttest.samenameab ( nesttest.samename1xy ( 0 , 1 ) , 2 ) , nesttest.samenameab2 ( nesttest.samename1xy2 ( 2 , 3 ) , 4 ) )
__label__0 class deprecatedargvaluestest ( test.testcase ) :
__label__0 * one of the recognized python collections , holding _nested structures_ ; * a value of any other type , typically a tensorflow data type like tensor , variable , or of compatible types such as int , float , ndarray , etc . these are commonly referred to as _atoms_ of the structure .
__label__0 if version.parse ( fake_tf.__version__ ) > = version.parse ( ' 2.14 ' ) : self.assertin ( ' < a id=add href= '' /tf/raw_ops/add.md '' > add < /a > | ✔️ | ✔️ | ' , raw_ops_page ) self.assertin ( ' < a id=print href= '' /tf/raw_ops/print.md '' > print < /a > | ✔️ | ❌ | ' , raw_ops_page , )
__label__0 self.assertequal ( [ 5 ] , nest.flatten ( 5 ) ) self.assertequal ( [ np.array ( [ 5 ] ) ] , nest.flatten ( np.array ( [ 5 ] ) ) )
__label__0 def get_next ( self ) : `` '' '' unlike __next__ , this may use a non-raising mechanism . '' '' ''
__label__0 def test_want_no_floats ( self ) : want = 'text ... text ' got = 'text 1.0 1.2 1.9 text ' output_checker = tf_doctest_lib.tfdoctestoutputchecker ( ) self.asserttrue ( output_checker.check_output ( want=want , got=got , optionflags=doctest.ellipsis ) )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tf.function tracing types .
__label__0 args : * func_and_positional : a callable , possibly decorated , followed by any positional arguments that would be passed to ` func ` . * * named : the named argument dictionary that would be passed to ` func ` .
__label__0 # save the initialized values in the file at `` save_path '' val = save.save ( sess , save_path ) self.assertisinstance ( val , str ) self.assertequal ( save_path , val )
__label__0 # start a second session . in that session the variables # have not been initialized either . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = variable_v1.variablev1 ( -1.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( -1.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) save = saver_module.saver ( [ v0 , v1 , v2.saveable ] )
__label__0 assert_return_type_comment = ( ast_edits.info , `` < function name > has been changed to return none , the `` `` data argument has been removed , and arguments have been reordered . '' `` \nthe calls have been converted to compat.v1 for safety ( even though `` `` they may already have been correct ) . '' )
__label__0 # __fspath__ is n't respected everywhere in doctest so convert paths to # strings . files = [ os.fspath ( f ) for f in files ]
__label__0 text = `` import tensorflow '' expected_text = ( `` import tensorflow.compat.v1 as tensorflow '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def g ( a , b , c , kw1 ) : ... def g2 ( a , b , c , d , kw1 ) : ...
__label__0 def testwritegraph ( self ) : test_dir = self._get_test_dir ( `` write_graph_dir '' ) variable_v1.variablev1 ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] , dtype=dtypes.float32 , name= '' v0 '' ) path = graph_io.write_graph ( ops_lib.get_default_graph ( ) , os.path.join ( test_dir , `` l1 '' ) , `` graph.pbtxt '' ) truth = os.path.join ( test_dir , `` l1 '' , `` graph.pbtxt '' ) self.assertequal ( path , truth ) self.asserttrue ( os.path.exists ( path ) )
__label__0 contrib_create_file_writer_comment = ( ast_edits.warning , `` tf.contrib.summary.create_file_writer ( ) has been ported to the new `` `` tf.compat.v2.summary.create_file_writer ( ) , which no longer re-uses `` `` existing event files for the same logdir ; instead it always opens a `` `` new writer/file . the python writer objects must be re-used explicitly `` `` if the reusing behavior is desired . '' )
__label__0 if you have previously disabled traceback filtering via ` tf.debugging.disable_traceback_filtering ( ) ` , you can re-enable it via ` tf.debugging.enable_traceback_filtering ( ) ` .
__label__0 s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) , write_meta_graph=false ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.assertfalse ( gfile.exists ( checkpoint_management.meta_graph_filename ( s1 ) ) )
__label__0 def process_file ( self , in_filename , out_filename , no_change_to_outfile_on_error=false ) : `` '' '' process the given python file for incompatible changes .
__label__0 here is another example using tf.tensorspec inputs :
__label__0 this method is useful for recovering the `` self._last_checkpoints '' state .
__label__0 header_path , header_version = _find_header ( base_paths , ( `` cudnn.h '' , `` cudnn_version.h '' ) , required_version , get_header_version ) cudnn_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 - modality.core follows tensorflow_core/tf.nest semantics .
__label__0 raises : typeerror : if ` sess ` is not a ` session ` . valueerror : if ` latest_filename ` contains path components , or if it collides with ` save_path ` . runtimeerror : if save and restore ops were n't built. `` '' '' # pylint : enable=line-too-long start_time = time.time ( ) if not self._is_built and not context.executing_eagerly ( ) : raise runtimeerror ( `` ` build ( ) ` should be called before save if defer_build==true '' ) if latest_filename is none : latest_filename = `` checkpoint '' if self._write_version ! = saver_pb2.saverdef.v2 : logging.warning ( `` * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * '' ) logging.warning ( `` tensorflow 's v1 checkpoint format has been deprecated . '' ) logging.warning ( `` consider switching to the more efficient v2 format : '' ) logging.warning ( `` ` tf.train.saver ( write_version=tf.train.saverdef.v2 ) ` `` ) logging.warning ( `` now on by default . '' ) logging.warning ( `` * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * '' )
__label__0 args : lines : array of strings corresponding to each line of the output from run_onednn_benchmarks.sh
__label__0 import abc
__label__0 text = `` import foo.test '' expected_text = `` import bar.test '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 input_graph_def_string = input_graph_def.serializetostring ( ) inputs_string = compat.as_bytes ( `` , '' .join ( inputs ) ) outputs_string = compat.as_bytes ( `` , '' .join ( outputs ) ) transforms_string = compat.as_bytes ( `` `` .join ( transforms ) ) output_graph_def_string = transformgraphwithstringinputs ( input_graph_def_string , inputs_string , outputs_string , transforms_string ) output_graph_def = graph_pb2.graphdef ( ) output_graph_def.parsefromstring ( output_graph_def_string ) return output_graph_def
__label__0 from tensorflow.python.eager import monitoring from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import fast_module_type from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect from tensorflow.tools.compatibility import all_renames_v2
__label__0 class samevariablesclearcontainertest ( test.testcase ) :
__label__0 def prepare_aot ( aot : list [ str ] , srcs_dir : str ) - > none : `` '' '' rearrange xla_aot files in target the target directory . args : aot : a list of paths to files that should be in xla_aot directory . srcs_dir : target directory where files are copied to. `` '' '' for file in aot : if `` external/local_tsl/ '' in file : copy_file ( file , srcs_dir , `` external/local_tsl/ '' ) elif `` external/local_xla/ '' in file : copy_file ( file , srcs_dir , `` external/local_xla/ '' ) else : copy_file ( file , srcs_dir )
__label__0 if i < len ( fields ) : # the next field must be from the value message . value_desc = parent_desc.fields_by_name [ `` value '' ] assert value_desc.message_type is not none parent_desc = value_desc.message_type
__label__0 text = `` from foo import a , b '' expected_text = `` from bar import a , b '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 you can pass any of the returned values to ` restore ( ) ` .
__label__0 that is , this function tests if the ` input_tree ` structure can be created from the ` shallow_tree ` structure by replacing its leaf nodes with deeper tree structures .
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # # ============================================================================== `` '' '' tests for protosplitter . '' '' ''
__label__0 args : session : a tensorflow session that has been created . coord : a coordinator object which keeps track of all threads. `` '' '' pass
__label__0 args : min_length : the minimum length of the list . max_length : the maximum length of the list .
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 subprocess.check_call ( [ 'bazel ' , 'build ' , '//tensorflow/java : java_op_gen_sources ' ] , cwd=tensorflow_root ) shutil.copytree ( op_source_path , merged_source / 'java/org/tensorflow/ops ' )
__label__0 deprecation_string = ' , '.join ( sorted ( deprecated_names ) )
__label__0 returns : ` bool ` : if ` fn ` has * * kwargs in its signature .
__label__0 from tensorflow.python.checkpoint import checkpoint as tracking_util from tensorflow.python.feature_column import feature_column_lib as fc from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import init_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import checkpoint_utils from tensorflow.python.training import saver as saver_lib from tensorflow.python.training import warm_starting_util as ws_util
__label__0 class checkpointreadertest ( test.testcase ) :
__label__0 def add_chunk ( self , chunk : union [ message.message , bytes ] , field_tags : util.fieldtypes , index=none , ) - > none : `` '' '' adds a new chunk and updates the chunkedmessage proto .
__label__0 def __getattr__ ( self , name ) : module = self._load ( ) return getattr ( module , name )
__label__0 # pylint : disable=g-import-not-at-top # pylint : disable=g-bad-import-order # pylint : disable=unused-import # note : cpuinfo and psutil are not installed for you in the tensorflow # oss tree . they are installable via pip . try : import cpuinfo import psutil except importerror as e : tf_logging.error ( `` \n\n\nerror : unable to import necessary library : { } . `` `` issuing a soft exit.\n\n\n '' .format ( e ) ) sys.exit ( 0 ) # pylint : enable=g-bad-import-order # pylint : enable=unused-import
__label__0 def debug_set_max_size ( value : int ) - > none : `` '' '' sets the max size allowed for each proto chunk ( used for debugging only ) .
__label__0 def __init__ ( self , type_ , repr_ , stack_frame , error_in_function , warn_in_eager ) : self._type = type_ self._repr = repr_ self._stack_frame = stack_frame self._error_in_function = error_in_function if context.executing_eagerly ( ) : # if warn_in_eager , sated == false . otherwise true . self._sated = not warn_in_eager elif ops.inside_function ( ) : if error_in_function : self._sated = false ops.add_exit_callback_to_default_func_graph ( lambda : self._check_sated ( raise_error=true ) ) else : self._sated = true else : # tf1 graph building mode self._sated = false
__label__0 def testnestmapstructure ( self ) : k = object_identity._objectidentitywrapper ( ' k ' ) v1 = object_identity._objectidentitywrapper ( 'v1 ' ) v2 = object_identity._objectidentitywrapper ( 'v2 ' ) struct = nest.map_structure ( lambda a , b : ( a , b ) , { k : v1 } , { k : v2 } ) self.assertequal ( struct , { k : ( v1 , v2 ) } )
__label__0 def get_slot_names ( self , * args , * * kwargs ) : `` '' '' return a list of the names of slots created by the ` optimizer ` .
__label__0 def set_last_checkpoints_with_time ( self , last_checkpoints_with_time ) : `` '' '' sets the list of old checkpoint filenames and timestamps .
__label__0 def test_variable_constructor ( self ) : def fn ( ) : _ = variables.variable ( )
__label__0 class tfupgradev2safetytest ( test_util.tensorflowtestcase ) :
__label__0 > > > structure = { `` foo '' : tf.ragged.constant ( [ [ 1 , 2 ] , [ 3 ] ] ) , ... `` bar '' : tf.constant ( [ [ 5 ] ] ) } > > > flat_sequence = [ `` one '' , `` two '' ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence , ... expand_composites=false ) { 'foo ' : 'two ' , 'bar ' : 'one ' }
__label__0 `` ` python x = tf.variable ( [ 1,2,3 ] , dtype=tf.float32 ) grad = tf.constant ( [ 0.1 , 0.2 , 0.3 ] ) optimizer = tf.compat.v1.train.rmspropoptimizer ( learning_rate=0.001 ) optimizer.apply_gradients ( zip ( [ grad ] , [ x ] ) ) `` `
__label__0 if __name__ == `` __main__ '' : args = parse_args ( ) temp_dir = tempfile.temporarydirectory ( prefix= '' tensorflow_wheel '' ) temp_dir_path = temp_dir.name try : prepare_wheel_srcs ( args.headers , args.srcs , args.xla_aot , temp_dir_path , args.version ) build_wheel ( os.path.join ( os.getcwd ( ) , args.output_name ) , temp_dir_path , args.project_name , args.collab ) finally : temp_dir.cleanup ( )
__label__0 need_to_bind_api_args = ( len ( api_signature.parameters ) > 3 or `` name '' not in api_signature.parameters )
__label__0 # the doc generator is n't aware of tf_export . # so prefix the score tuples with -1 when this is the canonical name , +1 # otherwise . the generator chooses the name with the lowest score . class tfexportawarevisitor ( doc_generator_visitor.docgeneratorvisitor ) : `` '' '' a ` tf_export ` , ` keras_export ` and ` estimator_export ` aware doc_visitor . '' '' ''
__label__0 later we can continue training from this saved ` meta_graph ` without building the model from scratch .
__label__0 def build_chunks ( self ) - > int : `` '' '' splits the proto , and returns the size of the chunks created . '' '' '' return 0
__label__0 self.assertequal ( ( ' a ' , ) , function_utils.fn_args ( wrapped_fn ) )
__label__0 def extra_method ( self , x ) : return self.a + x
__label__0 def __dir__ ( self ) : if not self._tfll_initialized : self._initialize ( ) return super ( ) .__dir__ ( )
__label__0 with self.assertraisesregex ( typeerror , `` signatures must be dictionaries mapping parameter `` `` names to type annotations '' ) :
__label__0 partial_func = functools.partial ( func , m=0 )
__label__0 def testinvalidhostname ( self ) : with self.assertraisesregex ( errors_impl.invalidargumenterror , `` port '' ) : _ = server_lib.server ( { `` local '' : [ `` localhost '' ] } , job_name= '' local '' , task_index=0 )
__label__0 def func ( a ) : return a
__label__0 @ property def name ( self ) : return self._name
__label__0 yields : tuples containing index or key values which form the path to a specific leaf value in the nested structure. `` '' '' is_nested_fn = _is_nested_or_composite if expand_composites else is_nested for k , _ in nest_util.yield_flat_up_to ( nest_util.modality.core , nest , nest , is_nested_fn ) : yield k
__label__0 def _unsortedsegmentmax ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.unsorted_segment_max , data , indices , num_segments )
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensor_slices ( [ 5. , 6. , 7. , 8 . ] ) .batch ( 2 ) > > > dataset_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > distributed_values = next ( dataset_iterator ) > > > reduced_value = strategy.reduce ( tf.distribute.reduceop.sum , ... distributed_values , ... axis = 0 ) > > > reduced_value < tf.tensor : shape= ( ) , dtype=float32 , numpy=11.0 >
__label__0 # replace the node if at least one import needs to be updated . if import_updated : assert self._stack [ -1 ] is node parent = self._stack [ -2 ]
__label__0 `` `` '' start a simple interactive console with tensorflow available . '' '' ''
__label__0 # the chunk_index of each nested chunkedmessage is set to the length of the # list when the chunk was added . this would be fine if the chunks were # always added to the end of the list . however , this is not always the case # the indices must be updated .
__label__0 def _rename_to_compat_v1 ( node , full_name , logs , reason ) : new_name = full_name.replace ( `` tf . `` , `` tf.compat.v1 . `` , 1 ) return _rename_func ( node , full_name , new_name , logs , reason )
__label__0 * float values are extracted from the text and replaced with wildcards . * the wildcard text is compared to the actual output . * the float values are compared using ` np.allclose ` .
__label__0 _ , biases2 = while_loop.while_loop ( loop_cond , loop_body , [ constant_op.constant ( 0 ) , variable_v1.variablev1 ( array_ops.zeros ( [ 32 ] ) ) ] ) hidden2 = nn_ops.relu ( math_ops.matmul ( hidden1 , weights2 ) + biases2 ) # linear with ops_lib.name_scope ( `` softmax_linear '' ) : weights3 = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 32 , 10 ] , stddev=1.0 / math.sqrt ( float ( 32 ) ) ) , name= '' weights '' ) biases3 = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' biases '' ) logits = math_ops.matmul ( hidden2 , weights3 ) + biases3 ops_lib.add_to_collection ( `` logits '' , logits )
__label__0 * use a pythonic for-loop construct :
__label__0 # empty structures self.assertequal ( ( ) , nest.map_structure ( lambda x : x + 1 , ( ) ) ) self.assertequal ( [ ] , nest.map_structure ( lambda x : x + 1 , [ ] ) ) self.assertequal ( { } , nest.map_structure ( lambda x : x + 1 , { } ) ) self.assertequal ( nesttest.emptynt ( ) , nest.map_structure ( lambda x : x + 1 , nesttest.emptynt ( ) ) )
__label__0 # output is : [ 'first_4_evens ' , [ 'first_5_odds ' , 'first_3_primes ' ] ] `` `
__label__0 def testgetsourcelines ( self ) : expected = inspect.getsourcelines ( test_decorated_function_with_defaults.decorated_target ) self.assertequal ( expected , tf_inspect.getsourcelines ( test_decorated_function_with_defaults ) )
__label__0 def test_contrib_summary_graph ( self ) : text = `` tf.contrib.summary.graph ( my_graph ) '' _ , _ , errors , _ = self._upgrade ( text ) expected_error = `` tf.compat.v2.summary.trace '' self.assertin ( expected_error , errors [ 0 ] )
__label__0 import importlib
__label__0 # collect list of files to process ( we do this to correctly handle if the # user puts the output directory in some sub directory of the input dir ) files_to_process = [ ] files_to_copy = [ ] for dir_name , _ , file_list in os.walk ( root_directory ) : py_files = [ f for f in file_list if f.endswith ( `` .py '' ) ] copy_files = [ f for f in file_list if not f.endswith ( `` .py '' ) ] for filename in py_files : fullpath = os.path.join ( dir_name , filename ) fullpath_output = os.path.join ( output_root_directory , os.path.relpath ( fullpath , root_directory ) ) files_to_process.append ( ( fullpath , fullpath_output ) ) if copy_other_files : for filename in copy_files : fullpath = os.path.join ( dir_name , filename ) fullpath_output = os.path.join ( output_root_directory , os.path.relpath ( fullpath , root_directory ) ) files_to_copy.append ( ( fullpath , fullpath_output ) )
__label__0 @ tf_export ( v1= [ `` train.export_meta_graph '' ] ) def export_meta_graph ( filename=none , meta_info_def=none , graph_def=none , saver_def=none , collection_list=none , as_text=false , graph=none , export_scope=none , clear_devices=false , clear_extraneous_savers=false , strip_default_attrs=false , save_debug_info=false , * * kwargs ) : # pylint : disable=line-too-long `` '' '' returns ` metagraphdef ` proto .
__label__0 args : sess : a ` session ` to use to restore the parameters . none in eager mode . save_path : path where parameters were previously saved .
__label__0 t , preprocess_logs , preprocess_errors = self._api_change_spec.preprocess ( t )
__label__0 from tensorflow.core.util import test_log_pb2 from tensorflow.python.framework import errors from tensorflow.python.platform import gfile
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def getter ( name ) : if getter not in _printed_warning and _print_deprecation_warnings : _printed_warning [ getter ] = true _log_deprecation ( 'please fix your imports . module % s has been moved to % s . the old ' 'module will be deleted in version % s . ' , deprecated_name , new_module.__name__ , deletion_version ) return getattr ( new_module , name )
__label__0 import re import sys
__label__0 # check the chunkedmessage proto . self.assertlen ( chunked_message.chunked_fields , 2 ) self.assertequal ( 1 , chunked_message.chunked_fields [ 0 ] .message.chunk_index ) self.assertequal ( 2 , chunked_message.chunked_fields [ 1 ] .message.chunk_index ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 2 , `` attr '' , `` value '' , `` tensor '' , `` tensor_content '' ] ) , chunked_message.chunked_fields [ 0 ] .field_tag , ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 4 , `` attr '' , `` value '' , `` tensor '' , `` tensor_content '' ] ) , chunked_message.chunked_fields [ 1 ] .field_tag , )
__label__0 wrapped_fn = functools.partial ( fn_has_kwargs , test_arg2=456 ) double_wrapped_fn = functools.partial ( wrapped_fn , test_arg1=123 )
__label__0 def start_queue_runners ( self , sess , queue_runners=none ) : `` '' '' start threads for ` queuerunners ` .
__label__0 return { `` nccl_version '' : nccl_version , `` nccl_include_dir '' : os.path.dirname ( header_path ) , `` nccl_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 with self.assertraises ( indexerror ) : _ = trace [ -len ( trace ) - 1 ]
__label__0 class shouldusewrapper ( object ) : pass
__label__0 args : func : a callable with the signature ` func ( tuple_path , * values , * * kwargs ) ` that is evaluated on the leaves of the structure . * structure : a variable number of compatible structures to process . * * kwargs : optional kwargs to be passed through to func . special kwarg ` check_types ` is not passed to func , but instead determines whether the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` .
__label__0 for cls in [ tf.module , tf.keras.layers.layer , tf.keras.optimizers.optimizer ] : doc_controls.decorate_all_class_attributes ( decorator=doc_controls.do_not_doc_in_subclasses , cls=cls , skip= [ `` __init__ '' ] )
__label__0 returns : a list containing all the modules in tensorflow.python. `` '' ''
__label__0 from tensorflow.core.util import test_log_pb2 from tensorflow.python.client import device_lib from tensorflow.python.framework import errors from tensorflow.python.platform import gfile from tensorflow.tools.test import gpu_info_lib
__label__0 def testunwrapboundmethods ( self ) : test_decorated_class = testdecoratedclass ( ) self.assertequal ( [ 2 , 2 , 3 ] , test_decorated_class.return_params ( 1 , 2 , 3 ) ) decorators , target = tf_decorator.unwrap ( test_decorated_class.return_params ) self.assertequal ( 'test_decorator_increment_first_int_arg ' , decorators [ 0 ] .decorator_name ) self.assertequal ( [ 1 , 2 , 3 ] , target ( test_decorated_class , 1 , 2 , 3 ) )
__label__0 def testreorder ( self ) : text = `` tf.concat ( a , b ) \ntf.split ( a , b , c ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.concat ( axis=a , values=b ) \n '' `` tf.split ( axis=a , num_or_size_splits=b , value=c ) \n '' )
__label__0 args : placeholder_context : a context reserved for internal/future usage . for the ` fruit ` example shared above , implementing :
__label__0 def roctracer_version_numbers ( path ) : possible_version_files = [ `` include/roctracer/roctracer.h '' , # rocm 5.2 `` roctracer/include/roctracer.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` roctracer version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` roctracer_version_major '' ) minor = _get_header_version ( version_file , `` roctracer_version_minor '' ) # roctracer header does not have a patch version number patch = 0 return major , minor , patch
__label__0 @ tf_export ( 'compat.path_to_str ' ) def path_to_str ( path ) : r '' '' '' converts input which is a ` pathlike ` object to ` str ` type .
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 assert self._chunks is not none assert self._chunked_message is not none
__label__0 import ast import copy import functools import sys
__label__0 # initialize all variables if not context.executing_eagerly ( ) : self.evaluate ( [ variables.global_variables_initializer ( ) , v2_init ] )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for rmsprop . '' '' ''
__label__0 `` ` a b `` `
__label__0 parser.add_argument ( `` -- debug '' , type=bool , help= '' print debugging information about paths '' , default=false )
__label__0 # create the test entity containing all the test information as a # non-indexed json blob . t_key = client.key ( `` test '' ) t_val = datastore.entity ( t_key , exclude_from_indexes= [ `` info '' ] ) t_val.update ( { `` test '' : test_name , `` start '' : start_time , `` info '' : str ( data ) } ) batch.append ( t_val )
__label__0 def _get_default_sycl_toolkit_path ( ) : return `` /opt/intel/oneapi/compiler/latest ''
__label__0 def _get_grouped_variables ( vars_to_warm_start ) : `` '' '' collects and groups ( possibly partitioned ) variables into a dictionary .
__label__0 if need_to_bind_api_args : tensor_api = lambda v : api ( v , * args , * * kwargs ) else : tensor_api = api
__label__0 def __init__ ( self , cluster ) : `` '' '' creates a ` clusterspec ` .
__label__0 keras_default_save_format_comment = ( ast_edits.warning , `` ( this warning is only applicable if the code saves a tf.keras model ) `` `` keras model.save now saves to the tensorflow savedmodel format by `` `` default , instead of hdf5 . to continue saving to hdf5 , add the `` `` argument save_format='h5 ' to the save ( ) function . '' )
__label__0 calls ` getfullargspec ` and assigns args , varargs , varkw , and defaults to a python 2/3 compatible ` argspec ` .
__label__0 > > > global_batch_size = 4 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensor_slices ( [ 1 , 2 , 3 , 4 ] ) .repeat ( ) .batch ( global_batch_size ) > > > distributed_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > print ( next ( distributed_iterator ) ) perreplica : { 0 : tf.tensor ( [ 1 2 ] , shape= ( 2 , ) , dtype=int32 ) , 1 : tf.tensor ( [ 3 4 ] , shape= ( 2 , ) , dtype=int32 ) }
__label__0 # disabling super class ' unwrapped field . unwrapped = property ( )
__label__0 if fullargspec.varkw is not none : parameters.append ( inspect.parameter ( fullargspec.varkw , inspect.parameter.var_keyword ) )
__label__0 def _yield_value ( iterable ) : return nest_util.yield_value ( nest_util.modality.core , iterable )
__label__0 def test_contrib_summary_scalar ( self ) : text = `` tf.contrib.summary.scalar ( 'foo ' , myval , 'fam ' , 42 ) '' expected = ( `` tf.compat.v2.summary.scalar ( name='foo ' , data=myval , `` `` step=42 ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'family ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 # the next one has the graph . ev = next ( rr ) ev_graph = graph_pb2.graphdef ( ) ev_graph.parsefromstring ( ev.graph_def ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_graph )
__label__0 the decorated function ( known as the `` elementwise api handler '' ) overrides the default implementation for any unary elementwise api whenever the value for the first argument ( typically named ` x ` ) matches the type annotation ` x_type ` . the elementwise api handler is called with two arguments :
__label__0 the new api is
__label__0 def _create_dummy_inputs ( self ) : return { `` sc_int '' : array_ops.sparse_placeholder ( dtypes.int32 ) , `` sc_hash '' : array_ops.sparse_placeholder ( dtypes.string ) , `` sc_keys '' : array_ops.sparse_placeholder ( dtypes.string ) , `` sc_vocab '' : array_ops.sparse_placeholder ( dtypes.string ) , `` real '' : array_ops.placeholder ( dtypes.float32 ) }
__label__0 def testcleardevicesonexport ( self ) : # test that we export a graph without its devices and run successfully . with ops_lib.graph ( ) .as_default ( ) : with ops_lib.device ( `` /job : ps/replica:0/task:0/device : gpu:0 '' ) : image = array_ops.placeholder ( dtypes.float32 , [ none , 784 ] , name= '' image '' ) label = array_ops.placeholder ( dtypes.float32 , [ none , 10 ] , name= '' label '' ) weights = variable_v1.variablev1 ( random_ops.random_uniform ( [ 784 , 10 ] ) , name= '' weights '' ) bias = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' bias '' ) logit = nn_ops.relu ( math_ops.matmul ( image , weights ) + bias ) nn_ops.softmax ( logit , name= '' prediction '' ) cost = nn_ops.softmax_cross_entropy_with_logits ( labels=label , logits=logit ) adam.adamoptimizer ( ) .minimize ( cost , name= '' optimize '' ) meta_graph_def = saver_module.export_meta_graph ( clear_devices=true ) graph_io.write_graph ( meta_graph_def , self.get_temp_dir ( ) , `` meta_graph.pbtxt '' )
__label__0 def start_loop ( self ) : self._last_time = time.time ( ) self._last_step = training_util.global_step ( self._sess , self._step_counter )
__label__0 def get_header_version ( path ) : version = int ( _get_header_version ( path , `` cuda_version '' ) ) if not version : return none return `` % d. % d '' % ( version // 1000 , version % 1000 // 10 )
__label__0 __slots__ = ( )
__label__0 - modality.data follows tf.data 's nest semantics .
__label__0 the script takes the directory specified by the rocm_path environment variable . the script looks for headers and library files in a hard-coded set of subdirectories from base path of the specified directory . if rocm_path is not specified , then `` /opt/rocm '' is used as it default value
__label__0 def func ( a , b ) : return ( a , b )
__label__0 def testdispatchwithiterableparams ( self ) : # the add_n api supports having ` inputs ` be an iterable ( and not just # a sequence ) . @ dispatch.dispatch_for_api ( math_ops.add_n , { `` inputs '' : typing.list [ maskedtensor ] } ) def masked_add_n ( inputs ) : masks = array_ops_stack.stack ( [ x.mask for x in inputs ] ) return maskedtensor ( math_ops.add_n ( [ x.values for x in inputs ] ) , math_ops.reduce_all ( masks , axis=0 ) )
__label__0 graph2 = ops_lib.graph ( ) with graph2.as_default ( ) : var_dict2 = meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden2 '' , to_scope= '' new_hidden2 '' , from_graph=graph , to_graph=graph2 ) self.assertequal ( 1 , len ( var_dict2 ) )
__label__0 def _init_ready_op ( self , ready_op=use_default , ready_for_local_init_op=use_default ) : `` '' '' initializes ready_op .
__label__0 `` ` python ... sv = supervisor ( logdir='/tmp/mydir ' ) with sv.managed_session ( flags.master ) as sess : sv.loop ( 60 , print_loss , ( sess , ) ) while not sv.should_stop ( ) : sess.run ( my_train_op ) `` `
__label__0 # ` ops.eagertensor ` subclasses ` symbol ` by way of subclassing ` tensor.tensor ` ; # care should be taken when performing ` isinstance ` checks on ` value ` , e.g . : # # `` ` # if isinstance ( core.symbol ) and not isinstance ( core.value ) : # ... # `` ` class symbol ( tensor ) : `` '' '' symbolic `` graph '' tensor .
__label__0 # remember which accumulator is on which device to set the initial step in # the accumulator to be global step . this list contains list of the # following format : ( accumulator , device ) . self._accumulator_list = [ ]
__label__0 def __dir__ ( self ) : module = self._load ( ) return dir ( module )
__label__0 args : meta_graph_def : metagraphdef protocol buffer . key : one of the graphkeys or user-defined string . export_scope : optional ` string ` . name scope to remove. `` '' '' meta_graph.add_collection_def ( meta_graph_def , key , export_scope=export_scope )
__label__0 this method currently blocks forever .
__label__0 def __repr__ ( self ) : # carefully to not trigger _load , since repr may be called in very # sensitive places . return f '' < lazyloader { self.__name__ } as { self._tfll_local_name } > ''
__label__0 # define stringify ( x ) # x # define tostring ( x ) stringify ( x )
__label__0 class sessionmanagertest ( test.testcase ) :
__label__0 def yield_value ( modality , iterable ) : `` '' '' yield elements of ` iterable ` in a deterministic order .
__label__0 import collections.abc import typing
__label__0 def testerrorconditions ( self ) : x = variable_scope.get_variable ( `` x '' , shape= [ 4 , 1 ] , initializer=ones ( ) , partitioner=lambda shape , dtype : [ 2 , 1 ] )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' tests for tensorflow.python.training.saver.py . '' '' ''
__label__0 @ end_compatibility `` '' '' graph = graph or ops.get_default_graph ( ) global_step_tensor = get_global_step ( graph ) if global_step_tensor is none : global_step_tensor = create_global_step ( graph ) return global_step_tensor
__label__0 @ test_util.run_v1_only ( `` queue-based input pipelines have been replaced by ` tf.data ` `` `` and not supported in v2 . '' ) def testaddcollectiondef ( self ) : test_dir = self._get_test_dir ( `` good_collection '' ) filename = os.path.join ( test_dir , `` metafile '' ) with self.cached_session ( ) : # creates a graph . v0 = variable_v1.variablev1 ( 1.0 , name= '' v0 '' ) cond.cond ( math_ops.less ( v0 , 10 ) , lambda : math_ops.add ( v0 , 1 ) , lambda : math_ops.subtract ( v0 , 1 ) ) while_loop.while_loop ( lambda i : math_ops.less ( i , 10 ) , lambda i : math_ops.add ( i , 1 ) , [ v0 ] ) var = variable_v1.variablev1 ( constant_op.constant ( 0 , dtype=dtypes.int64 ) ) count_up_to = var.count_up_to ( 3 ) input_queue = data_flow_ops.fifoqueue ( 30 , dtypes.float32 , shared_name= '' collection_queue '' ) qr = queue_runner_impl.queuerunner ( input_queue , [ count_up_to ] ) variables.global_variables_initializer ( ) # creates a saver . save = saver_module.saver ( { `` v0 '' : v0 } ) # adds a set of collections . ops_lib.add_to_collection ( `` int_collection '' , 3 ) ops_lib.add_to_collection ( `` float_collection '' , 3.5 ) ops_lib.add_to_collection ( `` string_collection '' , `` hello '' ) ops_lib.add_to_collection ( `` variable_collection '' , v0 ) # add queuerunners . queue_runner_impl.add_queue_runner ( qr ) # adds user_defined proto in three formats : string , bytes and any . queue_runner = queue_runner_pb2.queuerunnerdef ( queue_name= '' test_queue '' ) ops_lib.add_to_collection ( `` user_defined_string_collection '' , str ( queue_runner ) ) ops_lib.add_to_collection ( `` user_defined_bytes_collection '' , queue_runner.serializetostring ( ) ) any_buf = any ( ) any_buf.pack ( queue_runner ) ops_lib.add_to_collection ( `` user_defined_any_collection '' , any_buf )
__label__0 def _pool_seed_transformer ( parent , node , full_name , name , logs ) : `` '' '' removes seed2 and deterministic , and adds non-zero seed if needed . '' '' '' # this requires that this function uses all kwargs ( add to renames ! ) . seed_arg = none deterministic = false modified = false new_keywords = [ ]
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' verifies that a list of libraries is installed on the system .
__label__0 if not global_step_tensor.dtype.base_dtype.is_integer : raise typeerror ( 'existing `` global_step '' does not have integer type : % s ' % global_step_tensor.dtype )
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=true ) class myenum ( enum.enum ) : a = 1 b = 2
__label__0 def _find_rocrand_config ( rocm_install_path ) :
__label__0 applies ` func ( x [ 0 ] , x [ 1 ] , ... ) ` where x [ i ] enumerates all atoms in ` structure [ i ] ` . all items in ` structure ` must have the same arity , and the return value will contain results with the same structure layout .
__label__0 # tf_export generated two copies of the module objects . # this will just list compat.v2 as an alias for tf . close enough , let 's not # duplicate all the module skeleton files . tf.compat.v2 = tf
__label__0 @ test_util.run_v1_only ( `` sparseapplyftrl op returns a ref , so it is not `` `` supported in eager mode . '' ) def testsparseapplyftrldim1 ( self ) : for ( dtype , index_type , use_gpu ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ np.int32 , np.int64 ] , [ false , true ] ) : x_val = [ [ 0.0 ] , [ 0.0 ] , [ 0.0 ] ] y_val = [ [ 4.0 ] , [ 5.0 ] , [ 6.0 ] ] z_val = [ [ 0.0 ] , [ 0.0 ] , [ 0.0 ] ] x = np.array ( x_val ) .astype ( dtype ) y = np.array ( y_val ) .astype ( dtype ) z = np.array ( z_val ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) grad_val = [ [ 1.5 ] , [ 2.5 ] ] grad = np.array ( grad_val ) .astype ( dtype ) indices = np.array ( [ 0 , 2 ] ) .astype ( index_type ) self._testtypesforsparseftrl ( x , y , z , lr , grad , indices , use_gpu ) # empty sparse gradients . empty_grad = np.zeros ( [ 0 , 1 ] , dtype=dtype ) empty_indices = np.zeros ( [ 0 ] , dtype=index_type ) self._testtypesforsparseftrl ( x , y , z , lr , empty_grad , empty_indices , use_gpu )
__label__0 class reorderandrenamekeywordspec ( reorderkeywordspec , renamekeywordspec ) : `` '' '' a specification where kw2 gets moved in front of kw1 and is changed to kw3 .
__label__0 class tfdoctestoutputchecker ( doctest.outputchecker , object ) : `` '' '' customizes how ` want ` and ` got ` are compared , see ` check_output ` . '' '' ''
__label__0 raises : valueerror : if ` notice ` is empty. `` '' '' allowed_notice_types = [ 'deprecated ' , 'warning ' , 'caution ' , 'important ' , 'note ' ] if notice_type not in allowed_notice_types : raise valueerror ( f'unrecognized notice type . should be one of : { allowed_notice_types } ' )
__label__0 args : x_type : a type annotation indicating when the api handler should be called . see ` dispatch_for_api ` for a list of supported annotation types .
__label__0 returns : a group lock that can then be used to synchronize code .
__label__0 @ test_util.run_v1_only ( `` train.saver is v1 only api . '' ) def testgraphchangedforrestoreerrorraised ( self ) : checkpoint_directory = self.get_temp_dir ( ) checkpoint_prefix = os.path.join ( checkpoint_directory , `` ckpt '' )
__label__0 # restored to new shape with reshape=true with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : var = variable_v1.variablev1 ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] ) save = saver_module.saver ( reshape=true ) save.restore ( sess , save_path ) self.assertallclose ( [ [ 1.0 , 2.0 ] , [ 3.0 , 4.0 ] , [ 5.0 , 6.0 ] ] , self.evaluate ( var ) )
__label__0 @ tf_export ( `` nest.is_nested '' ) def is_nested ( seq ) : `` '' '' returns true if its input is a nested structure .
__label__0 class somethingelsewithfields ( tuple ) :
__label__0 class lazyloadingwrappertest ( test.testcase ) :
__label__0 # maybe it was a positional arg if len ( node.args ) > = 1 : _replace_scale_node ( node , node.args [ 0 ] )
__label__0 def miopen_version_numbers ( path ) : possible_version_files = [ `` include/miopen/version.h '' , # rocm 5.2 and prior `` miopen/include/miopen/version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( 'miopen version file `` { } '' not found'.format ( version_file ) ) major = _get_header_version ( version_file , `` miopen_version_major '' ) minor = _get_header_version ( version_file , `` miopen_version_minor '' ) patch = _get_header_version ( version_file , `` miopen_version_patch '' ) return major , minor , patch
__label__0 class ct2 ( composite_tensor.compositetensor ) : `` '' '' another compositetensor , used for constructing tests . '' '' ''
__label__0 < < api_list > > `` '' '' dispatcher = getattr ( api , type_based_dispatch_attr , none ) if dispatcher is none : raise valueerror ( f '' { api } does not support dispatch . '' )
__label__0 def test_brief_only ( self ) : expected = `` brief ( suffix ) \n\nwarning : go away\ninstructions '' self._check ( `` brief '' , expected ) self._check ( `` brief\n '' , expected ) self._check ( `` brief\n `` , expected ) self._check ( `` \nbrief\n `` , expected ) self._check ( `` \n brief\n `` , expected )
__label__0 def testshapechangingisolatestate ( self ) : server = self._cached_server sharing_config = config_pb2.configproto ( isolate_session_state=false ) isolate_config = config_pb2.configproto ( isolate_session_state=true )
__label__0 def _get_or_create_global_step_read ( graph=none ) : `` '' '' gets or creates global step read tensor in graph .
__label__0 def test_no_docstring ( self ) : expected = `` nothing here\n\nwarning : go away\ninstructions '' self._check ( none , expected ) self._check ( `` '' , expected )
__label__0 args : parent : parent of node . node : ast.call node to modify . full_name : full name of function to modify . name : name of function to modify . logs : list of logs to append to . arg_names : list of names of the argument to look for . arg_ok_predicate : predicate callable with the ast of the argument value , returns whether the argument value is allowed . remove_if_ok : remove the argument if present and ok as determined by arg_ok_predicate . message : message to print if a non-ok arg is found ( and hence , the function is renamed to its compat.v1 version ) .
__label__0 def sharded_filename ( self , filename_tensor , shard , num_shards ) : `` '' '' append sharding information to a filename .
__label__0 def test_undecorated_function ( ) : pass
__label__0 sess , is_loaded_from_checkpoint = self._restore_checkpoint ( master , saver , checkpoint_dir=checkpoint_dir , checkpoint_filename_with_path=checkpoint_filename_with_path , wait_for_checkpoint=wait_for_checkpoint , max_wait_secs=max_wait_secs , config=config ) if not is_loaded_from_checkpoint : if init_op is none and not init_fn and self._local_init_op is none : raise runtimeerror ( `` model is not initialized and no init_op or `` `` init_fn or local_init_op was given '' ) if init_op is not none : sess.run ( init_op , feed_dict=init_feed_dict ) if init_fn : init_fn ( sess )
__label__0 if not self._built : self.build_chunks ( ) self._fix_chunks ( ) self._built = true return self._chunks , self._chunked_message
__label__0 examples :
__label__0 self._maybe_add_module_deprecation_warning ( node , full_name , whole_name )
__label__0 class renamekeywordspec ( ast_edits.noupdatespec ) : `` '' '' a specification where kw2 gets renamed to kw3 .
__label__0 _do_not_doc_inheritable = `` _tf_docs_do_not_doc_inheritable ''
__label__0 result = { } for api , api_signatures in _type_based_dispatch_signatures.items ( ) : for _ , signatures in api_signatures.items ( ) : filtered = list ( filter ( contains_cls , signatures ) ) if filtered : result.setdefault ( api , [ ] ) .extend ( filtered ) return result
__label__0 def _rename_if_any_arg_found_transformer ( parent , node , full_name , name , logs , arg_names=none , arg_ok_predicate=none , remove_if_ok=false , message=none ) : `` '' '' replaces the given call with tf.compat.v1 if any of the arg_names is found .
__label__0 import itertools
__label__0 def testgetitem ( self ) : def func ( n ) : if n == 0 : return tf_stack.extract_stack ( ) # comment else : return func ( n - 1 )
__label__0 self.generic_visit ( node )
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 def is_tensor_tracer_arg ( self , value ) : return any ( isinstance ( x , tensortracer ) for x in self._flatten_with_slice_flattening ( value ) )
__label__0 if not os.path.isdir ( git_path ) : # no git directory spec [ `` git '' ] = false open ( os.path.join ( gen_path , `` head '' ) , `` w '' ) .write ( `` '' ) open ( os.path.join ( gen_path , `` branch_ref '' ) , `` w '' ) .write ( `` '' ) else : # git directory , possibly detached or attached spec [ `` git '' ] = true spec [ `` path '' ] = src_base_path git_head_path = os.path.join ( git_path , `` head '' ) spec [ `` branch '' ] = parse_branch_ref ( git_head_path ) link_map [ `` head '' ] = git_head_path if spec [ `` branch '' ] is not none : # attached method link_map [ `` branch_ref '' ] = os.path.join ( git_path , * os.path.split ( spec [ `` branch '' ] ) ) # create symlinks or dummy files for target , src in link_map.items ( ) : if src is none : open ( os.path.join ( gen_path , target ) , `` w '' ) .write ( `` '' ) elif not os.path.exists ( src ) : # git repo is configured in a way we do n't support such as having # packed refs . even though in a git repo , tf.__git_version__ will not # be accurate . # todo ( mikecase ) : support grabbing git info when using packed refs . open ( os.path.join ( gen_path , target ) , `` w '' ) .write ( `` '' ) spec [ `` git '' ] = false else : try : # in python 3.5 , symlink function exists even on windows . but requires # windows admin privileges , otherwise an oserror will be thrown . if hasattr ( os , `` symlink '' ) : os.symlink ( src , os.path.join ( gen_path , target ) ) else : shutil.copy2 ( src , os.path.join ( gen_path , target ) ) except oserror : shutil.copy2 ( src , os.path.join ( gen_path , target ) )
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , ( 0 , 1 ) , [ 0 , 1 ] )
__label__0 returns : the resulting code object. `` '' '' # doctest passes some dummy string as the file name , afaict # but tf.function freaks-out if this does n't look like a # python file name . del filename # doctest always passes `` single '' here , you need exec for multiple lines . del mode
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y1 = [ 10 , 20 , 30 , 40 , 50 ] y2 = np.array ( [ 10 , 20 , 30 , 40 , 50 ] ) y3 = constant_op.constant ( [ 10 , 20 , 30 , 40 , 50 ] ) y4 = variables.variable ( [ 5 , 4 , 3 , 2 , 1 ] ) if not context.executing_eagerly ( ) : self.evaluate ( variables.global_variables_initializer ( ) ) for y in [ y1 , y2 , y3 , y4 ] : z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y ) self.assertallequal ( z.mask , x.mask )
__label__0 below is a ` sequenceexample ` for a movie recommendation application recording a sequence of ratings by a user . the time-independent features ( `` locale '' , `` age '' , `` favorites '' ) describing the user are part of the context . the sequence of movies the user rated are part of the feature_lists . for each movie in the sequence we have information on its name and actors and the user 's rating . this information is recorded in three separate ` feature_list ` s . in the example below there are only two movies . all three ` feature_list ` s , namely `` movie_ratings '' , `` movie_names '' , and `` actors '' have a feature value for both movies . note , that `` actors '' is itself a ` bytes_list ` with multiple strings per movie .
__label__0 for call_saver_with_dict in { false , true } : # save partitionedvariable and restore into full variable . saved_full = _save ( partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=2 ) ) restored_full = _restore ( ) self.assertallequal ( saved_full , restored_full )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_no_date ( self , mock_warning ) : date = none instructions = `` this is how you update ... ''
__label__0 input_tree = 0 shallow_tree = [ 9 , 8 ] with self.assertraisesregex ( typeerror , expected_message ) : flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 # test restoring large tensors ( triggers a thread pool ) def testrestorelargetensors ( self ) : save_dir = self.get_temp_dir ( ) def _model ( ) : small_v = [ variable_scope.get_variable ( `` small % d '' % i , shape= [ 10 , 2 ] , use_resource=true ) for i in range ( 5 ) ] large_v = [ variable_scope.get_variable ( `` large % d '' % i , shape= [ 32000 , 1000 ] , use_resource=true ) for i in range ( 3 ) ] return small_v + large_v
__label__0 contrib_ps_strategy_warning = ( ast_edits.error , `` ( manual edit required ) `` `` tf.contrib.distribute.parameterserverstrategy has `` `` been migrated to `` `` tf.compat.v1.distribute.experimental.parameterserverstrategy ( multi `` `` machine ) and tf.distribute.experimental.centralstoragestrategy ( one `` `` machine ) . note the changes in constructors. `` + distribute_strategy_api_changes )
__label__0 args : filename : filename to write to . key_value_list : a list of `` key=value '' strings that will be added to the module 's `` build_info '' dictionary as additional entries. `` '' ''
__label__0 def _addshardedsaveopsforv2 ( self , checkpoint_prefix , per_device ) : `` '' '' add ops to save the params per shard , for the v2 format .
__label__0 sometimes we wish to apply a function to a partially flattened structure ( for example when the function itself takes structure inputs ) . we achieve this by specifying a shallow structure , ` shallow_tree ` we wish to flatten up to .
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 * run inference from a saved graph and checkpoints .
__label__0 @ property def save_path ( self ) : `` '' '' return the save path used by the supervisor .
__label__0 def _groupbydevices ( self , saveables ) : `` '' '' group variable tensor slices per device .
__label__0 def _get_full_name ( self , node ) : `` '' '' traverse an attribute node to generate a full name , e.g. , `` tf.foo.bar '' .
__label__0 # quite a lot of setup ( ) options are different if this is a collaborator package # build . we explicitly list the differences here , then unpack the dict as # options at the end of the call to setup ( ) below . for what each keyword does , # see https : //setuptools.pypa.io/en/latest/references/keywords.html . if collaborator_build : collaborator_build_dependent_options = { 'cmdclass ' : { } , 'distclass ' : none , 'entry_points ' : { } , 'headers ' : [ ] , 'include_package_data ' : none , 'packages ' : [ ] , 'package_data ' : { } , } else : collaborator_build_dependent_options = { 'cmdclass ' : { 'install_headers ' : installheaders , 'install ' : installcommand , } , 'distclass ' : binarydistribution , 'entry_points ' : { 'console_scripts ' : console_scripts , } , 'headers ' : headers , 'include_package_data ' : true , 'packages ' : find_namespace_packages ( ) , 'package_data ' : { 'tensorflow ' : [ extension_name ] + matches , } , }
__label__0 args : sync_optimizer : ` syncreplicasoptimizer ` which this hook will initialize . is_chief : ` bool ` , whether is this a chief replica or not . num_tokens : number of tokens to add to the queue. `` '' '' self._sync_optimizer = sync_optimizer self._is_chief = is_chief self._num_tokens = num_tokens
__label__0 thread_0 = self.checkedthread ( target=self._run , args= ( train_ops [ 0 ] , sessions [ 0 ] ) ) thread_1 = self.checkedthread ( target=self._run , args= ( train_ops [ 1 ] , sessions [ 1 ] ) )
__label__0 inner_decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , argspec ) outer_decorator = tf_decorator.tfdecorator ( `` , inner_decorator ) self.assertequal ( argspec , tf_inspect.getargspec ( outer_decorator ) )
__label__0 if not notice : raise valueerror ( 'the ` notice ` arg must not be empty . ' )
__label__0 `` ` python a `` `
__label__0 returns : checkpoint file name. `` '' '' name , _ = p return name
__label__0 def get_tf_dtype ( self , allowed_set=none ) : `` '' '' return a random tensorflow dtype .
__label__0 # stored metagraphdef ev = next ( rr ) ev_meta_graph = meta_graph_pb2.metagraphdef ( ) ev_meta_graph.parsefromstring ( ev.meta_graph_def ) self.assertprotoequals ( meta_graph_def , ev_meta_graph ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_meta_graph.graph_def )
__label__0 @ tf_contextlib.contextmanager def test_yield_append_before_and_after_yield ( x , before , after ) : x.append ( before ) yield x.append ( after )
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a nested structure .
__label__0 rocsolver_config = { `` rocsolver_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 def test_deprecated_missing_args ( self ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def _wait_for_glob ( self , pattern , timeout_secs , for_checkpoint=true ) : `` '' '' wait for a checkpoint file to appear .
__label__0 def do_not_generate_docs ( obj : t ) - > t : `` '' '' a decorator : do not generate docs for this object .
__label__0 import doctest
__label__0 def build_chunks ( self ) - > int : `` '' '' splits the proto , and returns the size of the chunks created . '' '' '' size_diff = 0
__label__0 def get_args ( symbol ) : if hasattr ( inspect , `` signature '' ) : signature = inspect.signature ( symbol ) # ignore * args and * * kwargs for now . return [ param.name for param in signature.parameters.values ( ) if param.kind == param.positional_or_keyword ] return tf_inspect.getargspec ( symbol ) [ 0 ]
__label__0 > > > from google.protobuf import text_format > > > example = text_format.parse ( `` ' ... features { ... feature { key : `` my_feature '' ... value { int64_list { value : [ 1 , 2 , 3 , 4 ] } } } ... } '' ' , ... tf.train.example ( ) ) > > > > > > example.features.feature [ 'my_feature ' ] .int64_list.value [ 1 , 2 , 3 , 4 ]
__label__0 def _find_rocfft_config ( rocm_install_path ) :
__label__0 @ parameterized.parameters ( { `` values '' : [ 1 , 2 , 3 ] } , { `` values '' : [ { `` b '' : 10 , `` a '' : 20 } , [ 1 , 2 ] , 3 ] } , { `` values '' : [ ( 1 , 2 ) , [ 3 , 4 ] , 5 ] } , { `` values '' : [ pointxy ( 1 , 2 ) , 3 , 4 ] } , ) @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testattrsmapstructure ( self , values ) : if attr is none : self.skiptest ( `` attr module is unavailable . '' )
__label__0 for kw in node.keywords : if sys.version_info [ :2 ] > = ( 3 , 5 ) and isinstance ( kw , ast.starred ) : pass elif kw.arg == `` seed '' : seed_arg = kw elif kw.arg == `` seed2 '' or kw.arg == `` deterministic '' : lineno = getattr ( kw , `` lineno '' , node.lineno ) col_offset = getattr ( kw , `` col_offset '' , node.col_offset ) logs.append ( ( ast_edits.info , lineno , col_offset , `` removed argument % s for function % s '' % ( kw.arg , full_name or name ) ) ) if kw.arg == `` deterministic '' : if not _is_ast_false ( kw.value ) : deterministic = true modified = true continue new_keywords.append ( kw )
__label__0 def testrename ( self ) : text = `` tf.conj ( a ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.math.conj ( a ) \n '' ) text = `` tf.rsqrt ( tf.log_sigmoid ( 3.8 ) ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.math.rsqrt ( tf.math.log_sigmoid ( 3.8 ) ) \n '' )
__label__0 expected_file_path = path + `` .pb '' self.asserttrue ( os.path.isfile ( expected_file_path ) ) self.assertequal ( returned_path , expected_file_path )
__label__0 args : last_checkpoints : a list of checkpoint filenames .
__label__0 def _find_hipsolver_config ( rocm_install_path ) :
__label__0 def gather_platform_info ( ) : `` '' '' gather platform info . '' '' '' platform_info = test_log_pb2.platforminfo ( ) ( platform_info.bits , platform_info.linkage ) = platform.architecture ( ) platform_info.machine = platform.machine ( ) platform_info.release = platform.release ( ) platform_info.system = platform.system ( ) platform_info.version = platform.version ( ) return platform_info
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 `` ` python ab_tuple = collections.namedtuple ( `` ab_tuple '' , `` a , b '' ) op_tuple = collections.namedtuple ( `` op_tuple '' , `` add , mul '' ) inp_val = ab_tuple ( a=2 , b=3 ) inp_ops = ab_tuple ( a=op_tuple ( add=1 , mul=2 ) , b=op_tuple ( add=2 , mul=3 ) ) out = map_structure_up_to ( inp_val , lambda val , ops : ( val + ops.add ) * ops.mul , inp_val , inp_ops )
__label__0 import ast import doctest import os import re import textwrap from typing import any , callable , dict , iterable , optional
__label__0 class repeatedmessagesplitter ( split.composablesplitter ) : `` '' '' splits a repeated message field on a proto . '' '' ''
__label__0 # remember which deprecation warnings have been printed already . _printed_warning = { }
__label__0 return tf.random.uniform ( shape=shape , minval=min_val , maxval=max_val , dtype=dtype , seed=seed )
__label__0 # initialize chunks and chunkedmessage ( optionally with the first chunk as # the user-provided proto . if parent_splitter is not none : # if this is not the root splitter class , skip the initialization of # the chunks/message since the parent 's will be updated instead . self._chunks = none self._chunked_message = none elif proto_as_initial_chunk : self._chunks = [ self._proto ] self._chunked_message = chunk_pb2.chunkedmessage ( chunk_index=0 ) self._add_chunk_order.append ( id ( self._proto ) ) else : self._chunks = [ ] self._chunked_message = chunk_pb2.chunkedmessage ( )
__label__0 # expected chunks ( max size = 500 ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # chunk # : contents # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 0 : graphdef # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 1 : graphdef.nodes [ 2 ] .attr [ `` value '' ] .tensor.tensor_content # -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # 2 : graphdef.nodes [ 4 ] .attr [ `` value '' ] .tensor.tensor_content # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
__label__0 @ dispatch_for_api ( api , { x_name : x_type } ) def dispatch_target ( * args , * * kwargs ) : args , kwargs , name = _extract_name_arg ( args , kwargs , name_index ) if args : x , args = args [ 0 ] , args [ 1 : ] else : x = kwargs.pop ( x_name )
__label__0 library_size = proto.library.bytesize ( ) approx_node_size = proto_size - library_size
__label__0 # save checkpoint from which to warm-start . _ , prev_bucket_val = self._create_prev_run_var ( `` linear_model/real_bucketized/weights '' , shape= [ 5 , 1 ] , initializer=norms ( ) )
__label__0 # adding s1 ( s3 should now be deleted as oldest in list ) s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s2 , s1 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s2 , s1 ] , save_dir=save_dir )
__label__0 returns : a new docstring , with the notice attached .
__label__0 @ tf_contextlib.contextmanager def silence ( ) : `` '' '' temporarily silence deprecation warnings . '' '' '' global _print_deprecation_warnings print_deprecation_warnings = _print_deprecation_warnings _print_deprecation_warnings = false yield _print_deprecation_warnings = print_deprecation_warnings
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 for shallow_branch , input_branch in zip ( _tf_core_yield_value ( shallow_tree ) , _tf_core_yield_value ( input_tree ) , ) : _tf_core_assert_shallow_structure ( shallow_branch , input_branch , check_types=check_types , expand_composites=expand_composites , )
__label__0 try : del examples except nameerror : pass
__label__0 def __str__ ( self ) : return self.string
__label__0 `` ` python class fruittracetype ( tf.types.experimental.tracetype ) : def __init__ ( self , fruit ) : self.fruit_type = type ( fruit ) self.fruit_value = fruit
__label__0 if temp_file and processed_file : new_notebook = _update_notebook ( notebook , raw_code , new_file_content.split ( `` \n '' ) ) json.dump ( new_notebook , temp_file ) else : raise syntaxerror ( `` was not able to process the file : \n % s\n '' % `` '' .join ( log ) )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for lock_util . '' '' ''
__label__0 returns : a python list , the partially flattened version of ` input_tree ` according to the structure of ` shallow_tree ` .
__label__0 def error_handler ( * args , * * kwargs ) : try : if not is_traceback_filtering_enabled ( ) : return fn ( * args , * * kwargs ) except nameerror : # in some very rare cases , # ` is_traceback_filtering_enabled ` ( from the outer scope ) may not be # accessible from inside this function return fn ( * args , * * kwargs )
__label__0 input_tree_smaller_than_shallow_tree = ( `` the input_tree has fewer items than the shallow_tree . input structure `` `` has length { input_size } , while shallow structure has length `` `` { shallow_size } . '' )
__label__0 an exception raised from the ` with ` block or one of the service threads is raised again when the block exits . this is done after stopping all threads and closing the session . for example , an ` abortederror ` exception , raised in case of preemption of one of the workers in a distributed model , is raised again when the block exits .
__label__0 results = set ( ) for doc_module in doc_string_modules : results.update ( [ m.group ( 1 ) for m in _reference_pattern.finditer ( doc_module.__doc__ ) if m.group ( 1 ) in cur_members ] ) return list ( results )
__label__0 `` ` python input_tree = [ [ ( ' a ' , 1 ) , [ ( ' b ' , 2 ) , [ ( ' c ' , 3 ) , [ ( 'd ' , 4 ) ] ] ] ] ] shallow_tree = [ [ 'level_1 ' , [ 'level_2 ' , [ 'level_3 ' , [ 'level_4 ' ] ] ] ] ]
__label__0 def after_create_session ( self , session , coord ) : # when this is called , the graph is finalized and # ops can no longer be added to the graph . print ( 'session created . ' )
__label__0 def deprecated_argument_lookup ( new_name , new_value , old_name , old_value ) : `` '' '' looks up deprecated argument name and ensures both are not used .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_positional_and_named_with_ok_vals ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 if __name__ == '__main__ ' : sys.exit ( main ( sys.argv ) )
__label__0 found_distribution = false for keyword_arg in node.keywords : if keyword_arg.arg == `` uniform '' : found_distribution = true keyword_arg.arg = `` distribution ''
__label__0 # using non-iterable elements . input_tree = 0 shallow_tree = 0 ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 def test_app_flags ( self ) : text = `` flags = tf.app.flags '' expected = `` flags = tf.compat.v1.app.flags '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 where ` x ` and ` y ` are the first two arguments to the binary elementwise assert operation , and ` assert_func ` is a tensorflow function that takes two parameters and performs the elementwise assert operation ( e.g. , ` tf.debugging.assert_equal ` ) .
__label__0 setattr ( obj , _doc_in_current_and_subclasses , none ) return obj
__label__0 def hipsolver_version_numbers ( path ) : possible_version_files = [ `` include/hipsolver/internal/hipsolver-version.h '' , # rocm 5.2 `` hipsolver/include/internal/hipsolver-version.h '' , # rocm 5.1 `` hipsolver/include/hipsolver-version.h '' , # rocm 5.0 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` hipsolver version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` hipsolverversionmajor '' ) minor = _get_header_version ( version_file , `` hipsolverversionminor '' ) patch = _get_header_version ( version_file , `` hipsolverversionpatch '' ) return major , minor , patch
__label__0 `` `` '' support for training models .
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( * , arg0 , arg1 , deprecated=none ) : return arg0 + arg1 if deprecated is not none else arg1 + arg0
__label__0 text = `` from foo.baz import a '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 if len ( repeated_msg_split ) > = 1 : total_size_diff += total_size
__label__0 def __tf_flatten__ ( self ) : `` '' '' flatten current object into ( metadata , components ) .
__label__0 def _log_deprecation ( msg , * args , * * kwargs ) : `` '' '' raises errors for deprecated methods if in strict mode , warns otherwise . '' '' '' if strict_mode.strict_mode : logging.error ( msg , * args , * * kwargs ) raise runtimeerror ( 'this behavior has been deprecated , which raises an error in strict ' ' mode . ' ) else : logging.warning ( msg , * args , * * kwargs )
__label__0 # generates a list of symbols , formatted as a list of c++ strings . with open ( os.path.join ( args.outdir , f ' { lib_name } .inc ' ) , ' w ' ) as f : sym_names = `` .join ( f ' `` { name } '' , \n ' for name in funs ) f.write ( sym_names )
__label__0 def build_headers ( output_dir ) : `` '' '' builds the headers files for tf . '' '' '' os.makedirs ( output_dir , exist_ok=true )
__label__0 class tfapichangespec ( ast_edits.apichangespec ) : `` '' '' list of maps that describe what changed in the api . '' '' ''
__label__0 # verifies savers is saved as bytes_list for meta_graph_def . collection_def = meta_graph_def.collection_def [ `` savers '' ] kind = collection_def.whichoneof ( `` kind '' ) self.assertequal ( kind , `` bytes_list '' )
__label__0 ` elementwise_api_handler ( api_func , x , y ) `
__label__0 def doc_in_current_and_subclasses ( obj : t ) - > t : `` '' '' overrides ` do_not_doc_in_subclasses ` decorator .
__label__0 class stacktracefilter ( stacktracetransform ) : `` '' '' allows filtering traceback information by removing superfluous frames . '' '' '' _stack_dict = _source_filter_stacks
__label__0 with open ( file_a , `` a '' ) as f : f.write ( `` import foo as f '' ) os.symlink ( file_a , file_b )
__label__0 def _segmentsumv2 ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.segment_sum_v2 , data , indices , num_segments )
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but ` input_tree ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` .
__label__0 args : target : the target object to inspect .
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities for proto splitter modules . '' '' ''
__label__0 class test ( object ) :
__label__0 def testwaitforsessionreturnsnoneaftertimeout ( self ) : with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , recovery_wait_secs=1 )
__label__0 args : * args : deprecated endpoint names .
__label__0 def prepare_session ( self , master : str , init_op : ops.operation = none , saver : saver_lib.saver = none , checkpoint_dir : str = none , checkpoint_filename_with_path : str = none , wait_for_checkpoint=false , max_wait_secs=7200 , config=none , init_feed_dict=none , init_fn=none , ) - > session.session : `` '' '' creates a ` session ` . makes sure the model is ready to be used .
__label__0 def begin ( self ) : `` '' '' called once before using the session .
__label__0 parser.add_argument ( `` -- git_tag_override '' , type=str , help= '' override git tag value in the __git_version__ string . useful when `` `` creating release builds before the release tag is created . '' )
__label__0 def testembeddinglookupsparse ( self ) : text = ( `` tf.nn.embedding_lookup_sparse ( params , sp_ids , sp_weights , `` `` partition_strategy , name , combiner , max_norm ) '' ) expected_text = ( `` tf.nn.embedding_lookup_sparse ( params , sp_ids , `` `` sp_weights , partition_strategy=partition_strategy , `` `` name=name , combiner=combiner , max_norm=max_norm ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def f ( a , b , kw1 , kw2 ) : ... def g ( a , b , kw1 , c , kw1_alias ) : ... def g2 ( a , b , kw1 , c , d , kw1_alias ) : ... def h ( a , kw1 , kw2 , kw1_alias , kw2_alias ) : ...
__label__0 if len ( dense_shapes ) ! = num_dense : raise valueerror ( `` len ( dense_shapes ) attribute does not match `` `` ndense attribute ( % d vs % d ) '' % ( len ( dense_shapes ) , num_dense ) )
__label__0 def func ( a=1 , b=2 ) : return ( a , b )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testnesteddataclassisnested ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) self.asserttrue ( nest.is_nested ( mt ) )
__label__0 return sv , sess , v , vadd , w
__label__0 shallow_tree = { ' a ' : none } map_structure_with_tuple_paths_up_to ( shallow_tree , print_path_and_values , lowercase , uppercase ) path : ( ' a ' , ) , values : ( ' a ' , ' a ' ) path : ( ' b ' , 0 ) , values : ( 'b0 ' , 'b0 ' ) path : ( ' b ' , 1 ) , values : ( 'b1 ' , 'b1 ' )
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a nested structure .
__label__0 > > > tf.nest.map_structure ( lambda x , y : x + y , 3 , 4 ) 7
__label__0 # pylint : disable=unused-import from tensorflow.python.summary.summary_iterator import summary_iterator from tensorflow.python.summary.writer.writer import filewriter as _filewriter from tensorflow.python.summary.writer.writer_cache import filewritercache as summarywritercache # pylint : enable=unused-import from tensorflow.python.util.deprecation import deprecated
__label__0 def _expand_variables ( input_str , cmake_vars ) : `` '' '' expands $ { variable } s and @ variable @ s in 'input_str ' , using dictionary 'cmake_vars ' .
__label__0 texts = [ `` object.foo ( ) '' , `` get_object ( ) .foo ( ) '' , `` get_object ( ) .foo ( ) '' , `` object.foo ( ) .bar ( ) '' ] for text in texts : ( _ , report , _ ) , _ = self._upgrade ( foowarningspec ( ) , text ) self.assertin ( `` not good '' , report )
__label__0 args : checkpoint_path : string , path to object-based checkpoint var_list : list of ` variables ` that appear in the checkpoint . if ` none ` , ` var_list ` will be set to all saveable objects . builder : a ` basesaverbuilder ` instance . if ` none ` , a new ` bulksaverbuilder ` will be created . names_to_keys : dict mapping string tensor names to checkpoint keys . if ` none ` , this dict will be generated from the checkpoint file . cached_saver : cached ` saver ` object with remapped variables .
__label__0 * checkpointing trained variables as the training progresses . * initializing variables on startup , restoring them from the most recent checkpoint after a crash , or wait for checkpoints to become available .
__label__0 def deprecated_wrapper ( func ) : `` '' '' deprecation decorator . '' '' '' decorator_utils.validate_callable ( func , 'deprecated_args ' )
__label__0 # pylint : disable=superfluous-parens
__label__0 def testnometagraph ( self ) : save_dir = self._get_test_dir ( `` no_meta_graph '' )
__label__0 def testchiefcanwriteevents ( self ) : logdir = self._test_dir ( `` can_write '' ) with ops.graph ( ) .as_default ( ) : summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sv = supervisor.supervisor ( is_chief=true , logdir=logdir , summary_op=none ) meta_graph_def = meta_graph.create_meta_graph_def ( ) sess = sv.prepare_or_wait_for_session ( `` '' ) sv.summary_computed ( sess , sess.run ( summ ) ) sess.close ( ) # wait to make sure everything is written to file before stopping . time.sleep ( 1 ) sv.stop ( )
__label__0 def testfullnamenode ( self ) : t = ast_edits.full_name_node ( `` a.b.c '' ) self.assertequal ( ast.dump ( t ) , `` attribute ( value=attribute ( value=name ( id= ' a ' , ctx=load ( ) ) , attr= ' b ' , `` `` ctx=load ( ) ) , attr= ' c ' , ctx=load ( ) ) '' )
__label__0 upgrade_dir = os.path.join ( self.get_temp_dir ( ) , `` foo '' ) os.mkdir ( upgrade_dir ) file_a = os.path.join ( upgrade_dir , `` a.py '' ) file_b = os.path.join ( upgrade_dir , `` b.py '' )
__label__0 # assert calling new fn with deprecated value issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 def testwithfunctoolspartial ( self ) : partial = functools.partial ( silly_example_function ) self.assertregex ( function_utils.get_func_name ( partial ) , ' < . * functools.partial . * > ' )
__label__0 from tensorflow.python.util import tf_inspect
__label__0 # exercise the first helper .
__label__0 class api_export ( object ) : # pylint : disable=invalid-name `` '' '' provides ways to export symbols to the tensorflow api . '' '' ''
__label__0 the ` run_context ` argument is a ` sessionruncontext ` that provides information about the upcoming ` run ( ) ` call : the originally requested op/tensors , the tensorflow session .
__label__0 # keywords are reordered , so we should reorder arguments too text = `` g ( a , b , x , c ) \n '' # do n't accept an output which does n't reorder c and d acceptable_outputs = [ `` g ( a , b , c , x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliasandreorderrest ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 w1 = resource_variable_ops.resourcevariable ( 0.0 , name= '' w1 '' ) w2 = resource_variable_ops.resourcevariable ( 0.0 , name= '' w2 '' )
__label__0 def get_session ( is_chief ) : g = ops.graph ( ) with g.as_default ( ) : with ops.device ( `` /job : localhost '' ) : v = variable_v1.variablev1 ( 1.0 , name= '' ready_for_local_init_op_restore_v_ '' + str ( uid ) ) vadd = v.assign_add ( 1 ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' ready_for_local_init_op_restore_w_ '' + str ( uid ) ) ready_for_local_init_op = variables.report_uninitialized_variables ( variables.global_variables ( ) ) sv = supervisor.supervisor ( logdir=logdir , is_chief=is_chief , graph=g , recovery_wait_secs=1 , ready_for_local_init_op=ready_for_local_init_op ) sess = sv.prepare_or_wait_for_session ( server.target )
__label__0 def testinitsetsdecoratorqualnametotargetqualname ( self ) : if hasattr ( tf_decorator.tfdecorator ( `` , test_function ) , '__qualname__ ' ) : self.assertequal ( 'test_function ' , tf_decorator.tfdecorator ( `` , test_function ) .__qualname__ )
__label__0 returns : the list of threads started for the ` queuerunners ` .
__label__0 < < api_list > > `` '' ''
__label__0 def testgetfullargspeconpartialwithvarkwargs ( self ) : `` '' '' tests getfullargspec .
__label__0 base_dir = os.path.abspath ( os.path.join ( os.path.dirname ( __file__ ) , ' .. / .. ' ) ) error_message = `` '' '' files with same name but different case detected in directory : { } `` '' ''
__label__0 def testdispatcherrornotcallable ( self ) : with self.assertraisesregex ( typeerror , `` expected dispatch_target to be callable '' ) : dispatch.dispatch_for_api ( math_ops.abs , { 0 : maskedtensor } ) ( `` not_callable '' )
__label__0 def testlazyloadwildcardimport ( self ) : # test that public apis are in __all__ . module = mockmodule ( 'test ' ) module._should_not_be_public = 5 apis = { 'cmd ' : ( `` , 'cmd ' ) } wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' , public_apis=apis , deprecation=false ) setattr ( wrapped_module , 'hello ' , 1 ) self.assertin ( 'hello ' , wrapped_module.__all__ ) self.assertin ( 'cmd ' , wrapped_module.__all__ ) self.assertnotin ( '_should_not_be_public ' , wrapped_module.__all__ )
__label__0 # nested dicts , ordereddicts and namedtuples . input_tree = collections.ordereddict ( [ ( `` a '' , ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) ) , ( `` c '' , { `` d '' : 3 , `` e '' : collections.ordereddict ( [ ( `` f '' , 4 ) ] ) } ) ] ) shallow_tree = input_tree ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( `` a '' , `` a '' , 0 ) , ( `` a '' , `` a '' , 1 , `` b '' ) , ( `` a '' , `` b '' ) , ( `` c '' , `` d '' ) , ( `` c '' , `` e '' , `` f '' ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ 0 , 1 , 2 , 3 , 4 ] ) shallow_tree = collections.ordereddict ( [ ( `` a '' , 0 ) , ( `` c '' , { `` d '' : 3 , `` e '' : 1 } ) ] ) ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( `` a '' , ) , ( `` c '' , `` d '' ) , ( `` c '' , `` e '' ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) , 3 , collections.ordereddict ( [ ( `` f '' , 4 ) ] ) ] ) shallow_tree = collections.ordereddict ( [ ( `` a '' , 0 ) , ( `` c '' , 0 ) ] ) ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( `` a '' , ) , ( `` c '' , ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ab_tuple ( a= [ 0 , { `` b '' : 1 } ] , b=2 ) , { `` d '' : 3 , `` e '' : collections.ordereddict ( [ ( `` f '' , 4 ) ] ) } ] )
__label__0 # ! /usr/bin/python # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # # automatically copy tensorflow binaries # # usage : # ./tensorflow/tools/ci_build/copy_binary.py -- filename # tf_nightly/tf_nightly_gpu-1.4.0.dev20170914-cp35-cp35m-manylinux1_x86_64.whl # -- new_py_ver 36 # `` '' '' copy binaries of tensorflow for different python versions . '' '' ''
__label__0 def testgetargspeconpartialkeywordargument ( self ) : `` '' '' tests getargspec on partial function that prunes some arguments . '' '' ''
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = constant_op.constant ( [ 10 , 20 , 30 , 40 , 50 ] ) z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y ) self.assertallequal ( z.mask , x.mask )
__label__0 expected_proto = `` '' '' job { name : 'ps ' tasks { key : 0 value : 'ps0:2222 ' } tasks { key : 1 value : 'ps1:2222 ' } } job { name : 'worker ' tasks { key : 1 value : 'worker1:2222 ' } } `` '' ''
__label__0 export_decorator_b = tf_export.tf_export ( 'testclassb1 ' ) export_decorator_b ( self._test_class_b ) self.assertequal ( ( 'testclassa1 ' , ) , self._test_class_a._tf_api_names ) self.assertequal ( ( 'testclassb1 ' , ) , self._test_class_b._tf_api_names ) self.assertequal ( [ 'testclassa1 ' ] , tf_export.get_v1_names ( self._test_class_a ) ) self.assertequal ( [ 'testclassb1 ' ] , tf_export.get_v1_names ( self._test_class_b ) )
__label__0 def _get_arg_names_to_ok_vals ( ) : `` '' '' returns a dict mapping arg_name to deprecatedargspec w/o position . '' '' '' d = { } for name_or_tuple in deprecated_arg_names_or_tuples : if isinstance ( name_or_tuple , tuple ) : d [ name_or_tuple [ 0 ] ] = deprecatedargspec ( -1 , true , name_or_tuple [ 1 ] ) else : d [ name_or_tuple ] = deprecatedargspec ( -1 , false , none ) return d
__label__0 self._chief_queue_runner = queue_runner.queuerunner ( sync_token_queue , [ sync_op ] ) for accum , dev in self._accumulator_list : with ops.device ( dev ) : chief_init_ops.append ( accum.set_global_step ( global_step , name= '' setglobalstep '' ) ) self.chief_init_op = control_flow_ops.group ( * ( chief_init_ops ) ) self._gradients_applied = true return train_op
__label__0 from tensorflow.python.util.tf_export import tf_export
__label__0 def func ( a=1 , b=2 ) : return ( a , b )
__label__0 from tensorflow_docs.api_generator import gen_java
__label__0 # mix old and new names text = `` h ( a , kw1=x , kw2_alias=y ) \n '' _ , new_text = self._upgrade ( removemultiplekeywordarguments ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 def _update_docstring_with_api_list ( target , api_list ) : `` '' '' replaces ` < < api_list > > ` in target.__doc__ with the given list of apis . '' '' '' lines = [ ] for func in api_list : name = tf_export_lib.get_canonical_name_for_symbol ( func , add_prefix_to_v1_names=true ) if name is not none : params = tf_inspect.signature ( func ) .parameters.keys ( ) lines.append ( f '' * ` tf . { name } ( { ' , '.join ( params ) } ) ` `` ) lines.sort ( ) target.__doc__ = target.__doc__.replace ( `` < < api_list > > '' , `` \n '' .join ( lines ) )
__label__0 nested_list = [ [ 2 ] ] nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 3 ] ) ) flattened_nmt = nest.flatten_up_to ( shallow_tree=nested_list , input_tree=nmt , check_types=false ) # expected flattened_nmt = [ tensor ( [ 3 ] ) ] self.assertallequal ( flattened_nmt [ 0 ] , [ 3 ] )
__label__0 args : output_file : output filename for the version info cc source_dir : base path of the source code git_tag_override : override the value for the git tag . this is useful for releases where we want to build the release before the git tag is created. `` '' ''
__label__0 results.entries.copyfrom ( process_benchmarks ( log_files ) ) results.run_configuration.argument.extend ( test_args ) results.machine_configuration.copyfrom ( system_info_lib.gather_machine_configuration ( ) ) return results
__label__0 # get current version information . version_file = open ( version_h , `` r '' ) for line in version_file : major_match = re.search ( `` ^ # define tf_major_version ( [ 0-9 ] + ) '' , line ) minor_match = re.search ( `` ^ # define tf_minor_version ( [ 0-9 ] + ) '' , line ) patch_match = re.search ( `` ^ # define tf_patch_version ( [ 0-9 ] + ) '' , line ) extension_match = re.search ( `` ^ # define tf_version_suffix \ '' ( . * ) \ '' '' , line ) if major_match : old_major = major_match.group ( 1 ) if minor_match : old_minor = minor_match.group ( 1 ) if patch_match : old_patch_num = patch_match.group ( 1 ) if extension_match : old_extension = extension_match.group ( 1 ) break
__label__0 return node
__label__0 def _init_local_init_op ( self , local_init_op=use_default ) : `` '' '' initializes local_init_op .
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } }
__label__0 contains a list of ` tf.train.feature ` s .
__label__0 @ classmethod def __tf_unflatten__ ( cls , metadata , components ) : mask = metadata [ 0 ] value = components [ 0 ] return maskedtensor ( mask=mask , value=value )
__label__0 args : ex : optional ` exception ` , or python ` exc_info ` tuple as returned by ` sys.exc_info ( ) ` . if this is the first call to ` request_stop ( ) ` the corresponding exception is recorded and re-raised from ` join ( ) ` . `` '' '' self._coord.request_stop ( ex=ex )
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' provides a list of renames between tensorflow 1 . * and 2.0 . '' '' '' from tensorflow.tools.compatibility import renames_v2
__label__1 def solvesudoku ( board ) : def is_valid ( row , col , num ) : for i in range ( 9 ) : if board [ row ] [ i ] == num or board [ i ] [ col ] == num or board [ ( row//3 ) * 3 + i//3 ] [ ( col//3 ) * 3 + i % 3 ] == num : return false return true def backtrack ( ) : for i in range ( 9 ) : for j in range ( 9 ) : if board [ i ] [ j ] == ' . ' : for num in map ( str , range ( 1 , 10 ) ) : if is_valid ( i , j , num ) : board [ i ] [ j ] = num if backtrack ( ) : return true board [ i ] [ j ] = ' . ' return false return true backtrack ( ) # test case board = [ [ `` 5 '' , '' 3 '' , '' . '' , '' . '' , '' 7 '' , '' . '' , '' . '' , '' . '' , '' . `` ] , [ `` 6 '' , '' . '' , '' . '' , '' 1 '' , '' 9 '' , '' 5 '' , '' . '' , '' . '' , '' . `` ] , [ `` . '' , '' 9 '' , '' 8 '' , '' . '' , '' . '' , '' . '' , '' . '' , '' 6 '' , '' . `` ] , [ `` 8 '' , '' . '' , '' . '' , '' . '' , '' 6 '' , '' . '' , '' . '' , '' . `` , '' 3 '' ] , [ `` 4 '' , '' . '' , '' . '' , '' 8 '' , '' . '' , '' 3 '' , '' . '' , '' . `` , '' 1 '' ] , [ `` 7 '' , '' . '' , '' . '' , '' . '' , '' 2 '' , '' . '' , '' . '' , '' . `` , '' 6 '' ] , [ `` . '' , '' 6 '' , '' . '' , '' . '' , '' . '' , '' . '' , '' 2 '' , '' 8 '' , '' . `` ] , [ `` . '' , '' . '' , '' . '' , '' 4 '' , '' 1 '' , '' 9 '' , '' . '' , '' . `` , '' 5 '' ] , [ `` . '' , '' . '' , '' . '' , '' . '' , '' 8 '' , '' . '' , '' . `` , '' 7 '' , '' 9 '' ] ] solvesudoku ( board ) print ( board )
__label__0 text = `` import foo '' expected_text = `` import bar as foo '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' tests for tensorflow.python.training.saver.py . '' '' ''
__label__0 def to_tensors ( self , value : any ) - > list [ core.tensor ] : `` '' '' breaks down a value of this type into tensors .
__label__0 attr = object.__getattribute__ ( self , name )
__label__0 if the arg is found , and if arg_ok_predicate is given , it is called with the ast expression representing the argument value found . if it returns true , the function is left alone .
__label__0 @ dispatch.dispatch_for_types ( some_op , customtensor ) def override_for_some_op ( x , y ) : # pylint : disable=unused-variable return x if x.score > 0 else y
__label__0 def _create_slot_var ( primary , val , scope , validate_shape , shape , dtype , * , copy_xla_sharding=false ) : `` '' '' helper function for creating a slot variable . '' '' ''
__label__0 assert self._stack [ -1 ] is node parent = self._stack [ -2 ]
__label__0 `` ` python data_list = [ [ 2 , 4 , 6 , 8 ] , [ [ 1 , 3 , 5 , 7 , 9 ] , [ 3 , 5 , 7 ] ] ] name_list = [ 'evens ' , [ 'odds ' , 'primes ' ] ] out = map_structure_up_to ( name_list , lambda name , sec : `` first_ { } _ { } '' .format ( len ( sec ) , name ) , name_list , data_list )
__label__0 def _is_callable_object ( obj ) : return hasattr ( obj , '__call__ ' ) and tf_inspect.ismethod ( obj.__call__ )
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' core tensorflow types . '' '' ''
__label__0 deprecated_arg_names = _get_arg_names_to_ok_vals ( )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassgettraverseshallowstructure ( self ) : nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) ) traverse_result = nest.get_traverse_shallow_structure ( lambda s : isinstance ( s , ( nestedmaskedtensor , maskedtensor ) ) , nmt ) self.assertisinstance ( traverse_result , nestedmaskedtensor ) self.assertequal ( traverse_result.mask , nmt.mask ) self.assertisinstance ( traverse_result.value , maskedtensor ) self.assertequal ( traverse_result.value.value , false ) nest.assert_shallow_structure ( traverse_result , nmt )
__label__0 takes a list of arguments with every two subsequent arguments being a logical tuple of ( path , check_soname ) . the path to the library and either true or false to indicate whether to check the soname field on the shared library .
__label__0 returns : an op for the chief/sync replica to fill the token queue .
__label__0 this function goes through the positional and keyword arguments to check whether a given argument was used , and if so , returns its value ( the node representing its value ) .
__label__0 self.assert_trace_line_count ( fn , count=15 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=30 , filtering_enabled=false )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for sync_replicas_optimizer.py . '' '' ''
__label__0 def visit_attribute ( self , node ) : # pylint : disable=invalid-name `` '' '' handle bare attributes i.e . [ tf.foo , tf.bar ] . '' '' '' assert self._stack [ -1 ] is node
__label__0 # need to keep track of scale because slim & keras # have different defaults found_scale = false for keyword_arg in node.keywords : if keyword_arg.arg == `` factor '' : keyword_arg.arg = `` scale '' found_scale = true if keyword_arg.arg == `` mode '' : _replace_mode ( keyword_arg , keyword_arg.value ) if keyword_arg.arg == `` uniform '' : keyword_arg.arg = `` distribution '' _replace_distribution ( keyword_arg , keyword_arg.value )
__label__0 def _call_location ( ) : `` '' '' extracts the caller filename and line number as a string .
__label__0 def save ( self , sess , save_path , global_step=none , latest_filename=none , meta_graph_suffix= '' meta '' , write_meta_graph=true , write_state=true , strip_default_attrs=false , save_debug_info=false ) : # pylint : disable=line-too-long `` '' '' saves variables .
__label__0 return re.search ( r '' \\\s * \n $ '' , code_line )
__label__0 def getdoc ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getdoc .
__label__0 if __name__ == `` __main__ '' : app.run ( main )
__label__0 # # # # structural mapping to native tf2
__label__0 returns : the docstring associated with the object .
__label__0 examples :
__label__0 import os import random import string
__label__0 fake_required_packages = [ # the depedencies here below are not actually used but are needed for # package managers like poetry to parse as they are confused by the # different architectures having different requirements . # the entries here should be a simple duplicate of those in the collaborator # build section . standard_or_nightly ( 'tensorflow-intel ' , 'tf-nightly-intel ' ) + '== ' + _version + ' ; platform_system== '' windows '' ' , ]
__label__0 # the names are different and will work . slice_saver = saver_module.saver ( { `` first '' : v1 , `` second '' : v2 } ) self.evaluate ( variables.global_variables_initializer ( ) ) # exports to meta_graph meta_graph_def = slice_saver.export_meta_graph ( filename )
__label__0 calling ` tf.debugging.disable_traceback_filtering ` disables this filtering mechanism , meaning that tensorflow exceptions stack traces will include all frames , in particular tensorflow-internal ones .
__label__0 def testglobaldispatcherconverttotensor ( self ) : original_global_dispatchers = dispatch._global_dispatchers try : tensortraceropdispatcher ( ) .register ( )
__label__0 > > > text_parts , floats = _floatextractor ( ) ( `` text 1.0 text '' ) > > > text_parts [ `` text `` , `` text '' ] > > > floats np.array ( [ 1.0 ] )
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but ` input_tree ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` . `` '' '' if modality == modality.core : return _tf_core_flatten_up_to ( shallow_tree , input_tree , check_types , expand_composites ) elif modality == modality.data : return _tf_data_flatten_up_to ( shallow_tree , input_tree ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 args : root : a python object with which to start the traversal . visit : a function taking arguments ` ( path , parent , children ) ` . will be called for each object found in the traversal. `` '' '' _traverse_internal ( root , visit , [ ] , `` )
__label__0 `` ` python ... # create a saver . saver = tf.compat.v1.train.saver ( ... variables ... ) # remember the training_op we want to run by adding it to a collection . tf.compat.v1.add_to_collection ( 'train_op ' , train_op ) sess = tf.compat.v1.session ( ) for step in range ( 1000000 ) : sess.run ( train_op ) if step % 1000 == 0 : # saves checkpoint , which by default also exports a meta_graph # named 'my-model-global_step.meta ' . saver.save ( sess , 'my-model ' , global_step=step ) `` `
__label__1 def calculate_hypotenuse ( a , b ) : return ( a * * 2 + b * * 2 ) * * 0.5
__label__0 # verifies resetting with config . # verifies that resetting target with no server times out . with self.assertraises ( errors_impl.deadlineexceedederror ) : session.session.reset ( `` grpc : //localhost:0 '' , [ `` test0 '' ] , config=config_pb2.configproto ( operation_timeout_in_ms=5 ) )
__label__0 def testgetargspeconnewclass ( self ) :
__label__0 __all__ = [ 'make_all ' , 'remove_undocumented ' , 'reveal_undocumented ' , ]
__label__0 class mysubclass ( myclass ) : pass
__label__0 # go/tf-wildcard-import from tensorflow.python.user_ops.ops.gen_user_ops import * # pylint : disable=wildcard-import from tensorflow.python.util.tf_export import tf_export
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf 2.0 upgrader in safety mode . '' '' '' import io
__label__1 import heapq class medianfinder : def __init__ ( self ) : self.min_heap = [ ] # stores the larger half of the numbers self.max_heap = [ ] # stores the smaller half of the numbers def addnum ( self , num : int ) - > none : # add the number to the appropriate heap if not self.max_heap or num < = -self.max_heap [ 0 ] : heapq.heappush ( self.max_heap , -num ) else : heapq.heappush ( self.min_heap , num ) # balance the heaps if necessary if len ( self.max_heap ) > len ( self.min_heap ) + 1 : heapq.heappush ( self.min_heap , -heapq.heappop ( self.max_heap ) ) elif len ( self.min_heap ) > len ( self.max_heap ) : heapq.heappush ( self.max_heap , -heapq.heappop ( self.min_heap ) ) def findmedian ( self ) - > float : # if the number of elements is even , return the average of the two middle elements if len ( self.max_heap ) == len ( self.min_heap ) : return ( -self.max_heap [ 0 ] + self.min_heap [ 0 ] ) / 2 else : return -self.max_heap [ 0 ] # if the number of elements is odd , return the middle element # example usage : medianfinder = medianfinder ( ) medianfinder.addnum ( 1 ) medianfinder.addnum ( 2 ) print ( medianfinder.findmedian ( ) ) # output : 1.5 medianfinder.addnum ( 3 ) print ( medianfinder.findmedian ( ) ) # output : 2.0
__label__0 # assert calls with the deprecated arguments log a warning . self.assertequal ( 12 , _fn ( 1 , 2 , kw1=3 , deprecated_arg1=2 , deprecated_arg2=4 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 # sparse features . for i in range ( num_sparse ) : key = sparse_keys [ i ] feature_config = config.feature_map [ key ] var_len_feature = feature_config.var_len_feature var_len_feature.dtype = sparse_types [ i ] .as_datatype_enum var_len_feature.indices_output_tensor_name = parse_example_op.outputs [ sparse_indices_start + i ] .name var_len_feature.values_output_tensor_name = parse_example_op.outputs [ sparse_values_start + i ] .name var_len_feature.shapes_output_tensor_name = parse_example_op.outputs [ sparse_shapes_start + i ] .name
__label__0 elif annotation is none : return make_type_checker ( type ( none ) )
__label__0 dense_types = parse_example_op.get_attr ( `` tdense '' ) num_sparse = parse_example_op.get_attr ( `` num_sparse '' ) sparse_types = parse_example_op.get_attr ( `` sparse_types '' ) ragged_value_types = parse_example_op.get_attr ( `` ragged_value_types '' ) ragged_split_types = parse_example_op.get_attr ( `` ragged_split_types '' ) dense_shapes = parse_example_op.get_attr ( `` dense_shapes '' )
__label__0 if check_types and not isinstance ( input_tree , shallow_type ) : # duck-typing means that nest should be fine with two different # namedtuples with identical name and fields . shallow_is_namedtuple = is_namedtuple ( shallow_tree , false ) input_is_namedtuple = is_namedtuple ( input_tree , false ) if shallow_is_namedtuple and input_is_namedtuple : if not same_namedtuples ( shallow_tree , input_tree ) : raise typeerror ( structures_have_mismatching_types.format ( input_type=type ( input_tree ) , shallow_type=type ( shallow_tree ) ) )
__label__0 filtered_tb = none try : return fn ( * args , * * kwargs ) except exception as e : filtered_tb = _process_traceback_frames ( e.__traceback__ ) raise e.with_traceback ( filtered_tb ) from none finally : del filtered_tb
__label__1 def is_even ( number ) : return number % 2 == 0
__label__0 args : module : tensorflow module .
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for sessionmanager . '' '' ''
__label__0 > > > g = tf.graph ( ) > > > with g.as_default ( ) : ... x = tf.compat.v1.placeholder ( tf.float32 , [ ] ) ... loss , var_list = compute_loss ( x ) ... global_step = tf.compat.v1.train.get_or_create_global_step ( ) ... global_init = tf.compat.v1.global_variables_initializer ( ) ... optimizer = tf.compat.v1.train.gradientdescentoptimizer ( 0.1 ) ... train_op = optimizer.minimize ( loss , global_step , var_list ) > > > sess = tf.compat.v1.session ( graph=g ) > > > sess.run ( global_init ) > > > print ( `` before training : '' , sess.run ( global_step ) ) before training : 0 > > > sess.run ( train_op , feed_dict= { x : 3 } ) > > > print ( `` after training : '' , sess.run ( global_step ) ) after training : 1
__label__0 def validate_callable ( func , decorator_name ) : if not hasattr ( func , '__call__ ' ) : raise valueerror ( ' % s is not a function . if this is a property , make sure ' ' @ property appears before @ % s in your source code : ' '\n\n @ property\n @ % s\ndef method ( ... ) ' % ( func , decorator_name , decorator_name ) )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_multiple_arg_values_once_each ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 returns a list of ( path , value ) pairs , where value a leaf node in the flattened tree , and path is the tuple path of that leaf in input_tree .
__label__0 # create a session with the new master and the worker . # the new master has the same task name ( '/job : master/replica:0/task:0 ' ) # as the old master , but is initiated from a different server thus has a # different incarnation . this triggers the workersession on worker with # the old master incarnation to be garbage collected .
__label__0 use case :
__label__0 example :
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import server_lib
__label__0 # # # fallback dispatch
__label__0 return _api_dispatcher.pysignaturechecker ( checkers )
__label__0 def testelementwiseapilists ( self ) : self.assertin ( math_ops.abs , dispatch.unary_elementwise_apis ( ) ) self.assertin ( math_ops.cos , dispatch.unary_elementwise_apis ( ) ) self.assertin ( math_ops.add , dispatch.binary_elementwise_apis ( ) ) self.assertin ( math_ops.multiply , dispatch.binary_elementwise_apis ( ) )
__label__0 raises : runtimeerror : if the whl file was not found `` '' ''
__label__0 # restore the metagraphdef into a new graph . with ops_lib.graph ( ) .as_default ( ) : with session.session ( ) as sess : saver = saver_module.import_meta_graph ( filename ) saver.restore ( sess , saver_ckpt )
__label__0 elif isinstance ( shallow_tree , list ) and isinstance ( input_tree , list ) : # list subclasses are considered the same , # e.g . python list vs. _listwrapper . pass
__label__0 @ property def save_model_secs ( self ) : `` '' '' return the delay between checkpoints .
__label__0 class deprecatedendpointstest ( test.testcase ) :
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenwithtuplepathsuptoincompatible ( self ) : simple_list = [ 2 ] mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( simple_list ) , input_type=type ( mt ) ) , ) : nest.flatten_with_tuple_paths_up_to ( shallow_tree=simple_list , input_tree=mt , check_types=true )
__label__0 returns : the number of tasks defined in the given job .
__label__0 def testnnerosion2d ( self ) : text = `` tf.nn.erosion2d ( v , k , s , r , p ) '' expected_text = `` tf.nn.erosion2d ( v , k , s , r , p , data_format='nhwc ' ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' generate java reference docs for tensorflow.org . '' '' '' import pathlib import shutil import subprocess import tempfile
__label__0 def testheterogeneouscomparison ( self ) : nest.assert_same_structure ( { `` a '' : 4 } , _custommapping ( a=3 ) ) nest.assert_same_structure ( _custommapping ( b=3 ) , { `` b '' : 4 } )
__label__0 def __init__ ( self , proto , repeated_field : util.fieldtypes , message_splitters : sequence [ type [ splitbasedonsize ] ] , * * kwargs , ) : `` '' '' initializer . '' '' '' super ( ) .__init__ ( proto , * * kwargs ) if not isinstance ( repeated_field , list ) : repeated_field = [ repeated_field ] self.repeated_field = repeated_field self.message_splitters = message_splitters
__label__0 # this could be done with a _rename_if_arg_not_found_transformer deprecate_partition_strategy_comment = ( ast_edits.warning , `` ` partition_strategy ` has been removed from < function name > . `` `` the 'div ' strategy will be used by default . '' )
__label__0 @ property @ deprecation.deprecated ( date , instructions ) def _prop ( self ) : `` '' '' prop doc .
__label__0 for the replicas :
__label__0 transforms : tf.uniform_unit_scaling_initializer ( factor , seed , dtype ) to tf.compat.v1.keras.initializers.variancescaling ( scale=factor , distribution= '' uniform '' , seed=seed )
__label__0 with self.session ( graph=graph2 ) as sess : saver_list2 [ 0 ] .restore ( sess , saver2_ckpt ) self.assertequal ( 2.0 , self.evaluate ( var_dict2 [ `` variable2:0 '' ] ) )
__label__0 the only change you have to do to the single program code is to indicate if the program is running as the * chief * .
__label__0 shallow_tree_has_invalid_keys = nest_util.shallow_tree_has_invalid_keys
__label__0 args : headers : a list of paths to header files . srcs_dir : target directory where headers are copied to. `` '' '' path_to_exclude = [ `` external/pypi '' , `` external/jsoncpp_git/src '' , `` local_config_cuda/cuda/_virtual_includes '' , `` local_config_tensorrt '' , `` python_x86_64 '' , `` python_aarch64 '' , `` llvm-project/llvm/ '' , ]
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' utility classes for testing checkpointing . '' '' ''
__label__0 sess_1 = session.session ( server.target ) sess_2 = session.session ( server.target )
__label__0 for i , name in enumerate ( funs ) : tramp_text = tramp_tpl.substitute ( lib_suffix=lib_name , sym=name , offset=i * ptr_size , number=i ) f.write ( tramp_text )
__label__0 feed_dict = { inp_a : ( np.random.randn ( 3 , 4 ) , np.random.randn ( 3 , 7 ) ) , inp_b : ( np.random.randn ( 3 , 4 ) , np.random.randn ( 3 , 7 ) ) }
__label__0 # the synchronization op will be executed in a queue runner which should # only be executed by one of the replicas ( usually the chief ) . self._chief_queue_runner = none
__label__0 def get_slot ( self , * args , * * kwargs ) : `` '' '' return a slot named `` name '' created for `` var '' by the optimizer .
__label__0 there are two ways to create decorators that tensorflow can introspect into . this is important for documentation generation purposes , so that function signatures are n't obscured by the ( * args , * * kwds ) signature that decorators often provide .
__label__0 def testdispatchfortypes_missingkwonly ( self ) : with self.assertraisesregex ( assertionerror , `` the decorated function 's non-default arguments must be identical to '' `` that of the overridden op . `` , ) :
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' prints rocm library and header directories and versions found on the system .
__label__0 on construction the summary writer creates a new event file in ` logdir ` . this event file will contain ` event ` protocol buffers constructed when you call one of the following functions : ` add_summary ( ) ` , ` add_session_log ( ) ` , ` add_event ( ) ` , or ` add_graph ( ) ` .
__label__0 initializers_no_dtype_comment = ( ast_edits.info , `` initializers no longer have the `` `` dtype argument in the constructor or partition_info argument in the `` `` __call__ method.\nthe calls have been converted to compat.v1 for `` `` safety ( even though they may already have been correct ) . '' )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 nvvm_path = _find_file ( base_paths , [ `` nvvm/libdevice '' , `` share/cuda '' , `` lib/nvidia-cuda-toolkit/libdevice '' , `` local/cuda/nvvm/libdevice '' , ] , `` libdevice * .10.bc '' )
__label__0 raw_code = [ ]
__label__0 return func
__label__0 summary_api_comment = ( ast_edits.info , `` the tf 1.x summary api can not be automatically migrated to tf 2.0 , so `` `` symbols have been converted to tf.compat.v1.summary . * and must be `` `` migrated manually . typical usage will only require changes to the `` `` summary writing logic , not to individual calls like scalar ( ) . `` `` for examples of the new summary api , see the effective tf 2.0 `` `` migration document or check the tf 2.0 tensorboard tutorials . '' )
__label__0 @ dispatch.dispatch_for_api ( array_ops.where_v2 , { `` condition '' : maskedtensor , `` x '' : typing.optional [ maskedtensor ] , `` y '' : typing.optional [ maskedtensor ] } ) def masked_where ( condition , x=none , y=none , name=none ) : del condition , x , y , name return `` stub ''
__label__0 def testcountnonzerochanges ( self ) : text = ( `` tf.count_nonzero ( input_tensor=input , dtype=dtype , name=name , `` `` reduction_indices=axis , keep_dims=keepdims ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) expected_text = ( `` tf.math.count_nonzero ( input=input , dtype=dtype , name=name , `` `` axis=axis , keepdims=keepdims ) \n '' ) self.assertequal ( new_text , expected_text )
__label__0 # creates 2 savers . saver0 = saver_module.saver ( { `` v0 '' : v0 } , name= '' saver0 '' ) saver1 = saver_module.saver ( { `` v1 '' : v1 } , name= '' saver1 '' ) ops_lib.add_to_collection ( `` savers '' , saver0 ) ops_lib.add_to_collection ( `` savers '' , saver1 ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 class test ( object ) :
__label__0 def _contrib_layers_l1_regularizer_transformer ( parent , node , full_name , name , logs ) : `` '' '' replace slim l1 regularizer with keras one .
__label__0 del masked_tensor_equals_2 # avoid pylint unused variable warning .
__label__0 class deprecationwrappertest ( test.testcase ) :
__label__0 args : module_name : the name of the module ( usually ` __name__ ` ) . allowed_exception_list : a list of names that should not be removed . doc_string_modules : a list of modules from which to take the docstrings . if none , then a list containing only the module named ` module_name ` is used .
__label__0 return b else : attributes_and_sizes = `` , `` .join ( [ f '' { key } : { util.format_bytes ( val.bytesize ( ) ) } '' for key , val in node.attr.items ( ) ] ) raise valueerror ( `` unable to split graphdef because at least one of the nodes `` `` individually exceeds the max size of `` f '' { util.format_bytes ( constants.max_size ( ) ) } . `` `` currently only const nodes can be further split . '' `` \nnode info : '' f '' \n\tsize : { util.format_bytes ( size ) } '' f '' \n\tname : { node.name } '' f '' \n\top : { node.op } '' f '' \n\tinputs : { node.input } '' f '' \n\top : { node.op } '' f '' \n\tdevice : { node.device } '' f '' \n\tattr ( and sizes ) : { attributes_and_sizes } '' )
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def get_const_value ( index ) : node_def = graph_def.library.function [ 0 ] .node_def [ index ] return node_def.attr [ `` value '' ] .tensor.tensor_content
__label__0 def testcolocategradientswithhessians ( self ) : text = `` tf.hessians ( ys=a , xs=b , colocate_gradients_with_ops=false ) \n '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` tf.hessians ( ys=a , xs=b ) \n '' , new_text ) self.assertin ( `` tf.hessians no longer takes '' , report )
__label__0 def isroutine ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isroutine . '' '' '' return _inspect.isroutine ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 examples :
__label__0 text = `` tf.contrib.distribute.tpustrategy '' expected = `` tf.contrib.distribute.tpustrategy '' _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` migrated to tf.distribute.tpustrategy '' , errors [ 0 ] )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 pasta.ast_utils.replace_child ( keyword_arg , old_value , new_value )
__label__0 # this version string is semver compatible , but incompatible with pip . # for pip , we will remove all '- ' characters from this string , and use the # result for pip . # also update tensorflow/tensorflow.bzl and # tensorflow/core/public/version.h _version = ' 2.18.0 '
__label__0 examples = [ # pyformat : disable ( 'simple ' , [ ( 'code ' , none ) ] , `` '' '' hello
__label__0 def test_sample_distorted_bounding_box ( self ) : text = ( `` tf.image.sample_distorted_bounding_box ( a , b , c , d , e , f , g , h , i , `` `` j ) '' ) expected = ( `` tf.image.sample_distorted_bounding_box ( a , b , c , `` `` min_object_covered=e , aspect_ratio_range=f , area_range=g , `` `` max_attempts=h , use_image_if_no_bounding_boxes=i , name=j ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 # test packing self.assertequal ( [ ( `` d '' , 3 ) , ( `` b '' , 1 ) , ( `` a '' , 0 ) , ( `` c '' , 2 ) ] , nest.pack_sequence_as ( ordered.items ( ) , ordered_items_flat ) )
__label__0 def fn ( self , x ) : del x self.assertfalse ( function_utils.has_kwargs ( foohasnokwargs ( ) .fn ) )
__label__0 more examples are available in the unit tests ( nest_test.py ) . `` '' ''
__label__0 raw tensorflow stack traces involve many internal frames , which can be challenging to read through , while not being actionable for end users . by default , tensorflow filters internal frames in most exceptions that it raises , to keep stack traces short , readable , and focused on what 's actionable for end users ( their own code ) .
__label__0 # output is : # [ ( ( 0 , 0 ) , [ 2 , 2 ] ) , # ( ( 0 , 1 ) , [ 3 , 3 ] ) , # ( ( 1 , 0 ) , [ 4 , 9 ] ) , # ( ( 1 , 1 ) , [ 5 , 5 ] ) ] # # [ ( ( 0 , 0 ) , true ) , # ( ( 0 , 1 ) , true ) , # ( ( 1 , 0 ) , false ) , # ( ( 1 , 1 ) , true ) ] `` `
__label__0 def testrecoversessionnochkptstillrunslocalinitop ( self ) : # this test checks for backwards compatibility . # in particular , we continue to ensure that recover_session will execute # local_init_op exactly once , regardless of whether the session was # successfully recovered . with ops.graph ( ) .as_default ( ) : w = variable_v1.variablev1 ( 1 , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=none , local_init_op=w.initializer ) # try to recover session from none sess , initialized = sm2.recover_session ( `` '' , saver=none , checkpoint_dir=none ) # succeeds because recover_session still run local_init_op self.assertfalse ( initialized ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( w ) )
__label__0 # a subclass of it . class subfoo ( foo ) :
__label__0 def _summary_iterator ( test_dir ) : `` '' '' reads events from test_dir/events .
__label__0 def test_contrib_summary_histogram ( self ) : text = `` tf.contrib.summary.histogram ( 'foo ' , myval , 'fam ' , 42 ) '' expected = ( `` tf.compat.v2.summary.histogram ( name='foo ' , data=myval , `` `` step=42 ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'family ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 the ` tf.train.sequenceexample ` proto can be thought of as a proto implementation of the following python type :
__label__0 sess.run ( enqueue_op ) sess.run ( dequeue_t )
__label__0 a detailed description of rmsprop .
__label__0 exceptions that indicate that the training inputs have been exhausted , ` tf.errors.outofrangeerror ` , also cause ` sv.should_stop ( ) ` to return ` true ` but are not re-raised from the ` with ` block : they indicate a normal termination .
__label__0 # if this cell ends with ` \ ` - > skip the next line is_line_split = check_line_split ( code_line )
__label__0 def __repr__ ( self ) : return `` < { } wrapping { ! r } > '' .format ( type ( self ) .__name__ , self._wrapped )
__label__0 this method is a convenience wrapper for creating a ` tf.distribute.server ` with a ` tf.train.serverdef ` that specifies a single-process cluster containing a single task in a job called ` `` local '' ` .
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 try : import tensorflow_estimator # pylint : disable= [ g-import-not-at-top , g-deprecated-tf-checker ] except importerror : tensorflow_estimator = none
__label__0 if _is_composite_tensor ( shallow_tree ) or _is_composite_tensor ( input_tree ) : if not ( ( _is_composite_tensor ( input_tree ) or _is_type_spec ( input_tree ) ) and ( _is_composite_tensor ( shallow_tree ) or _is_type_spec ( shallow_tree ) ) ) : raise typeerror ( structures_have_mismatching_types.format ( input_type=type ( input_tree ) , shallow_type=type ( shallow_tree ) ) ) # pylint : disable=protected-access type_spec_1 = ( shallow_tree if _is_type_spec ( shallow_tree ) else shallow_tree._type_spec ) ._without_tensor_names ( ) type_spec_2 = ( input_tree if _is_type_spec ( input_tree ) else input_tree._type_spec ) ._without_tensor_names ( ) # todo ( b/246356867 ) : replace the most_specific_common_supertype below # with get_structure . if hasattr ( type_spec_1 , `` _get_structure '' ) and hasattr ( type_spec_2 , `` _get_structure '' ) : result = ( type_spec_1._get_structure ( ) == type_spec_2._get_structure ( ) or none ) else : result = type_spec_1.most_specific_common_supertype ( [ type_spec_2 ] ) if result is none : raise valueerror ( `` incompatible compositetensor typespecs : % s vs. % s '' % ( type_spec_1 , type_spec_2 ) ) # pylint : enable=protected-access
__label__0 # gather_available_device_info must come before gather_gpu_devices # because the latter may access libcudart directly , which confuses # tensorflow streamexecutor . for d in gather_available_device_info ( ) : config.available_device_info.add ( ) .copyfrom ( d ) for gpu in gpu_info_lib.gather_gpu_devices ( ) : config.device_info.add ( ) .pack ( gpu )
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 args : obj : the object to print. `` '' '' if obj is not none : print ( repr ( obj ) )
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] ) self.assertequal ( math_ops.add ( x=x , y=y ) , `` stub '' )
__label__0 `` ` /parent.md # method2 /child.md # method2 `` `
__label__0 @ test_util.run_v1_only ( `` requires tf v1 variable behavior . '' ) def testpreparesessionwithpartialinitop ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) x = variable_v1.variablev1 ( 3 * v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' x '' ) # todo ( b/70206927 ) : use resourcevariables once they are handled properly . v_res = variable_v1.variablev1 ( 1 , name= '' v_res '' ) w_res = variable_v1.variablev1 ( v_res , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w_res '' ) x_res = variable_v1.variablev1 ( 3 * v_res , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' x_res '' )
__label__0 sometimes we may wish to partially flatten a nested sequence , retaining some of the nested structure . we achieve this by specifying a shallow structure , ` shallow_tree ` , we wish to flatten up to .
__label__0 most of the corrections are related to tsl/xla vendoring : these files used to be a part of source code but were moved to an external repo . to not break the tf api , we pretend that it 's still part of the it. `` '' ''
__label__0 bad_dictionary = mapping_type ( { ( 4 , 5 , ( 4 , 8 ) ) : ( `` a '' , `` b '' , ( `` c '' , `` d '' ) ) } ) with self.assertraisesregex ( valueerror , `` not unique '' ) : nest.flatten_dict_items ( bad_dictionary )
__label__0 returns : a docfilesuite containing the tests. `` '' '' if globs is none : globs = { }
__label__0 # it should get renamed and reordered even if it 's last text = `` g ( a , b , c=c , kw1_alias=x ) \n '' acceptable_outputs = [ `` g ( a , b , kw1=x , c=c ) \n '' , `` g ( a , b , c=c , kw1=x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , `` g ( a=a , b=b , c=c , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 import collections from tensorflow.python.util.tf_export import tf_export
__label__0 visit the [ tutorial ] ( https : //www.tensorflow.org/tutorials/distribute/input ) on distributed input for more examples and caveats. `` '' ''
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 class bulksaverbuilder ( basesaverbuilder ) : `` '' '' saverbuilder with support for bulk restoring multiple saveables . '' '' ''
__label__0 def testunwrapcontextmanager ( self ) : decorators , target = tf_decorator.unwrap ( test_params_and_defaults ) self.assertequal ( 1 , len ( decorators ) ) self.asserttrue ( isinstance ( decorators [ 0 ] , tf_decorator.tfdecorator ) ) self.assertequal ( 'contextmanager ' , decorators [ 0 ] .decorator_name ) self.assertfalse ( isinstance ( target , tf_decorator.tfdecorator ) )
__label__0 def testtwoserverssameport ( self ) : # starting a server with the same target as the cached server should fail . server = self._cached_server with self.assertraises ( errors_impl.unknownerror ) : _ = server_lib.server ( { `` local_2 '' : [ server.target [ len ( `` grpc : // '' ) : ] ] } )
__label__0 a sessionrunhook encapsulates a piece of reusable/composable computation that can piggyback a call to ` monitoredsession.run ( ) ` . a hook can add any ops-or-tensor/feeds to the run call , and when the run call finishes with success gets the outputs it requested . hooks are allowed to add ops to the graph in ` hook.begin ( ) ` . the graph is finalized after the ` begin ( ) ` method is called .
__label__0 returns : a ` tf.compat.v1.configproto ` . `` '' '' return config_pb2.configproto ( rpc_options=rpc_options_pb2.rpcoptions ( use_rpc_for_inprocess_master=true ) )
__label__0 raise configerror ( ' # define `` { } '' is either\n'.format ( name ) + `` not present in file { } or\n '' .format ( path ) + `` its value is not an integer literal '' )
__label__0 def __enter__ ( self ) : # any given instance is assumed to be used by a single thread , which reduces # expensive thread local lookups . if self._thread_key is none : self._thread_key = _get_thread_key ( ) else : assert self._thread_key == _get_thread_key ( ) , 'shared across threads ? '
__label__0 applies ` func ( path , x [ 0 ] , x [ 1 ] , ... , * * kwargs ) ` where x [ i ] is an entry in ` structure [ i ] ` and ` path ` is the common path to x [ i ] in the structures . all structures in ` structure ` must have the same arity , and the return value will contain the results with the same structure layout . special kwarg ` check_types ` determines whether the types of iterables within the structure must be the same -- see * * kwargs definition below .
__label__0 class validatecallabletest ( test.testcase ) :
__label__0 2. for a nested python tuple :
__label__0 # verifies round trip from proto- > spec- > proto is correct . cluster_spec = server_lib.clusterspec ( cluster_def ) self.assertprotoequals ( cluster_def , cluster_spec.as_cluster_def ( ) )
__label__0 for an example , see ` versionedtfimport ` . `` '' ''
__label__0 returns : ` saver ` .
__label__0 # initially all variables are initialized . for sess in [ sharing_sess_0 , sharing_sess_1 , isolate_sess_0 , isolate_sess_1 ] : with self.assertraises ( errors_impl.failedpreconditionerror ) : sess.run ( v )
__label__0 def testcreatezerosslotfromdynamicshapedvariable ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : dyn_shape = constant_op.constant ( [ 2 ] , dtype=dtypes.int32 ) dyn_shape = array_ops.placeholder_with_default ( dyn_shape , shape= [ none ] ) v = variable_scope.get_variable ( `` var '' , initializer=random_ops.random_uniform ( dyn_shape , dtype=dtypes.float64 ) , validate_shape=false ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 )
__label__0 # resets container `` local1 '' . verifies that v1 is no longer initialized . session.session.reset ( server1.target , [ `` local1 '' ] ) _ = session.session ( server1.target ) with self.assertraises ( errors_impl.failedpreconditionerror ) : self.evaluate ( v1 ) # verifies that v0 is still valid . _ = session.session ( server0.target ) self.assertallequal ( 1.0 , self.evaluate ( v0 ) )
__label__0 args : context : a context reserved for internal/future usage .
__label__0 also drops the scope argument. `` '' '' def _replace_scale_node ( parent , old_value ) : `` '' '' replaces old_value with 0.5 * ( old_value ) . '' '' '' half = ast.num ( n=0.5 ) half.lineno = 0 half.col_offset = 0 new_value = ast.binop ( left=half , op=ast.mult ( ) , right=old_value ) # this copies the prefix and suffix on old_value to new_value . pasta.ast_utils.replace_child ( parent , old_value , new_value )
__label__0 args : graph : the graph in which to create the global step tensor . if missing , use default graph .
__label__0 def __call__ ( self , path , parent , children ) : self.symbols.add ( path ) self.last_parent = parent self.last_children = list ( children ) # make a copy to preserve state .
__label__0 def request_stop ( self ) : `` '' '' sets stop requested field .
__label__0 # exercise the first helper .
__label__0 importrename = collections.namedtuple ( `` importrename '' , [ `` new_name '' , `` excluded_prefixes '' ] )
__label__0 # pylint : disable=wildcard-import # training data protos . from tensorflow.core.example.example_pb2 import * from tensorflow.core.example.feature_pb2 import * from tensorflow.core.protobuf.saver_pb2 import *
__label__0 @ deprecation.deprecated ( none , `` please switch to tf.train.monitoredtrainingsession '' ) def __init__ ( self , graph=none , ready_op=use_default , ready_for_local_init_op=use_default , is_chief=true , init_op=use_default , init_feed_dict=none , local_init_op=use_default , logdir=none , summary_op=use_default , saver=use_default , global_step=use_default , save_summaries_secs=120 , save_model_secs=600 , recovery_wait_secs=30 , stop_grace_secs=120 , checkpoint_basename= '' model.ckpt '' , session_manager=none , summary_writer=use_default , init_fn=none , local_init_run_options=none ) : `` '' '' create a ` supervisor ` .
__label__0 text = `` slim.xavier_initializer ( true or false ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution= ( \ '' uniform\ '' if true or false else `` `` \ '' truncated_normal\ '' ) ) \n '' , )
__label__0 self.assertequal ( 1 , sess0.run ( w0 ) ) self.assertequal ( 2 , sess1.run ( vadd1 ) ) self.assertequal ( 1 , sess1.run ( w1 ) ) self.assertequal ( 2 , sess0.run ( v0 ) )
__label__0 # sync_op will be assigned to the same device as the global step . with ops.device ( global_step.device ) , ops.name_scope ( `` '' ) : update_op = self._opt.apply_gradients ( aggregated_grads_and_vars , global_step )
__label__0 class keraslazyloader ( lazyloader ) : `` '' '' lazyloader that handles routing to different keras version . '' '' ''
__label__0 def most_specific_common_supertype ( self , other ) : # either the value is the same or other has a generalized value that # can represent any specific ones . if self.value == other.value : return self.value else : return dimension ( none ) `` ` `` '' ''
__label__0 `` ` def foo ( x : tensorlike ) : pass `` `
__label__0 `` `` '' given a .so file , lists symbols that should be included in a stub .
__label__0 with self.assertraisesregex ( assertionerror , `` dispatching not enabled for '' ) :
__label__0 x = tensortracer ( `` x '' ) trace = x [ 0 ] self.assertequal ( str ( trace ) , `` __operators__.getitem ( x , 0 ) '' )
__label__0 def testcreateslotfromvariable ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) slot = slot_creator.create_slot ( v , initialized_value ( v ) , name= '' slot '' )
__label__0 _write_version = saver_pb2.saverdef.v1
__label__0 for a tracetype instance , the number of tensors generated for corresponding value should be constant .
__label__0 `` `` '' standard functions for creating slots .
__label__0 text = `` % s ( a , b ) \n '' % decay _ , report , unused_errors , _ = self._upgrade ( text ) self.assertin ( `` switch to the schedules in `` `` ` tf.keras.optimizers.schedules ` `` , report )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 def test_assert_equal_graph_def ( self ) : text = ( `` tf.test.assert_equal_graph_def ( a , b , checkpoint_v2=x , `` `` hash_table_shared_name=y ) '' ) expected = `` tf.test.assert_equal_graph_def ( actual=a , expected=b ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 def _infer_var_name ( var ) : `` '' '' returns name of the ` var ` .
__label__0 is_varargs_deprecated = arg_spec.varargs in deprecated_arg_names is_kwargs_deprecated = arg_spec.varkw in deprecated_arg_names
__label__0 raises : typeerror : if 'names_to_saveables ' is not a dictionary mapping string keys to variable tensors . valueerror : if any of the keys or values in 'names_to_saveables ' is not unique. `` '' '' return self._build_internal ( names_to_saveables=names_to_saveables , reshape=reshape , sharded=sharded , max_to_keep=max_to_keep , keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours , name=name , restore_sequentially=restore_sequentially , filename=filename )
__label__0 # create a third helper , with the same configuration but no knowledge of # previous checkpoints . save3 = saver_module.saver ( saver_def=save.as_saver_def ( ) )
__label__0 # split module aliases into the ones that require import update # and those that do n't . for e.g . if we want to rename `` a '' to `` b '' # unless we import `` a.c '' in the following : # from a import c , d # we want to update import for `` d '' but not for `` c '' . updated_aliases = [ ] same_aliases = [ ] for import_alias in node.names : full_module_name = `` % s. % s '' % ( from_import , import_alias.name ) if excluded_from_module_rename ( full_module_name , import_rename_spec ) : same_aliases.append ( import_alias ) else : updated_aliases.append ( import_alias )
__label__0 def _add_dispatch_for_unary_elementwise_api ( api , x_type , elementwise_api_handler ) : `` '' '' registers a unary elementwise handler as a dispatcher for a given api . '' '' '' api_signature = tf_inspect.signature ( api ) x_name = list ( api_signature.parameters ) [ 0 ] name_index = _find_name_index ( api_signature )
__label__0 self.assertequal ( 3 , double_wrapped_fn ( 3 ) ) # pylint : disable=no-value-for-parameter self.assertequal ( 3 , double_wrapped_fn ( a=3 ) ) # pylint : disable=no-value-for-parameter
__label__0 # the custom page will be used for raw_ops.md not the one generated above . doc_controls.set_custom_page_builder_cls ( tf.raw_ops , rawopspageinfo )
__label__0 raises : typeerror : if ` shallow_tree ` is a sequence but ` input_tree ` is not . typeerror : if the sequence types of ` shallow_tree ` are different from ` input_tree ` . only raised if ` check_types ` is ` true ` . valueerror : if the sequence lengths of ` shallow_tree ` are different from ` input_tree ` . `` '' '' if modality == modality.core : _tf_core_assert_shallow_structure ( shallow_tree , input_tree , check_types , expand_composites ) elif modality == modality.data : _tf_data_assert_shallow_structure ( shallow_tree , input_tree , check_types ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassyieldflatpaths ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt_flat_paths = list ( nest.yield_flat_paths ( mt ) ) self.assertequal ( mt_flat_paths , [ ( 0 , ) ] )
__label__0 import os import re import shlex import subprocess import tempfile import time
__label__0 def _assert_cols_to_vars ( self , cols_to_vars , cols_to_expected_values , sess ) : for col , expected_values in cols_to_expected_values.items ( ) : for i , var in enumerate ( cols_to_vars [ col ] ) : self.assertallclose ( expected_values [ i ] , var.eval ( sess ) )
__label__0 > > > a = [ 24 , 76 , `` ab '' ] > > > tf.nest.map_structure ( lambda p : p * 2 , a ) [ 48 , 152 , 'abab ' ]
__label__0 def _rename_if_arg_found_transformer ( parent , node , full_name , name , logs , arg_name=none , arg_ok_predicate=none , remove_if_ok=false , message=none ) : `` '' '' replaces the given call with tf.compat.v1 if the given arg is found .
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( all_linear_cols , _partitioner ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=prev_vocab_path ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * ( sc_keys|sc_vocab ) . * '' , var_name_to_vocab_info= { ws_util._infer_var_name ( cols_to_vars [ sc_vocab ] ) : vocab_info } , var_name_to_prev_var_name= { ws_util._infer_var_name ( cols_to_vars [ sc_keys ] ) : `` some_other_name '' } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . var corresponding to # sc_hash should not be warm-started . var corresponding to sc_vocab # should be correctly warm-started after vocab remapping . self._assert_cols_to_vars ( cols_to_vars , { sc_keys : np.split ( prev_keys_val , 2 ) , sc_hash : [ np.zeros ( [ 8 , 1 ] ) , np.zeros ( [ 7 , 1 ] ) ] , sc_vocab : [ np.array ( [ [ 3 . ] , [ 2 . ] , [ 1 . ] ] ) , np.array ( [ [ 0.5 ] , [ 0 . ] , [ 0 . ] ] ) ] } , sess )
__label__1 def remove_punctuation ( text ) : return `` '' .join ( char for char in text if char.isalnum ( ) )
__label__0 def _write_vocab ( self , string_values , file_name ) : vocab_file = os.path.join ( self.get_temp_dir ( ) , file_name ) with open ( vocab_file , `` w '' ) as f : f.write ( `` \n '' .join ( string_values ) ) return vocab_file
__label__0 the order of tuples produced matches that of ` nest.flatten ` . this allows you to flatten a nested structure while keeping information about where in the structure each atom was located . see ` nest.yield_flat_paths ` for more information .
__label__0 # list of file paths containing build files that should not be included for the # pip smoke test . build_denylist = [ `` tensorflow/lite '' , `` tensorflow/compiler/mlir/lite '' , `` tensorflow/compiler/mlir/tfrt '' , `` tensorflow/core/runtime_fallback '' , `` tensorflow/core/tfrt '' , `` tensorflow/python/kernel_tests/signal '' , `` tensorflow/examples '' , `` tensorflow/tools/android '' , `` tensorflow/tools/toolchains '' , `` tensorflow/python/autograph/tests '' , `` tensorflow/python/eager/benchmarks '' , ]
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor } , { `` y '' : maskedtensor } ) def masked_add ( x , y , name=none ) : with ops.name_scope ( name ) : x_values = x.values if isinstance ( x , maskedtensor ) else x x_mask = x.mask if isinstance ( x , maskedtensor ) else true y_values = y.values if isinstance ( y , maskedtensor ) else y y_mask = y.mask if isinstance ( y , maskedtensor ) else true return maskedtensor ( x_values + y_values , x_mask & y_mask )
__label__0 args : proto : parent proto of any message type . fields : list of string/int/map key fields , e.g . [ `` nodes '' , `` attr '' , `` value '' ] can represent ` proto.nodes.attr [ `` value '' ] ` .
__label__0 * a single python list :
__label__0 - maintain a moving ( discounted ) average of the square of gradients - divide gradient by the root of this average
__label__0 # if some imports had to stay the same , add another import for them . additional_import_log = `` '' if same_aliases : same_node = ast.importfrom ( from_import , same_aliases , node.level , col_offset=node.col_offset , lineno=node.lineno ) ast.copy_location ( same_node , node ) parent.body.insert ( parent.body.index ( updated_node ) , same_node ) # apply indentation to new node . pasta.base.formatting.set ( same_node , `` prefix '' , pasta.base.formatting.get ( updated_node , `` prefix '' ) ) additional_import_log = `` and % r '' % pasta.dump ( same_node )
__label__0 def main ( ) : try : for key , value in sorted ( find_cuda_config ( ) .items ( ) ) : print ( `` % s : % s '' % ( key , value ) ) except configerror as e : sys.stderr.write ( str ( e ) + '\n ' ) sys.exit ( 1 )
__label__0 if fullargspec.kwonlydefaults is not none : defaults.update ( fullargspec.kwonlydefaults )
__label__1 def convert_to_celsius ( fahrenheit ) : return ( fahrenheit - 32 ) * 5 / 9
__label__0 def _patch_compile ( source , filename , mode , flags=0 , dont_inherit=false , optimize=-1 ) : `` '' '' patch ` doctest.compile ` to make doctest to behave like a notebook .
__label__0 `` ` python x = [ 1 ] y = [ 1 ]
__label__0 # this test does not test much . def testbasics ( self ) : logdir = self._test_dir ( `` basics '' ) with ops.graph ( ) .as_default ( ) : my_op = constant_op.constant ( 1.0 ) sv = supervisor.supervisor ( logdir=logdir ) sess = sv.prepare_or_wait_for_session ( `` '' ) for _ in range ( 10 ) : self.evaluate ( my_op ) sess.close ( ) sv.stop ( )
__label__0 args : tensors : an iterator from which the tensors can be pulled .
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' unit tests for tf_decorator . '' '' ''
__label__0 # # # # registered apis
__label__0 def tuple_path_sum ( tuple_path , * tensors ) : return ( tuple_path , sum ( tensors ) )
__label__0 now if we try calling it again : `` ` python get_mixed_flavor ( apple ( ) , mango ( ) ) # traces a new concrete function get_mixed_flavor ( apple ( ) , mango ( ) ) # re-uses the traced concrete function `` ` `` '' ''
__label__0 the method returns the path prefix of the newly created checkpoint files . this string can be passed directly to a call to ` restore ( ) ` .
__label__0 args : node : the ast.call node to check arg values for .
__label__0 class noopsplittertest ( test.testcase ) :
__label__0 for example , to set the device filters for a parameter server cluster :
__label__0 args : dirpath : directory pathname
__label__0 args : target : a callable to be wrapped in a contextmanager . returns : a callable that can be used inside of a ` with ` statement. `` '' '' context_manager = _contextlib.contextmanager ( target ) return tf_decorator.make_decorator ( target , context_manager , 'contextmanager ' )
__label__0 @ property def shape ( self ) : pass
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' locking related utils . '' '' ''
__label__0 if modified : node.keywords = new_keywords return node else : return
__label__0 # all versions of tf need these packages . we indicate the widest possible range # of package releases possible to be as up-to-date as possible as well as to # accomodate as many pre-installed packages as possible . # for packages that do n't have yet a stable release , we pin using ` ~= 0.x ` which # means we accept any ` 0.y ` version ( y > = x ) but not the first major release . we # will need additional testing for that . # note : this assumes that all packages follow semver . if a package follows a # different versioning scheme ( e.g. , pvp ) , we use different bound specifier and # comment the versioning scheme . required_packages = [ 'absl-py > = 1.0.0 ' , 'astunparse > = 1.6.0 ' , 'flatbuffers > = 24.3.25 ' , 'gast > =0.2.1 , ! =0.5.0 , ! =0.5.1 , ! =0.5.2 ' , 'google_pasta > = 0.1.1 ' , 'h5py > = 3.10.0 ' , 'libclang > = 13.0.0 ' , 'ml_dtypes ~= 0.3.1 ' , # todo ( b/304751256 ) : adjust the numpy pin to a single version , when ready 'numpy > = 1.23.5 , < 2.0.0 ; python_version < = `` 3.11 '' ' , 'numpy > = 1.26.0 , < 2.0.0 ; python_version > = `` 3.12 '' ' , 'opt_einsum > = 2.3.2 ' , 'packaging ' , # pylint : disable=line-too-long ( 'protobuf > =3.20.3 , < 5.0.0dev , ! =4.21.0 , ! =4.21.1 , ! =4.21.2 , ! =4.21.3 , ! =4.21.4 , ! =4.21.5 ' ) , 'requests > = 2.21.0 , < 3 ' , 'setuptools ' , 'six > = 1.12.0 ' , 'termcolor > = 1.1.0 ' , 'typing_extensions > = 3.6.6 ' , 'wrapt > = 1.11.0 ' , # todo ( b/305196096 ) : remove the < 3.12 condition once the pkg is updated 'tensorflow-io-gcs-filesystem > = 0.23.1 ; python_version < `` 3.12 '' ' , # grpcio does not build correctly on big-endian machines due to lack of # boringssl support . # see https : //github.com/tensorflow/tensorflow/issues/17882 . 'grpcio > = 1.24.3 , < 2.0 ' if sys.byteorder == 'little ' else none , # tensorflow exposes the tf api for certain tf ecosystem packages like # keras . when tf depends on those packages , the package version needs to # match the current tf version . for tf_nightly , we install the nightly # variant of each package instead , which must be one version ahead of the # current release version . these also usually have `` alpha '' or `` dev '' in their # version name . during the tf release process the version of these # dependencies on the release branch is updated to the stable releases ( rc # or final ) . for example , 'keras-nightly ~= 2.14.0.dev ' will be replaced by # 'keras > = 2.14.0rc0 , < 2.15 ' on the release branch after the branch cut . 'tb-nightly ~= 2.17.0.a ' , 'keras-nightly > = 3.2.0.dev ' , ] required_packages = [ p for p in required_packages if p is not none ]
__label__0 had_keras = any ( name == `` keras '' for name , child in children ) has_keras = any ( name == `` keras '' for name , child in new_children )
__label__0 class grouplocktest ( test.testcase , parameterized.testcase ) :
__label__0 def testallowempty ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` allow_empty '' ) # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : _ = constant_op.constant ( 1 ) save = saver_module.saver ( allow_empty=true ) val = save.save ( sess , save_path ) self.assertisnone ( val ) with ops_lib.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : save = saver_module.saver ( allow_empty=true ) save.restore ( sess , save_path )
__label__0 if_shallow_is_seq_input_must_be_seq = ( `` if shallow structure is a sequence , input must also be a sequence. `` `` input has type : { } . '' )
__label__0 # make change instead uniform_unit_scaling_initializer_comment = ( ast_edits.error , `` uniform_unit_scaling_initializer has been removed . please use '' `` tf.initializers.variance_scaling instead with distribution=uniform `` `` to get equivalent behaviour . '' )
__label__0 keras_api_name = 'keras ' tensorflow_api_name = 'tensorflow '
__label__0 if user called ` monitoredsession.run ( fetches=a , feed_dict=b ) ` , then this field is equal to sessionrunargs ( a , b ) .
__label__0 args : structure : substructure ( tuple of elements and/or tuples ) to mimic flat : flattened values to output substructure for . index : index at which to start reading from flat .
__label__0 @ compatibility ( eager ) ` supervisor ` s are not supported when eager execution is enabled . @ end_compatibility `` '' '' if context.executing_eagerly ( ) : raise runtimeerror ( `` supervisors are incompatible with eager execution . '' ) # set default values of arguments . if graph is none : graph = ops.get_default_graph ( ) with graph.as_default ( ) : self._init_ready_op ( ready_op=ready_op , ready_for_local_init_op=ready_for_local_init_op ) self._init_init_op ( init_op=init_op , init_feed_dict=init_feed_dict ) self._init_local_init_op ( local_init_op=local_init_op ) self._init_saver ( saver=saver ) self._init_summary_op ( summary_op=summary_op ) self._init_global_step ( global_step=global_step ) self._graph = graph self._meta_graph_def = meta_graph.create_meta_graph_def ( graph_def=graph.as_graph_def ( add_shapes=true ) , saver_def=self._saver.saver_def if self._saver else none ) self._is_chief = is_chief self._coord = coordinator.coordinator ( ) self._recovery_wait_secs = recovery_wait_secs self._stop_grace_secs = stop_grace_secs self._init_fn = init_fn self._local_init_run_options = local_init_run_options
__label__0 args : obj : a function , partial function , or callable object , possibly decorated .
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) ) self.asserttrue ( nest.is_nested ( nmt ) )
__label__0 def assert_shallow_structure ( modality , shallow_tree , input_tree , check_types=true , expand_composites=false , ) : `` '' '' asserts that ` shallow_tree ` is a shallow structure of ` input_tree ` .
__label__0 featurelists.__doc__ = `` '' '' \ mainly used as part of a ` tf.train.sequenceexample ` .
__label__0 # todo ( edloper ) : consider using a mechanism like this to automatically add # the ` name ` argument to all tensorflow apis that are implemented in python # ( so each python function does n't need to do it manually ) . def _add_name_scope_wrapper ( func , api_signature ) : `` '' '' wraps ` func ` to expect a `` name '' arg , and use it to call ` ops.name_scope ` .
__label__0 returns : a timestamp. `` '' '' return self._save_model_secs
__label__0 def __dir__ ( self ) : module = self._load ( ) return dir ( module )
__label__0 def _find_cusolver_config ( base_paths , required_version , cuda_version ) :
__label__0 > > > a = { `` hello '' : 24 , `` world '' : 76 } > > > tf.nest.map_structure ( lambda p : p * 2 , a ) { 'hello ' : 48 , 'world ' : 152 }
__label__0 if true , ` monitoredsession ` stops iterations . returns : a ` bool ` `` '' '' return self._stop_requested
__label__0 decorated = test_decorator_name ( test_wrapper ) decorator = getattr ( decorated , '_tf_decorator ' ) self.assertequal ( 'test_decorator_name ' , decorator.decorator_name )
__label__0 # at this step , the token queue is empty . so the 2 workers need to work # together to proceed . threads = [ ] threads.append ( self.checkedthread ( target=self._run , args= ( train_ops [ 0 ] , sessions [ 0 ] ) ) ) threads.append ( self.checkedthread ( target=self._run , args= ( train_ops [ 1 ] , sessions [ 1 ] ) ) )
__label__0 class multiplecontainerstest ( test.testcase ) :
__label__0 import os import sys import tempfile
__label__0 for k , v in result.items ( ) : if k.endswith ( `` _dir '' ) or k.endswith ( `` _path '' ) : result [ k ] = _normalize_path ( v )
__label__0 message = textwrap.dedent ( `` '' '' \n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # check the documentation ( https : //www.tensorflow.org/community/contribute/docs_ref ) on how to write testable docstrings . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # '' '' '' )
__label__0 example usage : $ bazel run -c opt @ local_tsl//third_party/implib_so : get_symbols /usr/local/cuda/lib64/libcudart.so > third_party/tsl/tsl/cuda/cudart.symbols `` '' ''
__label__0 * serialize a graph along with other python objects such as ` queuerunner ` , ` variable ` into a ` metagraphdef ` .
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` apple '' , `` banana '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_output_layer = variable_scope.get_variable ( `` fruit_output_layer '' , shape= [ 4 , 3 ] , initializer= [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) ws_util._warm_start_var_with_vocab ( fruit_output_layer , new_vocab_path , current_vocab_size=3 , prev_ckpt=self.get_temp_dir ( ) , prev_vocab_path=prev_vocab_path , axis=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.asserttrue ( isinstance ( fruit_output_layer , variables.partitionedvariable ) ) fruit_output_layer_vars = fruit_output_layer._get_variable_list ( ) self.assertallclose ( [ [ 0.3 , 0.5 , 0 . ] , [ 0.8 , 1.0 , 0 . ] ] , fruit_output_layer_vars [ 0 ] .eval ( sess ) ) self.assertallclose ( [ [ 1.2 , 1.5 , 0 . ] , [ 2.3 , 2. , 0 . ] ] , fruit_output_layer_vars [ 1 ] .eval ( sess ) )
__label__0 test_adjusted_name = name gpu_config = gpu_info_lib.gather_gpu_devices ( ) if gpu_config : gpu_name = gpu_config [ 0 ] .model gpu_short_name_match = re.search ( r '' ( tesla|nvidia ) ( k40|k80|p100|v100|a100 ) '' , gpu_name ) if gpu_short_name_match : gpu_short_name = gpu_short_name_match.group ( 0 ) test_adjusted_name = name + `` | '' + gpu_short_name.replace ( `` `` , `` _ '' )
__label__0 def test_reads_before_increments ( self ) : with ops.graph ( ) .as_default ( ) : training_util.create_global_step ( ) read_tensor = training_util._get_or_create_global_step_read ( ) inc_op = training_util._increment_global_step ( 1 ) inc_three_op = training_util._increment_global_step ( 3 ) with monitored_session.monitoredtrainingsession ( ) as sess : read_value , _ = sess.run ( [ read_tensor , inc_op ] ) self.assertequal ( 0 , read_value ) read_value , _ = sess.run ( [ read_tensor , inc_three_op ] ) self.assertequal ( 1 , read_value ) read_value = sess.run ( read_tensor ) self.assertequal ( 4 , read_value )
__label__0 self._learning_rate_tensor = ops.convert_to_tensor ( lr , name= '' learning_rate '' ) self._decay_tensor = ops.convert_to_tensor ( decay , name= '' decay '' ) self._momentum_tensor = ops.convert_to_tensor ( momentum , name= '' momentum '' ) self._epsilon_tensor = ops.convert_to_tensor ( epsilon , name= '' epsilon '' )
__label__0 sometimes we wish to apply a function to a partially flattened structure ( for example when the function itself takes structure inputs ) . we achieve this by specifying a shallow structure , ` shallow_tree ` we wish to flatten up to .
__label__0 build_info = collections.ordereddict ( % s ) `` '' '' % sorted_build_info_pairs open ( filename , `` w '' ) .write ( contents )
__label__0 raises : typeerror : if ` cluster ` is not a dictionary mapping strings to lists of strings , and not a ` tf.train.clusterdef ` protobuf. `` '' '' if isinstance ( cluster , dict ) : self._cluster_spec = { } for job_name , tasks in cluster.items ( ) : if isinstance ( tasks , ( list , tuple ) ) : job_tasks = { i : task for i , task in enumerate ( tasks ) } elif isinstance ( tasks , dict ) : job_tasks = { int ( i ) : task for i , task in tasks.items ( ) } else : raise typeerror ( `` the tasks for job % r must be a list or a dictionary `` `` from integers to strings . '' % job_name ) self._cluster_spec [ job_name ] = job_tasks self._make_cluster_def ( ) elif isinstance ( cluster , cluster_pb2.clusterdef ) : self._cluster_def = cluster self._cluster_spec = { } for job_def in self._cluster_def.job : self._cluster_spec [ job_def.name ] = { i : t for i , t in job_def.tasks.items ( ) } elif isinstance ( cluster , clusterspec ) : self._cluster_def = cluster_pb2.clusterdef ( ) self._cluster_def.mergefrom ( cluster.as_cluster_def ( ) ) self._cluster_spec = { } for job_def in self._cluster_def.job : self._cluster_spec [ job_def.name ] = { i : t for i , t in job_def.tasks.items ( ) } else : raise typeerror ( `` ` cluster ` must be a dictionary mapping one or more `` `` job names to lists of network addresses , or a `` `` ` clusterdef ` protocol buffer '' )
__label__0 returns : a ` saverdef ` protocol buffer. `` '' '' if export_scope is none : return self.saver_def
__label__0 # must set the vlog environment variables before importing tensorflow . import os os.environ [ `` tf_cpp_max_vlog_level '' ] = `` 5 '' os.environ [ `` tf_cpp_min_log_level '' ] = `` 0 ''
__label__0 class xlaopsetutilstest ( googletest.testcase ) :
__label__0 # using non-iterable elements . input_tree = [ 0 ] shallow_tree = 9 flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 meta_graph_def = save.export_meta_graph ( strip_default_attrs=false ) node_def = test_util.get_node_def_from_graph ( `` complex '' , meta_graph_def.graph_def ) self.assertin ( `` t '' , node_def.attr ) self.assertin ( `` tout '' , node_def.attr )
__label__0 # nested dict mapping ` api_func ` - > ` dispatch_target ` - > ` list [ signature ] ` , # which can be used for documentation generation and for improved error messages # when apis are called with unsupported types . _type_based_dispatch_signatures = { }
__label__0 from bazel_tools.tools.python.runfiles import runfiles
__label__0 def _handles ( self , args , kwargs ) : for arg in itertools.chain ( args , kwargs.values ( ) ) : if ( isinstance ( arg , self._types ) or ( isinstance ( arg , ( list , tuple ) ) and any ( isinstance ( elt , self._types ) for elt in arg ) ) ) : return true return false
__label__0 in addition to checkpoint files , savers keep a protocol buffer on disk with the list of recent checkpoints . this is used to manage numbered checkpoint files and by ` latest_checkpoint ( ) ` , which makes it easy to discover the path to the most recent checkpoint . that protocol buffer is stored in a file named 'checkpoint ' next to the checkpoint files .
__label__0 @ property def decorator_argspec ( self ) : return self._decorator_argspec
__label__0 class keepcheckpointeverynhourstest ( test.testcase ) :
__label__0 softmax_output_layer_bias_vocab_info = tf.vocabinfo ( new_vocab='class_vocab ' , new_vocab_size=5 , num_oov_buckets=0 , # no oov for classes . old_vocab='old_class_vocab ' , old_vocab_size=8 , backup_initializer=tf.compat.v1.zeros_initializer ( ) , axis=0 )
__label__0 def __delattr__ ( self , name ) : if name.startswith ( '_tfmw_ ' ) : super ( tfmodulewrapper , self ) .__delattr__ ( name ) else : delattr ( self._tfmw_wrapped_module , name ) self.__dict__.pop ( name ) if name in self.__all__ : self.__all__.remove ( name ) self._fastdict_pop ( name ) # delattr ( self._tfmw_wrapped_module , name )
__label__0 * ` max_to_keep ` indicates the maximum number of recent checkpoint files to keep . as new files are created , older files are deleted . if none or 0 , no checkpoints are deleted from the filesystem but only the last one is kept in the ` checkpoint ` file . defaults to 5 ( that is , the 5 most recent checkpoint files are kept . )
__label__0 def is_subtype_of ( self , other ) : return ( type ( other ) is fruittracetype and self.fruit_type is other.fruit_type )
__label__0 if not hasattr ( module , api_constants_attr_v1 ) : setattr ( module , api_constants_attr_v1 , [ ] ) getattr ( module , api_constants_attr_v1 ) .append ( ( self._names_v1 , name ) )
__label__0 `` ` python class fruittracetype : def placeholder_value ( self , placeholder_context ) : return fruit ( ) `` ` instructs tf.function to trace with the ` fruit ( ) ` objects instead of the actual ` apple ( ) ` and ` mango ( ) ` objects when it receives a call to ` get_mixed_flavor ( apple ( ) , mango ( ) ) ` . for example , tensor arguments are replaced with tensors of similar shape and dtype , output from a tf.placeholder op .
__label__0 * ` collections.abc.sequence ` ( except ` string ` and ` bytes ` ) . this includes ` list ` , ` tuple ` , and ` namedtuple ` . * ` collections.abc.mapping ` ( with sortable keys ) . this includes ` dict ` and ` collections.ordereddict ` . * ` collections.abc.mappingview ` ( with sortable keys ) . * [ ` attr.s ` classes ] ( https : //www.attrs.org/ ) . * classes ( including [ ` dataclass ` ] ( https : //docs.python.org/library/dataclasses.html ) ) that implement the ` __tf_flatten__ ` and ` __tf_unflatten__ ` methods . see examples in [ ` nest_util.py ` ] ( https : //github.com/tensorflow/tensorflow/blob/04869b4e63bfc03cb13627b3e1b879fdd0f69e34/tensorflow/python/util/nest_util.py # l97 )
__label__0 returns : packed : ` flat_sequence ` converted to have the same recursive structure as ` structure ` .
__label__0 raises : valueerror : if global step tensor is already defined .
__label__0 from tensorflow.python.eager import context from tensorflow.python.ops import logging_ops
__label__0 _rewriter_config_optimizer_disabled = none
__label__0 # with strip_default_attrs enabled , attributes `` t '' ( float32 ) and `` tout '' # ( complex64 ) in the `` complex '' op must be removed . # train.saver and train.export_meta_graph are v1 only apis . with ops_lib.graph ( ) .as_default ( ) , self.cached_session ( ) : real_num = variable_v1.variablev1 ( 1.0 , dtype=dtypes.float32 , name= '' real '' ) imag_num = variable_v1.variablev1 ( 2.0 , dtype=dtypes.float32 , name= '' imag '' ) math_ops.complex ( real_num , imag_num , name= '' complex '' )
__label__0 def contains_deprecation_decorator ( decorators ) : return any ( d.decorator_name == 'deprecated ' for d in decorators )
__label__0 returns : a tuple ( arg_present , arg_value ) containing a boolean indicating whether the argument is present , and its value in case it is. `` '' '' # check keyword args if arg_name is not none : for kw in node.keywords : if kw.arg == arg_name : return ( true , kw.value )
__label__0 provides information about original request to ` session.run ( ) ` function . sessionrunhook objects can stop the loop by calling ` request_stop ( ) ` of ` run_context ` . in the future we may use this object to add more information about run without changing the hook api. `` '' ''
__label__0 # overridden to maintain a stack of nodes to allow for parent access def visit ( self , node ) : self._stack.append ( node ) super ( _pastaeditvisitor , self ) .visit ( node ) self._stack.pop ( )
__label__0 # using positional arguments ( in proper order ) - > no change text = `` h ( a , x , y ) \n '' _ , new_text = self._upgrade ( removemultiplekeywordarguments ( ) , text ) self.assertequal ( new_text , text )
__label__0 def _allclose ( self , want , got , rtol=1e-3 , atol=1e-3 ) : return np.allclose ( want , got , rtol=rtol , atol=atol )
__label__0 simple usage example with two groups accessing the same resource :
__label__0 def testwarmstartfromobjectbasedcheckpoint ( self ) : prev_val = [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) : prev_var = variable_scope.get_variable ( `` fruit_weights '' , initializer=prev_val ) self.evaluate ( variables.global_variables_initializer ( ) ) # save object-based checkpoint . tracking_util.checkpoint ( v=prev_var ) .save ( os.path.join ( self.get_temp_dir ( ) , `` checkpoint '' ) )
__label__0 def testunboundfuncwithoneparamkeyword ( self ) :
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testmapstructurewithstrings ( self ) : inp_a = nesttest.abtuple ( a= '' foo '' , b= ( `` bar '' , `` baz '' ) ) inp_b = nesttest.abtuple ( a=2 , b= ( 1 , 3 ) ) out = nest.map_structure ( lambda string , repeats : string * repeats , inp_a , inp_b ) self.assertequal ( `` foofoo '' , out.a ) self.assertequal ( `` bar '' , out.b [ 0 ] ) self.assertequal ( `` bazbazbaz '' , out.b [ 1 ] )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 args : obj : an method , function , or functool.partial , possibly decorated by tfdecorator .
__label__0 def testsamename ( self ) : with ops_lib.graph ( ) .as_default ( ) : v0 = variable_v1.variablev1 ( [ 10.0 ] , name= '' v0 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' )
__label__0 ` head_symlink ` is a filename to head that is cross-referenced against what is contained in the json branch designation .
__label__0 note : restarting training from saved ` meta_graph ` only works if the device assignments have not changed .
__label__0 self._api_change_spec.clear_preprocessing ( )
__label__0 # generate assembly code , containing a table for the resolved symbols and the # trampolines . lib_name , _ = os.path.splitext ( os.path.basename ( args.symbols ) )
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 class utiltest ( test.testcase ) :
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 def verify_compat_v1_rename_correctness ( self , values , ns_prefix= '' '' ) : if ns_prefix : ns_prefix += `` . '' for v in values : text = `` tf . '' + ns_prefix + v + `` ( a , b ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( `` tf.compat.v1 . '' + ns_prefix + v + `` ( a , b ) '' , new_text )
__label__0 import numpy as np
__label__0 def testwarmstartvarwithvocab ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] )
__label__0 raises : typeerror : if ` bytes_or_text ` is not a binary or unicode string. `` '' '' # validate encoding , a lookuperror will be raised if invalid . encoding = codecs.lookup ( encoding ) .name if isinstance ( bytes_or_text , bytearray ) : return bytes ( bytes_or_text ) elif isinstance ( bytes_or_text , str ) : return bytes_or_text.encode ( encoding ) elif isinstance ( bytes_or_text , bytes ) : return bytes_or_text else : raise typeerror ( 'expected binary or unicode string , got % r ' % ( bytes_or_text , ) )
__label__0 args : filename_tensor : a scalar string tensor . per_device : a list of ( device , basesaverbuilder.saveableobject ) pairs , as returned by _groupbydevices ( ) .
__label__0 class child2 ( parent ) : def method1 ( self ) : pass def method2 ( self ) : pass
__label__0 raises : runtimeerror : if the program reaches an unknown state. `` '' '' idx = 0 batch , onednn , model = none , none , none state = state.find_config_or_model while idx < len ( lines ) : if state is state.find_config_or_model : config = re.match ( r '' \+ echo 'batch= ( ? p < batch > [ \d ] + ) , onednn= ( ? p < onednn > [ \d ] + ) '' , lines [ idx ] ) if config : batch = int ( config.group ( `` batch '' ) ) onednn = int ( config.group ( `` onednn '' ) ) batch_sizes.add ( batch ) else : model_re = re.search ( r '' tf-graphs\/ ( ? p < model > [ \w\d_- ] + ) .pb '' , lines [ idx ] ) assert model_re model = model_re.group ( `` model '' ) models.add ( model ) state = state.find_running_time elif state is state.find_running_time : match = re.search ( r '' no stats : ( ? p < avg > [ \d . ] + ) '' , lines [ idx ] ) state = state.find_config_or_model if match : avg = float ( match.group ( `` avg '' ) ) key = ( model , batch , onednn ) assert none not in key db [ key ] = avg else : # some models such as ssd-resnet34 ca n't run on cpu with vanilla tf and # wo n't have results . this line contains either a config or model name . continue else : raise runtimeerror ( `` reached the unreachable code . '' ) idx = idx + 1
__label__0 `` '' ''
__label__0 def testv1keywordargnames ( self ) : all_keyword_renames = ( tf_upgrade_v2.tfapichangespec ( ) .function_keyword_renames )
__label__0 # the two workers starts to execute the train op . for thread in threads : thread.start ( ) for thread in threads : thread.join ( )
__label__0 returns : result of repeatedly applying ` func ` . has the same structure layout as ` shallow_tree ` . `` '' '' return nest_util.map_structure_up_to ( nest_util.modality.core , shallow_tree , func , * inputs , * * kwargs )
__label__0 text = `` import tensorflow.google.compat.v2 as tf '' expected_text = `` import tensorflow.google.compat.v2 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 with ops_lib.graph ( ) .as_default ( ) : # restores from metagraphdef . new_saver = saver_module.import_meta_graph ( filename ) self.assertisnotnone ( new_saver ) # generates a new metagraphdef . new_meta_graph_def = new_saver.export_meta_graph ( ) # it should be the same as the original . test_util.assert_meta_graph_protos_equal ( self , meta_graph_def , new_meta_graph_def )
__label__0 # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 20.0 , name= '' v1 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 } , restore_sequentially=true ) init_all_op = variables.global_variables_initializer ( )
__label__0 returns : a saver object. `` '' '' return self._saver
__label__0 example usage :
__label__0 # test that we still get the right behavior when using normal tensors . a = [ 1. , 2. , 3 . ] b = [ 7. , 8. , 2 . ] a_plus_b = gen_math_ops.atan2 ( a , b ) self.assertallclose ( a_plus_b , [ 0.14189707 , 0.24497867 , 0.98279375 ] )
__label__0 this uploader script is typically run periodically as a cron job . it locates , in a specified data directory , files that contain benchmark test results . the results are written by the `` run_and_gather_logs.py '' script using the json-format serialization of the `` testresults '' protobuf message ( core/util/test_log.proto ) .
__label__0 args : graph : the graph in which to create the global step tensor . if missing , use default graph .
__label__0 # adding s2 again ( old s2 is removed first , then new s2 appended ) s2 = save2.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s3 , s2 ] , save2.last_checkpoints ) # created by the first helper . self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) # deleted by the first helper . self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s3 , s2 ] , save_dir=save_dir )
__label__0 ` var_list ` specifies the variables that will be saved and restored . it can be passed as a ` dict ` or a list :
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests util functions . '' '' ''
__label__0 # comparing tf.math.angle with tf.compat.v1.angle . input_supported_dtypes = [ tf.float32 , tf.float64 ] random_dtype_index = fh.get_int ( min_int=0 , max_int=1 ) input_dtype = input_supported_dtypes [ random_dtype_index ] input_shape = fh.get_int_list ( min_length=0 , max_length=6 , min_int=0 , max_int=10 ) seed = fh.get_int ( ) input_tensor = tf.random.uniform ( shape=input_shape , dtype=input_dtype , seed=seed , maxval=10 ) name = fh.get_string ( 5 ) v2_output = tf.math.angle ( input=input_tensor , name=name ) v1_output = tf.compat.v1.angle ( input=input_tensor , name=name ) try : tf.debugging.assert_equal ( v1_output , v2_output ) tf.debugging.assert_equal ( v1_output.shape , v2_output.shape ) except exception as e : # pylint : disable=broad-except print ( `` input tensor : { } '' .format ( input_tensor ) ) print ( `` input dtype : { } '' .format ( input_dtype ) ) print ( `` v1_output : { } '' .format ( v1_output ) ) print ( `` v2_output : { } '' .format ( v2_output ) ) raise e
__label__0 args : filename : optional filename including the path for writing the generated ` metagraphdef ` protocol buffer . meta_info_def : ` metainfodef ` protocol buffer . graph_def : ` graphdef ` protocol buffer . saver_def : ` saverdef ` protocol buffer . collection_list : list of string keys to collect . as_text : if ` true ` , writes the ` metagraphdef ` as an ascii proto . graph : the ` graph ` to export . if ` none ` , use the default graph . export_scope : optional ` string ` . name scope under which to extract the subgraph . the scope name will be striped from the node definitions for easy import later into new name scopes . if ` none ` , the whole graph is exported . graph_def and export_scope can not both be specified . clear_devices : whether or not to clear the device field for an ` operation ` or ` tensor ` during export . clear_extraneous_savers : remove any saver-related information from the graph ( both save/restore ops and saverdefs ) that are not associated with the provided saverdef . strip_default_attrs : boolean . if ` true ` , default-valued attributes will be removed from the nodedefs . for a detailed guide , see [ stripping default-valued attributes ] ( https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/readme.md # stripping-default-valued-attributes ) . save_debug_info : if ` true ` , save the graphdebuginfo to a separate file , which in the same directory of filename and with ` _debug ` added before the file extend . * * kwargs : optional keyed arguments .
__label__0 class apianalysisspec : `` '' '' this class defines how ` analysisresult ` s should be generated .
__label__0 _mockop = collections.namedtuple ( `` mockop '' , [ `` name '' ] )
__label__0 def decorator ( handler ) : if ( x_type , ) in _elementwise_api_handlers : raise valueerror ( `` a unary elementwise dispatch handler `` f '' ( { _elementwise_api_handlers [ ( x_type , ) ] } ) `` f '' has already been registered for { x_type } . '' ) _elementwise_api_handlers [ ( x_type , ) ] = handler for api in _unary_elementwise_apis : _add_dispatch_for_unary_elementwise_api ( api , x_type , handler )
__label__0 def _get_attrs_items ( obj ) : `` '' '' returns a list of ( name , value ) pairs from an attrs instance .
__label__0 @ tf_export ( `` types.experimental.concretefunction '' , v1= [ ] ) class concretefunction ( callable , metaclass=abc.abcmeta ) : `` '' '' base class for differentiable graph functions .
__label__0 self.assertallclose ( 0 - ( 0.1 + 0.3 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_0_g_1 ) ) self.assertallclose ( 1 - ( 0.9 + 1.1 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_1_g_1 ) ) self.assertallclose ( [ [ 3.0 ] , [ 4.0 - ( 0.1 + 0.3 ) / 2 * 2.0 ] ] , sessions [ 1 ] .run ( var_sparse_g_1 ) )
__label__0 def testunsortedsegmentsum0dindices1ddata ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ [ 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 2 , 3 , 4 , 5 ] , [ 0 , 0 , 0 , 0 , 0 , 0 ] ] , dtype=dtype ) , self._unsortedsegmentsum ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , 2 , 4 ) )
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensor_slices ( [ 5. , 6. , 7. , 8 . ] ) .batch ( 2 ) > > > dataset_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > distributed_values = next ( dataset_iterator ) > > > distributed_values perreplica : { 0 : < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 5 . ] , dtype=float32 ) > , 1 : < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 6 . ] , dtype=float32 ) > }
__label__0 @ dispatch.dispatch_for_types ( test_op_with_kwonly , customtensor ) def override_for_test_op ( x , z , y ) : # pylint : disable=unused-variable return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 , )
__label__0 def process_opened_file ( self , in_filename , in_file , out_filename , out_file ) : `` '' '' process the given python file for incompatible changes .
__label__0 > > > optimizer = tf.keras.optimizers.sgd ( .01 ) > > > print ( `` before training : '' , optimizer.iterations.numpy ( ) ) before training : 0 > > > with tf.gradienttape ( ) as tape : ... loss , var_list = compute_loss ( 3 ) ... grads = tape.gradient ( loss , var_list ) ... optimizer.apply_gradients ( zip ( grads , var_list ) ) > > > print ( `` after training : '' , optimizer.iterations.numpy ( ) ) after training : 1
__label__0 with self.assertraises ( valueerror ) : nest.pack_sequence_as ( [ 5 , 6 , [ 7 , 8 ] ] , [ `` a '' , `` b '' , `` c '' ] )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_one_line_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__1 class dog : def __init__ ( self , name , age ) : self.name = name self.age = age
__label__0 if not is_nested_fn ( structure ) : if len ( flat_sequence ) ! = 1 : raise valueerror ( `` the target structure is of type ` { } ` \n { } \nhowever the input `` `` is a sequence ( { } ) of length { } .\n { } \nnest can not `` `` guarantee that it is safe to map one to the other . `` .format ( type ( structure ) , truncate ( structure , 100 ) , type ( flat_sequence ) , len ( flat_sequence ) , truncate ( flat_sequence , 100 ) , ) ) return flat_sequence [ 0 ]
__label__0 for i , task_address in sorted ( tasks.items ( ) ) : try : task_address = compat.as_bytes ( task_address ) except typeerror : raise typeerror ( `` task address % r must be bytes or unicode '' % task_address ) job_def.tasks [ i ] = task_address
__label__0 the ` get_examples ` method receives a string and returns an iterable of ` doctest.example ` objects. `` '' '' patched = false
__label__0 def test_strict_mode_deprecation ( self ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def appendgraphdebuginfo ( self , fn_name , fn_debug_info ) : debug_info_str = fn_debug_info.serializetostring ( ) super ( ) .appendgraphdebuginfo ( fn_name , debug_info_str )
__label__0 def is_python ( cell ) : `` '' '' checks if the cell consists of python code . '' '' '' return ( cell [ `` cell_type '' ] == `` code '' # code cells only and cell [ `` source '' ] # non-empty cells and not cell [ `` source '' ] [ 0 ] .startswith ( `` % % '' ) ) # multiline eg : % % bash
__label__0 self.evaluate ( variables.global_variables_initializer ( ) ) if call_saver_with_dict : saver = saver_module.saver ( { var_name : new_vs [ 0 ] } ) else : saver = saver_module.saver ( new_vs ) saver.restore ( sess , saved_path )
__label__0 goodbye `` '' '' ) , ( 'output ' , [ ( 'code ' , 'result ' ) ] , `` '' '' hello
__label__0 the decorated function is used to override ` op ` if any of the arguments or keyword arguments ( including elements of lists or tuples ) have one of the specified types .
__label__0 `` ` `` '' '' if hasattr ( path , '__fspath__ ' ) : path = as_str_any ( path.__fspath__ ( ) ) return path
__label__0 def get_session ( is_chief ) : g = ops.graph ( ) with g.as_default ( ) : with ops.device ( `` /job : localhost '' ) : v = variable_v1.variablev1 ( 1 , name= '' default_ready_for_local_init_op_v_ '' + str ( uid ) ) vadd = v.assign_add ( 1 ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' default_ready_for_local_init_op_w_ '' + str ( uid ) ) ready_for_local_init_op = variables.report_uninitialized_variables ( variables.global_variables ( ) ) sv = supervisor.supervisor ( logdir=logdir , is_chief=is_chief , graph=g , recovery_wait_secs=1 , init_op=v.initializer , ready_for_local_init_op=ready_for_local_init_op ) sess = sv.prepare_or_wait_for_session ( server.target )
__label__0 if __name__ == `` __main__ '' : test_lib.main ( )
__label__0 _ , report , _ , _ = self._upgrade ( `` model.save ( path ) '' ) self.assertin ( `` saves to the tensorflow savedmodel format by default '' , report )
__label__0 # api label for cell name used in checkpoint metrics . _saver_label = `` saver_v1 ''
__label__0 class writegraphtest ( test.testcase ) :
__label__0 class child ( parent ) : def method1 ( self ) : pass def method2 ( self ) : pass `` `
__label__0 wrapped_fn = functools.partial ( fn_has_no_kwargs , test_arg2=456 ) double_wrapped_fn = functools.partial ( wrapped_fn , test_arg1=123 )
__label__0 def clear_preprocessing ( self ) : self.__init__ ( import_rename=self.import_rename , upgrade_compat_v1_import=self.upgrade_compat_v1_import )
__label__0 def _create_prev_run_vars ( self , var_names , shapes , initializers ) : with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : all_vars = [ ] for var_name , shape , initializer in zip ( var_names , shapes , initializers ) : all_vars.append ( variable_scope.get_variable ( var_name , shape=shape , initializer=initializer ) ) self._write_checkpoint ( sess ) return [ self.evaluate ( var ) for var in all_vars ]
__label__0 tensorflow is an open source software library for high performance numerical computation . its flexible architecture allows easy deployment of computation across a variety of platforms ( cpus , gpus , tpus ) , and from desktops to clusters of servers to mobile and edge devices .
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 def testnologdirsucceeds ( self ) : with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] ) sv = supervisor.supervisor ( logdir= '' '' , summary_op=none ) sess = sv.prepare_or_wait_for_session ( `` '' ) sess.close ( ) sv.stop ( )
__label__0 import numpy as np
__label__0 def test_decorator_name ( wrapper ) : return tf_decorator.make_decorator ( test_function , wrapper )
__label__0 def __dir__ ( self ) : module = self._load ( ) return dir ( module )
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' a module to support operations on ipynb files '' '' ''
__label__0 finally : try : gfile.deleterecursively ( temp_directory ) except oserror : pass
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 return grouped_variables
__label__0 # add the save ops . if sharded : per_device = self._groupbydevices ( saveables ) if build_save : save_tensor = self._addshardedsaveops ( filename_tensor , per_device ) if build_restore : restore_op = self._addshardedrestoreops ( filename_tensor , per_device , restore_sequentially , reshape ) else : if build_save : save_tensor = self._addsaveops ( filename_tensor , saveables ) if build_restore : restore_op = self._addrestoreops ( filename_tensor , saveables , restore_sequentially , reshape )
__label__0 args : shallow_tree : a possibly pruned structure of input_tree . input_tree : an atom or a nested structure . note , numpy arrays are considered atoms . check_types : bool . if true , check that each node in shallow_tree has the same type as the corresponding node in input_tree . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 with self.assertraisesregex ( typeerror , `` returned a non-bool scalar '' ) : nest.get_traverse_shallow_structure ( lambda _ : 1 , [ 1 ] )
__label__0 # put parentheses around scale.value ( and remove the old prefix/ # suffix , they should only be around new_value ) . pasta.base.formatting.set ( old_value , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( old_value , `` suffix '' , `` ) '' )
__label__0 1. python dict :
__label__0 def __init__ ( self ) : self.internal_map = _tf_stack.pybindsourcemap ( )
__label__0 checkpoint_prefix = compat.as_text ( save_path ) if not checkpoint_management.checkpoint_exists_internal ( checkpoint_prefix ) : raise valueerror ( `` the passed save_path is not a valid checkpoint : `` + checkpoint_prefix )
__label__0 def testsaverdef ( self ) : # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.cached_session ( ) : v0 = variable_v1.variablev1 ( 123 , name= '' v0 '' ) save = saver_module.saver ( { `` v0 '' : v0 } , sharded=true ) sd = save.as_saver_def ( ) self.asserttrue ( sd.sharded )
__label__0 major , minor , patch = rocm_version_numbers ( rocm_install_path )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 from google.protobuf import message from google.protobuf import text_format from tensorflow.core.framework import function_pb2 from tensorflow.core.framework import graph_pb2 from tensorflow.python.platform import test from tensorflow.tools.proto_splitter import chunk_pb2 from tensorflow.tools.proto_splitter import constants from tensorflow.tools.proto_splitter import split_graph_def from tensorflow.tools.proto_splitter import util from tensorflow.tools.proto_splitter.python import test_util
__label__0 def teststatelessmultinomial ( self ) : text = ( `` tf.random.stateless_multinomial ( logits , num_samples , seed , `` `` output_dtype=dtype , name=name ) '' ) expected_text = ( `` tf.random.stateless_categorical ( logits , num_samples , seed , `` `` dtype=dtype , name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 * for a given test ( from the list above ) , get all its entry names . the list of entry names can be extracted from the test `` info '' metadata for a given test name and start time ( e.g . pick the latest start time for that test ) . select * from test where test = < test-name > and start = < latest-datetime > `` '' ''
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 for ( i , index ) in enumerate ( indices ) : self.assertallcloseaccordingtotype ( x [ index ] - lr * grad [ i ] * ( y [ index ] + grad [ i ] * grad [ i ] ) * * ( -0.5 ) , self.evaluate ( var ) [ index ] ) self.assertallcloseaccordingtotype ( y [ index ] + grad [ i ] * grad [ i ] , self.evaluate ( accum ) [ index ] )
__label__0 def testoneinput ( data ) : tf.constant ( data )
__label__0 def __eq__ ( self , other ) : return self.mask == other.mask and self.value == other.value
__label__0 args : parse_example_op : a parseexample or parseexamplev2 ` operation ` sess : a tf.compat.v1.session needed to obtain some configuration values . returns : a exampleparserconfig proto .
__label__0 @ tf_export ( `` types.experimental.distributed.mirrored '' , v1= [ ] ) class mirrored ( distributedvalues ) : `` '' '' holds a distributed value : a map from replica id to synchronized values .
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maybemasked , `` y '' : maybemasked } ) def masked_add ( x , y , name=none ) : with ops.name_scope ( name ) : x_values = x.values if isinstance ( x , maskedtensor ) else x x_mask = x.mask if isinstance ( x , maskedtensor ) else true y_values = y.values if isinstance ( y , maskedtensor ) else y y_mask = y.mask if isinstance ( y , maskedtensor ) else true return maskedtensor ( x_values + y_values , x_mask & y_mask )
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 1000.0 , self.evaluate ( v0_2 ) ) self.assertequal ( 2000.0 , self.evaluate ( v1_2 ) ) self.assertequal ( b '' k1000 '' , self.evaluate ( v2_2.keys ( ) ) ) self.assertequal ( 3000.0 , self.evaluate ( v2_2.values ( ) ) ) # restore the values saved earlier in the parameter nodes . save2.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0_2 ) ) self.assertequal ( 20.0 , self.evaluate ( v1_2 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2_2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2_2.values ( ) ) )
__label__0 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # type-based dispatch # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
__label__0 def __init__ ( self , wrapped ) : super ( _weakobjectidentitywrapper , self ) .__init__ ( weakref.ref ( wrapped ) )
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but ` input_tree ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` .
__label__0 for an example , see ` tfapichangespec ` . `` '' ''
__label__0 test_util.assert_meta_graph_protos_equal ( self , meta_graph_def , new_meta_graph_def )
__label__0 del properties
__label__0 # output is : ab_tuple ( a=6 , b=15 ) `` `
__label__0 fastmoduletype = fast_module_type.get_fast_module_type_class ( ) _per_module_warning_limit = 1 compat_v1_usage_gauge = monitoring.boolgauge ( '/tensorflow/api/compat/v1 ' , 'compat.v1 usage ' )
__label__0 with ops.graph ( ) .as_default ( ) : w_vector = variable_v1.variablev1 ( [ 4 , 5 , 6 ] , name= '' w '' ) with session.session ( server.target , config=sharing_config ) as sess : self.assertallequal ( [ 1 , 2 , 3 ] , sess.run ( w_vector ) ) sess.run ( w_vector.initializer ) self.assertallequal ( [ 4 , 5 , 6 ] , sess.run ( w_vector ) )
__label__0 def _restore ( partitioner=none ) : # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.session ( ) as sess : if partitioner : new_vs = [ variable_scope.get_variable ( var_name , shape=var_full_shape , initializer=array_ops.zeros ( var_full_shape ) , partitioner=partitioner ) ] else : new_vs = [ variable_v1.variablev1 ( array_ops.zeros ( shape=var_full_shape ) , # ! = original contents . name=var_name ) ]
__label__0 with self.session ( graph=ops_lib.graph ( ) ) as sess : # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 20.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) v2_init = v2.insert ( `` k1 '' , 30.0 ) save = saver_module.saver ( [ v0 , v1 , v2.saveable ] ) self.evaluate ( variables.global_variables_initializer ( ) ) v2_init.run ( )
__label__0 def getexpectedconfig ( self , op_type ) : expected = example_parser_configuration_pb2.exampleparserconfiguration ( ) if op_type == 'parseexamplev2 ' : text_format.parse ( expected_config_v2 , expected ) else : text_format.parse ( expected_config_v1 , expected ) return expected
__label__0 wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' ) self.asserttrue ( tf_inspect.ismodule ( wrapped_module ) )
__label__0 def testconv2dbackpropfilter ( self ) : text = ( `` tf.nn.conv2d_backprop_filter ( input , filter_sizes , out_backprop , `` `` strides , padding , use_cudnn_on_gpu , data_format ) '' ) expected_text = ( `` tf.compat.v1.nn.conv2d_backprop_filter ( input , filter_sizes , `` `` out_backprop , strides , padding , use_cudnn_on_gpu , data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 stack = self._stack_dict [ self._thread_key ] self.parent = stack [ -1 ] stack.append ( self ) self.update ( ) return self
__label__0 def testpreparesessionafterstopforchief ( self ) : logdir = self._test_dir ( `` prepare_after_stop_chief '' ) with ops.graph ( ) .as_default ( ) : sv = supervisor.supervisor ( logdir=logdir , is_chief=true )
__label__0 def release ( self , group_id ) : `` '' '' release the group lock for a specific group ` group_id ` . '' '' '' self._validate_group_id ( group_id )
__label__0 return device_info_list
__label__0 # passthrough xla_flags . config.environment [ `` xla_flags '' ] = os.environ.get ( `` xla_flags '' , `` '' )
__label__0 > > > from google.protobuf import text_format > > > example = text_format.parse ( `` ' ... features { ... feature { key : `` my_feature '' ... value { int64_list { value : [ 1 , 2 , 3 , 4 ] } } } ... } '' ' , ... tf.train.example ( ) )
__label__0 with graph.as_default ( ) , self.session ( ) as sess : self.evaluate ( variables.global_variables_initializer ( ) ) saver = saver_module.saver ( var_list=var_list , max_to_keep=1 ) saver.save ( sess , os.path.join ( test_dir , ckpt_filename ) , write_state=false )
__label__0 def _testscopedrestore ( self , test_dir , exported_filename , new_exported_filename , ckpt_filename ) : graph = ops_lib.graph ( ) # create all the missing inputs . with graph.as_default ( ) : new_image = constant_op.constant ( 1.2 , dtypes.float32 , shape= [ 100 , 28 ] , name= '' images '' ) var_list = meta_graph.import_scoped_meta_graph ( os.path.join ( test_dir , exported_filename ) , graph=graph , input_map= { `` $ unbound_inputs_images '' : new_image } , import_scope= '' new_hidden1 '' ) self.assertequal ( [ `` biases:0 '' , `` weights:0 '' ] , sorted ( var_list.keys ( ) ) ) hidden1 = graph.as_graph_element ( `` new_hidden1/relu:0 '' ) weights1 = graph.as_graph_element ( `` new_hidden1/weights:0 '' ) biases1 = graph.as_graph_element ( `` new_hidden1/biases:0 '' )
__label__0 text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=foo ( bar ) ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( foo ( bar ) ) ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.0 ] } } feature : { float_list : { value : [ 5.0 , 3.0 ] } } } } `` ` `` '' ''
__label__0 import collections
__label__0 try : x = maskedtensor ( [ true , false , true , true , true ] , [ 1 , 0 , 1 , 1 , 1 ] ) self.assertequal ( array_ops.where_v2 ( x ) , `` stub '' ) self.assertequal ( array_ops.where_v2 ( x , x , x ) , `` stub '' )
__label__0 # in a member of group 1 : with lock.group ( 1 ) : # do stuff , access the resource # ... `` `
__label__0 # note ( mrry ) : sort by job_name to produce deterministic protobufs . for job_name , tasks in sorted ( self._cluster_spec.items ( ) ) : try : job_name = compat.as_bytes ( job_name ) except typeerror : raise typeerror ( `` job name % r must be bytes or unicode '' % job_name )
__label__0 def __tf_flatten__ ( self ) : metadata = ( self.mask , ) components = ( self.value , ) return metadata , components
__label__0 def add_result ( self , analysis_result ) : self._results.append ( analysis_result )
__label__0 translate a qualified name into nested attribute nodes ( and a name node ) .
__label__0 def update_setup_dot_py ( old_version , new_version ) : `` '' '' update setup.py . '' '' '' replace_string_in_line ( `` _version = ' % s ' '' % old_version.string , `` _version = ' % s ' '' % new_version.string , setup_py )
__label__0 graph_def = self._make_graph_def_with_constant_nodes ( sizes ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , chunked_message = s.split ( ) self.assertlen ( chunks , 3 ) self._assert_chunk_sizes ( chunks , max_size )
__label__0 def testinvalidchecktypes ( self ) : with self.assertraises ( ( valueerror , typeerror ) ) : nest.assert_same_structure ( nest1=array_ops.zeros ( ( 1 ) ) , nest2=array_ops.ones ( ( 1 , 1 , 1 ) ) , check_types=array_ops.ones ( ( 2 ) ) ) with self.assertraises ( ( valueerror , typeerror ) ) : nest.assert_same_structure ( nest1=array_ops.zeros ( ( 1 ) ) , nest2=array_ops.ones ( ( 1 , 1 , 1 ) ) , expand_composites=array_ops.ones ( ( 2 ) ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenwithstringpaths ( self ) : sep = `` / '' mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt_flat_paths = nest.flatten_with_joined_string_paths ( mt , separator=sep ) self.assertequal ( mt_flat_paths [ 0 ] [ 0 ] , `` 0 '' ) self.assertallequal ( mt_flat_paths [ 0 ] [ 1 ] , [ 1 ] )
__label__0 # if the docstring 's output is empty and there is some output generated # after running the snippet , return true . this is because if the user # does n't want to display output , respect that over what the doctest wants . if got and not want : return true
__label__0 def testassertshallowstructure ( self ) : inp_ab = [ `` a '' , `` b '' ] inp_abc = [ `` a '' , `` b '' , `` c '' ] with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises valueerror , nest.structures_have_mismatching_lengths.format ( input_length=len ( inp_ab ) , shallow_length=len ( inp_abc ) ) , ) : nest.assert_shallow_structure ( inp_abc , inp_ab )
__label__0 class sparsejobtest ( test.testcase ) :
__label__0 from tensorflow.tools.compatibility import ast_edits
__label__0 def test_partial_function_with_positional_args ( self ) : expected_test_arg = 123
__label__0 def main ( _ ) : `` '' '' run an interactive console . '' '' '' code.interact ( ) return 0
__label__0 the input dictionary must satisfy two properties :
__label__0 this is needed if preprocessing a file changed any rewriting rules. `` '' '' pass
__label__0 def testinplaceemptyoutputonerror ( self ) : `` '' '' in place file becomes empty when parsing error is not handled . '' '' '' temp_file = tempfile.namedtemporaryfile ( `` w '' , delete=false ) original = `` print ' a ' \n '' upgraded = `` '' temp_file.write ( original ) temp_file.close ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2.tfapichangespec ( ) ) upgrader.process_file ( temp_file.name , temp_file.name ) self.assertallequal ( open ( temp_file.name ) .read ( ) , upgraded ) os.unlink ( temp_file.name )
__label__0 def _find_name_index ( signature ) : `` '' '' returns the index of the ` name ` parameter , or -1 if it 's not present . '' '' '' try : return list ( signature.parameters ) .index ( `` name '' ) except valueerror : return -1
__label__0 def override_for_test_op ( x , y , z ) : # pylint : disable=unused-variable return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 , )
__label__0 def testunboundfuncwithtwoparamskeyword ( self ) :
__label__0 # the diagonal tensor addition gets traced even though the linear_operator # api only uses dispatchable ops instead of directly exposing dispatching . trace = linear_operator_diag.linearoperatordiag ( x ) .add_to_tensor ( x ) self.assertin ( `` linalg.set_diag ( convert_to_tensor ( x , name=x ) , __operators__.add ( `` `` convert_to_tensor ( x , dtype=none , dtype_hint=none , name=diag ) , `` `` linalg.diag_part ( convert_to_tensor ( x , name=x ) ) , `` `` name= '' , str ( trace ) )
__label__0 text = `` from tensorflow import contrib '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 @ tf_export ( v1= [ `` train.sessionmanager '' ] ) class sessionmanager : `` '' '' training helper that restores from checkpoint and creates session .
__label__0 def test_contrib_summary_flush ( self ) : text = `` tf.contrib.summary.flush ( writer=foo ) '' expected = `` tf.compat.v2.summary.flush ( writer=foo ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated argument values ) '' `` \n '' `` \ndeprecated : some argument values are deprecated : ` ( deprecated=true ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : \n % s '' `` \n '' `` \nargs : '' `` \n arg0 : arg 0 . '' `` \n arg1 : arg 1 . '' `` \n deprecated : deprecated ! '' `` \n '' `` \nreturns : '' `` \n sum of args . '' % ( date , instructions ) , _fn.__doc__ )
__label__0 @ property def target ( self ) : `` '' '' returns the target for a ` tf.compat.v1.session ` to connect to this server .
__label__0 def rocfft_version_numbers ( path ) : possible_version_files = [ `` include/rocfft/rocfft-version.h '' , # rocm 5.2 `` rocfft/include/rocfft-version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` rocfft version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` rocfft_version_major '' ) minor = _get_header_version ( version_file , `` rocfft_version_minor '' ) patch = _get_header_version ( version_file , `` rocfft_version_patch '' ) return major , minor , patch
__label__0 def testrename ( self ) : text = `` tf.mul ( a , tf.sub ( b , c ) ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.multiply ( a , tf.subtract ( b , c ) ) \n '' )
__label__0 # saving one variable under two names raises an error . with self.assertraisesregex ( valueerror , `` the same saveable will be restored with two names : v0 '' ) : saver_module.saver ( { `` v0 '' : v0 , `` v0too '' : v0 } )
__label__0 def testchunkgraphdefandfunctions ( self ) : sizes = [ 50 , 50 , 50 , 50 , 50 , 50 ] fn1 = [ 50 , 50 , 50 ] fn2 = [ 50 ] fn3 = [ 50 ] fn4 = [ 50 ] max_size = 200 constants.debug_set_max_size ( max_size )
__label__0 if hasattr ( _inspect , 'fullargspec ' ) : fullargspec = _inspect.fullargspec # pylint : disable=invalid-name else : fullargspec = collections.namedtuple ( 'fullargspec ' , [ 'args ' , 'varargs ' , 'varkw ' , 'defaults ' , 'kwonlyargs ' , 'kwonlydefaults ' , 'annotations ' ] )
__label__0 threads = [ ] if self._save_summaries_secs and self._summary_writer : if self._summary_op is not none : threads.append ( svsummarythread ( self , sess ) ) if self._global_step is not none : threads.append ( svstepcounterthread ( self , sess ) ) if self.saver and self._save_model_secs : threads.append ( svtimercheckpointthread ( self , sess ) ) for t in threads : t.start ( ) return threads
__label__0 class lazyloader ( types.moduletype ) : `` '' '' lazily import a module , mainly to avoid pulling in large dependencies .
__label__0 self._ready.acquire ( ) while self._another_group_active ( group_id ) : self._ready.wait ( ) self._group_member_counts [ group_id ] += 1 self._ready.release ( )
__label__0 @ test_tfdecorator ( 'decorator ' ) class testdecoratedclass ( object ) : `` '' '' test decorated class . '' '' ''
__label__0 > print myclass.value 123 `` '' ''
__label__0 # check if dispatch_target registered by ` @ dispatch_for_api ` for api , signatures in _type_based_dispatch_signatures.items ( ) : if dispatch_target in signatures : dispatcher = getattr ( api , type_based_dispatch_attr ) dispatcher.unregister ( dispatch_target ) del signatures [ dispatch_target ] found = true
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 4 ] ) ) nmt2 = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=false , inner_value=constant_op.constant ( [ 5 ] ) ) nmt_combined_with_path = nest.map_structure_with_tuple_paths ( tuple_path_sum , nmt , nmt2 ) self.assertisinstance ( nmt_combined_with_path , nestedmaskedtensor ) self.assertequal ( nmt_combined_with_path.mask , true ) self.assertequal ( nmt_combined_with_path.value.mask , false ) self.assertallequal ( nmt_combined_with_path.value.value [ 0 ] , ( 0 , 0 ) ) self.assertallequal ( nmt_combined_with_path.value.value [ 1 ] , [ 9 ] )
__label__0 class missinglogserror ( exception ) : pass
__label__0 def __ne__ ( self , other ) : return self._cluster_spec ! = other
__label__1 def find_gcd ( a , b ) : while b : a , b = b , a % b return a
__label__0 so to let doctest act like a notebook :
__label__0 def __init__ ( self , func ) : self._func = func
__label__0 args : bytes_or_text : a ` bytearray ` , ` bytes ` , ` str ` , or ` unicode ` object . encoding : a string indicating the charset for encoding unicode .
__label__0 def testisfunction ( self ) : self.asserttrue ( tf_inspect.isfunction ( test_decorated_function ) ) self.assertfalse ( tf_inspect.isfunction ( testdecoratedclass ) )
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 graph = ops_lib.graph ( ) with session.session ( graph=graph ) as sess : with ops_lib.name_scope ( `` new_model '' ) : new_saver = saver_module.import_meta_graph ( filename + `` .meta '' , graph=graph )
__label__0 > > > array = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) > > > tf.nest.flatten ( array ) [ array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ]
__label__0 def testexportsinglefunction ( self ) : export_decorator = tf_export.tf_export ( 'namea ' , 'nameb ' ) decorated_function = export_decorator ( self._test_function ) self.assertequal ( decorated_function , self._test_function ) self.assertequal ( ( 'namea ' , 'nameb ' ) , decorated_function._tf_api_names ) self.assertequal ( [ 'namea ' , 'nameb ' ] , tf_export.get_v1_names ( decorated_function ) ) self.assertequal ( [ 'namea ' , 'nameb ' ] , tf_export.get_v2_names ( decorated_function ) ) self.assertequal ( tf_export.get_symbol_from_name ( 'namea ' ) , decorated_function ) self.assertequal ( tf_export.get_symbol_from_name ( 'nameb ' ) , decorated_function ) self.assertequal ( tf_export.get_symbol_from_name ( tf_export.get_canonical_name_for_symbol ( decorated_function ) ) , decorated_function )
__label__0 @ compatibility ( tf2 ) ` tf.compat.v1.train.saver ` is not supported for saving and restoring checkpoints in tf2 . please switch to ` tf.train.checkpoint ` or ` tf.keras.model.save_weights ` , which perform a more robust [ object-based saving ] ( https : //www.tensorflow.org/guide/checkpoint # loading_mechanics ) .
__label__0 def testnonreshapevariable ( self ) : self._testnonreshape ( variables.variable )
__label__0 for splitter_cls in self.message_splitters : splitter = splitter_cls ( ele , size , parent_splitter=self , fields_in_parent=self.repeated_field + [ n ] , ) size_diff = splitter.build_chunks ( ) total_size_diff += size_diff size -= size_diff
__label__0 @ abc.abstractmethod def placeholder_value ( self , placeholder_context ) - > any : `` '' '' creates a placeholder for tracing .
__label__0 text = `` import tensorflow.foo as bar '' expected_text = `` import tensorflow.compat.v1.foo as bar '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # it 's not clear why this step is necessary . ` compile ` is supposed to handle # ast directly . source = astor.to_source ( source_ast )
__label__1 def reverse_list ( lst ) : return lst [ : :-1 ]
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _object ( ) ._fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 returns : a ` tf.train.serverdef ` protocol buffer that describes the configuration of this server. `` '' '' return self._server_def
__label__0 raises : valueerror : if ` job_name ` does not name a job in this cluster. `` '' '' try : job = self._cluster_spec [ job_name ] except keyerror : raise valueerror ( `` no such job in cluster : % r '' % job_name ) return len ( job )
__label__0 with ops.graph ( ) .as_default ( ) : init_value = array_ops.placeholder ( dtypes.int32 ) v = variable_v1.variablev1 ( init_value , validate_shape=false , name= '' v '' )
__label__0 transforms : tf.contrib.layers.xavier_initializer ( uniform , seed , dtype ) to tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , mode= '' fan_avg '' , distribution= ( `` uniform '' if uniform else `` truncated_normal '' ) , seed=seed , dtype=dtype )
__label__0 @ tf_export ( v1= [ `` train.sessionruncontext '' ] ) class sessionruncontext : `` '' '' provides information about the ` session.run ( ) ` call being made .
__label__0 class tfcontextlibtest ( test.testcase ) :
__label__0 def _do_test ( self , expected_example_tuples , string ) : parser = fenced_doctest_lib.fencedcellparser ( fence_label='python ' )
__label__0 for node , chunk in zip ( graph_def.library.function [ 0 ] .node_def , itertools.chain ( chunks [ 0 ] .library.function [ 0 ] .node_def , chunks [ 1 ] .node_def ) , ) : self.assertprotoequals ( node , chunk )
__label__0 # ignored : bulk restore is internally sequential . del restore_sequentially restore_specs = [ ] for saveable in saveables : for spec in saveable.specs : restore_specs.append ( ( spec.name , spec.slice_spec , spec.dtype ) )
__label__0 def testunboundfuncwithoneparamdefaultonekeyword ( self ) :
__label__0 def testwarmstart_sparsecolumnhashed ( self ) : # create feature column . sc_hash = fc.categorical_column_with_hash_bucket ( `` sc_hash '' , hash_bucket_size=15 )
__label__0 * when this is called , the graph is finalized and ops can no longer be added to the graph . * this method will also be called as a result of recovering a wrapped session , not only at the beginning of the overall session .
__label__0 returns : the tracing type of this object. `` '' ''
__label__0 with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = resource_variable_ops.resourcevariable ( -1.0 , name= '' v0 '' ) v1 = resource_variable_ops.resourcevariable ( -1.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } ) save.restore ( sess , save_path )
__label__0 returns : a timestamp. `` '' '' return self._save_summaries_secs
__label__0 like map_structure_up_to ( ) , except that the 'func ' argument takes a path tuple as its first argument , followed by the corresponding values from * inputs .
__label__0 self.assertequal ( { ' a ' : 3 , ' b ' : 2 } , tf_inspect.getcallargs ( func , a=3 ) )
__label__0 def before_run ( self , run_context ) : # pylint : disable=unused-argument `` '' '' called before each call to run ( ) .
__label__0 def f ( a , b , kw1 , kw3 ) : ... def f ( a , b , kw2 , kw1 ) : ... def f ( a , b , kw3 , kw1 ) : ... def g ( a , b , kw1 , c ) : ... def g ( a , b , c , kw1 ) : ... def g2 ( a , b , kw1 , c , d ) : ... def g2 ( a , b , c , d , kw1 ) : ... def h ( a , kw1 , kw2 ) : ...
__label__0 def testreadingclassattributeondecoratedclass ( self ) : self.assertequal ( 2 , testdecoratedclass ( ) .two_attr )
__label__0 def get_clear_session_function ( ) : global _keras_clear_session_function return _keras_clear_session_function
__label__0 def __get__ ( self , obj , objtype ) : if objtype not in self._cache : self._cache [ objtype ] = self._func ( objtype ) return self._cache [ objtype ]
__label__0 build_info = { }
__label__0 this configuration ensures that we continue to exercise the grpc stack when testing , rather than using the in-process optimization , which avoids using grpc as the transport between a client and master in the same process .
__label__0 def __init__ ( self , * args : str , api_name : str = tensorflow_api_name , v1 : optional [ sequence [ str ] ] = none , allow_multiple_exports : bool = true , # pylint : disable=unused-argument ) : `` '' '' export under the names * args ( first one is considered canonical ) .
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 the new api is
__label__0 if __name__ == '__main__ ' : absltest.main ( )
__label__0 def testwarmstartvarwithvocabcurrentvarpartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] )
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 # python3 ast requires the args for the attribute , but codegen will mess up # the arg order if we just set them to 0. new_arg.value.lineno = node.lineno new_arg.value.col_offset = node.col_offset+100
__label__0 def begin ( self ) : if self._sync_optimizer._gradients_applied is false : # pylint : disable=protected-access raise valueerror ( `` syncreplicasoptimizer.apply_gradient should be called before using `` `` the hook . '' ) if self._is_chief : self._local_init_op = self._sync_optimizer.chief_init_op self._ready_for_local_init_op = ( self._sync_optimizer.ready_for_local_init_op ) self._q_runner = self._sync_optimizer.get_chief_queue_runner ( ) self._init_tokens_op = self._sync_optimizer.get_init_tokens_op ( self._num_tokens ) else : self._local_init_op = self._sync_optimizer.local_step_init_op self._ready_for_local_init_op = ( self._sync_optimizer.ready_for_local_init_op ) self._q_runner = none self._init_tokens_op = none
__label__0 `` ` python a `` `
__label__0 if collaborator_build : # if this is a collaborator build , then build an `` installer '' wheel and # add the collaborator packages as the only dependencies . required_packages = [ # install the tensorflow package built by intel if the user is on a # windows machine . standard_or_nightly ( 'tensorflow-intel ' , 'tf-nightly-intel ' ) + '== ' + _version + ' ; platform_system== '' windows '' ' , ]
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.acos . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 returns : a ` vocabinfo ` which represents the vocabulary information for warm-starting .
__label__0 `` ` checkpoint = tf.train.checkpoint ( model ) manager = tf.train.checkpointmanager ( checkpoint ) status = checkpoint.restore ( manager.latest_checkpoint ) `` `
__label__0 rr = _summary_iterator ( logdir )
__label__0 - a regular expression ( string ) that captures which variables to warm-start ( see tf.compat.v1.get_collection ) . this expression will only consider variables in the trainable_variables collection . - a list of strings , each representing a full variable name to warm-start . these will consider variables in global_variables collection . - a list of variables to warm-start . - ` none ` , in which case all variables in trainable_variables will be used . returns : a dictionary mapping variable names ( strings ) to lists of variables . raises : valueerror : if vars_to_warm_start is not a string , ` none ` , a list of ` variables ` , or a list of strings. `` '' '' # todo ( b/143899805 ) : remove unicode checks when deprecating python2 . if isinstance ( vars_to_warm_start , str ) or vars_to_warm_start is none : # both vars_to_warm_start = ' . * ' and vars_to_warm_start = none will match # everything ( in trainable_variables ) here . logging.info ( `` warm-starting variables only in trainable_variables . '' ) list_of_vars = ops.get_collection ( ops.graphkeys.trainable_variables , scope=vars_to_warm_start ) elif isinstance ( vars_to_warm_start , list ) : if all ( isinstance ( v , str ) for v in vars_to_warm_start ) : list_of_vars = [ ] for v in vars_to_warm_start : list_of_vars += ops.get_collection ( ops.graphkeys.global_variables , scope=v ) elif all ( checkpoint_utils._is_variable ( v ) for v in vars_to_warm_start ) : # pylint : disable=protected-access list_of_vars = vars_to_warm_start else : raise valueerror ( `` if ` vars_to_warm_start ` is a list , it must be all `` `` ` variable ` or all ` str ` . given types are { } '' .format ( [ type ( v ) for v in vars_to_warm_start ] ) ) else : raise valueerror ( `` ` vars_to_warm_start must be a ` list ` or ` str ` . given `` `` type is { } '' .format ( type ( vars_to_warm_start ) ) ) # we have to deal with partitioned variables , since get_collection flattens # out the list . grouped_variables = { } for v in list_of_vars : t = [ v ] if not isinstance ( v , list ) else v var_name = _infer_var_name ( t ) grouped_variables.setdefault ( var_name , [ ] ) .append ( v )
__label__0 self.evaluate ( variables.global_variables_initializer ( ) ) if call_saver_with_dict : saver = saver_module.saver ( { var_name : vs [ 0 ] } ) else : saver = saver_module.saver ( vs ) actual_path = saver.save ( sess , saved_path ) self.assertequal ( saved_path , actual_path )
__label__0 @ dispatch.register_unary_elementwise_api @ dispatch.add_dispatch_support def some_op ( x ) : return x * 2
__label__0 def reveal_undocumented ( symbol_name , target_module=none ) : `` '' '' reveals a symbol that was previously removed by ` remove_undocumented ` .
__label__0 # construct the vocab_info for the embedding weight . vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=prev_vocab_path , # ca n't use constant_initializer with load_and_remap . in practice , # use a truncated normal initializer . backup_initializer=init_ops.random_uniform_initializer ( minval=0.42 , maxval=0.42 ) ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * sc_vocab . * '' , var_name_to_vocab_info= { `` linear_model/sc_vocab_embedding/embedding_weights '' : vocab_info } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . var corresponding to # emb_vocab should be correctly warm-started after vocab remapping . # missing values are filled in with the embeddingcolumn 's initializer . self._assert_cols_to_vars ( cols_to_vars , { emb_vocab : [ # linear weights part 0. np.array ( [ [ 0.69 ] ] ) , # linear weights part 1. np.array ( [ [ 0.71 ] ] ) , # embedding_weights part 0. np.array ( [ [ 3. , 3.3 ] , [ 2. , 2.2 ] , [ 1. , 1.1 ] ] ) , # embedding_weights part 1. np.array ( [ [ 0.5 , 0.4 ] , [ 0.42 , 0.42 ] , [ 0.42 , 0.42 ] ] ) ] } , sess )
__label__0 mt_combined_with_path = nest.map_structure_with_tuple_paths ( tuple_path_sum , mt , mt2 , mt3 ) self.assertisinstance ( mt_combined_with_path , maskedtensor ) # metadata uses the one from the first input ( mt ) . self.assertequal ( mt_combined_with_path.mask , false ) # tesnor index is 0 for the only compoenent in maskedtensor . self.assertallequal ( mt_combined_with_path.value [ 0 ] , ( 0 , ) ) # sum of all input tensors . self.assertallequal ( mt_combined_with_path.value [ 1 ] , [ 6 ] )
__label__0 # make a mock tensorflow package that wo n't take too long to test . fake_tf = automodule ( 'faketensorflow ' ) fake_tf.module = tf.module # pylint : disable=invalid-name fake_tf.feature_column.nummeric_column = tf.feature_column.numeric_column fake_tf.keras.model = tf.keras.model fake_tf.keras.preprocessing = tf.keras.preprocessing fake_tf.keras.layers.layer = tf.keras.layers.layer fake_tf.keras.optimizers.optimizer = tf.keras.optimizers.optimizer fake_tf.nn.sigmoid_cross_entropy_with_logits = ( tf.nn.sigmoid_cross_entropy_with_logits ) fake_tf.raw_ops.add = tf.raw_ops.add fake_tf.raw_ops.print = tf.raw_ops.print # op with no xla support fake_tf.summary.audio = tf.summary.audio fake_tf.summary.audio2 = tf.summary.audio fake_tf.__version__ = tf.__version__
__label__0 > > > dict = { `` key3 '' : `` value3 '' , `` key1 '' : `` value1 '' , `` key2 '' : `` value2 '' } > > > tf.nest.flatten ( dict ) [ 'value1 ' , 'value2 ' , 'value3 ' ]
__label__0 def testwithclassmethod ( self ) : code = function_utils.get_func_code ( self.testwithclassmethod ) self.assertisnotnone ( code ) self.assertregex ( code.co_filename , 'function_utils_test.py ' )
__label__0 _fn ( ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' unit tests for object_identity . '' '' ''
__label__0 note , certain behaviors can not be tracked - for these the object may not be marked as used . examples include :
__label__0 def get_inputs ( self ) : return self.distribution.headers or [ ]
__label__0 headers = ( list ( find_files ( ' * .proto ' , 'tensorflow/compiler ' ) ) + list ( find_files ( ' * .proto ' , 'tensorflow/core ' ) ) + list ( find_files ( ' * .proto ' , 'tensorflow/python ' ) ) + list ( find_files ( ' * .proto ' , 'tensorflow/python/framework ' ) ) + list ( find_files ( ' * .proto ' , 'tensorflow/tsl ' ) ) + list ( find_files ( ' * .def ' , 'tensorflow/compiler ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/c ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/cc ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/compiler ' ) ) + list ( find_files ( ' * .h.inc ' , 'tensorflow/compiler ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/core ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/lite/kernels/shim ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/python ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/python/client ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/python/framework ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/stream_executor ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/compiler/xla/stream_executor ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/tsl ' ) ) + list ( find_files ( ' * .h ' , 'google/com_google_protobuf/src ' ) ) + list ( find_files ( ' * .inc ' , 'google/com_google_protobuf/src ' ) ) + list ( find_files ( ' * ' , 'third_party/gpus ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/include/external/com_google_absl ' ) ) + list ( find_files ( ' * .inc ' , 'tensorflow/include/external/com_google_absl ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/include/external/ducc/google ' ) ) + list ( find_files ( ' * ' , 'tensorflow/include/external/eigen_archive ' ) ) + list ( find_files ( ' * .h ' , 'tensorflow/include/external/ml_dtypes ' ) ) )
__label__0 # build the first session . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = variable_op ( 10.0 , name= '' v0 '' , dtype=dtypes.float32 )
__label__0 def __call__ ( self , * v2 : str , v1 : optional [ sequence [ str ] ] = none , allow_multiple_exports : bool = true , # deprecated , no-op ) - > api_export : ...
__label__0 code examples :
__label__0 users must not modify any collections used in nest while this function is running .
__label__0 if the structure is an atom , then returns a single-item list : [ structure ] .
__label__0 def testsparseadd ( self ) : text = `` tf.sparse.add ( a , b , thresh=t ) '' expected_text = `` tf.sparse.add ( a , b , threshold=t ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 @ parameterized.parameters ( [ ( typing.union [ int , float ] , 'union ' ) , ( typing.tuple [ int , ... ] , 'tuple ' ) , ( typing.tuple [ int , float , float ] , 'tuple ' ) , ( typing.mapping [ int , float ] , 'mapping ' ) , ( typing.union [ typing.tuple [ int ] , typing.tuple [ int , ... ] ] , 'union ' ) , # these predicates return false for generic types w/ no parameters : ( typing.union , none ) , ( typing.tuple , none ) , ( typing.mapping , none ) , ( int , none ) , ( 12 , none ) , ] ) def testgenerictypepredicates ( self , tp , expected ) : self.assertequal ( type_annotations.is_generic_union ( tp ) , expected == 'union ' ) self.assertequal ( type_annotations.is_generic_tuple ( tp ) , expected == 'tuple ' ) self.assertequal ( type_annotations.is_generic_mapping ( tp ) , expected == 'mapping ' )
__label__0 # creates and returns all the workers . sessions , graphs , train_ops = get_workers ( num_workers , replicas_to_aggregate , workers )
__label__0 return deprecated_wrapper
__label__0 def testembeddinglookup ( self ) : text = ( `` tf.nn.embedding_lookup ( params , ids , partition_strategy , name , `` `` validate_indices , max_norm ) '' ) expected_text = ( `` tf.nn.embedding_lookup ( params , ids , `` `` partition_strategy=partition_strategy , name=name , `` `` max_norm=max_norm ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # chief should have already initialized all the variables . var_0_g_0 = graphs [ 0 ] .get_tensor_by_name ( `` v0:0 '' ) var_1_g_0 = graphs [ 0 ] .get_tensor_by_name ( `` v1:0 '' ) local_step_0 = graphs [ 0 ] .get_tensor_by_name ( `` sync_rep_local_step:0 '' ) self.assertallequal ( 0.0 , sessions [ 0 ] .run ( var_0_g_0 ) ) self.assertallequal ( 1.0 , sessions [ 0 ] .run ( var_1_g_0 ) ) self.assertallequal ( 0 , sessions [ 0 ] .run ( local_step_0 ) )
__label__0 import numpy as np
__label__0 def testdispatchforpositionalsignature ( self ) :
__label__0 # todo ( b/219556836 ) : direct tf_export decorator adds non-method members to the # protocol which breaks @ runtime_checkable since it does not support them . tf_export ( `` types.experimental.supportstracingprotocol '' , v1= [ ] ) .export_constant ( __name__ , `` supportstracingprotocol '' )
__label__0 - for modality.core : refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 from tensorflow.python.compiler.xla.experimental import xla_sharding from tensorflow.python.distribute import distribute_lib from tensorflow.python.ops import array_ops from tensorflow.python.ops import cond from tensorflow.python.ops import init_ops from tensorflow.python.ops import ref_variable from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables
__label__0 returns : obj `` '' '' setattr ( obj , _for_subclass_implementers , none ) return obj
__label__0 # endif // tensorflow_core_util_version_info_h_ `` '' '' % git_version.decode ( `` utf-8 '' ) open ( filename , `` w '' ) .write ( contents )
__label__0 returns : a list of ( string , atom ) tuples. `` '' '' flat_paths = yield_flat_paths ( structure , expand_composites=expand_composites ) def stringify_and_join ( path_elements ) : return separator.join ( str ( path_element ) for path_element in path_elements )
__label__0 def testkeywordreorder ( self ) : `` '' '' test that we get the expected result if kw2 is now before kw1 . '' '' '' text = `` f ( a , b , kw1=c , kw2=d ) \n '' acceptable_outputs = [ # no change is a valid output text , # just reordering the kw .. args is also ok `` f ( a , b , kw2=d , kw1=c ) \n '' , # also cases where all arguments are fully specified are allowed `` f ( a=a , b=b , kw1=c , kw2=d ) \n '' , `` f ( a=a , b=b , kw2=d , kw1=c ) \n '' , ] ( _ , report , _ ) , new_text = self._upgrade ( reorderkeywordspec ( ) , text ) self.assertin ( new_text , acceptable_outputs ) self.assertnotin ( `` manual check required '' , report )
__label__0 # this should n't add a variable to the variables collection responsible # for variables that are saved/restored from checkpoints . self.assertequal ( len ( variables.global_variables ( ) ) , 0 )
__label__0 from typing import iterable
__label__0 # replace python 's addresses with ellipsis ( ` ... ` ) since it can change on # each execution . want = self._address_re.sub ( 'at ... > ' , want )
__label__0 @ tf_export ( `` types.experimental.atomicfunction '' , v1= [ ] ) class atomicfunction ( callable ) : `` '' '' base class for graph functions .
__label__0 _reference_pattern = _re.compile ( r'^ @ @ ( \w+ ) $ ' , flags=_re.multiline )
__label__0 * a ` dict ` of names to variables : the keys are the names that will be used to save or restore the variables in the checkpoint files . * a list of variables : the variables will be keyed with their op name in the checkpoint files .
__label__0 in a nutshell this script does : 1 ) takes lists of paths to .h/.py/.so/etc files . 2 ) creates a temporary directory . 3 ) copies files from # 1 to # 2 with some exceptions and corrections . 4 ) a wheel is created from the files in the temp directory .
__label__0 # fallback dispatch system ( dispatch v1 ) : try : return dispatch_target ( * args , * * kwargs ) except ( typeerror , valueerror ) : # note : convert_to_eager_tensor currently raises a valueerror , not a # typeerror , when given unexpected types . so we need to catch both . result = dispatch ( op_dispatch_handler , args , kwargs ) if result is not opdispatcher.not_supported : return result else : raise
__label__0 _enable_traceback_filtering = threading.local ( ) _excluded_paths = ( os.path.abspath ( os.path.join ( __file__ , ' .. ' , ' .. ' ) ) , )
__label__0 return node
__label__0 if _is_windows ( ) : return [ os.environ.get ( `` cuda_path '' , `` c : \\program files\\nvidia gpu computing toolkit\\cuda\\v % s\\ '' % cuda_version ) ] return [ `` /usr/local/cuda- % s '' % cuda_version , `` /usr/local/cuda '' , `` /usr '' , `` /usr/local/cudnn '' ] + _get_ld_config_paths ( )
__label__0 # check that flatten fails if attributes are not iterable with self.assertraisesregex ( typeerror , `` object is not iterable '' ) : flat = nest.flatten ( nesttest.badattr ( ) )
__label__0 def loop_body ( it , biases ) : biases += constant_op.constant ( 0.1 , shape= [ 32 ] ) return it + 1 , biases
__label__0 elif type_annotations.is_generic_list ( annotation ) : type_args = type_annotations.get_generic_type_args ( annotation ) if len ( type_args ) ! = 1 : raise assertionerror ( `` expected list [ ... ] to have a single type parameter '' ) elt_type = make_type_checker ( type_args [ 0 ] ) return _api_dispatcher.makelistchecker ( elt_type )
__label__0 sessions [ 0 ] .run ( train_ops [ 0 ] ) sessions [ 1 ] .run ( train_ops [ 1 ] ) # although the global step should still be 1 as explained above , the local # step should now be updated to 1. the variables are still the same . self.assertallequal ( 1 , sessions [ 1 ] .run ( global_step ) ) self.assertallequal ( 1 , sessions [ 0 ] .run ( local_step_0 ) ) self.assertallequal ( 1 , sessions [ 1 ] .run ( local_step_1 ) ) self.assertallclose ( 0 - ( 0.1 + 0.3 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_0_g_1 ) ) self.assertallclose ( 1 - ( 0.9 + 1.1 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_1_g_1 ) )
__label__0 def _getattribute2 ( self , name ) : # pylint : disable=unused-argument raise attributeerror ( `` pass to getattr '' )
__label__1 def fulljustify ( words , maxwidth ) : result = [ ] line = [ ] line_length = 0 for word in words : if line_length + len ( word ) + len ( line ) > maxwidth : for i in range ( maxwidth - line_length ) : line [ i % ( len ( line ) - 1 or 1 ) ] += ' ' result.append ( `` .join ( line ) ) line = [ ] line_length = 0 line.append ( word ) line_length += len ( word ) result.append ( ' '.join ( line ) .ljust ( maxwidth ) ) return result # test cases words1 = [ `` this '' , `` is '' , `` an '' , `` example '' , `` of '' , `` text '' , `` justification . '' ] maxwidth1 = 16 print ( fulljustify ( words1 , maxwidth1 ) ) words2 = [ `` what '' , '' must '' , '' be '' , '' acknowledgment '' , '' shall '' , '' be '' ] maxwidth2 = 16 print ( fulljustify ( words2 , maxwidth2 ) ) words3 = [ `` science '' , '' is '' , '' what '' , '' we '' , '' understand '' , '' well '' , '' enough '' , '' to '' , '' explain '' , '' to '' , '' a '' , '' computer . `` , '' art '' , '' is '' , '' everything '' , '' else '' , '' we '' , '' do '' ] maxwidth3 = 20 print ( fulljustify ( words3 , maxwidth3 ) )
__label__0 returns : json-serializable structure representing ` obj ` .
__label__0 returns : a list of strings , corresponding to the names of jobs in this cluster. `` '' '' return list ( self._cluster_spec.keys ( ) )
__label__0 if not import_rename_spec or excluded_from_module_rename ( import_alias.name , import_rename_spec ) : continue
__label__0 def convert_variables_to_tensors ( values ) : `` '' '' converts ` resourcevariable ` s in ` values ` to ` tensor ` s .
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclasscustomprotocol ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) self.assertisinstance ( mt , customnestprotocol )
__label__0 import atheris import tensorflow as tf
__label__0 # using non-iterable elements . input_tree = 0 shallow_tree = 0 flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 # creates a graph with 2 variables . v0 = variables.variable ( [ [ 2 , 1 ] ] , name= '' v0 '' ) v1 = variables.variable ( [ [ 1 ] , [ 2 ] ] , name= '' v1 '' ) v2 = math_ops.matmul ( v0 , v1 )
__label__0 with session.session ( server.target ) as sess_1 : v0 = variable_v1.variablev1 ( [ [ 2 , 1 ] ] , name= '' v0 '' ) v1 = variable_v1.variablev1 ( [ [ 1 ] , [ 2 ] ] , name= '' v1 '' ) v2 = math_ops.matmul ( v0 , v1 ) sess_1.run ( [ v0.initializer , v1.initializer ] ) self.assertallequal ( [ [ 4 ] ] , sess_1.run ( v2 ) )
__label__0 text = `` tf.nn.dropout ( x ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , text ) self.assertin ( `` tf.nn.dropout called without arguments '' , errors [ 0 ] )
__label__0 @ property def session_manager ( self ) : `` '' '' return the sessionmanager used by the supervisor .
__label__0 # check the summary was written to 'logdir ' rr = _summary_iterator ( logdir )
__label__0 input_tree = `` input_tree '' shallow_tree = [ `` shallow_tree_9 '' , `` shallow_tree_8 '' ] with self.assertraisesregex ( typeerror , expected_message ) : flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 def traverse ( root , visit ) : `` '' '' recursively enumerate all members of ` root ` .
__label__0 # remove the scope keyword or arg if it is present if scope_keyword : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` dropping scope arg from tf.contrib.layers.l1_regularizer , '' `` because it is unsupported in tf.keras.regularizers.l1\n '' ) ) node.keywords.remove ( scope_keyword ) if len ( node.args ) > 1 : node.args = node.args [ :1 ] logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` dropping scope arg from tf.contrib.layers.l1_regularizer , '' `` because it is unsupported in tf.keras.regularizers.l1\n '' ) )
__label__0 > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( `` bar '' , `` a b '' ) ( 1 , 2 ) , ... { `` a '' : 1 , `` b '' : 2 } , ... check_types=false )
__label__0 from tensorflow.core.framework.summary_pb2 import summary from tensorflow.core.util.event_pb2 import sessionlog from tensorflow.python.eager import context from tensorflow.python.framework import dtypes from tensorflow.python.framework import meta_graph from tensorflow.python.framework import ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import lookup_ops from tensorflow.python.ops import variables from tensorflow.python.platform import tf_logging as logging from tensorflow.python.summary import summary as _summary from tensorflow.python.training import coordinator from tensorflow.python.training import saver as saver_mod from tensorflow.python.training import session_manager as session_manager_mod from tensorflow.python.training import training_util from tensorflow.python.util import deprecation from tensorflow.python.util.tf_export import tf_export
__label__0 def get_field ( proto : message.message , fields : fieldtypes ) - > tuple [ any , optional [ descriptor.fielddescriptor ] ] : `` '' '' returns the field and field descriptor from the proto .
__label__0 returns : sha hash of the git commit used for the build , if available `` '' ''
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import constant_op from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.platform import test from tensorflow.python.training import server_lib
__label__0 # pylint : disable=unused-import
__label__0 def teststack ( self ) : expected_stack = inspect.stack ( ) actual_stack = tf_inspect.stack ( ) self.assertequal ( len ( expected_stack ) , len ( actual_stack ) ) self.assertequal ( expected_stack [ 0 ] [ 0 ] , actual_stack [ 0 ] [ 0 ] ) # frame object self.assertequal ( expected_stack [ 0 ] [ 1 ] , actual_stack [ 0 ] [ 1 ] ) # filename self.assertequal ( expected_stack [ 0 ] [ 2 ] , actual_stack [ 0 ] [ 2 ] - 1 ) # line number self.assertequal ( expected_stack [ 0 ] [ 3 ] , actual_stack [ 0 ] [ 3 ] ) # function name self.assertequal ( expected_stack [ 1 : ] , actual_stack [ 1 : ] )
__label__0 this simply wraps the compute_gradients ( ) from the real optimizer . the gradients will be aggregated in the apply_gradients ( ) so that user can modify the gradients like clipping with per replica global norm if needed . the global norm with aggregated gradients can be bad as one replica 's huge gradients can hurt the gradients from other replicas .
__label__0 if copy_xla_sharding and _has_same_rank ( primary.shape , slot.shape ) : slot = xla_sharding.copy_sharding ( primary , slot , use_sharding_op=false ) return slot
__label__0 def testlazyloadlocaloverride ( self ) : # test that we can override and add fields to the wrapped module . module = mockmodule ( 'test ' ) apis = { 'cmd ' : ( `` , 'cmd ' ) } wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' , public_apis=apis , deprecation=false ) import cmd as _cmd # pylint : disable=g-import-not-at-top self.assertequal ( wrapped_module.cmd , _cmd ) setattr ( wrapped_module , 'cmd ' , 1 ) setattr ( wrapped_module , 'cgi ' , 2 ) self.assertequal ( wrapped_module.cmd , 1 ) # override # verify that the values are also updated in the cache # of the fastmoduletype object self.assertequal ( wrapped_module._fastdict_get ( 'cmd ' ) , 1 ) self.assertequal ( wrapped_module.cgi , 2 ) # add self.assertequal ( wrapped_module._fastdict_get ( 'cgi ' ) , 2 )
__label__0 ` managed_session ( ) ` launches the checkpoint and summary services ( threads ) . if you need more services to run you can simply launch them in the block controlled by ` managed_session ( ) ` .
__label__0 before migrating :
__label__0 def testinitsetsdecoratornametotargetname ( self ) : self.assertequal ( 'test_function ' , tf_decorator.tfdecorator ( `` , test_function ) .__name__ )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' generate c++ reference docs for tensorflow.org . '' '' '' import os import pathlib import subprocess
__label__0 # clean up test_op_with_optional._tf_fallback_dispatchers = original_handlers
__label__0 # group the vocabless vars into one call to init_from_checkpoint . vocabless_vars = { } for var_name , variable in grouped_variables.items ( ) : prev_var_name = var_name_to_prev_var_name.get ( var_name ) if prev_var_name : prev_var_name_used.add ( var_name ) vocab_info = var_name_to_vocab_info.get ( var_name ) if vocab_info : vocab_info_used.add ( var_name ) warmstarted_count += 1 logging.debug ( `` warm-starting variable : { } ; current_vocab : { } current_vocab_size : { } '' `` prev_vocab : { } prev_vocab_size : { } current_oov : { } prev_tensor : { } '' `` initializer : { } '' .format ( var_name , vocab_info.new_vocab , vocab_info.new_vocab_size , vocab_info.old_vocab , ( vocab_info.old_vocab_size if vocab_info.old_vocab_size > 0 else `` all '' ) , vocab_info.num_oov_buckets , prev_var_name or `` unchanged '' , vocab_info.backup_initializer or `` zero-initialized '' ) ) _warm_start_var_with_vocab ( variable , current_vocab_path=vocab_info.new_vocab , current_vocab_size=vocab_info.new_vocab_size , prev_ckpt=ckpt_to_initialize_from , prev_vocab_path=vocab_info.old_vocab , previous_vocab_size=vocab_info.old_vocab_size , current_oov_buckets=vocab_info.num_oov_buckets , prev_tensor_name=prev_var_name , initializer=vocab_info.backup_initializer , axis=vocab_info.axis ) else : # for the special value of vars_to_warm_start = none , # we only warm-start variables with explicitly specified vocabularies . if vars_to_warm_start : warmstarted_count += 1 logging.debug ( `` warm-starting variable : { } ; prev_var_name : { } '' .format ( var_name , prev_var_name or `` unchanged '' ) ) # because we use a default empty list in grouped_variables , single # unpartitioned variables will be lists here , which we rectify in order # for init_from_checkpoint logic to work correctly . if len ( variable ) == 1 : variable = variable [ 0 ] prev_tensor_name , var = _get_var_info ( variable , prev_var_name ) if prev_tensor_name in vocabless_vars : # the api for checkpoint_utils.init_from_checkpoint accepts a mapping # from checkpoint tensor names to model variable names , so it does not # support warm-starting two variables from the same tensor . our work- # around is to run init_from_checkpoint multiple times , each time we # encounter a new variable that should be initialized by a previously- # used tensor . logging.debug ( `` requested prev_var_name { } initialize both { } and { } ; `` `` calling init_from_checkpoint . `` .format ( prev_tensor_name , vocabless_vars [ prev_tensor_name ] , var ) ) checkpoint_utils.init_from_checkpoint ( ckpt_to_initialize_from , vocabless_vars ) vocabless_vars.clear ( ) vocabless_vars [ prev_tensor_name ] = var
__label__0 args : config : ( options . ) a ` tf.compat.v1.configproto ` that specifies default configuration options for all sessions that run on this server . start : ( optional . ) boolean , indicating whether to start the server after creating it . defaults to ` true ` .
__label__0 for d in decorators : if d.decorator_argspec is not none : return _convert_maybe_argspec_to_fullargspec ( d.decorator_argspec ) return _getfullargspec ( target )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testassertsamestructure ( self ) : structure1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) structure2 = ( ( ( `` foo1 '' , `` foo2 '' ) , `` foo3 '' ) , `` foo4 '' , ( `` foo5 '' , `` foo6 '' ) ) structure_different_num_elements = ( `` spam '' , `` eggs '' ) structure_different_nesting = ( ( ( 1 , 2 ) , 3 ) , 4 , 5 , ( 6 , ) ) nest.assert_same_structure ( structure1 , structure2 ) nest.assert_same_structure ( `` abc '' , 1.0 ) nest.assert_same_structure ( `` abc '' , np.array ( [ 0 , 1 ] ) ) nest.assert_same_structure ( `` abc '' , constant_op.constant ( [ 0 , 1 ] ) )
__label__0 args : iterable : an iterable .
__label__1 def bark ( self ) : print ( `` woof ! '' )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( all_linear_cols , partitioner ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=vocab_path ) ws_util.warm_start ( self.get_temp_dir ( ) , var_name_to_vocab_info= { `` linear_model/sc_vocab/weights '' : vocab_info } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { sc_int : [ prev_int_val ] , sc_hash : [ prev_hash_val ] , sc_keys : [ prev_keys_val ] , sc_vocab : [ prev_vocab_val ] , real_bucket : [ prev_bucket_val ] , cross : [ prev_cross_val ] , `` bias '' : [ prev_bias_val ] , } , sess )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_one_line_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 @ typing.runtime_checkable class customnestprotocol ( protocol ) : `` '' '' protocol for adding custom tf.nest support in user-defined classes .
__label__0 define a dummy model and loss :
__label__0 def testinitopfailsfortransientvariable ( self ) : server = server_lib.server.create_local_server ( ) logdir = self._test_dir ( `` default_init_op_fails_for_local_variable '' ) with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' , collections= [ ops.graphkeys.local_variables ] ) variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' w '' , collections= [ ops.graphkeys.local_variables ] ) # w will not be initialized . sv = supervisor.supervisor ( logdir=logdir , local_init_op=v.initializer ) with self.assertraisesregex ( runtimeerror , `` variables not initialized : w '' ) : sv.prepare_or_wait_for_session ( server.target )
__label__0 # if we used the alias , it should get renamed text = `` g ( a , b , kw1_alias=x , c=c ) \n '' acceptable_outputs = [ `` g ( a , b , kw1=x , c=c ) \n '' , `` g ( a , b , c=c , kw1=x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , `` g ( a=a , b=b , c=c , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 below is an example of migrating away from using a global step to using a keras optimizer :
__label__0 args : x : python object . error_in_function : python bool . if ` true ` , a ` runtimeerror ` is raised if the returned value is never used when created during ` tf.function ` tracing . warn_in_eager : python bool . if ` true ` raise warning if in eager mode as well as graph mode .
__label__0 field , _ = util.get_field ( proto , [ `` nested_map_bool '' , true , `` string_field '' ] ) self.assertequal ( `` string_true '' , field )
__label__0 returns : true if the node uses starred variadic positional args or keyword args . false if it does not. `` '' '' if sys.version_info [ :2 ] > = ( 3 , 5 ) : # check for an * args usage in python 3.5+ for arg in node.args : if isinstance ( arg , ast.starred ) : return true else : if node.starargs : return true return false
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 def _testrestorefromtraingraphwithcontrolcontext ( self , test_dir ) : train_filename = os.path.join ( test_dir , `` train_metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : # restores from metagraphdef . new_saver = saver_module.import_meta_graph ( train_filename ) # restores from checkpoint . new_saver.restore ( sess , saver0_ckpt ) train_op = ops_lib.get_collection ( `` train_op '' ) [ 0 ] self.evaluate ( train_op )
__label__1 class solution : def ispathcrossing ( self , distance : list [ int ] ) - > bool : # initialize position and visited set x , y = 0 , 0 visited = { ( 0 , 0 ) } # define directions : north , west , south , east directions = [ ( 0 , 1 ) , ( -1 , 0 ) , ( 0 , -1 ) , ( 1 , 0 ) ] direction_index = 0 # traverse the distance array for d in distance : dx , dy = directions [ direction_index ] # update position for _ in range ( d ) : x += dx y += dy # check if current position is visited if ( x , y ) in visited : return true visited.add ( ( x , y ) ) # update direction direction_index = ( direction_index + 1 ) % 4 return false
__label__0 `` `` ''
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 for testing ` deprecation.deprecate_moved_module ` . `` '' ''
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_no_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 @ tf_export ( `` config.experimental.clusterdevicefilters '' ) class clusterdevicefilters : `` '' '' represent a collection of device filters for the remote workers in cluster .
__label__0 def testintopk ( self ) : text = `` tf.math.in_top_k ( a , b , c , n ) '' expected_text = ( `` tf.math.in_top_k ( predictions=a , targets=b , k=c , name=n ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def _gather_saveables_for_checkpoint ( self ) : return { trackable_base.variable_value_key : self.non_dep_variable }
__label__0 4. numpy array ( considered a scalar ) :
__label__0 def testposonly2 ( self ) : self.assertequal ( self._matmul_func.canonicalize ( 2 , 3 , true , false , true ) , [ 2 , 3 , true , false , true , false , false , false , none ] )
__label__0 # value to pass for the 'ready_op ' , 'init_op ' , 'summary_op ' , 'saver ' , # and 'global_step ' parameters of supervisor.__init__ ( ) to indicate that # the default behavior should be used . use_default = 0
__label__0 def __delattr__ ( self , name ) : if name.startswith ( _tensorflow_lazy_loader_prefix ) : super ( ) .__delattr__ ( name ) else : module = self._load ( ) delattr ( module , name ) self.__dict__.pop ( name ) try : # check if the module has __all__ if name in self.__all__ : self.__all__.remove ( name ) except attributeerror : pass
__label__0 @ property def server_def ( self ) : `` '' '' returns the ` tf.train.serverdef ` for this server .
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' parses results from run_onednn_benchmarks.sh .
__label__0 args : node : current node `` '' '' assert self._stack [ -1 ] is node
__label__0 for ( kwarg , arg ) , ( level , warning ) in sorted ( arg_warnings.items ( ) ) : present , _ = get_arg_value ( node , kwarg , arg ) or variadic_args if present : warned = true warning_message = warning.replace ( `` < function name > '' , full_name or name ) template = `` % s called with % s argument , requires manual check : % s '' if variadic_args : template = ( `` % s called with * args or * * kwargs that may include % s , `` `` requires manual check : % s '' ) self.add_log ( level , node.lineno , node.col_offset , template % ( full_name or name , kwarg , warning_message ) )
__label__0 # find first argument with default value set . first_default = next ( ( idx for idx , x in enumerate ( all_defaults ) if x is not no_default ) , none )
__label__0 def _testtypesforftrlmultiplylinearbylr ( self , x , y , z , lr , grad , use_gpu=none , l1=0.0 , l2=0.0 , lr_power=-0.5 ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) linear = variable_v1.variablev1 ( z ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def update ( self ) : raise notimplementederror ( 'subclasses need to override this ' )
__label__1 def square_root ( x ) : return x * * 0.5
__label__0 def _is_bound_method ( fn ) : _ , fn = tf_decorator.unwrap ( fn ) return tf_inspect.ismethod ( fn ) and ( fn.__self__ is not none )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 raises : runtimeerror : if called with eager execution enabled .
__label__0 # verifies copy to different graph . graph2 = ops_lib.graph ( ) with graph2.as_default ( ) : new_var_list_1 = meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden1 '' , to_scope= '' new_hidden1 '' , from_graph=graph1 , to_graph=graph2 )
__label__0 def prepare_wheel_srcs ( headers : list [ str ] , srcs : list [ str ] , aot : list [ str ] , srcs_dir : str , version : str ) - > none : `` '' '' rearrange source and header files . args : headers : a list of paths to header files . srcs : a list of paths to the rest of files . aot : a list of paths to files that should be in xla_aot directory . srcs_dir : directory to copy files to . version : tensorflow version. `` '' '' prepare_headers ( headers , os.path.join ( srcs_dir , `` tensorflow/include '' ) ) prepare_srcs ( srcs , srcs_dir ) prepare_aot ( aot , os.path.join ( srcs_dir , `` tensorflow/xla_aot_runtime_src '' ) )
__label__0 def doc_private ( obj : t ) - > t : `` '' '' a decorator : generates docs for private methods/functions .
__label__0 class samenamedtype1 ( samenameab ) : pass
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( all_linear_cols , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , all weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_int : [ np.zeros ( [ 10 , 1 ] ) ] , sc_hash : [ np.zeros ( [ 15 , 1 ] ) ] , sc_keys : [ np.zeros ( [ 4 , 1 ] ) ] , sc_vocab : [ np.zeros ( [ 4 , 1 ] ) ] , real_bucket : [ np.zeros ( [ 5 , 1 ] ) ] , cross : [ np.zeros ( [ 20 , 1 ] ) ] , } , sess )
__label__0 if flags.gen_ops : # ` $ yes | configure ` yes = subprocess.popen ( [ 'yes ' , `` ] , stdout=subprocess.pipe ) configure = subprocess.popen ( [ tensorflow_root / 'configure ' ] , stdin=yes.stdout , cwd=tensorflow_root ) configure.communicate ( )
__label__0 class configerror ( exception ) : pass
__label__0 a subclass instance of ` tf.distribute.distributedvalues ` is created when creating variables within a distribution strategy , iterating a ` tf.distribute.distributeddataset ` or through ` tf.distribute.strategy.run ` . this base class should never be instantiated directly . ` tf.distribute.distributedvalues ` contains a value per replica . depending on the subclass , the values could either be synced on update , synced on demand , or never synced .
__label__0 # verifies behavior of tf.session.reset ( ) with multiple containers using # default container names as defined by the target name . # todo ( b/34465411 ) : starting multiple servers with different configurations # in the same test is flaky . move this test case back into # `` server_lib_test.py '' when this is no longer the case . def testsamevariablesclearcontainer ( self ) : # starts two servers with different names so they map to different # resource `` containers '' . server0 = server_lib.server ( { `` local0 '' : [ `` localhost:0 '' ] } , protocol= '' grpc '' , start=true ) server1 = server_lib.server ( { `` local1 '' : [ `` localhost:0 '' ] } , protocol= '' grpc '' , start=true )
__label__0 def __repr__ ( self ) : return `` objectidentitydictionary ( % s ) '' % repr ( self._storage )
__label__0 def _get_deprecated_positional_arguments ( names_to_ok_vals , arg_spec ) : `` '' '' builds a dictionary from deprecated arguments to their spec .
__label__0 # saves to different checkpoints . saver0.save ( sess , saver0_ckpt ) saver1.save ( sess , saver1_ckpt )
__label__0 def _wrap_restore_error_with_msg ( err , extra_verbiage ) : err_msg = ( `` restoring from checkpoint failed . this is most likely `` `` due to { } from the checkpoint . please ensure that you `` `` have not altered the graph expected based on the checkpoint. `` `` original error : \n\n { } '' ) .format ( extra_verbiage , err.message ) return err.__class__ ( err.node_def , err.op , err_msg )
__label__0 yields : pairs of ( path , value ) , where path the tuple path of a leaf node in shallow_tree , and value is the value of the corresponding node in input_tree. `` '' '' if not is_nested_fn ( shallow_tree ) : yield ( path , input_tree ) else : input_tree = dict ( _tf_core_yield_sorted_items ( input_tree ) ) for ( shallow_key , shallow_subtree , ) in _tf_core_yield_sorted_items ( shallow_tree ) : subpath = path + ( shallow_key , ) input_subtree = input_tree [ shallow_key ] for leaf_path , leaf_value in _tf_core_yield_flat_up_to ( shallow_subtree , input_subtree , is_nested_fn , path=subpath ) : yield ( leaf_path , leaf_value )
__label__0 # # # # how to map arguments | tf1 arg name | tf2 arg name | note | | -- -- -- -- -- -- -- -- -- | -- -- -- -- -- -- - | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | | ` learning_rate ` | ` learning_rate ` | be careful of setting | : : : learning_rate tensor value computed from the global step . : : : : in tf1 this was usually meant to imply a dynamic learning rate and : : : : would recompute in each step . in tf2 ( eager + function ) it will : : : : treat it as a scalar value that only gets computed once instead of : : : : a symbolic placeholder to be computed each time . : | ` decay ` | ` rho ` | - | | ` momentum ` | ` momentum ` | - | | ` epsilon ` | ` epsilon ` | default value is 1e-10 in tf1 , | : : : but 1e-07 in tf2 . : | ` use_locking ` | - | not applicable in tf2 . |
__label__0 if prev_var_name_not_used : raise valueerror ( `` you provided the following variables in `` `` var_name_to_prev_var_name that were not used : `` `` { 0 } . perhaps you misspelled them ? here is the list of viable `` `` variable names : { 1 } '' .format ( prev_var_name_not_used , grouped_variables.keys ( ) ) ) if vocab_info_not_used : raise valueerror ( `` you provided the following variables in `` `` var_name_to_vocab_info that were not used : { 0 } . `` `` perhaps you misspelled them ? here is the list of viable variable `` `` names : { 1 } '' .format ( vocab_info_not_used , grouped_variables.keys ( ) ) )
__label__0 if __name__ == `` __main__ '' : test_lib.main ( )
__label__0 args : sess : a ` session ` .
__label__0 def testwithsimplefunction ( self ) : self.assertequal ( 'silly_example_function ' , function_utils.get_func_name ( silly_example_function ) )
__label__0 with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) prev_tensor_name , var = ws_util._get_var_info ( fruit_weights ) checkpoint_utils.init_from_checkpoint ( self.get_temp_dir ( ) , { prev_tensor_name : var } ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( prev_val , fruit_weights.eval ( sess ) )
__label__0 note the method does not check the types of atoms inside the structures .
__label__0 this decorator logs a deprecation warning whenever the decorated function is called with the deprecated argument . it has the following format :
__label__0 class objectidentitydictionary ( collections_abc.mutablemapping ) : `` '' '' a mutable mapping data structure which compares using `` is '' .
__label__0 the ` inputs ` are flattened up to ` shallow_tree ` before being mapped .
__label__0 def testprint ( self ) : # tf.print ( ) can not be parsed unless we import print_function text = `` '' '' from __future__ import print_function tf.print ( ) tf.print ( 'abc ' ) `` '' '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , text ) # text should stay the same
__label__0 originally developed by researchers and engineers from the google brain team within google 's ai organization , it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains . tensorflow is licensed under [ apache 2.0 ] ( https : //github.com/tensorflow/tensorflow/blob/master/license ) . `` '' ''
__label__0 * check the types of iterables :
__label__0 import numpy as _np
__label__0 @ tf_export ( v1= [ `` train.sessionrunargs '' ] ) class sessionrunargs ( collections.namedtuple ( `` sessionrunargs '' , [ `` fetches '' , `` feed_dict '' , `` options '' ] ) ) : `` '' '' represents arguments to be added to a ` session.run ( ) ` call .
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 def testwithfunctoolspartial ( self ) : partial = functools.partial ( silly_example_function ) code = function_utils.get_func_code ( partial ) self.assertisnone ( code )
__label__0 if not isinstance ( names_to_saveables , dict ) : names_to_saveables = saveable_object_util.op_list_to_dict ( names_to_saveables ) saveables = saveable_object_util.validate_and_slice_inputs ( names_to_saveables ) if max_to_keep is none : max_to_keep = 0
__label__0 def wrapper ( * args , * * kwargs ) : if args : raise typeerror ( ' { f } only takes keyword args ( possible keys : { kwargs } ) . ' 'please pass these args as kwargs instead . '.format ( f=f.__name__ , kwargs=f_argspec.args ) ) return f ( * * kwargs )
__label__0 config_path = r.rlocation ( f'implib_so/arch/ { args.target } /config.ini ' ) table_path = r.rlocation ( f'implib_so/arch/ { args.target } /table.s.tpl ' ) trampoline_path = r.rlocation ( f'implib_so/arch/ { args.target } /trampoline.s.tpl ' )
__label__0 note that the sharded save procedure for the v2 format is different from v1 : there is a special `` merge '' step that merges the small metadata produced from each device .
__label__0 args : required_version : the version specified by the user . actual_version : the version detected from the cuda installation . returns : whether the actual version matches the required one. `` '' '' if actual_version is none : return false
__label__0 usage is identical to ` contextlib.contextmanager ` .
__label__0 logging.info ( `` warm-started % d variables . `` , warmstarted_count )
__label__0 _keep_tensor_proto_fields = ( `` dtype '' , `` tensor_shape '' , `` version_number '' )
__label__0 if deterministic : if seed_arg is none : new_keywords.append ( ast.keyword ( arg= '' seed '' , value=ast.num ( 42 ) ) ) logs.add ( ( ast_edits.info , node.lineno , node.col_offset , `` adding seed=42 to call to % s since determinism was requested '' % ( full_name or name ) ) ) else : logs.add ( ( ast_edits.warning , node.lineno , node.col_offset , `` the deterministic argument is deprecated for % s , pass a `` `` non-zero seed for determinism . the deterministic argument is `` `` present , possibly not false , and the seed is already set . the `` `` converter can not determine whether it is nonzero , please check . '' ) )
__label__0 def testserializesaverwithscope ( self ) : test_dir = self._get_test_dir ( `` export_graph_def '' ) saver1_ckpt = os.path.join ( test_dir , `` saver1.ckpt '' ) saver2_ckpt = os.path.join ( test_dir , `` saver2.ckpt '' ) graph = ops_lib.graph ( ) with graph.as_default ( ) : with ops_lib.name_scope ( `` hidden1 '' ) : variable1 = variable_v1.variablev1 ( [ 1.0 ] , name= '' variable1 '' ) saver1 = saver_module.saver ( var_list= [ variable1 ] ) graph.add_to_collection ( ops_lib.graphkeys.savers , saver1 )
__label__0 * a single python dict :
__label__0 import io import os import glob import platform import re import subprocess import sys
__label__0 def _get_header_version ( path , name ) : `` '' '' returns preprocessor defines in c header file . '' '' '' for line in io.open ( path , `` r '' , encoding= '' utf-8 '' ) : match = re.match ( r '' # define % s + ( \d+ ) '' % name , line ) if match : value = match.group ( 1 ) return int ( value )
__label__0 def is_real_file ( dirpath , fname ) : fpath = os.path.join ( dirpath , fname ) return os.path.isfile ( fpath ) and not os.path.islink ( fpath )
__label__0 this proto implements the ` dict [ str , featurelist ] ` portion. `` '' ''
__label__0 def for_subclass_implementers ( obj : t ) - > t : `` '' '' a decorator : only generate docs for this method in the defining class .
__label__0 _ , packed = _tf_data_packed_nest_with_indices ( structure , flat_sequence , 0 ) return sequence_like ( structure , packed ) # pylint : disable=protected-access
__label__0 with self.assertraises ( lookuperror ) : compat.as_text ( b '' hello '' , `` invalid '' )
__label__0 def testsavetononexistingpath ( self ) : file_io.write_string_to_file ( os.path.join ( self.get_temp_dir ( ) , `` actually_a_file '' ) , `` '' ) paths = [ os.path.join ( self.get_temp_dir ( ) , `` nonexisting_dir/path '' ) , os.path.join ( self.get_temp_dir ( ) , `` other_nonexisting_dir/path1/path2 '' ) , os.path.join ( self.get_temp_dir ( ) , `` actually_a_file/path '' ) , ]
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] ) z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y.values ) self.assertallequal ( z.mask , x.mask & y.mask )
__label__0 with session.session ( config=no_constfold_config ) as sess : self.evaluate ( init_op ) actual_grad_value = self.evaluate ( grad ) self.assertequal ( expected_grad_value , actual_grad_value )
__label__0 `` ` python ... # create a saver . saver = tf.compat.v1.train.saver ( ... variables ... ) # launch the graph and train , saving the model every 1,000 steps . sess = tf.compat.v1.session ( ) for step in range ( 1000000 ) : sess.run ( .. training_op .. ) if step % 1000 == 0 : # append the step number to the checkpoint name : saver.save ( sess , 'my-model ' , global_step=step ) `` `
__label__0 # register functions @ tf_export ( '__internal__.register_call_context_function ' , v1= [ ] ) def register_call_context_function ( func ) : global _keras_call_context_function _keras_call_context_function = func
__label__0 returns : a ` session ` object that can be used to drive the model .
__label__0 def _do_not_descend ( self , path , name ) : `` '' '' safely queries if a specific fully qualified name should be excluded . '' '' '' return ( path in self._do_not_descend_map and name in self._do_not_descend_map [ path ] )
__label__0 return hipruntime_config
__label__0 def testmapstructureoverplaceholders ( self ) : # test requires placeholders and thus requires graph mode with ops.graph ( ) .as_default ( ) : inp_a = ( array_ops.placeholder ( dtypes.float32 , shape= [ 3 , 4 ] ) , array_ops.placeholder ( dtypes.float32 , shape= [ 3 , 7 ] ) ) inp_b = ( array_ops.placeholder ( dtypes.float32 , shape= [ 3 , 4 ] ) , array_ops.placeholder ( dtypes.float32 , shape= [ 3 , 7 ] ) )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_prop_with_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 returns : a list of valid task indices in the given job .
__label__0 # upgrading explicitly-versioned tf code is unsafe , but we do n't # need to throw errors when we detect explicitly-versioned tf . import_header = `` import tensorflow.compat.v1 as tf\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` ` tensorflow.compat.v1 ` was directly imported as ` tf ` `` , report ) self.assertempty ( errors )
__label__0 # check that values stayed the same self.assertequal ( lazy_loader_module.foo , foo )
__label__0 def __len__ ( self ) : # used by ` nest.map_structure_up_to ` and releatd functions to verify the # arity compatibility . return 1
__label__1 def addoperators ( num , target ) : def generateexpressions ( idx , expr , evaluated , target ) : if idx == len ( num ) : if evaluated == target : result.append ( expr ) return for i in range ( idx , len ( num ) ) : if i > idx and num [ idx ] == ' 0 ' : # skip leading zeros break curr_str = num [ idx : i+1 ] curr_val = int ( curr_str ) if idx == 0 : generateexpressions ( i + 1 , curr_str , curr_val , target ) else : generateexpressions ( i + 1 , expr + '+ ' + curr_str , evaluated + curr_val , target ) generateexpressions ( i + 1 , expr + '- ' + curr_str , evaluated - curr_val , target ) generateexpressions ( i + 1 , expr + ' * ' + curr_str , evaluated - prev_val + prev_val * curr_val , target ) result = [ ] generateexpressions ( 0 , `` '' , 0 , target ) return result # test cases num1 , target1 = `` 123 '' , 6 print ( addoperators ( num1 , target1 ) ) # output : [ `` 1 * 2 * 3 '' , '' 1+2+3 '' ] num2 , target2 = `` 232 '' , 8 print ( addoperators ( num2 , target2 ) ) # output : [ `` 2 * 3+2 '' , '' 2+3 * 2 '' ] num3 , target3 = `` 3456237490 '' , 9191 print ( addoperators ( num3 , target3 ) ) # output : [ ]
__label__0 # lint.ifchange class _objectidentitywrapper : `` '' '' wraps an object , mapping __eq__ on wrapper to `` is '' on wrapped .
__label__0 returns : the tuple ( new_index , child ) , where : * new_index - the updated index into ` flat ` having processed ` structure ` . * packed - the subset of ` flat ` corresponding to ` structure ` , having started at ` index ` , and packed into the same nested format .
__label__0 func_signature = tf_inspect.signature ( func ) func_argspec = tf_inspect.getargspec ( func ) if `` name '' in func_signature.parameters or func_argspec.keywords is not none : return func # no wrapping needed ( already has name parameter ) .
__label__0 @ parameterized.parameters ( [ ( typing.union [ int , float ] , ( int , float ) ) , ( typing.tuple [ int , ... ] , ( int , ellipsis ) ) , ( typing.tuple [ int , float , float ] , ( int , float , float , ) ) , ( typing.mapping [ int , float ] , ( int , float ) ) , ( typing.union [ typing.tuple [ int ] , typing.tuple [ int , ... ] ] , ( typing.tuple [ int ] , typing.tuple [ int , ... ] ) ) , ] ) def testgetgenerictypeargs ( self , tp , expected ) : self.assertequal ( type_annotations.get_generic_type_args ( tp ) , expected )
__label__0 def _expand_cmakedefines ( line , cmake_vars ) : `` '' '' expands # cmakedefine declarations , using a dictionary 'cmake_vars ' . '' '' ''
__label__0 frames = func ( ) # callsite
__label__0 copy the following code into the old module :
__label__0 def testdispatchforunion ( self ) : maybemasked = typing.union [ maskedtensor , tensor_lib.tensor ]
__label__0 def testpreparesessionsucceedswithinitfeeddict ( self ) : with ops.graph ( ) .as_default ( ) : p = array_ops.placeholder ( dtypes.float32 , shape= ( 3 , ) ) v = variable_v1.variablev1 ( p , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) , init_feed_dict= { p : [ 1.0 , 2.0 , 3.0 ] } ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) )
__label__0 # we use the same setup.py for all tensorflow_ * packages and for the nightly # equivalents ( tf_nightly_ * ) . the package is controlled from the argument line # when building the pip package . project_name = 'tensorflow ' if os.environ.get ( 'project_name ' , none ) : project_name = os.environ [ 'project_name ' ]
__label__0 v = variable_v1.variablev1 ( 10.0 , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } , max_to_keep=2 ) self.evaluate ( variables.global_variables_initializer ( ) ) if not context.executing_eagerly ( ) : self.assertequal ( [ ] , save.last_checkpoints )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 if ` timer_interval_secs ` is none the thread calls ` target ( * args , * * kwargs ) ` repeatedly . otherwise it calls it every ` timer_interval_secs ` seconds . the thread terminates when a stop is requested .
__label__0 def _wrap_key ( self , key ) : return _weakobjectidentitywrapper ( key )
__label__0 return handler
__label__0 job_def = self._cluster_def.job.add ( ) job_def.name = job_name
__label__0 # # type collections
__label__0 def func ( a=1 ) : return a
__label__0 returns : output as string. `` '' '' try : output = subprocess.check_output ( args , shell=true , stderr=subprocess.stdout ) except subprocess.calledprocesserror as e : output = e.output return output.strip ( )
__label__0 def testallapi ( self ) : if not hasattr ( tf.compat , `` v2 '' ) : return
__label__0 def get_wrapper ( func ) :
__label__0 class _mirroringsaveable ( saver_module.basesaverbuilder.resourcevariablesaveable ) :
__label__0 class supervisortest ( test.testcase ) :
__label__0 if op_pos_names ! = func_pos_names : raise assertionerror ( `` the decorated function 's non-default arguments must be identical '' `` to that of the overridden op . '' f '' func has { func_pos_names } . op has { op_pos_names } . '' )
__label__0 # todo ( b/225045380 ) : move to a `` leaf '' library to use in trace_type . def is_namedtuple ( instance , strict=false ) : `` '' '' returns true iff ` instance ` is a ` namedtuple ` .
__label__0 # create a second helper , identical to the first . save2 = saver_module.saver ( saver_def=save.as_saver_def ( ) ) save2.set_last_checkpoints ( save.last_checkpoints )
__label__0 returns : a python list , the partially flattened version of ` input_tree ` according to the structure of ` shallow_tree ` .
__label__0 flattened_input_tree = flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = flatten_up_to ( shallow_tree , shallow_tree )
__label__0 this method runs the ops added by the constructor for restoring variables . it requires a session in which the graph was launched . the variables to restore do not have to have been initialized , as restoring is itself a way to initialize variables .
__label__0 flat_structure = _tf_data_flatten ( structure ) if len ( flat_structure ) ! = len ( flat_sequence ) : raise valueerror ( `` could not pack sequence . argument ` structure ` had `` f '' { len ( flat_structure ) } elements , but argument ` flat_sequence ` had `` f '' { len ( flat_sequence ) } elements . received structure : `` f '' { structure } , flat_sequence : { flat_sequence } . '' )
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` cusolver_ver_major '' , `` cusolver_ver_minor '' , `` cusolver_ver_patch '' ) ) return `` . `` .join ( version )
__label__0 with open ( file_c , `` a '' ) as f : f.write ( `` import foo as f '' ) os.symlink ( file_c , file_d )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenandpack ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) leaves = nest.flatten ( mt ) reconstructed_mt = nest.pack_sequence_as ( mt , leaves ) self.assertisinstance ( reconstructed_mt , maskedtensor ) self.assertequal ( reconstructed_mt , mt )
__label__0 @ tf_export ( v1= [ `` train.import_meta_graph '' ] ) def import_meta_graph ( meta_graph_or_file , clear_devices=false , import_scope=none , * * kwargs ) : `` '' '' recreates a graph saved in a ` metagraphdef ` proto .
__label__0 class testclassa ( object ) : pass
__label__0 copy_tx = types.new_class ( tx.__name__ , bases , exec_body=set_body ) copy_tx.__init__ = _new__init__ copy_tx.__getattribute__ = _new__getattribute__ for op in overloadable_operators : if hasattr ( type_x , op ) : setattr ( copy_tx , op , getattr ( type_x , op ) )
__label__0 def get_symbol_for_name ( root , name ) : name_parts = name.split ( `` . '' ) symbol = root # iterate starting with second item since 1st item is `` tf. '' . for part in name_parts [ 1 : ] : symbol = getattr ( symbol , part ) return symbol
__label__0 fixed_config.default_value.copyfrom ( tensor_util.make_tensor_proto ( fetched [ dense_def_start + i ] ) ) # convert the shape from the attributes # into a tensorshapeproto . fixed_config.shape.copyfrom ( tensor_shape.tensorshape ( dense_shapes [ i ] ) .as_proto ( ) )
__label__1 class solution : def countsmaller ( self , nums : list [ int ] ) - > list [ int ] : def mergesort ( nums , indices ) : if len ( nums ) < = 1 : return nums , [ 0 ] # base case : a single element has 0 smaller elements to the right mid = len ( nums ) // 2 left , left_indices = mergesort ( nums [ : mid ] , indices [ : mid ] ) right , right_indices = mergesort ( nums [ mid : ] , indices [ mid : ] ) merged = [ ] merged_indices = [ ] count = 0 i , j = 0 , 0 while i < len ( left ) or j < len ( right ) : if j == len ( right ) or ( i < len ( left ) and left [ i ] < = right [ j ] ) : merged.append ( left [ i ] ) merged_indices.append ( left_indices [ i ] ) result [ left_indices [ i ] ] += count # update the count of smaller elements i += 1 else : merged.append ( right [ j ] ) merged_indices.append ( right_indices [ j ] ) count += 1 # increment count for each element from the right half j += 1 return merged , merged_indices result = [ 0 ] * len ( nums ) mergesort ( nums , list ( range ( len ( nums ) ) ) ) return result
__label__0 def clear_preprocessing ( self ) : `` '' '' restore this apichangespec to before it preprocessed a file .
__label__0 args : opts : command line options object
__label__0 return miopen_config
__label__0 class tfmodulewrapper ( fastmoduletype ) : `` '' '' wrapper for tf modules to support deprecation messages and lazyloading . '' '' '' # ensures that compat.v1 api usage is recorded at most once compat_v1_usage_recorded = false
__label__0 args : seq : the value to test .
__label__0 # this should create 6 chunks : # [ parent graphdef , node [ 1 ] , node [ 2 ] , node [ 3 ] , node [ 5 ] , chunkedmessage ] graph_def = self._make_graph_def_with_constant_nodes ( sizes ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , chunked_message = s.split ( ) self.assertlen ( chunks , 5 ) self._assert_chunk_sizes ( chunks , max_size )
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for functions used to extract and analyze stacks . '' '' ''
__label__0 def test_convert_variables_in_composite_tensor ( self ) : ct2 = ct2 ( resource_variable_ops.resourcevariable ( 1 ) ) if not context.executing_eagerly ( ) : self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def _cartesian_product ( first , second ) : `` '' '' returns all path combinations of first and second . '' '' '' return [ os.path.join ( f , s ) for f in first for s in second ]
__label__0 * ` bytes_or_text_types ` * ` complex_types ` * ` integral_types ` * ` real_types `
__label__0 def test_non_class ( self ) : integer = 5 visitor = testvisitor ( ) traverse.traverse ( integer , visitor ) self.assertequal ( [ ] , visitor.call_log )
__label__0 > > > from google.protobuf import text_format > > > example = text_format.parse ( `` ' ... features { ... feature { key : `` my_feature '' ... value { bytes_list { value : [ 'abc ' , '12345 ' ] } } } ... } '' ' , ... tf.train.example ( ) ) > > > > > > example.features.feature [ 'my_feature ' ] .bytes_list.value [ `` abc '' , `` 12345 '' ]
__label__0 class configerror ( exception ) : pass
__label__0 text = `` tf.gradients ( yx=a , colocate_gradients_with_ops=false ) \n '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` tf.gradients ( yx=a ) \n '' , new_text ) self.assertin ( `` tf.gradients no longer takes '' , report )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testisnested ( self ) : self.assertfalse ( nest.is_nested ( `` 1234 '' ) ) self.asserttrue ( nest.is_nested ( [ 1 , 3 , [ 4 , 5 ] ] ) ) self.asserttrue ( nest.is_nested ( ( ( 7 , 8 ) , ( 5 , 6 ) ) ) ) self.asserttrue ( nest.is_nested ( [ ] ) ) self.asserttrue ( nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } ) ) self.asserttrue ( nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .keys ( ) ) ) self.asserttrue ( nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .values ( ) ) ) self.asserttrue ( nest.is_nested ( { `` a '' : 1 , `` b '' : 2 } .items ( ) ) ) self.assertfalse ( nest.is_nested ( set ( [ 1 , 2 ] ) ) ) ones = array_ops.ones ( [ 2 , 3 ] ) self.assertfalse ( nest.is_nested ( ones ) ) self.assertfalse ( nest.is_nested ( math_ops.tanh ( ones ) ) ) self.assertfalse ( nest.is_nested ( np.ones ( ( 4 , 5 ) ) ) )
__label__0 from absl import app from absl import flags
__label__0 def _checkpointfilename ( self , p ) : `` '' '' returns the checkpoint filename given a ` ( filename , time ) ` pair .
__label__0 def _get_checkpoint_size ( prefix ) : `` '' '' calculates filesize of checkpoint based on prefix . '' '' '' size = 0 # gather all files beginning with prefix ( .index plus sharded data files ) . files = glob.glob ( `` { } * '' .format ( prefix ) ) for file in files : # use tensorflow 's c++ filesystem api . size += metrics.calculatefilesize ( file ) return size
__label__0 def __setattr__ ( self , arg , val ) : if not arg.startswith ( '_tfmw_ ' ) : setattr ( self._tfmw_wrapped_module , arg , val ) self.__dict__ [ arg ] = val if arg not in self.__all__ and arg ! = '__all__ ' : self.__all__.append ( arg ) # update the cache if self._fastdict_key_in ( arg ) : self._fastdict_insert ( arg , val ) super ( tfmodulewrapper , self ) .__setattr__ ( arg , val )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 import abc from collections.abc import sequence import time from typing import optional , union
__label__0 the script takes a list of base directories specified by the tf_cuda_paths environment variable as comma-separated glob list . the script looks for headers and library files in a hard-coded set of subdirectories from these base paths . if tf_cuda_paths is not specified , a os specific default is used :
__label__0 `` ` python ab_tuple = collections.namedtuple ( `` ab_tuple '' , `` a , b '' ) op_tuple = collections.namedtuple ( `` op_tuple '' , `` add , mul '' ) inp_val = ab_tuple ( a=2 , b=3 ) inp_ops = ab_tuple ( a=op_tuple ( add=1 , mul=2 ) , b=op_tuple ( add=2 , mul=3 ) ) out = map_structure_up_to ( inp_val , lambda val , ops : ( val + ops.add ) * ops.mul , inp_val , inp_ops )
__label__0 we check if deprecation decorator is in decorators as well as whether symbol is a class whose __init__ method has a deprecation decorator . args : symbol : python object .
__label__0 if defer_build and var_list : raise valueerror ( `` if ` var_list ` is provided then build can not be deferred. `` `` either set defer_build=false or var_list=none . '' ) if context.executing_eagerly ( ) : logging.warning ( `` saver is deprecated , please switch to tf.train.checkpoint or `` `` tf.keras.model.save_weights for training checkpoints . when `` `` executing eagerly variables do not necessarily have unique names , `` `` and so the variable.name-based lookups saver performs are `` `` error-prone . '' ) if var_list is none : raise runtimeerror ( `` when eager execution is enabled , ` var_list ` must specify a list `` `` or dict of variables to save '' ) self._var_list = var_list self._reshape = reshape self._sharded = sharded self._max_to_keep = max_to_keep self._keep_checkpoint_every_n_hours = keep_checkpoint_every_n_hours self._name = name self._restore_sequentially = restore_sequentially self.saver_def = saver_def self._builder = builder self._is_built = false self._allow_empty = allow_empty self._is_empty = none self._write_version = write_version self._pad_step_number = pad_step_number self._filename = filename self._last_checkpoints = [ ] self._checkpoints_to_be_deleted = [ ] if context.executing_eagerly ( ) : self._next_checkpoint_time = ( time.time ( ) + self._keep_checkpoint_every_n_hours * 3600 ) elif not defer_build : self.build ( ) if self.saver_def : self._check_saver_def ( ) self._write_version = self.saver_def.version self._save_relative_paths = save_relative_paths # for compatibility with object-based checkpoints , we may build a second # saver to read the renamed keys . self._object_restore_saver = none
__label__0 def _add_argument_transformer ( parent , node , full_name , name , logs , arg_name , arg_value_ast ) : `` '' '' adds an argument ( as a final kwarg arg_name=arg_value_ast ) . '' '' '' node.keywords.append ( ast.keyword ( arg=arg_name , value=arg_value_ast ) ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` adding argument ' % s ' to call to % s . '' % ( pasta.dump ( node.keywords [ -1 ] ) , full_name or name ) ) ) return node
__label__0 setattr ( target , type_based_dispatch_attr , _api_dispatcher.pythonapidispatcher ( unwrapped.__name__ , target_argspec.args , target_argspec.defaults ) ) _type_based_dispatch_signatures [ target ] = collections.defaultdict ( list ) return target
__label__0 the uploader uses file-level exclusive locking ( non-blocking flock ) which allows multiple instances of this script to run concurrently if desired , splitting the task among them , each one processing and archiving different files .
__label__0 args : args : positional rguments to a function kwargs : keyword arguments to a function . iterable_params : a list of ( name , index ) tuples for iterable parameters .
__label__0 args : path : the path to the library . check_soname : whether to check the soname as well .
__label__0 ` polymorphicfunction ` assigns types to call arguments , forming a signature . function signatures are used to match arguments to ` concretefunction ` s . for example , when a new ` concretefunction ` is traced , it is assigned a the signature of the arguments it was traced with . subsequent call arguments which match its signature will be dispatched to the same ` concretefunction ` . if no ` concretefunction ` with a matching signature is found , a new one may be traced ( a process known as retracing ) . `` '' ''
__label__0 self.report_benchmark ( iters=test_iter , wall_time= ( t1 - t0 ) / test_iter , name=name )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 if self._is_chief : sess = self._session_manager.prepare_session ( master , init_op=self.init_op , saver=self.saver , checkpoint_dir=self._logdir , wait_for_checkpoint=wait_for_checkpoint , max_wait_secs=max_wait_secs , config=config , init_feed_dict=self._init_feed_dict , init_fn=self._init_fn ) self._write_graph ( ) if start_standard_services : logging.info ( `` starting standard services . '' ) self.start_standard_services ( sess ) else : sess = self._session_manager.wait_for_session ( master , config=config , max_wait_secs=max_wait_secs ) if start_standard_services : logging.info ( `` starting queue runners . '' ) self.start_queue_runners ( sess ) return sess
__label__0 def _rmsprop_update_numpy ( self , var , g , mg , rms , mom , lr , decay , momentum , epsilon , centered ) : rms_t = rms * decay + ( 1 - decay ) * g * g denom_t = rms_t + epsilon if centered : mg_t = mg * decay + ( 1 - decay ) * g denom_t -= mg_t * mg_t else : mg_t = mg mom_t = momentum * mom + lr * g / np.sqrt ( denom_t , dtype=denom_t.dtype ) var_t = var - mom_t return var_t , mg_t , rms_t , mom_t
__label__0 this module encapsulates different semantics of handling nests by the public tf.nest apis and internal tf.data apis . the difference in semantics exists for historic reasons and reconciliation would require a non-backwards compatible change .
__label__0 # we upgrade the base un-versioned tensorflow aliased as tf import_header = `` import tensorflow as tf\n '' text = import_header + old_symbol expected_text = import_header + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 def __init__ ( # pylint : disable=super-init-not-called self , parent_module_globals , mode=none , submodule=none , name= '' keras '' ) : self._tfll_parent_module_globals = parent_module_globals self._tfll_mode = mode self._tfll_submodule = submodule self._tfll_name = name self._tfll_initialized = false
__label__0 def __repr__ ( self ) : return `` . ''
__label__0 from tensorflow.python.eager import context from tensorflow.python.eager import def_function from tensorflow.python.framework import constant_op from tensorflow.python.framework import test_util from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging from tensorflow.python.util import tf_should_use
__label__0 for example the following classes :
__label__0 `` ` b `` `
__label__0 set device filters for selective jobs and tasks . for each remote worker , the device filters are a list of strings . when any filters are present , the remote worker will ignore all devices which do not match any of its filters . each filter can be partially specified , e.g . `` /job : ps '' , `` /job : worker/replica:3 '' , etc . note that a device is always visible to the worker it is located on .
__label__1 import random
__label__0 @ tf_export ( v1= [ `` train.sessionrunhook '' ] ) class sessionrunhook : `` '' '' hook to extend calls to monitoredsession.run ( ) . '' '' ''
__label__0 args : node : current node `` '' '' for import_alias in node.names : # detect based on full import name and alias ) full_import = ( import_alias.name , import_alias.asname ) detection = ( self._api_analysis_spec .imports_to_detect.get ( full_import , none ) ) if detection : self.add_result ( detection ) self.add_log ( detection.log_level , node.lineno , node.col_offset , detection.log_message )
__label__0 copy_tx.mark_used = _new_mark_used copy_tx.__setattr__ = _new__setattr__ _wrappers [ type_x ] = copy_tx
__label__0 text = `` tf.contrib.distribute.foo '' expected = `` tf.contrib.distribute.foo '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` tf.contrib.distribute . * have been migrated '' , report )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( all_linear_cols , _partitioner ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=prev_vocab_path ) ws_util.warm_start ( self.get_temp_dir ( ) , # the special value of none here will ensure that only the variable # specified in var_name_to_vocab_info ( sc_vocab embedding ) is # warm-started . vars_to_warm_start=none , var_name_to_vocab_info= { ws_util._infer_var_name ( cols_to_vars [ sc_vocab ] ) : vocab_info } , # even though this is provided , the none value for # vars_to_warm_start overrides the logic , and this will not be # warm-started . var_name_to_prev_var_name= { ws_util._infer_var_name ( cols_to_vars [ sc_keys ] ) : `` some_other_name '' } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . var corresponding to # sc_vocab should be correctly warm-started after vocab remapping , # and neither of the other two should be warm-started .. self._assert_cols_to_vars ( cols_to_vars , { sc_keys : [ np.zeros ( [ 2 , 1 ] ) , np.zeros ( [ 2 , 1 ] ) ] , sc_hash : [ np.zeros ( [ 8 , 1 ] ) , np.zeros ( [ 7 , 1 ] ) ] , sc_vocab : [ np.array ( [ [ 3 . ] , [ 2 . ] , [ 1 . ] ] ) , np.array ( [ [ 0.5 ] , [ 0 . ] , [ 0 . ] ] ) ] } , sess )
__label__0 args : pattern : a string . timeout_secs : how long to wait for in seconds . for_checkpoint : whether we 're globbing for checkpoints. `` '' '' end_time = time.time ( ) + timeout_secs while time.time ( ) < end_time : if for_checkpoint : if checkpoint_management.checkpoint_exists ( pattern ) : return else : if len ( gfile.glob ( pattern ) ) > = 1 : return time.sleep ( 0.05 ) self.assertfalse ( true , `` glob never matched any file : % s '' % pattern )
__label__0 args : learning_rate : a tensor or a floating point value . the learning rate . decay : discounting factor for the history/coming gradient momentum : a scalar tensor . epsilon : small value to avoid zero denominator . use_locking : if true use locks for update operation . centered : if true , gradients are normalized by the estimated variance of the gradient ; if false , by the uncentered second moment . setting this to true may help with training , but is slightly more expensive in terms of computation and memory . defaults to false . name : optional name prefix for the operations created when applying gradients . defaults to `` rmsprop '' .
__label__0 with context.eager_mode ( ) : ops_lib._default_graph_stack.reset ( ) # pylint : disable=protected-access ops_lib.reset_default_graph ( )
__label__0 since __eq__ is based on object identity , it 's safe to also define __hash__ based on object ids . this lets us add unhashable types like trackable _listwrapper objects to object-identity collections. `` '' ''
__label__0 paths are lists of objects which can be str-converted , which may include integers or other types which are used as indices in a dict .
__label__0 int64list.__doc__ = `` '' '' \ used in ` tf.train.example ` protos . holds a list of int64s .
__label__0 save = saver_module.saver ( { `` real_num '' : real_num , `` imag_num '' : imag_num } ) variables.global_variables_initializer ( )
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 def test_contrib_summary_full_example ( self ) : deindent = lambda n , s : `` \n '' .join ( line [ n : ] for line in s.split ( `` \n '' ) ) text = deindent ( 4 , `` '' '' import tensorflow as tf tf.enable_eager_execution ( ) writer = tf.contrib.summary.create_file_writer ( `` /tmp/migration_test '' , flush_millis=1000 ) with writer.as_default ( ) , tf.contrib.summary.always_record_summaries ( ) : tf.contrib.summary.scalar ( `` loss '' , 0.42 ) tf.contrib.summary.histogram ( `` weights '' , [ 1.0 , 2.0 ] , step=7 ) tf.contrib.summary.flush ( ) `` '' '' ) expected = deindent ( 4 , `` '' '' import tensorflow as tf tf.compat.v1.enable_eager_execution ( ) writer = tf.compat.v2.summary.create_file_writer ( logdir= '' /tmp/migration_test '' , flush_millis=1000 ) with writer.as_default ( ) , tf.compat.v2.summary.record_if ( true ) : tf.compat.v2.summary.scalar ( name= '' loss '' , data=0.42 , step=tf.compat.v1.train.get_or_create_global_step ( ) ) tf.compat.v2.summary.histogram ( name= '' weights '' , data= [ 1.0 , 2.0 ] , step=7 ) tf.compat.v2.summary.flush ( ) `` '' '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 __slots__ = [ `` _storage '' , `` __weakref__ '' ]
__label__0 decorators , target = tf_decorator.unwrap ( obj )
__label__0 self.assertis ( myenum ( 1 ) , myenum.a ) self.assertequal ( 1 , mock_warning.call_count ) self.assertis ( myenum ( 2 ) , myenum.b ) self.assertequal ( 1 , mock_warning.call_count ) self.assertin ( `` is deprecated '' , myenum.__doc__ )
__label__0 def get_func_and_args_from_str ( call_str ) : `` '' '' parse call string to get function and argument names .
__label__0 class nestbenchmark ( test.benchmark ) :
__label__0 def testcreatezerosslotfromvariablecopyxlasharding ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) v = xla_sharding.mesh_split ( v , np.array ( [ 0 , 1 ] ) , [ 0 ] , use_sharding_op=false ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 , copy_xla_sharding=true ) self.assertequal ( xla_sharding.get_tensor_sharding ( v ) , xla_sharding.get_tensor_sharding ( slot ) )
__label__0 if __name__ == '__main__ ' : absltest.main ( )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_no_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 mg0 = opt.get_slot ( var0 , `` mg '' ) self.assertequal ( mg0 is not none , centered ) mg1 = opt.get_slot ( var1 , `` mg '' ) self.assertequal ( mg1 is not none , centered ) rms0 = opt.get_slot ( var0 , `` rms '' ) self.asserttrue ( rms0 is not none ) rms1 = opt.get_slot ( var1 , `` rms '' ) self.asserttrue ( rms1 is not none ) mom0 = opt.get_slot ( var0 , `` momentum '' ) self.asserttrue ( mom0 is not none ) mom1 = opt.get_slot ( var1 , `` momentum '' ) self.asserttrue ( mom1 is not none )
__label__0 def recover_session ( self , master : str , saver : saver_lib.saver = none , checkpoint_dir : str = none , checkpoint_filename_with_path : str = none , wait_for_checkpoint=false , max_wait_secs=7200 , config=none , ) - > tuple [ session.session , bool ] : `` '' '' creates a ` session ` , recovering if possible .
__label__0 def testwarmstartvarwithvocabconstrainedoldvocabsize ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] )
__label__0 # define tf_git_version `` % s '' # ifdef _msc_ver # define tf_compiler_version `` msvc `` tostring ( _msc_full_ver ) # else # define tf_compiler_version __version__ # endif # ifdef _glibcxx_use_cxx11_abi # define tf_cxx11_abi_flag _glibcxx_use_cxx11_abi # else # define tf_cxx11_abi_flag 0 # endif # define tf_cxx_version __cplusplus # ifdef tensorflow_monolithic_build # define tf_monolithic_build 1 # else # define tf_monolithic_build 0 # endif
__label__0 # we do n't handle unaliased tensorflow imports currently , # so the upgrade script show log errors import_header = `` import tensorflow\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` unaliased ` import tensorflow ` `` , `` \n '' .join ( errors ) )
__label__0 after :
__label__0 # shallow tree ends at string . input_tree = [ [ ( `` a '' , 1 ) , [ ( `` b '' , 2 ) , [ ( `` c '' , 3 ) , [ ( `` d '' , 4 ) ] ] ] ] ] shallow_tree = [ [ `` level_1 '' , [ `` level_2 '' , [ `` level_3 '' , [ `` level_4 '' ] ] ] ] ] input_tree_flattened_as_shallow_tree = nest.flatten_up_to ( shallow_tree , input_tree ) input_tree_flattened = nest.flatten ( input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ ( `` a '' , 1 ) , ( `` b '' , 2 ) , ( `` c '' , 3 ) , ( `` d '' , 4 ) ] ) self.assertequal ( input_tree_flattened , [ `` a '' , 1 , `` b '' , 2 , `` c '' , 3 , `` d '' , 4 ] )
__label__0 def getmodule ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getmodule . '' '' '' return _inspect.getmodule ( object )
__label__0 def _upgrade ( self , old_file_text , import_rename=false , upgrade_compat_v1_import=false ) : in_file = io.stringio ( old_file_text ) out_file = io.stringio ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2.tfapichangespec ( import_rename , upgrade_compat_v1_import=upgrade_compat_v1_import ) ) count , report , errors = ( upgrader.process_opened_file ( `` test.py '' , in_file , `` test_out.py '' , out_file ) ) return count , report , errors , out_file.getvalue ( )
__label__0 def _extract_from_parse_example_v2 ( parse_example_op , sess ) : `` '' '' extract exampleparserconfig from parseexamplev2 op . '' '' '' config = example_parser_configuration_pb2.exampleparserconfiguration ( )
__label__0 args : module_name : ( string ) name of the module to store constant at . name : ( string ) current constant name. `` '' '' module = sys.modules [ module_name ] api_constants_attr = api_attrs [ self._api_name ] .constants api_constants_attr_v1 = api_attrs_v1 [ self._api_name ] .constants
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_in_graph_and_eager_modes def test_deprecated_arg_values_when_value_is_none ( self , mock_warning ) :
__label__0 this function flattens the keys and values of a dictionary , which can be arbitrarily nested structures , and returns the flattened version of such structures :
__label__0 `` ` python # create a supervisor with no automatic summaries . sv = supervisor ( logdir='/tmp/mydir ' , is_chief=is_chief , summary_op=none ) # as summary_op was none , managed_session ( ) does not start the # summary thread . with sv.managed_session ( flags.master ) as sess : for step in range ( 1000000 ) : if sv.should_stop ( ) : break if is_chief and step % 100 == 0 : # create the summary every 100 chief steps . sv.summary_computed ( sess , sess.run ( my_summary_op ) ) else : # train normally sess.run ( my_train_op ) `` `
__label__0 > > > skip_magic ( ' ! ls -laf ' , [ ' % ' , ' ! ' , ' ? ' ] ) true `` '' ''
__label__0 device_count = ct.c_int ( ) libcudart.cudagetdevicecount ( ct.byref ( device_count ) )
__label__0 flags.define_bool ( 'search_hints ' , true , ' [ unused ] include metadata search hints in the generated files ' )
__label__0 if full_name in function_reorders : if uses_star_args_in_call ( node ) : self.add_log ( warning , node.lineno , node.col_offset , `` ( manual check required ) upgrading % s may require `` `` re-ordering the call arguments , but it was passed `` `` variable-length positional * args . the upgrade `` `` script can not handle these automatically . '' % full_name )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 args : deps : a list of paths to files . srcs_dir : target directory where files are copied to. `` '' '' path_to_replace = { `` external/local_xla/ '' : `` tensorflow/compiler '' , `` external/local_tsl/ '' : `` tensorflow '' , }
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tf_export tests . '' '' ''
__label__0 @ tf_export ( `` experimental.dispatch_for_unary_elementwise_apis '' ) def dispatch_for_unary_elementwise_apis ( x_type ) : `` '' '' decorator to override default implementation for unary elementwise apis .
__label__0 return dispatch_target
__label__0 def build ( self ) : # skip the modulepage implementation , which does n't use a template . content = base_page.pageinfo.build ( self )
__label__0 # the global step should have been updated and the variables should now have # the new values after the average of the gradients are applied . while sessions [ 1 ] .run ( global_step ) ! = 1 : time.sleep ( 0.01 )
__label__0 > > > @ dispatch_for_api ( tf.math.add , { ' x ' : maskedtensor } , { ' y ' : maskedtensor } ) ... def masked_add ( x , y ) : ... x_values = x.values if isinstance ( x , maskedtensor ) else x ... x_mask = x.mask if isinstance ( x , maskedtensor ) else true ... y_values = y.values if isinstance ( y , maskedtensor ) else y ... y_mask = y.mask if isinstance ( y , maskedtensor ) else true ... return maskedtensor ( x_values + y_values , x_mask & y_mask )
__label__0 # note : this is in a separate file from saver_test.py because the # large allocations do not play well with tsan , and cause flaky # failures . def testlargevariable ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` large_variable '' ) with session.session ( `` '' , graph=ops.graph ( ) ) as sess : # declare a variable that is exactly 2gb . this should fail , # because a serialized checkpoint includes other header # metadata . with ops.device ( `` /cpu:0 '' ) : var = variables.variable ( constant_op.constant ( false , shape= [ 2 , 1024 , 1024 , 1024 ] , dtype=dtypes.bool ) ) save = saver.saver ( { var.op.name : var } , write_version=saver_pb2.saverdef.v1 ) var.initializer.run ( ) with self.assertraisesregex ( errors_impl.invalidargumenterror , `` tensor slice is too large to serialize '' ) : save.save ( sess , save_path )
__label__0 note : to apply this transformation , symbol must be added to reordered_function_names above. `` '' '' for keyword_arg in node.keywords : if keyword_arg.arg == `` factor '' : keyword_arg.arg = `` scale ''
__label__0 def test_import_rename_analysis ( self ) : old_symbol = `` tf.conj ( a ) '' new_symbol = `` tf.math.conj ( a ) ''
__label__0 with sess.graph.device ( `` /cpu:1 '' ) : ds1 = dataset_ops.dataset.range ( 20 ) it1 = dataset_ops.make_initializable_iterator ( ds1 ) get_next1 = it1.get_next ( ) saveable1 = iterator_ops._iteratorsaveable ( it1._iterator_resource , name= '' saveable_it1 '' ) saver = saver_module.saver ( { `` it0 '' : saveable0 , `` it1 '' : saveable1 } , write_version=self._write_version , sharded=false ) self.evaluate ( it0.initializer ) self.evaluate ( it1.initializer ) saver.restore ( sess , save_path ) self.assertequal ( 2 , self.evaluate ( get_next0 ) ) self.assertequal ( 1 , self.evaluate ( get_next1 ) )
__label__0 > > > structure1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > structure2 = ( ( ( `` foo1 '' , `` foo2 '' ) , `` foo3 '' ) , `` foo4 '' , ( `` foo5 '' , `` foo6 '' ) ) > > > structure3 = [ ( ( `` a '' , `` b '' ) , `` c '' ) , `` d '' , [ `` e '' , `` f '' ] ] > > > tf.nest.assert_same_structure ( structure1 , structure2 ) > > > tf.nest.assert_same_structure ( structure1 , structure3 , check_types=false )
__label__0 export_decorator = tf_export.tf_export ( 'namea ' , 'nameb ' ) exported_function = export_decorator ( decorated_function ) self.assertequal ( decorated_function , exported_function ) self.assertequal ( ( 'namea ' , 'nameb ' ) , self._test_function._tf_api_names )
__label__0 returns : obj `` '' '' setattr ( obj , _do_not_doc , none ) return obj
__label__0 def testwarmstart_listofstrings ( self ) : # save checkpoint from which to warm-start . _ , prev_int_val = self._create_prev_run_var ( `` v1 '' , shape= [ 10 , 1 ] , initializer=ones ( ) ) # verify we initialized the values correctly . self.assertallequal ( np.ones ( [ 10 , 1 ] ) , prev_int_val )
__label__0 flattened = nest.flatten ( mess ) self.assertequal ( flattened , [ `` z '' , 3 , 4 , 5 , 1 , 2 , 3 , 4 , 17 ] )
__label__0 def unlock ( fd ) : fcntl.flock ( fd , fcntl.lock_un )
__label__0 the optimizer adds nodes to the graph to collect gradients and pause the trainers until variables are updated . for the parameter server job :
__label__0 def testreadingclasspropertyondecoratedclass ( self ) : self.assertequal ( 2 , testdecoratedclass ( ) .two_prop )
__label__0 major , minor , extension = string.split ( `` . `` , 2 )
__label__0 file_count += 1 _ , l_report , l_errors = self.process_file ( input_path , output_path ) tree_errors [ input_path ] = l_errors report += l_report
__label__0 raw_ops_page = ( output_dir/'tf/raw_ops.md ' ) .read_text ( ) self.assertin ( '/tf/raw_ops/add.md ' , raw_ops_page )
__label__1 class treenode : def __init__ ( self , val=0 , left=none , right=none ) : self.val = val self.left = left self.right = right class codec : def serialize ( self , root ) : def serializehelper ( node ) : if not node : return `` null '' return str ( node.val ) + `` , '' + serializehelper ( node.left ) + `` , '' + serializehelper ( node.right ) return serializehelper ( root ) def deserialize ( self , data ) : def deserializehelper ( queue ) : if queue [ 0 ] == `` null '' : queue.popleft ( ) return none val = int ( queue.popleft ( ) ) node = treenode ( val ) node.left = deserializehelper ( queue ) node.right = deserializehelper ( queue ) return node data = data.split ( `` , '' ) return deserializehelper ( deque ( data ) ) # test case root1 = treenode ( 1 ) root1.left = treenode ( 2 ) root1.right = treenode ( 3 ) root1.right.left = treenode ( 4 ) root1.right.right = treenode ( 5 ) codec = codec ( ) serialized_tree = codec.serialize ( root1 ) print ( `` serialized tree : '' , serialized_tree ) # output : `` 1,2 , null , null,3,4 , null , null,5 , null , null '' deserialized_tree = codec.deserialize ( serialized_tree ) print ( `` deserialized tree : '' , deserialized_tree ) # output : node object of the deserialized tree
__label__0 # argument checking args.filename = os.path.abspath ( args.filename ) check_existence ( args.filename ) regex_groups = re.search ( tf_nightly_regex , args.filename ) directory = regex_groups.group ( 1 ) package = regex_groups.group ( 2 ) version = regex_groups.group ( 3 ) origin_tag = regex_groups.group ( 4 ) old_py_ver = re.search ( r '' ( cp\d\d ) '' , origin_tag ) .group ( 1 )
__label__0 # save checkpoint from which to warm-start . _ , prev_vocab_val = self._create_prev_run_var ( `` linear_model/sc_vocab/weights '' , shape= [ 4 , 1 ] , initializer=ones ( ) )
__label__0 `` ` shell list ( nest.yield_flat_paths ( { ' a ' : [ 3 ] } ) ) [ ( ' a ' , 0 ) ] list ( nest.yield_flat_paths ( { ' a ' : 3 } ) ) [ ( ' a ' , ) ] `` `
__label__0 @ abc.abstractmethod def get_concrete_function ( self , * args , * * kwargs ) - > concretefunction : `` '' '' returns a ` concretefunction ` specialized to input types .
__label__0 def _segmentminv2 ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.segment_min_v2 , data , indices , num_segments )
__label__0 # if necessary , add a call to .to_sparse ( ) to convert the output of # strings.split from a raggedtensor to a sparsetensor . if need_to_sparse : if ( isinstance ( parent , ast.attribute ) and parent.attr == `` to_sparse '' ) : return # prevent infinite recursion ( since child nodes are transformed ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` adding call to raggedtensor.to_sparse ( ) to result of strings.split , `` `` since it now returns a raggedtensor . '' ) ) node = ast.attribute ( value=copy.deepcopy ( node ) , attr= '' to_sparse '' ) try : node = ast.call ( node , [ ] , [ ] ) except typeerror : node = ast.call ( node , [ ] , [ ] , none , none )
__label__0 return rocrand_config
__label__0 def deprecated_wrapper ( func ) : `` '' '' deprecation decorator . '' '' '' decorator_utils.validate_callable ( func , 'deprecated_arg_values ' )
__label__0 @ test.mock.patch.object ( tf_logging , `` warning '' , autospec=true ) def testinteractionwithdeprecationwarning ( self , mock_warning ) :
__label__0 def testwarmstart_twovarsfromthesameprevvar ( self ) : # save checkpoint from which to warm-start . _ , prev_int_val = self._create_prev_run_var ( `` v1 '' , shape= [ 10 , 1 ] , initializer=ones ( ) ) # verify we initialized the values correctly . self.assertallequal ( np.ones ( [ 10 , 1 ] ) , prev_int_val )
__label__0 returns : ` saver ` with remapped variables for reading from an object-based checkpoint .
__label__0 def testwarmstartvarcurrentvarpartitioned ( self ) : _ , prev_val = self._create_prev_run_var ( `` fruit_weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] )
__label__0 class cachedclasspropertytest ( test.testcase ) :
__label__0 returns : a ` variable ` object. `` '' '' if dtype is none : dtype = primary.dtype slot_shape = primary.get_shape ( ) if slot_shape.is_fully_defined ( ) : initializer = init_ops.zeros_initializer ( ) return create_slot_with_initializer ( primary , initializer , slot_shape , dtype , name , colocate_with_primary=colocate_with_primary , copy_xla_sharding=copy_xla_sharding ) else : if isinstance ( primary , variables.variable ) : slot_shape = array_ops.shape ( cond.cond ( variable_v1.is_variable_initialized ( primary ) , primary.read_value , lambda : primary.initial_value ) ) else : slot_shape = array_ops.shape ( primary ) val = array_ops.zeros ( slot_shape , dtype=dtype ) return create_slot ( primary , val , name , colocate_with_primary=colocate_with_primary , copy_xla_sharding=copy_xla_sharding )
__label__0 # use the address of each chunk ( python ` id ` ) as lookup keys to the # ordered chunk indices . chunk_indices = { id ( chunk ) : i for i , chunk in enumerate ( self._chunks ) }
__label__0 import functools
__label__0 from local_xla.xla import xla_data_pb2 from tensorflow.python.compiler.xla.experimental import xla_sharding from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import cond from tensorflow.python.ops import partitioned_variables from tensorflow.python.ops import random_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import slot_creator
__label__0 # pylint : enable=unused-argument
__label__0 def test_sdca_to_raw_ops ( self ) : text = `` tf.train.sdca_fprint ( input_tensor ) '' expected_text = `` tf.raw_ops.sdcafprint ( input_tensor ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def deprecated_endpoints ( * args ) : `` '' '' decorator for marking endpoints deprecated .
__label__0 def rewrap ( decorator_func , previous_target , new_target ) : `` '' '' injects a new target into a function built by make_decorator .
__label__0 def _validate_deprecation_args ( date , instructions ) : if date is not none and not re.match ( r'20\d\d- [ 01 ] \d- [ 0123 ] \d ' , date ) : raise valueerror ( f'date must be in format yyyy-mm-dd . received : { date } ' ) if not instructions : raise valueerror ( 'don\'t deprecate things without conversion instructions ! specify ' 'the ` instructions ` argument . ' )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor , `` y '' : maskedtensor } ) def masked_add ( x , y ) : return maskedtensor ( x.values + y.values , x.mask & y.mask )
__label__0 proto_val = tensortracer ( `` proto '' ) trace = decode_proto ( proto_val , `` message_type '' , [ `` field '' ] , [ `` float32 '' ] ) self.assertin ( `` io.decode_proto ( bytes=proto , '' , str ( trace ) )
__label__0 def __len__ ( self ) : return 1
__label__0 @ deprecation.deprecated ( `` 2016-07-04 '' , `` instructions . '' ) @ property def _prop ( self ) : return `` prop_wrong_order ''
__label__0 parameter = _inspect.parameter signature = _inspect.signature
__label__0 core = `` core '' data = `` data ''
__label__0 # create feature columns . sc_int = fc.categorical_column_with_identity ( `` sc_int '' , num_buckets=10 ) sc_hash = fc.categorical_column_with_hash_bucket ( `` sc_hash '' , hash_bucket_size=15 ) sc_keys = fc.categorical_column_with_vocabulary_list ( `` sc_keys '' , vocabulary_list= [ `` a '' , `` b '' , `` c '' , `` e '' ] ) sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=vocab_path , vocabulary_size=4 ) real = fc.numeric_column ( `` real '' ) real_bucket = fc.bucketized_column ( real , boundaries= [ 0. , 1. , 2. , 3 . ] ) cross = fc.crossed_column ( [ sc_keys , sc_vocab ] , hash_bucket_size=20 ) all_linear_cols = [ sc_int , sc_hash , sc_keys , sc_vocab , real_bucket , cross ]
__label__0 hide_attribute_from_api = hiddentfapiattribute # pylint : disable=invalid-name
__label__0 def __init ( self ) : pass
__label__0 class descr ( object ) :
__label__0 returns : a random type from the list containing all tensorflow types. `` '' '' if allowed_set : index = self.get_int ( 0 , len ( allowed_set ) - 1 ) if allowed_set [ index ] not in _tf_dtypes : raise tf.errors.invalidargumenterror ( none , none , 'given dtype { } is not accepted . '.format ( allowed_set [ index ] ) ) return allowed_set [ index ] else : index = self.get_int ( 0 , len ( _tf_dtypes ) - 1 ) return _tf_dtypes [ index ]
__label__0 # # # # # custom model initialization
__label__0 yield field_proto , field_desc , map_key , index
__label__0 def main ( ) : `` '' '' this script runs the pip smoke test .
__label__0 def testtensorflowimport ( self ) : text = `` import tensorflow as tf '' expected_text = ( `` import tensorflow.compat.v1 as tf '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 `` ` result `` `
__label__0 # this list should contain all modules _immediately_ under tensorflow _top_level_modules = [ `` tensorflow._api '' , `` tensorflow.python '' , `` tensorflow.tools '' , `` tensorflow.core '' , `` tensorflow.compiler '' , `` tensorflow.lite '' , `` tensorflow.keras '' , `` tensorflow.compat '' , `` tensorflow.summary '' , # tensorboard `` tensorflow.examples '' , ]
__label__0 partial_function = functools.partial ( func , 1 ) argspec = tf_inspect.fullargspec ( args= [ ' b ' ] , varargs=none , varkw=none , defaults=none , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def testkeywordrename ( self ) : `` '' '' test that we get the expected result if renaming kw2 to kw3 . '' '' '' text = `` f ( a , b , kw1=c , kw2=d ) \n '' expected = `` f ( a , b , kw1=c , kw3=d ) \n '' ( _ , report , _ ) , new_text = self._upgrade ( renamekeywordspec ( ) , text ) self.assertequal ( new_text , expected ) self.assertnotin ( `` manual check required '' , report )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 if index is none : self._chunks.append ( chunk ) else : self._chunks.insert ( index , chunk ) self._fix_chunk_order = true
__label__0 cudnn_install_path , nccl_install_path , tensorrt_install_path set to '/usr/lib/x86_64-linux-gnu ' would previously find both library and header paths . detect those and return '/usr ' , otherwise forward to _list_from_env ( ) . `` '' '' if env_name in os.environ : match = re.match ( r '' ^ ( / [ ^/ ] * ) +/lib/\w+-linux-gnu/ ? $ '' , os.environ [ env_name ] ) if match : return [ match.group ( 1 ) ] return _list_from_env ( env_name , default )
__label__0 def _start_standard_services ( ) : with ops.graph ( ) .as_default ( ) : sv = supervisor.supervisor ( is_chief=false ) sess = sv.prepare_or_wait_for_session ( `` '' ) sv.start_standard_services ( sess )
__label__0 def getbuild ( dir_base ) : `` '' '' get the list of build file all targets recursively startind at dir_base . '' '' '' items = [ ] for root , _ , files in os.walk ( dir_base ) : for name in files : if ( name == `` build '' and not any ( x in root for x in build_denylist ) ) : items.append ( `` // '' + root + `` : all '' ) return items
__label__0 finally : # clean up . dispatch._global_dispatchers = original_global_dispatchers
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def __init__ ( self , name ) : self.__name__ = name
__label__0 i += 1 parent_desc = field_desc.message_type if field_proto is not none : field_proto = getattr ( field_proto , field_desc.name )
__label__0 defaults = fullargspecs.defaults or ( ) if fullargspecs.kwonlydefaults : defaults += tuple ( fullargspecs.kwonlydefaults.values ( ) )
__label__0 this lock provides a way to allow access to a resource by multiple threads belonging to a logical group at the same time , while restricting access to threads from all other groups . you can think of this as an extension of a reader-writer lock , where you allow multiple writers at the same time . we made it generic to support multiple groups instead of just two - readers and writers .
__label__0 # maps from a function name to a dictionary that describes how to # map from an old argument keyword to the new argument keyword . # if the new argument is none , it will be removed . # only keyword args are handled , so make sure to also put any function in # function_reorders to ensure that all args are made into keywords first . self.function_keyword_renames = { # todo ( b/129398290 ) # `` tf.string_split '' : { # `` delimiter '' : `` sep '' , # } , `` tf.test.assert_equal_graph_def '' : { `` checkpoint_v2 '' : none , `` hash_table_shared_name '' : none , } , `` tf.autograph.to_code '' : { `` arg_types '' : none , `` arg_values '' : none , `` indentation '' : none , } , `` tf.autograph.to_graph '' : { `` arg_types '' : none , `` arg_values '' : none , } , `` tf.nn.embedding_lookup '' : { `` validate_indices '' : none , } , `` tf.image.sample_distorted_bounding_box '' : { `` seed2 '' : none , } , `` tf.gradients '' : { `` colocate_gradients_with_ops '' : none , } , `` tf.hessians '' : { `` colocate_gradients_with_ops '' : none , } , `` * .minimize '' : { `` colocate_gradients_with_ops '' : none , } , `` * .compute_gradients '' : { `` colocate_gradients_with_ops '' : none , } , `` tf.cond '' : { `` strict '' : none , `` fn1 '' : `` true_fn '' , `` fn2 '' : `` false_fn '' } , `` tf.argmin '' : { `` dimension '' : `` axis '' , } , `` tf.argmax '' : { `` dimension '' : `` axis '' , } , `` tf.arg_min '' : { `` dimension '' : `` axis '' , } , `` tf.arg_max '' : { `` dimension '' : `` axis '' , } , `` tf.math.argmin '' : { `` dimension '' : `` axis '' , } , `` tf.math.argmax '' : { `` dimension '' : `` axis '' , } , `` tf.image.crop_and_resize '' : { `` box_ind '' : `` box_indices '' , } , `` tf.extract_image_patches '' : { `` ksizes '' : `` sizes '' , } , `` tf.image.extract_image_patches '' : { `` ksizes '' : `` sizes '' , } , `` tf.image.resize '' : { `` align_corners '' : none , } , `` tf.image.resize_images '' : { `` align_corners '' : none , } , `` tf.expand_dims '' : { `` dim '' : `` axis '' , } , `` tf.batch_to_space '' : { `` block_size '' : `` block_shape '' , } , `` tf.space_to_batch '' : { `` block_size '' : `` block_shape '' , } , `` tf.nn.space_to_batch '' : { `` block_size '' : `` block_shape '' , } , `` tf.constant '' : { `` verify_shape '' : `` verify_shape_is_now_always_true '' , } , `` tf.convert_to_tensor '' : { `` preferred_dtype '' : `` dtype_hint '' } , `` tf.nn.softmax_cross_entropy_with_logits '' : { `` dim '' : `` axis '' , } , `` tf.nn.softmax_cross_entropy_with_logits_v2 '' : { `` dim '' : `` axis '' } , `` tf.linalg.l2_normalize '' : { `` dim '' : `` axis '' , } , `` tf.linalg.norm '' : { `` keep_dims '' : `` keepdims '' , } , `` tf.norm '' : { `` keep_dims '' : `` keepdims '' , } , `` tf.load_file_system_library '' : { `` library_filename '' : `` library_location '' , } , `` tf.count_nonzero '' : { `` input_tensor '' : `` input '' , `` keep_dims '' : `` keepdims '' , `` reduction_indices '' : `` axis '' , } , `` tf.math.count_nonzero '' : { `` input_tensor '' : `` input '' , `` keep_dims '' : `` keepdims '' , `` reduction_indices '' : `` axis '' , } , `` tf.nn.erosion2d '' : { `` kernel '' : `` filters '' , `` rates '' : `` dilations '' , } , `` tf.math.l2_normalize '' : { `` dim '' : `` axis '' , } , `` tf.math.log_softmax '' : { `` dim '' : `` axis '' , } , `` tf.math.softmax '' : { `` dim '' : `` axis '' } , `` tf.nn.l2_normalize '' : { `` dim '' : `` axis '' , } , `` tf.nn.log_softmax '' : { `` dim '' : `` axis '' , } , `` tf.nn.moments '' : { `` keep_dims '' : `` keepdims '' , } , `` tf.nn.pool '' : { `` dilation_rate '' : `` dilations '' } , `` tf.nn.separable_conv2d '' : { `` rate '' : `` dilations '' } , `` tf.nn.depthwise_conv2d '' : { `` rate '' : `` dilations '' } , `` tf.nn.softmax '' : { `` dim '' : `` axis '' } , `` tf.nn.sufficient_statistics '' : { `` keep_dims '' : `` keepdims '' } , `` tf.debugging.assert_all_finite '' : { `` t '' : `` x '' , `` msg '' : `` message '' , } , `` tf.verify_tensor_all_finite '' : { `` t '' : `` x '' , `` msg '' : `` message '' , } , `` tf.sparse.add '' : { `` thresh '' : `` threshold '' , } , `` tf.sparse_add '' : { `` thresh '' : `` threshold '' , } , `` tf.sparse.concat '' : { `` concat_dim '' : `` axis '' , `` expand_nonconcat_dim '' : `` expand_nonconcat_dims '' , } , `` tf.sparse_concat '' : { `` concat_dim '' : `` axis '' , `` expand_nonconcat_dim '' : `` expand_nonconcat_dims '' , } , `` tf.sparse.split '' : { `` split_dim '' : `` axis '' , } , `` tf.sparse_split '' : { `` split_dim '' : `` axis '' , } , `` tf.sparse.reduce_max '' : { `` reduction_axes '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.sparse_reduce_max '' : { `` reduction_axes '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.sparse.reduce_sum '' : { `` reduction_axes '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.sparse_reduce_sum '' : { `` reduction_axes '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.nn.max_pool_with_argmax '' : { `` targmax '' : `` output_dtype '' , } , `` tf.nn.max_pool '' : { `` value '' : `` input '' } , `` tf.nn.avg_pool '' : { `` value '' : `` input '' } , `` tf.nn.avg_pool2d '' : { `` value '' : `` input '' } , `` tf.multinomial '' : { `` output_dtype '' : `` dtype '' , } , `` tf.random.multinomial '' : { `` output_dtype '' : `` dtype '' , } , `` tf.reverse_sequence '' : { `` seq_dim '' : `` seq_axis '' , `` batch_dim '' : `` batch_axis '' , } , `` tf.nn.batch_norm_with_global_normalization '' : { `` t '' : `` input '' , `` m '' : `` mean '' , `` v '' : `` variance '' , } , `` tf.nn.dilation2d '' : { `` filter '' : `` filters '' , `` rates '' : `` dilations '' , } , `` tf.nn.conv3d '' : { `` filter '' : `` filters '' } , `` tf.zeros_like '' : { `` tensor '' : `` input '' , } , `` tf.ones_like '' : { `` tensor '' : `` input '' , } , `` tf.nn.conv2d_transpose '' : { `` value '' : `` input '' , `` filter '' : `` filters '' , } , `` tf.nn.conv3d_transpose '' : { `` value '' : `` input '' , `` filter '' : `` filters '' , } , `` tf.nn.convolution '' : { `` filter '' : `` filters '' , `` dilation_rate '' : `` dilations '' , } , `` tf.gfile.exists '' : { `` filename '' : `` path '' , } , `` tf.gfile.remove '' : { `` filename '' : `` path '' , } , `` tf.gfile.stat '' : { `` filename '' : `` path '' , } , `` tf.gfile.glob '' : { `` filename '' : `` pattern '' , } , `` tf.gfile.mkdir '' : { `` dirname '' : `` path '' , } , `` tf.gfile.makedirs '' : { `` dirname '' : `` path '' , } , `` tf.gfile.deleterecursively '' : { `` dirname '' : `` path '' , } , `` tf.gfile.isdirectory '' : { `` dirname '' : `` path '' , } , `` tf.gfile.listdirectory '' : { `` dirname '' : `` path '' , } , `` tf.gfile.copy '' : { `` oldpath '' : `` src '' , `` newpath '' : `` dst '' , } , `` tf.gfile.rename '' : { `` oldname '' : `` src '' , `` newname '' : `` dst '' , } , `` tf.gfile.walk '' : { `` in_order '' : `` topdown '' , } , `` tf.random.stateless_multinomial '' : { `` output_dtype '' : `` dtype '' , } , `` tf.string_to_number '' : { `` string_tensor '' : `` input '' , } , `` tf.strings.to_number '' : { `` string_tensor '' : `` input '' , } , `` tf.string_to_hash_bucket '' : { `` string_tensor '' : `` input '' , } , `` tf.strings.to_hash_bucket '' : { `` string_tensor '' : `` input '' , } , `` tf.reduce_all '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_all '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_any '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_any '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_min '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_min '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_max '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_max '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_sum '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_sum '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_mean '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_mean '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_prod '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_prod '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_logsumexp '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.math.reduce_logsumexp '' : { `` reduction_indices '' : `` axis '' , `` keep_dims '' : `` keepdims '' , } , `` tf.reduce_join '' : { `` keep_dims '' : `` keepdims '' , `` reduction_indices '' : `` axis '' } , `` tf.strings.reduce_join '' : { `` keep_dims '' : `` keepdims '' , `` reduction_indices '' : `` axis '' } , `` tf.squeeze '' : { `` squeeze_dims '' : `` axis '' , } , `` tf.nn.weighted_moments '' : { `` keep_dims '' : `` keepdims '' } , `` tf.nn.conv1d '' : { `` value '' : `` input '' , `` use_cudnn_on_gpu '' : none , } , `` tf.nn.conv2d '' : { `` filter '' : `` filters '' , `` use_cudnn_on_gpu '' : none , } , `` tf.nn.conv2d_backprop_input '' : { `` use_cudnn_on_gpu '' : none , `` input_sizes '' : `` output_shape '' , `` out_backprop '' : `` input '' , `` filter '' : `` filters '' , } , `` tf.contrib.summary.audio '' : { `` tensor '' : `` data '' , `` family '' : none , } , `` tf.contrib.summary.create_file_writer '' : { `` name '' : none , } , `` tf.contrib.summary.generic '' : { `` name '' : `` tag '' , `` tensor '' : `` data '' , `` family '' : none , } , `` tf.contrib.summary.histogram '' : { `` tensor '' : `` data '' , `` family '' : none , } , `` tf.contrib.summary.image '' : { `` tensor '' : `` data '' , `` bad_color '' : none , `` max_images '' : `` max_outputs '' , `` family '' : none , } , `` tf.contrib.summary.scalar '' : { `` tensor '' : `` data '' , `` family '' : none , } , `` tf.nn.weighted_cross_entropy_with_logits '' : { `` targets '' : `` labels '' , } , `` tf.decode_raw '' : { `` bytes '' : `` input_bytes '' , } , `` tf.io.decode_raw '' : { `` bytes '' : `` input_bytes '' , } , `` tf.contrib.framework.load_variable '' : { `` checkpoint_dir '' : `` ckpt_dir_or_file '' , } } all_renames_v2.add_contrib_direct_import_support ( self.function_keyword_renames )
__label__0 migrating to a keras optimizer :
__label__0 # partitioned variables also cause name conflicts . p_v1 = variable_scope.get_variable ( `` p_v1 '' , shape= [ 4 , 5 ] , partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=2 ) ) p_v2 = variable_scope.get_variable ( `` p_v2 '' , shape= [ 4 , 5 ] , partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=2 ) ) p_v2._name = `` p_v1 '' with self.assertraisesregex ( valueerror , `` same name : p_v1 '' ) : saver_module.saver ( [ p_v1 , p_v2 ] )
__label__0 def testflattencustomsequencethatraisesexception ( self ) : # b/140746865 seq = _customsequencethatraisesexception ( ) with self.assertraisesregex ( valueerror , `` can not get item '' ) : nest.flatten ( seq )
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 @ property def warnings_and_errors ( self ) : return [ log for log in self._log if log [ 0 ] in ( warning , error ) ]
__label__0 def num_tasks ( self , job_name ) : `` '' '' returns the number of tasks defined in the given job .
__label__0 self.assertequal ( ( ' a ' , ) , function_utils.fn_args ( wrapped_fn ) )
__label__0 `` `` '' functions that work with structures .
__label__0 def _tf_core_sorted ( dict_ ) : `` '' '' returns a sorted list of the dict keys , with error if keys not sortable . '' '' '' try : return sorted ( dict_.keys ( ) ) except typeerror : # pylint : disable=raise-missing-from raise typeerror ( `` nest only supports dicts with sortable keys . '' )
__label__0 override = dispatch.dispatch_for_types ( test_op_with_optional , customtensor ) ( override_for_test_op )
__label__0 @ tf.function ( jit_compile=true ) def f ( x ) : return x + y
__label__0 def testdebugstring ( self ) : # builds a graph . v0 = variable_v1.variablev1 ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] , dtype=dtypes.float32 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( [ [ [ 1 ] , [ 2 ] ] , [ [ 3 ] , [ 4 ] ] , [ [ 5 ] , [ 6 ] ] ] , dtype=dtypes.float32 , name= '' v1 '' ) init_all_op = variables.global_variables_initializer ( ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 } , write_version=self._write_version ) save_path = os.path.join ( self.get_temp_dir ( ) , `` ckpt_for_debug_string '' + str ( self._write_version ) ) with self.cached_session ( ) as sess : self.evaluate ( init_all_op ) # saves a checkpoint . save.save ( sess , save_path )
__label__0 # output is : # [ ( ' a ' , 1 ) , ( ' b ' , 2 ) , ( ' c ' , 3 ) , ( 'd ' , 4 ) ] # [ ' a ' , 1 , ' b ' , 2 , ' c ' , 3 , 'd ' , 4 ] `` `
__label__0 if len ( node.args ) > = 1 : found_distribution = true dist = _get_distribution ( node.args [ 0 ] ) new_keywords.append ( ast.keyword ( arg= '' distribution '' , value=dist ) ) if not found_distribution : # parse with pasta instead of ast to avoid emitting a spurious trailing \n . uniform_dist = pasta.parse ( `` \ '' uniform\ '' '' ) new_keywords.append ( ast.keyword ( arg= '' distribution '' , value=uniform_dist ) ) if len ( node.args ) > = 2 : new_keywords.append ( ast.keyword ( arg= '' seed '' , value=node.args [ 1 ] ) ) if len ( node.args ) > = 3 : new_keywords.append ( ast.keyword ( arg= '' dtype '' , value=node.args [ 2 ] ) ) node.args = [ ]
__label__0 # maybe it was a positional arg if len ( node.args ) < 2 : logs.append ( ( ast_edits.error , node.lineno , node.col_offset , `` tf.nn.dropout called without arguments , so `` `` automatic fix was disabled . tf.nn.dropout has changed `` `` the semantics of the second argument . '' ) ) else : rate_arg = ast.keyword ( arg= '' rate '' , value=node.args [ 1 ] ) _replace_keep_prob_node ( rate_arg , rate_arg.value ) node.keywords.append ( rate_arg ) del node.args [ 1 ] logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changing keep_prob arg of tf.nn.dropout to rate , and `` `` recomputing value.\n '' ) )
__label__0 def __init__ ( self ) : # ` _device_filters ` is a dict mapping job names to job device filters . # job device filters further maps task ids to task device filters . # task device filters are a list of strings , each one is a device filter . self._device_filters = { }
__label__0 class _ownsavariablesimple ( trackable_base.trackable ) : `` '' '' a trackable object which can be saved using a tf.train.saver . '' '' ''
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( partial_func ) )
__label__0 args : structure : structure to mimic . flat : flattened values to output substructure for . index : index at which to start reading from flat . is_nested_fn : function used to test if a value should be treated as a nested structure . sequence_fn : function used to generate a new strcuture instance .
__label__0 import copy import itertools import math
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' upgrader for python scripts according to an api change specification . '' '' ''
__label__0 def get_json_type ( obj ) : `` '' '' serializes any object to a json-serializable structure .
__label__0 flat_path_list = nest.flatten_with_tuple_paths_up_to ( shallow_tree=mt , input_tree=simple_list , check_types=false ) self.assertallequal ( flat_path_list , [ [ ( 0 , ) , 2 ] ] )
__label__0 decorator_utils.validate_callable ( testclass ( ) , `` test '' )
__label__0 # try to get the cpu governor try : cpu_governors = set ( [ gfile.gfile ( f , ' r ' ) .readline ( ) .rstrip ( ) for f in glob.glob ( '/sys/devices/system/cpu/cpu * /cpufreq/scaling_governor ' ) ] ) if cpu_governors : if len ( cpu_governors ) > 1 : cpu_info.cpu_governor = 'mixed ' else : cpu_info.cpu_governor = list ( cpu_governors ) [ 0 ] except errors.operror : pass
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 stacksummary = _tf_stack.stacktrace framesummary = _tf_stack.stackframe
__label__0 def finalize_options ( self ) : ret = installcommandbase.finalize_options ( self ) # pylint : disable=assignment-from-no-return self.install_headers = os.path.join ( self.install_platlib , 'tensorflow ' , 'include ' ) self.install_lib = self.install_platlib return ret
__label__0 args : min_int : minimum allowed integer . max_int : maximum allowed integer .
__label__0 text = `` tf.train.sdca_shrink_l1 ( w , l , ll ) '' expected_text = `` tf.raw_ops.sdcashrinkl1 ( w , l , ll ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def _assert_field_tags ( self , expected_fields , actual_fields ) : self.assertlen ( actual_fields , len ( expected_fields ) ) for expected , actual in zip ( expected_fields , actual_fields ) : self.assertprotoequals ( expected , actual )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add ) def masked_add ( x : maskedtensor , y : maskedtensor ) : del x , y
__label__0 def testdispatchforbinaryelementwiseapis ( self ) :
__label__0 @ compatibility ( eager ) when eager execution is enabled , ` var_list ` must specify a ` list ` or ` dict ` of variables to save . otherwise , a ` runtimeerror ` will be raised .
__label__0 # note that foo ( ) wo n't result in a warning , because in this case foo is # not an attribute , but a name . false_alarms = [ `` foo '' , `` foo ( ) '' , `` foo.bar ( ) '' , `` obj.run_foo ( ) '' , `` obj.foo '' ] for text in false_alarms : ( _ , report , _ ) , _ = self._upgrade ( foowarningspec ( ) , text ) self.assertnotin ( `` not good '' , report )
__label__0 returns : the normalized docstring `` '' '' if not docstring : return `` # convert tabs to spaces ( following the normal python rules ) # and split into a list of lines : lines = docstring.expandtabs ( ) .splitlines ( ) # determine minimum indentation ( first line does n't count ) : # ( we use sys.maxsize because sys.maxint does n't exist in python 3 ) indent = sys.maxsize for line in lines [ 1 : ] : stripped = line.lstrip ( ) if stripped : indent = min ( indent , len ( line ) - len ( stripped ) ) # remove indentation ( first line is special ) : trimmed = [ lines [ 0 ] .strip ( ) ] if indent < sys.maxsize : for line in lines [ 1 : ] : trimmed.append ( line [ indent : ] .rstrip ( ) ) # strip off trailing and leading blank lines : while trimmed and not trimmed [ -1 ] : trimmed.pop ( ) while trimmed and not trimmed [ 0 ] : trimmed.pop ( 0 ) # return a single string : return '\n'.join ( trimmed )
__label__0 def test_get_field_tag_invalid ( self ) : proto = test_message_pb2.manyfields ( ) with self.assertraisesregex ( keyerror , `` unable to find field 'not_a_field ' '' ) : util.get_field_tag ( proto , [ `` field_one '' , `` not_a_field '' ] ) with self.assertraisesregex ( keyerror , `` unable to find field number 10000 '' ) : util.get_field_tag ( proto , [ 1 , 10000 ] ) with self.assertraisesregex ( valueerror , `` unable to find fields . * in proto '' ) : util.get_field_tag ( proto , [ `` string_field '' , 1 ] )
__label__0 logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changed % s call to tf.cast ( ... , dtype=tf. % s ) . '' % ( full_name , dtype_str ) ) ) return node
__label__0 # sometimes , jupyter has more than python code # idea is to comment these lines , for upgrade time if skip_magic ( code_line , [ `` % '' , `` ! `` , `` ? '' ] ) or is_line_split : # found a special character , need to `` encode '' code_line = `` # # # ! ! ! '' + code_line
__label__0 args : proto : parent proto of any message type . fields : list of string/int/map key fields , e.g . [ `` nodes '' , `` attr '' , `` value '' ] can represent ` proto.nodes.attr [ `` value '' ] ` .
__label__0 if check_types and not isinstance ( input_tree , type ( shallow_tree ) ) : raise typeerror ( `` the two structures do n't have the same sequence type . input `` f '' structure has type ' { type ( input_tree ) .__name__ } ' , while shallow `` f '' structure has type ' { type ( shallow_tree ) .__name__ } ' . '' )
__label__0 # the saver sorts by name before parsing , so we need a name property . @ property def name ( self ) : return self.non_dep_variable.name
__label__0 def unary_elementwise_apis ( ) : `` '' '' returns a list of apis that have been registered as unary elementwise . '' '' '' return tuple ( _unary_elementwise_apis )
__label__0 def test_double_partial_with_positional_args_in_outer_layer ( self ) : expected_test_arg1 = 123 expected_test_arg2 = 456
__label__0 args : values : a nested structure of ` resourcevariable ` s , or any other objects .
__label__0 toc = yaml.safe_load ( ( output_dir / 'tf/_toc.yaml ' ) .read_text ( ) ) self.assertequal ( { 'title ' : 'overview ' , 'path ' : '/tf_overview ' } , toc [ 'toc ' ] [ 0 ] [ 'section ' ] [ 0 ] ) redirects = yaml.safe_load ( ( output_dir / 'tf/_redirects.yaml ' ) .read_text ( ) ) self.assertin ( { 'from ' : '/tf_overview ' , 'to ' : '/tf ' } , redirects [ 'redirects ' ] )
__label__1 class solution : def palindromepairs ( self , words : list [ str ] ) - > list [ list [ int ] ] : # initialize hash table to store indices of words word_indices = { word : i for i , word in enumerate ( words ) } result = [ ] # helper function to check if a string is palindrome def is_palindrome ( s ) : return s == s [ : :-1 ] # iterate over words for i , word in enumerate ( words ) : # check if the word itself is palindrome and its reverse exists in the hash table if word and is_palindrome ( word ) and `` '' in word_indices : j = word_indices [ `` '' ] if i ! = j : result.append ( [ i , j ] ) result.append ( [ j , i ] ) # check if the word can form palindrome pairs with its prefix or suffix for j in range ( len ( word ) ) : prefix = word [ : j ] suffix = word [ j : ] if is_palindrome ( prefix ) : reverse_suffix = suffix [ : :-1 ] if reverse_suffix in word_indices and word_indices [ reverse_suffix ] ! = i : result.append ( [ i , word_indices [ reverse_suffix ] ] ) if is_palindrome ( suffix ) : reverse_prefix = prefix [ : :-1 ] if reverse_prefix in word_indices and word_indices [ reverse_prefix ] ! = i : result.append ( [ word_indices [ reverse_prefix ] , i ] ) return result
__label__0 text = `` import tensorflow.google '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 with self.assertraises ( typeerror ) : nest.flatten_dict_items ( 4 )
__label__0 wrapped_fn = functools.partial ( fn , test_arg=123 )
__label__0 @ tf_export ( v1= [ 'train.global_step ' ] ) def global_step ( sess , global_step_tensor ) : `` '' '' small helper to get the global step .
__label__0 returns : a context handler. `` '' '' return self._coord.stop_on_exception ( )
__label__0 this wrapper emulates ` inspect.getfullargspec ` in [ ^ ) ] * python2 .
__label__0 def isclass ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isclass . '' '' '' return _inspect.isclass ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 # using non-iterable elements . input_tree = [ 0 ] shallow_tree = 9 ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 @ dispatch.register_binary_elementwise_api @ dispatch.add_dispatch_support def some_op ( x , y ) : return x * 2 + y
__label__0 if __name__ == `` __main__ '' : test_lib.main ( )
__label__0 def testgetargspecondecoratorsthatdontprovideargspec ( self ) : argspec = tf_inspect.getargspec ( test_decorated_function_with_defaults ) self.assertequal ( [ ' a ' , ' b ' , ' c ' ] , argspec.args ) self.assertequal ( ( 2 , 'hello ' ) , argspec.defaults )
__label__0 def _as_cluster_device_filters ( self ) : `` '' '' returns a serialized protobuf of cluster device filters . '' '' '' if self._cluster_device_filters : return self._cluster_device_filters
__label__0 for input_path , output_path in files_to_copy : output_directory = os.path.dirname ( output_path ) if not os.path.isdir ( output_directory ) : os.makedirs ( output_directory ) shutil.copy ( input_path , output_path ) return file_count , report , tree_errors
__label__0 try : generator = ( maskedtensor ( [ i ] , [ true ] ) for i in range ( 5 ) ) y = math_ops.add_n ( generator ) self.assertallequal ( y.values , [ 0 + 1 + 2 + 3 + 4 ] ) self.assertallequal ( y.mask , [ true ] )
__label__0 args : * args : positional argument for this call * * kwargs : keyword arguments for this call returns : the execution results. `` '' ''
__label__0 for backwards compatibility , some libraries also use alternative base directories from other environment variables if they are specified . list of library-specific environment variables :
__label__0 self.assertequal ( { ' a ' : 3 , ' b ' : 4 } , tf_inspect.getcallargs ( func , 3 , 4 ) )
__label__0 this decorator allows you to skip classes or methods :
__label__0 return decorator
__label__0 def _update_string ( self ) : self.string = `` % s. % s. % s % s '' % ( self.major , self.minor , self.patch , self.identifier_string )
__label__0 with session.session ( graph=ops_lib.graph ( ) ) as sess : saver_module.import_meta_graph ( meta_graph_def , import_scope= '' new_model '' ) self.evaluate ( variables.global_variables_initializer ( ) ) sess.run ( [ `` new_model/optimize '' ] , { `` new_model/image:0 '' : np.random.random ( [ 1 , 784 ] ) , `` new_model/label:0 '' : np.random.randint ( 10 , size= [ 1 , 10 ] ) } )
__label__0 # include additional substitutions that may be defined via params config.substitutions.extend ( ( `` % % { % s } '' % key , val ) for key , val in lit_config.params.items ( ) )
__label__0 def fn_has_no_kwargs ( x , test_arg1 , test_arg2 ) : if test_arg1 ! = expected_test_arg1 or test_arg2 ! = expected_test_arg2 : return valueerror ( 'partial does not work correctly ' ) return x
__label__0 # # # how to rewrite checkpoints
__label__0 major , minor , patch = roctracer_version_numbers ( rocm_install_path )
__label__0 def testwrappernotequaltowrapped ( self ) : class settablehash ( object ) :
__label__0 @ parameterized.parameters ( [ 'text [ 1.0 ] text ' , 'text [ 1.00 ] text ' ] , [ 'text [ 1.0 ] text ' , 'text [ 1.0 ] text ' ] , [ 'text [ 1.0 ] text ' , 'text [ 1.0 ] text ' ] , [ 'text [ 1.000 ] text ' , 'text [ 1.0 ] text ' ] ) def test_extra_spaces ( self , want , got ) : output_checker = tf_doctest_lib.tfdoctestoutputchecker ( ) self.asserttrue ( output_checker.check_output ( want=want , got=got , optionflags=doctest.ellipsis ) )
__label__0 tf.function does not know when to re-use an existing concrete function in regards to the ` fruit ` class so naively it retraces for every new instance . `` ` python get_mixed_flavor ( apple ( ) , mango ( ) ) # traces a new concrete function get_mixed_flavor ( apple ( ) , mango ( ) ) # traces a new concrete function again `` `
__label__0 _fn ( ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : # initialize with zeros . v1 = variable_scope.get_variable ( `` v1 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) v1_momentum = variable_scope.get_variable ( `` v1/momentum '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) v2 = variable_scope.get_variable ( `` v2 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) v2_momentum = variable_scope.get_variable ( `` v2/momentum '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) ws_util.warm_start ( self.get_temp_dir ( ) , # this warm-starts both v1 and v1/momentum , but only # v2 ( and not v2/momentum ) . vars_to_warm_start= [ `` v1 '' , `` v2 [ ^/ ] '' ] ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify the selection of weights were correctly warm-started ( init # overridden to ones ) . self.assertallequal ( v1 , prev_v1_val ) self.assertallequal ( v1_momentum , prev_v1_momentum_val ) self.assertallequal ( v2 , prev_v2_val ) self.assertallequal ( v2_momentum , np.zeros ( [ 10 , 1 ] ) )
__label__0 class fastmoduletypetest ( test.testcase ) :
__label__0 summary.scalar ( `` loss '' , loss ) # creates the gradient descent optimizer with the given learning rate . optimizer = gradient_descent.gradientdescentoptimizer ( 0.01 )
__label__0 jdf = self._cluster_device_filters.jobs.add ( ) jdf.name = job_name
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 returns : a single ` tf.tensor ` or a ` tf.distribute.distributedvalues ` which contains the next input for all replicas .
__label__0 # todo ( b/34465411 ) : starting multiple servers with different configurations # in the same test is flaky . move this test case back into # `` server_lib_test.py '' when this is no longer the case . @ test_util.run_deprecated_v1 def testsparsejob ( self ) : server = server_lib.server ( { `` local '' : { 37 : `` localhost:0 '' } } ) with ops.device ( `` /job : local/task:37 '' ) : a = constant_op.constant ( 1.0 )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor } ) def masked_add ( x , y ) : if y is none : return x y_values = y.values if isinstance ( y , maskedtensor ) else y y_mask = y.mask if isinstance ( y , maskedtensor ) else true return maskedtensor ( x.values + y_values , x.mask & y_mask )
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 def testautograph ( self ) : text = `` tf.autograph.to_graph ( f , true , arg_values=none , arg_types=none ) '' expected_text = `` tf.autograph.to_graph ( f , true ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # output is : # [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 9 ] , [ 5 , 5 ] ] # [ true , true , false , true ] `` `
__label__0 # ! /usr/bin/python # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # # automatically update tensorflow version in source files # # usage : # ./tensorflow/tools/ci_build/update_version.py -- version 1.4.0-rc1 # ./tensorflow/tools/ci_build/update_version.py -- nightly # `` '' '' update version of tensorflow script . '' '' ''
__label__0 # although the global step should still be 1 as explained above , the local # step should now be updated to 1. just check worker 1 as an example . self.assertallequal ( 1 , sessions [ 1 ] .run ( global_step ) ) self.assertallequal ( 1 , sessions [ 1 ] .run ( local_step_1 ) )
__label__0 class stacktracemapper ( stacktracetransform ) : `` '' '' allows remapping traceback information to different source code . '' '' '' _stack_dict = _source_mapper_stacks
__label__0 def _recordlastcheckpoint ( self , latest_save_path ) : `` '' '' manages the list of the latest checkpoints . '' '' '' if not self.saver_def.max_to_keep : return # remove first from list if the same name was used before . for p in self._last_checkpoints [ : ] : if latest_save_path == self._checkpointfilename ( p ) : self._last_checkpoints.remove ( p )
__label__0 the following collection types are recognized by ` tf.nest ` as nested structures :
__label__0 def _tf_data_sorted ( dict_ ) : `` '' '' returns a sorted list of the dict keys , with error if keys not sortable . '' '' '' try : return sorted ( list ( dict_ ) ) except typeerror as e : # pylint : disable=raise-missing-from raise typeerror ( f '' nest only supports dicts with sortable keys . error : { e.message } '' )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 def silly_example_function ( ) : pass
__label__0 def loadtracesfromdebuginfo ( debug_info ) : return _tf_stack.loadtracesfromdebuginfo ( debug_info.serializetostring ( ) )
__label__0 partial_func = functools.partial ( func , n=7 )
__label__0 # resets container `` local0 '' . verifies that v0 is no longer initialized . session.session.reset ( server0.target , [ `` local0 '' ] ) _ = session.session ( server0.target ) with self.assertraises ( errors_impl.failedpreconditionerror ) : self.evaluate ( v0 ) # reinitializes v0 for the following test . self.evaluate ( v0.initializer )
__label__0 def _gather_gpu_devices_proc ( ) : `` '' '' try to gather nvidia gpu device information via /proc/driver . '' '' '' dev_info = [ ] for f in gfile.glob ( `` /proc/driver/nvidia/gpus/ * /information '' ) : bus_id = f.split ( `` / '' ) [ 5 ] key_values = dict ( line.rstrip ( ) .replace ( `` \t '' , `` '' ) .split ( `` : '' , 1 ) for line in gfile.gfile ( f , `` r '' ) ) key_values = dict ( ( k.lower ( ) , v.strip ( `` `` ) .rstrip ( `` `` ) ) for ( k , v ) in key_values.items ( ) ) info = test_log_pb2.gpuinfo ( ) info.model = key_values.get ( `` model '' , `` unknown '' ) info.uuid = key_values.get ( `` gpu uuid '' , `` unknown '' ) info.bus_id = bus_id dev_info.append ( info ) return dev_info
__label__0 pip_package_query_expression = ( `` deps ( //tensorflow/tools/pip_package : build_pip_package ) '' )
__label__0 the hlo module accepts a flat list of inputs . to retrieve the order of these inputs signatures , users can call the ` concrete_fn.structured_input_signature ` and ` concrete_fn.captured_inputs ` :
__label__0 mytuple = deprecation.deprecated ( date , instructions , warn_once=true ) ( collections.namedtuple ( `` my_tuple '' , [ `` field1 '' , `` field2 '' ] ) )
__label__0 returns : list of all api names for this symbol. `` '' '' names_v2 = [ ] tensorflow_api_attr = api_attrs [ tensorflow_api_name ] .names keras_api_attr = api_attrs [ keras_api_name ] .names
__label__0 def bound ( self ) : pass
__label__0 def tear_down ( self , test ) : self.teardown ( )
__label__0 from google.protobuf import message from tensorflow.python.lib.io import file_io from tensorflow.tools.proto_splitter import chunk_pb2 from tensorflow.tools.proto_splitter import util from tensorflow.tools.proto_splitter import version as version_lib from tensorflow.tools.proto_splitter import versions_pb2
__label__0 def is_generic_tuple ( tp ) : `` '' '' returns true if ` tp ` is a parameterized typing.tuple value . '' '' '' return ( tp not in ( tuple , typing.tuple ) and getattr ( tp , '__origin__ ' , none ) in ( tuple , typing.tuple ) )
__label__0 _contrib_dist_strat_warning = ( ast_edits.warning , `` ( manual edit required ) tf.contrib.distribute . * have been migrated to `` `` tf.distribute. * . please check out the new module for updated apis . '' )
__label__0 if keras.__version__.startswith ( `` 3 . `` ) : # this is the keras 3.x case . keras_version = `` keras_3 '' package_name = `` keras._tf_keras.keras '' else : # this is the keras 2.x case . keras_version = `` keras_2 '' if self._tfll_mode == `` v1 '' : package_name = `` keras.api._v1.keras '' else : package_name = `` keras.api._v2.keras '' except importerror : raise importerror ( # pylint : disable=raise-missing-from `` keras can not be imported . check that it is installed . '' )
__label__0 def _test_function2 ( unused_arg=0 ) : pass
__label__0 return nest.map_structure ( _convert_resource_variable_to_tensor , values )
__label__0 # currently , only axis=0 and axis=1 are supported . `` ` `` '' ''
__label__0 # this assertion is expected to pass : two namedtuples with the same # name and field names are considered to be identical . inp_shallow = nesttest.samenameab ( 1 , 2 ) inp_deep = nesttest.samenameab2 ( 1 , [ 1 , 2 , 3 ] ) nest.assert_shallow_structure ( inp_shallow , inp_deep , check_types=false ) nest.assert_shallow_structure ( inp_shallow , inp_deep , check_types=true )
__label__0 def get_v1_constants ( module : any ) - > sequence [ str ] : `` '' '' get a list of tf 1 . * constants in this module .
__label__0 text = `` tf.arg_min ( input , 0 ) '' expected_text = `` tf.argmin ( input , 0 ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 returns : a new structure with ` resourcevariable ` s in ` values ` converted to ` tensor ` s. `` '' '' def _convert_resource_variable_to_tensor ( x ) : if _pywrap_utils.isresourcevariable ( x ) : return ops.convert_to_tensor ( x ) elif isinstance ( x , composite_tensor.compositetensor ) : return composite_tensor.convert_variables_to_tensors ( x ) else : return x
__label__0 doc_generator.build ( output_dir )
__label__0 @ test_decorator ( 'decorator ' ) def func ( m=1 , n=2 ) : return 2 * m + n
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' library for getting system information during tensorflow tests . '' '' ''
__label__0 def testgetdoc ( self ) : self.assertequal ( 'test decorated function with defaults docstring . ' , tf_inspect.getdoc ( test_decorated_function_with_defaults ) )
__label__0 __slots__ = [ `` _ready '' , `` _num_groups '' , `` _group_member_counts '' ]
__label__0 import code import sys
__label__0 r = runfiles.create ( )
__label__0 def testconv2dbackpropinput ( self ) : text = ( `` tf.nn.conv2d_backprop_input ( input_sizes , filter , out_backprop , `` `` strides , padding , use_cudnn_on_gpu , data_format ) '' ) expected_text = ( `` tf.nn.conv2d_transpose ( output_shape=input_sizes , filters=filter , `` `` input=out_backprop , strides=strides , padding=padding , `` `` data_format=data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 if self._is_empty : return none else : metrics.recordcheckpointsize ( api_label=_saver_label , filesize=_get_checkpoint_size ( model_checkpoint_path ) ) return model_checkpoint_path
__label__0 canonical_score = 1 if canonical is not none and name == `` tf . '' + canonical : canonical_score = -1
__label__0 if __name__ == '__main__ ' : sys.exit ( main ( sys.argv ) )
__label__0 def testvarlistshouldbeemptyindeferredbuild ( self ) : with ops_lib.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1.0 ) with self.assertraisesregex ( valueerror , `` defer_build '' ) : saver_module.saver ( [ v ] , defer_build=true )
__label__0 cuda_version = result [ `` cuda_version '' ] cublas_paths = base_paths if tuple ( int ( v ) for v in cuda_version.split ( `` . '' ) ) < ( 10 , 1 ) : # before cuda 10.1 , cublas was in the same directory as the toolkit . cublas_paths = cuda_paths cublas_version = os.environ.get ( `` tf_cublas_version '' , `` '' ) result.update ( _find_cublas_config ( cublas_paths , cublas_version , cuda_version ) )
__label__0 def testsignaturefollowsnesteddecorators ( self ) : signature = tf_inspect.signature ( test_decorated_function )
__label__0 shallow_tree_has_invalid_keys = ( `` the shallow_tree 's keys are not a subset of the input_tree 's keys . the `` `` shallow_tree has the following keys that are not in the input_tree : { } . '' )
__label__0 def testwithsimplefunction ( self ) : code = function_utils.get_func_code ( silly_example_function ) self.assertisnotnone ( code ) self.assertregex ( code.co_filename , 'function_utils_test.py ' )
__label__0 # check the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) - ( 0.5 * ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) + ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1e-5 ) ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) - ( 0.5 * ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) + ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1e-5 ) ) ) ] ) , self.evaluate ( var0 ) )
__label__0 def testcreateslotfromvariablecopyxlasharding ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) v = xla_sharding.mesh_split ( v , np.array ( [ 0 , 1 ] ) , [ 0 ] , use_sharding_op=false ) slot = slot_creator.create_slot ( v , initialized_value ( v ) , name= '' slot '' , copy_xla_sharding=true ) self.assertequal ( xla_sharding.get_tensor_sharding ( v ) , xla_sharding.get_tensor_sharding ( slot ) )
__label__0 def testwithcallableclass ( self ) : callable_instance = sillycallableclass ( ) self.assertregex ( function_utils.get_func_name ( callable_instance ) , ' < . * sillycallableclass . * > ' )
__label__0 mystr ( `` abc '' ) self.assertequal ( 1 , mock_warning.call_count ) mystr ( `` abc '' ) self.assertequal ( 1 , mock_warning.call_count ) self.assertin ( `` is deprecated '' , mystr.__doc__ )
__label__0 converts ` import tensorflow.compat.v1 as tf ` to ` import tensorflow as tf ` `` '' ''
__label__0 self.assertequal ( [ tf_inspect.parameter ( ' a ' , tf_inspect.parameter.positional_or_keyword ) , tf_inspect.parameter ( ' b ' , tf_inspect.parameter.positional_or_keyword , default=2 ) , tf_inspect.parameter ( ' c ' , tf_inspect.parameter.positional_or_keyword , default='hello ' ) ] , list ( signature.parameters.values ( ) ) )
__label__0 return tf_decorator.make_decorator ( target , wrapper )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testnesteddataclassmapstructure ( self ) : nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) ) mt_doubled = nest.map_structure ( lambda x : x * 2 , nmt ) expected = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 2 ] ) )
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } }
__label__0 args : arglist : should be a sequence that contains spec , head_symlink , ref_symlink , destination_file .
__label__0 def testrewriteargumentdocstring ( self ) : docs = `` '' '' add ` a ` and ` b `
__label__0 ` elementwise_api_handler ( api_func , x ) `
__label__0 with self.assertraises ( errors_impl.deadlineexceedederror ) : # time-out because w fails to be initialized , # because of overly restrictive ready_for_local_init_op sm.wait_for_session ( `` '' , max_wait_secs=3 )
__label__0 args : grads_and_vars : list of ( gradient , variable ) pairs as returned by compute_gradients ( ) . global_step : optional variable to increment by one after the variables have been updated . name : optional name for the returned operation . default to the name passed to the optimizer constructor .
__label__0 def fn_args ( fn ) : `` '' '' get argument names for function-like object .
__label__0 def test_callable ( self ) :
__label__0 from absl import app from absl import flags
__label__0 @ deprecation.deprecated_arg_values ( date , instructions , warn_once=true , deprecated=true ) def _fn ( deprecated ) : # pylint : disable=unused-argument pass
__label__0 # tf.train.featurelist featurelist = list [ feature ]
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 > > > structure = [ ' a ' ] > > > flat_sequence = [ tf.constant ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] ] ) ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) [ < tf.tensor : shape= ( 2 , 3 ) , dtype=float32 , numpy= array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] ] , dtype=float32 ) > ]
__label__0 # dense features . for i in range ( num_dense ) : key = dense_keys [ i ] feature_config = config.feature_map [ key ] # convert the default value numpy array fetched from the session run # into a tensorproto . fixed_config = feature_config.fixed_len_feature
__label__0 # output tensor indices . sparse_indices_start = 0 sparse_values_start = num_sparse sparse_shapes_start = sparse_values_start + num_sparse dense_values_start = sparse_shapes_start + num_sparse ragged_values_start = dense_values_start + num_dense ragged_row_splits_start = ragged_values_start + num_ragged
__label__0 def keyword_args_only ( func ) : `` '' '' decorator for marking specific function accepting keyword args only .
__label__0 from tensorflow.tools.docs import tf_doctest_lib
__label__0 def func ( ) : pass
__label__0 class versionedtfimport ( ast_edits.analysisresult ) :
__label__0 def test_partial_function ( self ) : expected_test_arg = 123
__label__0 # restore with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : ds0 = dataset_ops.dataset.range ( 10 ) it0 = dataset_ops.make_initializable_iterator ( ds0 ) get_next0 = it0.get_next ( ) saveable0 = iterator_ops._iteratorsaveable ( it0._iterator_resource , name= '' saveable_it0 '' )
__label__0 return handler
__label__0 | op name | has gradient | | -- -- -- -- -| : -- -- -- -- -- -- : | '' '' '' )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' one-line documentation for rmsprop module .
__label__0 cusparse_paths = base_paths if tuple ( int ( v ) for v in cuda_version.split ( `` . '' ) ) < ( 11 , 0 ) : cusparse_paths = cuda_paths cusparse_version = os.environ.get ( `` tf_cusparse_version '' , `` '' ) result.update ( _find_cusparse_config ( cusparse_paths , cusparse_version , cuda_version ) )
__label__0 # output is : # [ ( ( 0 , 0 ) , ( ' a ' , 1 ) ) , # ( ( 0 , 1 , 0 ) , ( ' b ' , 2 ) ) , # ( ( 0 , 1 , 1 , 0 ) , ( ' c ' , 3 ) ) , # ( ( 0 , 1 , 1 , 1 ) , ( 'd ' , 4 ) ) ] # [ ' a ' , 1 , ' b ' , 2 , ' c ' , 3 , 'd ' , 4 ] `` `
__label__0 # pylint : disable=g-doc-return-or-yield def dispatch_for_types ( op , * types ) : `` '' '' decorator to declare that a python function overrides an op for a type .
__label__0 def write_build_info ( filename , key_value_list ) : `` '' '' writes a python that describes the build .
__label__0 check_types = kwargs.pop ( `` check_types '' , true ) expand_composites = kwargs.pop ( `` expand_composites '' , false ) is_nested_fn = ( _is_nested_or_composite if expand_composites else _tf_core_is_nested )
__label__0 import functools
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` in a future version '' , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 def __init__ ( self , primary_variable , mirrored_variable , name ) : self._primary_variable = primary_variable self._mirrored_variable = mirrored_variable super ( _mirroringsaveable , self ) .__init__ ( self._primary_variable , `` '' , name )
__label__0 def compute_gradients ( self , * args , * * kwargs ) : `` '' '' compute gradients of `` loss '' for the variables in `` var_list '' .
__label__0 with session.session ( server.target ) as sess_2 : new_v0 = ops.get_default_graph ( ) .get_tensor_by_name ( `` v0:0 '' ) new_v1 = ops.get_default_graph ( ) .get_tensor_by_name ( `` v1:0 '' ) new_v2 = math_ops.matmul ( new_v0 , new_v1 ) self.assertallequal ( [ [ 4 ] ] , sess_2.run ( new_v2 ) )
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 ] )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 args : * args : compilation args supports inputs either : ( 1 ) all inputs are tensorspec or ( 2 ) all inputs are tf.tensor/python variables . * * kwargs : keyword arguments used for compilation . same requirement as compiliation args .
__label__0 # split ` graphdef.library.function ` function_splitter = repeatedmessagesplitter ( proto.library , [ `` function '' ] , [ functiondefsplitter ] , parent_splitter=self , fields_in_parent= [ `` library '' ] , )
__label__0 converts from any python constant representation of a ` pathlike ` object to a string . if the input is not a ` pathlike ` object , simply returns the input .
__label__0 sets the list of old checkpoint filenames .
__label__0 if ` self ` and ` others ` have no common supertype , this returns ` none ` .
__label__0 ` self._checkpoints_to_be_deleted ` is going to contain checkpoints that are over ` max_to_keep ` . they are going to be deleted . if ` keep_checkpoint_every_n_hours ` was specified , keep an additional checkpoint every ` n ` hours . for example , if ` n ` is 0.5 , an additional checkpoint is kept for every 0.5 hours of training ; if ` n ` is 10 , an additional checkpoint is kept for every 10 hours of training .
__label__0 with ops_lib.name_scope ( `` hidden2 '' ) : variable2 = variable_v1.variablev1 ( [ 2.0 ] , name= '' variable2 '' ) saver2 = saver_module.saver ( var_list= [ variable2 ] , name= '' hidden2/ '' ) graph.add_to_collection ( ops_lib.graphkeys.savers , saver2 )
__label__0 lineno = node.func.value.lineno col_offset = node.func.value.col_offset node.func.value = ast_edits.full_name_node ( `` tf.compat.v1.keras.initializers '' ) node.func.value.lineno = lineno node.func.value.col_offset = col_offset node.func.attr = `` variancescaling ''
__label__0 class testclassb ( testclassa ) : pass
__label__0 class sillytensor ( extension_type.extensiontype ) : `` '' '' simple extensiontype for testing v2 dispatch . '' '' '' value : tensor_lib.tensor how_silly : float
__label__0 def test_contrib_summary_import_event ( self ) : text = `` tf.contrib.summary.import_event ( my_event ) '' _ , _ , errors , _ = self._upgrade ( text ) expected_error = `` tf.compat.v2.summary.experimental.write_raw_pb '' self.assertin ( expected_error , errors [ 0 ] )
__label__0 library_path = _find_library ( base_paths , `` cufft '' , cufft_version )
__label__0 def _update_notebook ( original_notebook , original_raw_lines , updated_code_lines ) : `` '' '' updates notebook , once migration is done . '' '' ''
__label__0 finally : dispatch.unregister_dispatch_for ( handler )
__label__0 with self.assertraisesregex ( valueerror , `` only valid keyword argument . * foo '' ) : nest.map_structure ( lambda x : none , structure1 , foo= '' a '' )
__label__0 # unused ref_symlink arg spec , head_symlink , _ , dest_file = arglist data = json.load ( open ( spec ) ) git_version = none if not data [ `` git '' ] : git_version = b '' unknown '' else : old_branch = data [ `` branch '' ] new_branch = parse_branch_ref ( head_symlink ) if new_branch ! = old_branch : raise runtimeerror ( `` run ./configure again , branch was ' % s ' but is now ' % s ' '' % ( old_branch , new_branch ) ) git_version = get_git_version ( data [ `` path '' ] , git_tag_override ) write_version_info ( dest_file , git_version )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( all_linear_cols , partitioner=none ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=prev_vocab_path ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * ( sc_keys|sc_vocab ) . * '' , var_name_to_vocab_info= { ws_util._infer_var_name ( cols_to_vars [ sc_vocab ] ) : vocab_info } , var_name_to_prev_var_name= { ws_util._infer_var_name ( cols_to_vars [ sc_keys ] ) : `` some_other_name '' } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . var corresponding to # sc_hash should not be warm-started . var corresponding to sc_vocab # should be correctly warm-started after vocab remapping . self._assert_cols_to_vars ( cols_to_vars , { sc_keys : [ prev_keys_val ] , sc_hash : [ np.zeros ( [ 15 , 1 ] ) ] , sc_vocab : [ np.array ( [ [ 3 . ] , [ 2 . ] , [ 1 . ] , [ 0.5 ] , [ 0 . ] , [ 0 . ] ] ) ] } , sess )
__label__0 conformant ` featurelists ` :
__label__0 with graph.as_default ( ) : # hidden 2 with ops_lib.name_scope ( `` hidden2 '' ) : weights = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 128 , 32 ] , stddev=1.0 / math.sqrt ( float ( 128 ) ) ) , name= '' weights '' )
__label__0 major , minor , patch = hipsolver_version_numbers ( rocm_install_path )
__label__0 class hiddentfapiattribute ( property ) : `` '' '' hides a class attribute from the public api .
__label__0 def testunsortedsegmentsum2dindices3ddata ( self ) : for dtype in self.numeric_types : data = np.array ( [ [ [ 0 , 1 , 2 ] , [ 10 , 11 , 12 ] ] , [ [ 100 , 101 , 102 ] , [ 110 , 111 , 112 ] ] , [ [ 200 , 201 , 202 ] , [ 210 , 211 , 212 ] ] , [ [ 300 , 301 , 302 ] , [ 310 , 311 , 312 ] ] ] , dtype=dtype ) indices = np.array ( [ [ 3 , 5 ] , [ 3 , 1 ] , [ 5 , 0 ] , [ 6 , 2 ] ] , dtype=np.int32 ) num_segments = 8 y = self._unsortedsegmentsum ( data , indices , num_segments ) self.assertallclose ( np.array ( [ [ 210 , 211 , 212 ] , [ 110 , 111 , 112 ] , [ 310 , 311 , 312 ] , [ 100 , 102 , 104 ] , [ 0 , 0 , 0 . ] , [ 210 , 212 , 214 ] , [ 300 , 301 , 302 ] , [ 0 , 0 , 0 ] ] , dtype=dtype ) , y )
__label__0 def __str__ ( self ) : return `` . ''
__label__0 returns : a python list , the flattened version of the input .
__label__0 # # # type-based dispatch
__label__0 raises : assertionerror : when _cast is not overloaded in subclass , the value is returned directly , and it should be the same to self.placeholder_value ( ) . `` '' '' del cast_context assert value == self.placeholder_value ( placeholdercontext ( ) ) , f '' can not cast { value ! r } to type { self ! r } '' return value
__label__0 hello
__label__0 def test_contrib_initialize ( self ) : text = `` tf.contrib.summary.initialize '' expected = `` tf.compat.v1.summary.initialize '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 # replace tf.tensor strings with only their numpy field values . want , want_changed = self._tf_tensor_numpy_output ( want ) if want_changed : got , _ = self._tf_tensor_numpy_output ( got )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add ) def masked_add ( x : maskedortensorlike , y : maskedortensorlike , name=none ) : with ops.name_scope ( name ) : x_values = x.values if isinstance ( x , maskedtensor ) else x x_mask = x.mask if isinstance ( x , maskedtensor ) else true y_values = y.values if isinstance ( y , maskedtensor ) else y y_mask = y.mask if isinstance ( y , maskedtensor ) else true return maskedtensor ( x_values + y_values , x_mask & y_mask )
__label__0 pasta.ast_utils.replace_child ( parent , old_value , new_value )
__label__0 def testextractimagepatches ( self ) : text = ( `` tf.extract_image_patches ( images , ksizes=ksizes , strides=strides , '' `` rates=rates , padding=padding , name=name ) '' ) expected_text = ( `` tf.image.extract_patches ( images , sizes=ksizes , strides=strides , '' `` rates=rates , padding=padding , name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 import io import os import tempfile
__label__0 class child11 ( child1 ) : pass `` `
__label__0 def configure ( src_base_path , gen_path , debug=false ) : `` '' '' configure ` src_base_path ` to embed git hashes if available . '' '' ''
__label__0 def __init__ ( self ) : self.symbols_to_detect = { } self.imports_to_detect = { ( `` tensorflow '' , none ) : unaliasedtfimport ( ) , ( `` tensorflow.compat.v1 '' , `` tf '' ) : compat_v1_import , ( `` tensorflow.compat.v2 '' , `` tf '' ) : compat_v2_import , }
__label__0 # assert calling new fn with default deprecated value issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 def build_chunks ( self ) - > int : `` '' '' splits a nodedef proto , and returns the size of the chunks created . '' '' '' if _above_max_size ( self.proto_size ) : constant_bytes = chunk_constant_value ( self._proto , self.proto_size ) self.add_chunk ( constant_bytes , [ `` attr '' , `` value '' , `` tensor '' , `` tensor_content '' ] , ) return len ( constant_bytes ) return 0
__label__0 def register ( self ) : `` '' '' register this dispatcher as a handler for all ops . '' '' '' _global_dispatchers.append ( self )
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) apply_adagrad = gen_training_ops.apply_adagrad ( var , accum , lr , grad ) out = self.evaluate ( apply_adagrad ) self.assertshapeequal ( out , apply_adagrad ) self.assertallcloseaccordingtotype ( x - lr * grad * ( y + grad * grad ) * * ( -0.5 ) , out ) self.assertallcloseaccordingtotype ( y + grad * grad , self.evaluate ( accum ) )
__label__0 def __init__ ( self , api_analysis_spec ) : super ( pastaanalyzevisitor , self ) .__init__ ( noupdatespec ( ) ) self._api_analysis_spec = api_analysis_spec self._results = [ ] # holds analysisresult objects
__label__0 def _get_bound_instance ( target ) : `` '' '' returns the instance any of the targets is attached to . '' '' '' decorators , target = unwrap ( target ) for decorator in decorators : if inspect.ismethod ( decorator.decorated_target ) : return decorator.decorated_target.__self__
__label__0 import itertools import threading
__label__0 returns : a boolean indicating whether the two inputs are the same value for the purposes of deprecation. `` '' '' if a is b : return true try : equality = a == b if isinstance ( equality , bool ) : return equality except typeerror : return false return false
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > @ tf.function ... def run ( ) : ... ctx = tf.distribute.get_replica_context ( ) ... return ctx.replica_id_in_sync_group > > > distributed_values = strategy.run ( run ) > > > distributed_values perreplica : { 0 : < tf.tensor : shape= ( ) , dtype=int32 , numpy=0 > , 1 : < tf.tensor : shape= ( ) , dtype=int32 , numpy=1 > }
__label__0 _min_length = 0 _max_length = 10000
__label__0 def most_specific_common_supertype ( self , others ) : return self if all ( self == other for other in others ) else none
__label__0 # sort by job_name to produce deterministic protobufs . for job_name , tasks in sorted ( self._device_filters.items ( ) ) : try : job_name = compat.as_bytes ( job_name ) except typeerror : raise typeerror ( `` job name % r must be bytes or unicode '' % job_name )
__label__0 the binary elementwise assert apis are :
__label__0 # this is required to so that we initialize the tpu device before # restoring from checkpoint since we 'll be placing variables on the device # and tpuinitialize wipes out the memory of the device . strategy = distribute_lib.get_strategy ( ) if strategy and hasattr ( strategy.extended , `` _experimental_initialize_system '' ) : strategy.extended._experimental_initialize_system ( ) # pylint : disable=protected-access
__label__0 # make a wrapper for the original @ functools.wraps ( func_or_class ) def new_func ( * args , * * kwargs ) : # pylint : disable=missing-docstring if _print_deprecation_warnings : # we 're making the alias as we speak . the original may have other # aliases , so we can not use it to check for whether it 's already been # warned about . if new_func not in _printed_warning : if warn_once : _printed_warning [ new_func ] = true _log_deprecation ( 'from % s : the name % s is deprecated . please use % s instead.\n ' , _call_location ( ) , deprecated_name , name ) return func_or_class ( * args , * * kwargs )
__label__0 `` ` python # use concrete_fn to get the hlo_module flat_args . concrete_fn = f.get_concrete_function ( tf.tensorspec ( shape= ( 10 , 20 ) ) ) flat_args = list ( tf.nest.flatten ( concrete_fn.structured_input_signature ) ) + concrete_fn.captured_inputs `` `
__label__0 def _import_meta_graph_with_return_elements ( meta_graph_or_file , clear_devices=false , import_scope=none , return_elements=none , * * kwargs ) : `` '' '' import metagraph , and return both a saver and returned elements . '' '' '' if context.executing_eagerly ( ) : raise runtimeerror ( `` exporting/importing meta graphs is not supported when `` `` eager execution is enabled . no graph exists when eager `` `` execution is enabled . '' ) if not isinstance ( meta_graph_or_file , meta_graph_pb2.metagraphdef ) : meta_graph_def = meta_graph.read_meta_graph_file ( meta_graph_or_file ) else : meta_graph_def = meta_graph_or_file
__label__0 def task_address ( self , job_name , task_index ) : `` '' '' returns the address of the given task in the given job .
__label__0 the ` run_values ` argument contains results of requested ops/tensors by ` before_run ( ) ` .
__label__0 v1 = resource_variable_ops.resourcevariable ( 3.14 , name= '' v1 '' ) v2 = resource_variable_ops.resourcevariable ( [ 1 , 2 ] , name= '' v2 '' ) save = saver_module.saver ( [ v1 , v2 ] ) save.save ( none , ckpt_prefix )
__label__0 import os
__label__0 _const_op = `` const ''
__label__0 returns : a tuple of files processed , the report string for all files , and a dict mapping filenames to errors encountered in that file. `` '' ''
__label__0 self.generic_visit ( node )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a module target for traversetest.test_module . '' '' ''
__label__0 # initialize v but not w s = session_lib.session ( server.target , graph=graph ) s.run ( v.initializer )
__label__0 if ` date ` is none , 'after < date > ' is replaced with 'in a future version ' . < function > includes the class name if it is a method .
__label__0 for import_alias in node.names : # detect based on full import name ( to & as ) full_module_name = `` % s. % s '' % ( from_import , import_alias.name ) full_import = ( full_module_name , import_alias.asname ) detection = ( self._api_analysis_spec .imports_to_detect.get ( full_import , none ) ) if detection : self.add_result ( detection ) self.add_log ( detection.log_level , node.lineno , node.col_offset , detection.log_message )
__label__0 _major_api_version = 1
__label__0 def test_invalid_shape ( self ) : with ops.graph ( ) .as_default ( ) as g : self.assertisnone ( training_util.get_global_step ( ) ) variable_v1.variablev1 ( [ 0 ] , trainable=false , dtype=dtypes.int32 , name=ops.graphkeys.global_step , collections= [ ops.graphkeys.global_step ] ) self.assertraisesregex ( typeerror , 'not scalar ' , training_util.get_global_step ) self.assertraisesregex ( typeerror , 'not scalar ' , training_util.get_global_step , g )
__label__0 # type-based dispatch system ( dispatch v2 ) : if api_dispatcher is not none : if iterable_params is not none : args , kwargs = replace_iterable_params ( args , kwargs , iterable_params ) result = api_dispatcher.dispatch ( args , kwargs ) if result is not notimplemented : return result
__label__0 traverse_result3 = nest.get_traverse_shallow_structure ( lambda s : isinstance ( s , tensor.tensor ) , nmt ) # expected ` traverse_result3 = false ` because ` nmt ` does n't pass the # traverse function . self.assertequal ( traverse_result3 , false ) nest.assert_shallow_structure ( traverse_result3 , nmt )
__label__0 these objects represent the ( usually future ) output of executing an op immediately. `` '' ''
__label__0 mock_tf = mockmodule ( 'tensorflow ' ) mock_tf_wrapped = module_wrapper.tfmodulewrapper ( mock_tf , 'test ' , public_apis=apis ) mock_tf_wrapped.cosh # pylint : disable=pointless-statement self.assertfalse ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded )
__label__0 return make_tf_decorator
__label__0 args : var : current graph 's variable that needs to be warm-started ( initialized ) . can be either of the following : ( i ) ` variable ` ( ii ) ` resourcevariable ` ( iii ) list of ` variable ` : the list must contain slices of the same larger variable . ( iv ) ` partitionedvariable ` prev_tensor_name : name of the tensor to lookup in provided ` prev_ckpt ` . if none , we lookup tensor with same name as given ` var ` .
__label__0 mt2 = maskedtensor ( mask=true , value=constant_op.constant ( [ 3 ] ) ) nmt2 = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=false , inner_value=constant_op.constant ( [ 2 ] ) ) results = nest.list_to_tuple ( input_sequence ) expected = ( mt2 , ( nmt2 , { `` a '' : ( mt2 , nmt2 , ( mt2 , ) ) } , none , nmt2 , ( ( ( mt2 , ) , ) , ) ) , ) nest.assert_same_structure ( results , expected )
__label__0 result.update ( res )
__label__0 < < api_list > > `` '' ''
__label__0 you can return from this call a ` sessionrunargs ` object indicating ops or tensors to add to the upcoming ` run ( ) ` call . these ops/tensors will be run together with the ops/tensors originally passed to the original run ( ) call . the run args you return can also contain feeds to be added to the run ( ) call .
__label__0 f.experimental_get_compiler_ir ( tf.random.normal ( [ 10 , 10 ] ) ( stage='hlo ' ) `` `
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_namedtuple ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 for example , for
__label__0 class tfdecoratortest ( test.testcase ) :
__label__0 if ops.inside_function ( ) and not error_in_function : # we do n't currently log warnings in tf.function calls , so just skip it . return x
__label__0 from tensorflow.tools.compatibility import ast_edits
__label__0 * these atom vs. atom comparisons will pass :
__label__0 # in a member of group 0 : with lock.group ( 0 ) : # do stuff , access the resource # ...
__label__0 @ test_injectable_decorator_square @ test_injectable_decorator_increment def test_rewrappable_decorated ( x ) : return x * 2
__label__0 self.assertequal ( `` var/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , array_ops.shape ( slot ) .eval ( ) ) self.assertequal ( dtypes.float64 , slot.dtype.base_dtype ) self.assertallequal ( [ 0.0 , 0.0 ] , self.evaluate ( slot ) )
__label__0 @ test_util.run_v1_only ( `` applygradientdescent op returns a ref , so it is not `` `` supported in eager mode . '' ) def testapplygradientdescent ( self ) : for ( dtype , use_gpu ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ false , true ] ) : x = np.arange ( 100 ) .astype ( dtype ) alpha = np.array ( 2.0 ) .astype ( dtype ) delta = np.arange ( 100 ) .astype ( dtype ) self._testtypes ( x , alpha , delta , use_gpu )
__label__0 args : other : a tracetype object to be compared against .
__label__0 def path_to_bytes ( path ) : r '' '' '' converts input which is a ` pathlike ` object to ` bytes ` .
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 class clusterspectest ( test.testcase ) :
__label__0 # if we used the alias , it should get renamed text = `` g ( a , b , kw1_alias=x , c=c ) \n '' acceptable_outputs = [ `` g ( a , b , kw1=x , c=c ) \n '' , `` g ( a , b , c=c , kw1=x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , `` g ( a=a , b=b , c=c , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 def _get_basekit_version ( ) : return _get_toolkit_path ( ) .split ( `` /compiler/ '' ) [ 1 ] .split ( `` / '' ) [ 0 ]
__label__0 _dot = _dotstring ( )
__label__0 return deprecated_wrapper
__label__0 def _unsortedsegmentmin ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.unsorted_segment_min , data , indices , num_segments )
__label__0 # opdispatchers which should be used for all operations . _global_dispatchers = [ ]
__label__0 new_keywords = [ ] scale = pasta.parse ( `` 1.0 '' ) new_keywords.append ( ast.keyword ( arg= '' scale '' , value=scale ) )
__label__0 export_decorator1 = tf_export.tf_export ( 'name_a ' , 'name_b ' ) export_decorator2 = tf_export.tf_export ( 'name_c ' , 'name_d ' ) export_decorator3 = tf_export.tf_export ( 'name_e ' , 'name_f ' ) export_decorator1.export_constant ( 'module1 ' , test_constant1 ) export_decorator2.export_constant ( 'module2 ' , test_constant2 ) export_decorator3.export_constant ( 'module2 ' , test_constant3 ) self.assertequal ( [ ( ( 'name_a ' , 'name_b ' ) , 123 ) ] , module1._tf_api_constants ) self.assertequal ( [ ( ( 'name_c ' , 'name_d ' ) , 'abc ' ) , ( ( 'name_e ' , 'name_f ' ) , 0.5 ) ] , module2._tf_api_constants )
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 > > > skip_magic ( `` ! gcloud ml-engine models create $ { model } \\\n '' ) true `` '' ''
__label__0 rocfft_config = { `` rocfft_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 args : output_dir : a string path , where to put the files . code_url_prefix : prefix for `` defined in '' links . search_hints : bool . include meta-data search hints at the top of each file. `` '' '' output_dir = pathlib.path ( output_dir ) site_path = pathlib.path ( `` / '' , flags.site_path )
__label__0 @ tf_export ( `` distribute.server '' , v1= [ `` distribute.server '' , `` train.server '' ] ) @ deprecation.deprecated_endpoints ( `` train.server '' ) class server : `` '' '' an in-process tensorflow server , for use in distributed training .
__label__0 the input , ` input_tree ` , can be thought of as having the same structure layout as ` shallow_tree ` , but with leaf nodes that are themselves tree structures .
__label__0 zout = zipfile.zipfile ( directory + new_binary , `` w '' , zipfile.zip_deflated ) zip_these_files = [ `` % s- % s.dist-info '' % ( package , version ) , `` % s- % s.data '' % ( package , version ) , `` tensorflow '' , `` tensorflow_core '' , ] for dirname in zip_these_files : for root , _ , files in os.walk ( dirname ) : for filename in files : zout.write ( os.path.join ( root , filename ) ) zout.close ( ) finally : shutil.rmtree ( tmpdir )
__label__0 with open ( input_file ) as in_file : notebook = json.load ( in_file )
__label__0 try : del python except nameerror : pass
__label__0 api_signature = tf_inspect.signature ( api ) signature_checkers = [ _make_signature_checker ( api_signature , signature ) for signature in signatures ]
__label__0 def testgetattributecallback ( self ) : # tests that functionality of __getattribute__ can be set as a callback . module = childfastmodule ( `` test '' ) fastmoduletype.set_getattribute_callback ( module , childfastmodule._getattribute1 ) self.assertequal ( 2 , module.foo )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.sparsecountsparseoutput . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 from tensorflow.python.framework import strict_mode from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import decorator_utils from tensorflow.python.util import is_in_graph_mode from tensorflow.python.util import tf_contextlib from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect from tensorflow.tools.docs import doc_controls
__label__0 header_path , header_version = _find_header ( base_paths , `` nvinferversion.h '' , required_version , get_header_version )
__label__0 returns : dictionary from arg_name to deprecatedargspec. `` '' '' # extract argument list arg_space = arg_spec.args + arg_spec.kwonlyargs arg_name_to_pos = { name : pos for pos , name in enumerate ( arg_space ) } deprecated_positional_args = { } for arg_name , spec in iter ( names_to_ok_vals.items ( ) ) : if arg_name in arg_name_to_pos : pos = arg_name_to_pos [ arg_name ] deprecated_positional_args [ arg_name ] = deprecatedargspec ( pos , spec.has_ok_value , spec.ok_value ) return deprecated_positional_args
__label__0 from tensorflow.python.eager import def_function from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.util import traceback_utils
__label__0 `` `` '' keyword args functions . '' '' ''
__label__0 args : sess : a session to use to save the variables . save_path : string . prefix of filenames created for the checkpoint . global_step : if provided the global step number is appended to ` save_path ` to create the checkpoint filenames . the optional argument can be a ` tensor ` , a ` tensor ` name or an integer . latest_filename : optional name for the protocol buffer file that will contains the list of most recent checkpoints . that file , kept in the same directory as the checkpoint files , is automatically managed by the saver to keep track of recent checkpoints . defaults to 'checkpoint ' . meta_graph_suffix : suffix for ` metagraphdef ` file . defaults to 'meta ' . write_meta_graph : ` boolean ` indicating whether or not to write the meta graph file . write_state : ` boolean ` indicating whether or not to write the ` checkpointstateproto ` . strip_default_attrs : boolean . if ` true ` , default-valued attributes will be removed from the nodedefs . for a detailed guide , see [ stripping default-valued attributes ] ( https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/readme.md # stripping-default-valued-attributes ) . save_debug_info : if ` true ` , save the graphdebuginfo to a separate file , which in the same directory of save_path and with ` _debug ` added before the file extension . this is only enabled when ` write_meta_graph ` is ` true `
__label__0 args : graph : the graph to find the global step in . if missing , use default graph .
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] shallow_tree : a possibly pruned structure of input_tree . input_tree : an atom or a nested structure . note , numpy arrays are considered atoms . check_types : bool . if true , check that each node in shallow_tree has the same type as the corresponding node in input_tree . expand_composites : arg valid for modality.core only . if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 `` ` python a b `` `
__label__0 toc_path = output_dir / `` tf/_toc.yaml '' with edit_yaml_file ( toc_path ) as toc : # replace the overview path for 'tensorflow ' to # ` /api_docs/python/tf_overview ` . this will be redirected to # ` /api_docs/python/tf ` . toc [ `` toc '' ] [ 0 ] [ `` section '' ] [ 0 ] [ `` path '' ] = str ( site_path / `` tf_overview '' )
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 2 ] ) ) nmt_flat_paths = nest.flatten_with_joined_string_paths ( nmt , separator=sep ) self.assertequal ( nmt_flat_paths [ 0 ] [ 0 ] , `` 0/0 '' ) self.assertallequal ( nmt_flat_paths [ 0 ] [ 1 ] , [ 2 ] )
__label__0 def testpywraptensorflowwarning ( self ) : text = `` tf.pywrap_tensorflow.foo ( ) '' expected = `` tf.pywrap_tensorflow.foo ( ) '' _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` ` tf.pywrap_tensorflow ` will not be distributed '' , errors [ 0 ] )
__label__0 self.assertraises ( stopiteration , lambda : next ( rr ) ) # there should be a checkpoint file with the variable `` foo '' with ops.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : v = variable_v1.variablev1 ( [ 10.10 ] , name= '' foo '' ) sav = saver_lib.saver ( [ v ] ) sav.restore ( sess , save_path ) self.assertequal ( 1.0 , self.evaluate ( v ) [ 0 ] )
__label__0 returns : a pair ( sess , is_restored ) where 'is_restored ' is ` true ` if the session could be restored , ` false ` otherwise .
__label__0 self.assertequal ( 0 , mock_warning.call_count ) bar = wrapped_module.bar self.assertequal ( 1 , mock_warning.call_count ) foo = wrapped_module.foo self.assertequal ( 1 , mock_warning.call_count ) baz = wrapped_module.baz # pylint : disable=unused-variable self.assertequal ( 2 , mock_warning.call_count ) baz = wrapped_module.baz self.assertequal ( 2 , mock_warning.call_count )
__label__0 wrapped_fn = functools.partial ( fn_has_no_kwargs , test_arg=123 ) self.assertfalse ( function_utils.has_kwargs ( wrapped_fn ) ) some_arg = 1 self.assertequal ( wrapped_fn ( some_arg ) , some_arg )
__label__0 cuda_library_path = _find_library ( base_paths , `` cudart '' , cuda_version )
__label__0 # manual mapping of function names to be reordered to their list of argument # names , in order . only use this if argument names can not be autodetected , # e.g . if the functions are in contrib . self.manual_function_reorders = { `` tf.contrib.summary.audio '' : [ `` name '' , `` tensor '' , `` sample_rate '' , `` max_outputs '' , `` family '' , `` step '' ] , `` tf.contrib.summary.create_file_writer '' : [ `` logdir '' , `` max_queue '' , `` flush_millis '' , `` filename_suffix '' , `` name '' ] , `` tf.contrib.summary.generic '' : [ `` name '' , `` tensor '' , `` metadata '' , `` family '' , `` step '' ] , `` tf.contrib.summary.histogram '' : [ `` name '' , `` tensor '' , `` family '' , `` step '' ] , `` tf.contrib.summary.image '' : [ `` name '' , `` tensor '' , `` bad_color '' , `` max_images '' , `` family '' , `` step '' ] , `` tf.contrib.summary.scalar '' : [ `` name '' , `` tensor '' , `` family '' , `` step '' ] , } # functions that were reordered should be changed to the new keyword args # for safety , if positional arguments are used . if you have reversed the # positional arguments yourself , this could do the wrong thing . self.function_reorders = dict ( reorders_v2.reorders ) self.function_reorders.update ( self.manual_function_reorders )
__label__0 * ` @ dispatch_for_api ( tf.add , { ' x ' : union [ maskedtensor , tensor ] , ' y ' : union [ maskedtensor , tensor ] } ) ` : will not dispatch to the decorated dispatch target when the user calls ` tf.add ( tf.constant ( 1 ) , tf.constant ( 2 ) ) ` .
__label__0 args : var : a list . the list can contain either of the following : ( i ) a single ` variable ` ( ii ) a single ` resourcevariable ` ( iii ) multiple ` variable ` objects which must be slices of the same larger variable . ( iv ) a single ` partitionedvariable `
__label__0 import io import os import re import sys
__label__0 class rawopspageinfo ( module_page.modulepageinfo ) : `` '' '' generates a custom page for ` tf.raw_ops ` . '' '' ''
__label__0 class placeholdercontext ( ) : `` '' '' contains context information for generating placeholders within a scope . '' '' ''
__label__0 args : file_prefix : string prefix of the filepath .
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' reads summaries from and writes summaries to event files . '' '' ''
__label__0 # pass the variables as a dict : saver = tf.compat.v1.train.saver ( { 'v1 ' : v1 , 'v2 ' : v2 } )
__label__0 the following accumulators/queue are created :
__label__0 input_tree_smaller_than_shallow_tree = ( nest_util.input_tree_smaller_than_shallow_tree )
__label__0 @ property @ abc.abstractmethod def version_def ( self ) - > versions_pb2.versiondef : `` '' '' version info about the splitter and merge implementation required . '' '' ''
__label__0 def testwarmstartvarwithvocabprevvarpartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] )
__label__0 @ tf_export ( v1= [ `` __internal__.types.data.dataset '' ] ) class datasetv1 ( datasetv2 , abc.abc ) : `` '' '' represents the tensorflow 1 type ` tf.data.dataset ` . '' '' ''
__label__0 returns : a list of variables. `` '' '' return self._opt.variables ( )
__label__0 # pylint : disable=line-too-long # this list contains names of functions that had their arguments reordered . # after modifying this list , run the following to update reorders_v2.py : # bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map # pylint : enable=line-too-long self.reordered_function_names = { `` tf.io.serialize_sparse '' , `` tf.io.serialize_many_sparse '' , `` tf.argmax '' , `` tf.argmin '' , `` tf.batch_to_space '' , `` tf.cond '' , `` tf.nn.space_to_batch '' , `` tf.boolean_mask '' , `` tf.convert_to_tensor '' , `` tf.nn.conv1d '' , `` tf.nn.conv2d '' , `` tf.nn.conv2d_backprop_input '' , `` tf.nn.ctc_beam_search_decoder '' , `` tf.nn.moments '' , `` tf.nn.convolution '' , `` tf.nn.crelu '' , `` tf.nn.weighted_moments '' , `` tf.nn.pool '' , `` tf.nn.separable_conv2d '' , `` tf.nn.depthwise_conv2d '' , `` tf.multinomial '' , `` tf.random.multinomial '' , `` tf.pad '' , `` tf.quantize_v2 '' , `` tf.feature_column.categorical_column_with_vocabulary_file '' , `` tf.shape '' , `` tf.size '' , # todo ( b/129398290 ) # `` tf.string_split '' , `` tf.random.poisson '' , `` tf.sparse.add '' , `` tf.sparse_add '' , `` tf.sparse.concat '' , `` tf.sparse_concat '' , `` tf.sparse.segment_mean '' , `` tf.sparse.segment_sqrt_n '' , `` tf.sparse.segment_sum '' , `` tf.sparse_matmul '' , `` tf.sparse.reduce_max '' , `` tf.sparse_reduce_max '' , `` tf.io.decode_csv '' , `` tf.strings.length '' , `` tf.strings.reduce_join '' , `` tf.strings.substr '' , `` tf.substr '' , `` tf.transpose '' , `` tf.tuple '' , `` tf.parse_example '' , `` tf.parse_single_example '' , `` tf.io.parse_example '' , `` tf.io.parse_single_example '' , `` tf.while_loop '' , `` tf.reduce_all '' , `` tf.math.reduce_all '' , `` tf.reduce_any '' , `` tf.math.reduce_any '' , `` tf.reduce_min '' , `` tf.math.reduce_min '' , `` tf.reduce_max '' , `` tf.math.reduce_max '' , `` tf.reduce_sum '' , `` tf.math.reduce_sum '' , `` tf.reduce_mean '' , `` tf.math.reduce_mean '' , `` tf.reduce_prod '' , `` tf.math.reduce_prod '' , `` tf.reduce_logsumexp '' , `` tf.math.reduce_logsumexp '' , `` tf.reduce_join '' , `` tf.confusion_matrix '' , `` tf.math.confusion_matrix '' , `` tf.math.in_top_k '' , `` tf.nn.depth_to_space '' , `` tf.nn.embedding_lookup '' , `` tf.nn.embedding_lookup_sparse '' , `` tf.nn.in_top_k '' , `` tf.nn.space_to_depth '' , `` tf.test.assert_equal_graph_def '' , `` tf.linalg.norm '' , `` tf.norm '' , `` tf.reverse_sequence '' , `` tf.sparse_split '' , # tf.nn.softmax_cross_entropy_with_logits * must * be called with # keyword arguments . add keyword arguments in rare case when they # are not specified . `` tf.nn.softmax_cross_entropy_with_logits '' , `` tf.nn.fractional_avg_pool '' , `` tf.nn.fractional_max_pool '' , `` tf.image.sample_distorted_bounding_box '' , `` tf.gradients '' , `` tf.hessians '' , `` tf.nn.max_pool '' , `` tf.nn.avg_pool '' , `` tf.initializers.uniform_unit_scaling '' , `` tf.uniform_unit_scaling_initializer '' , `` tf.data.experimental.tensorstructure '' , `` tf.data.experimental.sparsetensorstructure '' , `` tf.data.experimental.raggedtensorstructure '' , `` tf.data.experimental.tensorarraystructure '' , `` tf.debugging.assert_all_finite '' , `` tf.gather_nd '' , }
__label__0 returns : an operation that restores the variables. `` '' '' sharded_restores = [ ] for shard , ( device , saveables ) in enumerate ( per_device ) : with ops.device ( device ) : sharded_restores.append ( self._addrestoreops ( filename_tensor , saveables , restore_sequentially , reshape , preferred_shard=shard , name= '' restore_shard '' ) ) return control_flow_ops.group ( * sharded_restores , name= '' restore_all '' )
__label__0 > > > text_parts , floats = _floatextractor ( ) ( `` text 1.0 text '' ) > > > text_parts [ `` text `` , `` text '' ] > > > floats np.array ( [ 1.0 ] ) `` '' ''
__label__0 class warmstartingutiltest ( test.testcase ) :
__label__0 def export_constant ( self , module_name : str , name : str ) - > none : `` '' '' store export information for constants/string literals .
__label__0 args : cluster : a dictionary mapping one or more job names to ( i ) a list of network addresses , or ( ii ) a dictionary mapping integer task indices to network addresses ; or a ` tf.train.clusterdef ` protocol buffer .
__label__0 3. for a nested dictionary of dictionaries :
__label__0 def testassertrankstatements ( self ) : for name in [ `` assert_rank '' , `` assert_rank_at_least '' , `` assert_rank_in '' ] : text = `` tf. % s ( a ) '' % name expected_text = `` tf.compat.v1. % s ( a ) '' % name _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` % s has been '' % name , report )
__label__0 if var_name_to_vocab_info is none : var_name_to_vocab_info = { }
__label__0 returns : true if ` instance ` is a ` namedtuple ` . `` '' '' return _pywrap_utils.isnamedtuple ( instance , strict )
__label__0 > > > g = tf.graph ( ) > > > with g.as_default ( ) : ... x = tf.compat.v1.placeholder ( tf.float32 , [ ] ) ... loss , var_list = compute_loss ( x ) ... global_step = tf.compat.v1.train.get_or_create_global_step ( ) ... global_init = tf.compat.v1.global_variables_initializer ( ) ... optimizer = tf.compat.v1.train.gradientdescentoptimizer ( 0.1 ) ... train_op = optimizer.minimize ( loss , global_step , var_list ) > > > sess = tf.compat.v1.session ( graph=g ) > > > sess.run ( global_init ) > > > print ( `` before training : '' , sess.run ( global_step ) ) before training : 0 > > > sess.run ( train_op , feed_dict= { x : 3 } ) > > > print ( `` after training : '' , sess.run ( global_step ) ) after training : 1
__label__0 a conformant ` sequenceexample ` data set obeys the following conventions :
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testinitopwithfeeddict ( self ) : logdir = self._test_dir ( `` feed_dict_init_op '' ) with ops.graph ( ) .as_default ( ) : p = array_ops.placeholder ( dtypes.float32 , shape= ( 3 , ) ) v = variable_v1.variablev1 ( p , name= '' v '' ) sv = supervisor.supervisor ( logdir=logdir , init_op=variables.global_variables_initializer ( ) , init_feed_dict= { p : [ 1.0 , 2.0 , 3.0 ] } ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) sv.stop ( )
__label__0 if ` shallow_tree ` and ` input_tree ` are atoms , this returns a single-item list : ` [ input_tree ] ` .
__label__0 args : api_names : api names iterable . deprecated_api_names : deprecated api names iterable .
__label__0 def testtensorflowimportinindent ( self ) : text = `` '' '' try : import tensorflow as tf # import line
__label__0 # we put doctest after absltest so that it picks up the unittest monkeypatch . # otherwise doctest tests are n't runnable at all . import doctest # pylint : disable=g-bad-import-order
__label__0 def test_decorator_increment_first_int_arg ( target ) : `` '' '' this test decorator skips past ` self ` as args [ 0 ] in the bound case . '' '' ''
__label__0 for name in names_v1 : name = `` tf. % s '' % name if name not in all_keyword_renames : continue arg_names_v1 = tf_inspect.getargspec ( attr ) [ 0 ] keyword_renames = all_keyword_renames [ name ] self.assertequal ( type ( keyword_renames ) , dict )
__label__0 _is_mapping_view = _pywrap_utils.ismappingview _is_attrs = _pywrap_utils.isattrs _is_composite_tensor = _pywrap_utils.iscompositetensor _is_type_spec = _pywrap_utils.istypespec _is_mutable_mapping = _pywrap_utils.ismutablemapping _is_mapping = _pywrap_utils.ismapping _tf_data_is_nested = _pywrap_utils.isnestedfordata _tf_data_flatten = _pywrap_utils.flattenfordata _tf_core_is_nested = _pywrap_utils.isnested _is_nested_or_composite = _pywrap_utils.isnestedorcomposite # see the swig file ( util.i ) for documentation . same_namedtuples = _pywrap_utils.samenamedtuples
__label__0 # if decorator_func is not a decorator , new_target replaces it directly . if innermost_decorator is none : # consistency check . the caller should always pass the result of # tf_decorator.unwrap as previous_target . if decorator_func is not a # decorator , that will have returned decorator_func itself . assert decorator_func is previous_target return new_target
__label__0 returns : an operation that save the variables .
__label__0 # runs train_op . train_op = optimizer.minimize ( loss ) ops_lib.add_to_collection ( `` train_op '' , train_op )
__label__0 gives access to all graph transforms available through the command line tool . see documentation at https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/readme.md for full details of the options available .
__label__0 sess , is_loaded_from_checkpoint = self._restore_checkpoint ( master , saver , checkpoint_dir=checkpoint_dir , checkpoint_filename_with_path=checkpoint_filename_with_path , wait_for_checkpoint=wait_for_checkpoint , max_wait_secs=max_wait_secs , config=config )
__label__0 from tensorflow.python.util import tf_inspect
__label__0 def test_decode_raw ( self ) : text = `` tf.io.decode_raw ( bytes= [ 1,2,3 ] , output_dtype=tf.int32 ) '' expected_text = ( `` tf.io.decode_raw ( input_bytes= [ 1,2,3 ] , output_dtype=tf.int32 ) '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 from_import = node.module
__label__0 git_path = os.path.join ( src_base_path , `` .git '' )
__label__0 class publicapitest ( googletest.testcase ) :
__label__0 def process_test_logs ( name , test_name , test_args , benchmark_type , start_time , run_time , log_files ) : `` '' '' gather test information and put it in a testresults proto .
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( self , arg0 , arg1 ) : return arg0 + arg1
__label__0 with ops_lib.graph ( ) .as_default ( ) as g : a = variable_v1.variablev1 ( 1. , name= '' a '' ) a_saver = saver_module.saver ( [ a ] )
__label__0 users must not modify any collections used in nest while this function is running .
__label__0 def test_no_descent_child_removal ( self ) : visitor = self.testvisitor ( ) children = [ ( 'name1 ' , 'thing1 ' ) , ( 'mock ' , 'thing2 ' ) ] public_api.publicapivisitor ( visitor ) ( 'test ' , 'dummy ' , children ) # make sure not-to-be-descended-into symbols are removed after the visitor # is called . self.assertequal ( [ ( 'name1 ' , 'thing1 ' ) , ( 'mock ' , 'thing2 ' ) ] , visitor.last_children ) self.assertequal ( [ ( 'name1 ' , 'thing1 ' ) ] , children )
__label__0 def testisclass ( self ) : self.asserttrue ( tf_inspect.isclass ( testdecoratedclass ) ) self.assertfalse ( tf_inspect.isclass ( test_decorated_function ) )
__label__0 def _extract_glimpse_transformer ( parent , node , full_name , name , logs ) :
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # insert any followup lines that should happen after this import . full_import = ( import_alias.name , import_alias.asname ) insert_offset = 1 for line_to_insert in inserts_after_imports.get ( full_import , [ ] ) : assert self._stack [ -1 ] is node parent = self._stack [ -2 ]
__label__0 # when calling func : # - positional args without default must be in the same order . # - ignore missing optional arguments from op
__label__0 threads = qr.create_threads ( sess , start=true ) for t in threads : t.join ( ) exceptions = qr.exceptions_raised self.assertequal ( 1 , len ( exceptions ) ) self.asserttrue ( `` operation not in the graph '' in str ( exceptions [ 0 ] ) )
__label__0 def testnomaxtokeep ( self ) : save_dir = self._get_test_dir ( `` no_max_to_keep '' ) save_dir2 = self._get_test_dir ( `` max_to_keep_0 '' )
__label__0 new_code = [ updated_code_lines [ idx ] for idx in applicable_lines ]
__label__1 import numpy as np def create_matrix ( n ) : return np.zeros ( ( n , n ) )
__label__0 # we can not define namedtuples within @ parameterized argument lists . # pylint : disable=invalid-name foo = collections.namedtuple ( `` foo '' , [ `` a '' , `` b '' ] ) bar = collections.namedtuple ( `` bar '' , [ `` c '' , `` d '' ] ) # pylint : enable=invalid-name
__label__0 def testnotransformifnothingissupplied ( self ) : text = `` f ( a , b , kw1=c , kw2=d ) \n '' _ , new_text = self._upgrade ( ast_edits.noupdatespec ( ) , text ) self.assertequal ( new_text , text )
__label__0 # start a second session . in that session the parameter nodes # have not been initialized either . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = variable_op ( -1.0 , name= '' v0 '' ) v1 = variable_op ( -1.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' )
__label__1 def calculate_area_of_circle ( radius ) : return 3.14 * radius * * 2
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 args : job_name : the string name of a job in this cluster .
__label__0 if _print_deprecation_warnings : # we 're making the alias as we speak . the original may have other # aliases , so we can not use it to check for whether it 's already been # warned about . if _newclass.__init__ not in _printed_warning : if warn_once : _printed_warning [ _newclass.__init__ ] = true _log_deprecation ( 'from % s : the name % s is deprecated . please use % s instead.\n ' , _call_location ( ) , deprecated_name , name ) super ( _newclass , self ) .__init__ ( * args , * * kwargs )
__label__0 > > > structure = [ ' a ' ] > > > flat_sequence = [ tf.constant ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] ] ) ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) [ < tf.tensor : shape= ( 2 , 3 ) , dtype=float32 , numpy= array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] ] , dtype=float32 ) > ]
__label__0 self.assertprotoequals ( `` '' '' cluster { job { name : 'ps ' tasks { key : 0 value : 'ps0:2222 ' } tasks { key : 1 value : 'ps1:2222 ' } } job { name : 'worker ' tasks { key : 0 value : 'worker0:2222 ' } tasks { key : 2 value : 'worker2:2222 ' } } } job_name : 'worker ' task_index : 2 protocol : 'grpc ' `` '' '' , server_def )
__label__0 `` ` python @ deprecated def deprecated_alias ( original_args ) : real_function ( original_args ) `` `
__label__0 returns : a tensor with the filename used to save. `` '' '' save = self.save_op ( filename_tensor , saveables ) return control_flow_ops.with_dependencies ( [ save ] , filename_tensor )
__label__0 parts = [ warning , table_header ]
__label__0 args : sess : a ` session ` . `` '' '' # pylint : disable=broad-except try : sess.close ( ) except exception : # intentionally not logging to avoid user complaints that # they get cryptic errors . we really do not care that close # fails . pass # pylint : enable=broad-except
__label__0 def testdeprecatedargumentlookup ( self ) : good_value = 3 self.assertequal ( deprecation.deprecated_argument_lookup ( `` val_new '' , good_value , `` val_old '' , none ) , good_value ) self.assertequal ( deprecation.deprecated_argument_lookup ( `` val_new '' , none , `` val_old '' , good_value ) , good_value ) with self.assertraisesregex ( valueerror , `` can not specify both 'val_old ' and 'val_new ' '' ) :
__label__0 if the structure is an atom , then returns a single-item list : [ structure ] .
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' basic interface for python-based splitter . '' '' ''
__label__0 def make_all ( module_name , doc_string_modules=none ) : `` '' '' generates ` __all__ ` from the docstring of one or more modules .
__label__0 this library is a common interface that contains keras functions needed by tensorflow and tensorflow lite and is required as per the dependency inversion principle ( https : //en.wikipedia.org/wiki/dependency_inversion_principle ) . as per this principle , high-level modules ( eg : tensorflow and tensorflow lite ) should not depend on low-level modules ( eg : keras ) and instead both should depend on a common interface such as this file. `` '' ''
__label__0 def has_kwargs ( fn ) : `` '' '' returns whether the passed callable has * * kwargs in its signature .
__label__0 class deprecatednamesalreadyseterror ( exception ) : `` '' '' raised when setting deprecated names multiple times for the same symbol . '' '' ''
__label__0 # some regular expressions we will need for parsing find_open = re.compile ( r '' ^\s * ( \ [ ) . * $ '' ) find_string_chars = re.compile ( r '' [ '\ '' ] '' )
__label__0 # same as teststandardservicesnoglobalstep but with a global step . # we should get a summary about the step time . @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def teststandardserviceswithglobalstep ( self ) : logdir = self._test_dir ( `` standard_services_with_global_step '' ) # create a checkpoint . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 123 ] , name= '' global_step '' ) sv = supervisor.supervisor ( logdir=logdir ) meta_graph_def = meta_graph.create_meta_graph_def ( saver_def=sv.saver.saver_def ) sess = sv.prepare_or_wait_for_session ( `` '' ) # this is where the checkpoint will appear , with step number 123. save_path = `` % s-123 '' % sv.save_path self._wait_for_glob ( save_path , 3.0 ) self._wait_for_glob ( os.path.join ( logdir , `` * events * '' ) , 3.0 , for_checkpoint=false ) # wait to make sure everything is written to file before stopping . time.sleep ( 1 ) sv.stop ( ) # there should be an event file with a version number . rr = _summary_iterator ( logdir ) ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version ) ev = next ( rr ) ev_graph = graph_pb2.graphdef ( ) ev_graph.parsefromstring ( ev.graph_def ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_graph ) ev = next ( rr ) ev_meta_graph = meta_graph_pb2.metagraphdef ( ) ev_meta_graph.parsefromstring ( ev.meta_graph_def ) self.assertprotoequals ( meta_graph_def , ev_meta_graph ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_meta_graph.graph_def ) ev = next ( rr ) # it is actually undeterministic whether sessionlog.start gets written # before the summary or the checkpoint , but this works when run 10000 times . self.assertequal ( 123 , ev.step ) self.assertequal ( event_pb2.sessionlog.start , ev.session_log.status ) first = next ( rr ) second = next ( rr ) # it is undeterministic whether the value gets written before the checkpoint # since they are on separate threads , so we check for both conditions . if first.hasfield ( `` summary '' ) : self.assertprotoequals ( `` '' '' value { tag : 'global_step/sec ' simple_value : 0.0 } '' '' '' , first.summary ) self.assertequal ( 123 , second.step ) self.assertequal ( event_pb2.sessionlog.checkpoint , second.session_log.status ) else : self.assertequal ( 123 , first.step ) self.assertequal ( event_pb2.sessionlog.checkpoint , first.session_log.status ) self.assertprotoequals ( `` '' '' value { tag : 'global_step/sec ' simple_value : 0.0 } '' '' '' , second.summary ) ev = next ( rr ) self.assertequal ( event_pb2.sessionlog.stop , ev.session_log.status ) self.assertraises ( stopiteration , lambda : next ( rr ) ) # there should be a checkpoint file with the variable `` foo '' with ops.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : v = variable_v1.variablev1 ( [ -12 ] , name= '' global_step '' ) sav = saver_lib.saver ( [ v ] ) sav.restore ( sess , save_path ) self.assertequal ( 123 , self.evaluate ( v ) [ 0 ] )
__label__0 def __getattr__ ( self , item ) : module = self._load ( ) return getattr ( module , item )
__label__0 from typing_extensions import protocol from typing_extensions import runtime_checkable
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_enum ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def testlargenodes ( self ) : # large nodes are greedily split from the original proto if they are # larger than max_size / 3. sizes = [ 50 , 95 , 95 , 95 , 50 , 95 ] max_size = 200 constants.debug_set_max_size ( max_size )
__label__0 self.assertequal ( `` const/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float32 , slot.dtype.base_dtype ) self.assertallequal ( [ 2.0 , 5.0 ] , self.evaluate ( slot ) )
__label__0 self.assertequal ( ( ) , function_utils.fn_args ( foo ( ) .bar ) )
__label__0 * multiple python dictionaries :
__label__0 args : parent : parent of node . node : ast.call node to maybe modify . full_name : full name of function to modify name : name of function to modify logs : list of logs to append to arg_name : name of the argument to look for arg_ok_predicate : predicate callable with the ast of the argument value , returns whether the argument value is allowed . remove_if_ok : remove the argument if present and ok as determined by arg_ok_predicate . message : message to print if a non-ok arg is found ( and hence , the function is renamed to its compat.v1 version ) .
__label__0 graph_def = self._make_graph_def_with_constant_nodes ( sizes , fn=fn1 ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , chunked_message = s.split ( )
__label__0 returns : list of tensors. `` '' '' del value return [ ]
__label__0 def test_global_step_read_is_none_if_there_is_no_global_step ( self ) : with ops.graph ( ) .as_default ( ) : self.assertisnone ( training_util._get_or_create_global_step_read ( ) ) training_util.create_global_step ( ) self.assertisnotnone ( training_util._get_or_create_global_step_read ( ) )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a function that tells you if the program is running in graph mode . '' '' '' # call is_in_graph_mode ( ) when you want to know whether the thread is in # graph mode . by default , we always are . is_in_graph_mode = lambda : true
__label__0 see the [ ` tf.train.example ` ] ( https : //www.tensorflow.org/tutorials/load_data/tfrecord # tftrainexample ) guide for usage details. `` '' ''
__label__0 args : byte_count : byte count that defaults to _max_int .
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 def uses_star_args_in_call ( node ) : `` '' '' check if an ast.call node uses arbitrary-length positional * args .
__label__0 graph_def = self._make_graph_def_with_constant_nodes ( sizes , fn1=fn1 , fn2=fn2 , fn3=fn3 , fn4=fn4 ) s = split_graph_def.graphdefsplitter ( self._copy_graph ( graph_def ) ) chunks , _ = s.split ( )
__label__0 > > > mt3 = tf.nest.pack_sequence_as ( mt , leaves ) > > > mt3 maskedtensor ( mask=true , value= < tf.tensor : ... numpy=array ( [ 1 ] , dtype=int32 ) > ) > > > bool ( mt == mt3 ) true
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 4 ] ) ) nmt2 = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 5 ] ) ) nmt_out = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=false , inner_value=constant_op.constant ( [ 6 ] ) ) nmt_combined_with_path = nest.map_structure_with_tuple_paths_up_to ( nmt_out , tuple_path_sum , nmt , nmt2 ) self.assertisinstance ( nmt_combined_with_path , nestedmaskedtensor ) self.assertequal ( nmt_combined_with_path.mask , false ) self.assertequal ( nmt_combined_with_path.value.mask , true ) self.assertallequal ( nmt_combined_with_path.value.value [ 0 ] , ( 0 , 0 ) ) self.assertallequal ( nmt_combined_with_path.value.value [ 1 ] , [ 9 ] )
__label__0 class tfdecoratorunwraptest ( test.testcase ) :
__label__0 # check if we have a scale or scope keyword arg scope_keyword = none for keyword in node.keywords : if keyword.arg == `` scale '' : keyword.arg = `` l '' _replace_scale_node ( keyword , keyword.value ) if keyword.arg == `` scope '' : scope_keyword = keyword
__label__0 `` ` python def train ( ) : sv = tf.compat.v1.train.supervisor ( ... ) with sv.managed_session ( < master > ) as sess : for step in range ( .. ) : if sv.should_stop ( ) : break sess.run ( < my training op > ) ... do other things needed at each training step ... `` `
__label__0 import wrapt as _wrapt
__label__0 def testweightedmoments ( self ) : text = `` tf.nn.weighted_moments ( x , axes , freq , name , kd ) '' expected_text = ( `` tf.nn.weighted_moments ( x , axes , freq , name=name , keepdims=kd ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 @ property @ abc.abstractmethod def function_type ( self ) - > functiontype : `` '' '' returns a functiontype describing this callable . '' '' ''
__label__0 returns : a tuple ( is_successful , msg ) , where is_successful is true if _local_init_op is none , or we ran _local_init_op , and false otherwise ; and msg is a ` string ` with the reason why the model was not ready to run local init. `` '' '' if self._local_init_op is not none : is_ready_for_local_init , msg = self._model_ready_for_local_init ( sess ) if is_ready_for_local_init : logging.info ( `` running local_init_op . '' ) sess.run ( self._local_init_op , feed_dict=self._local_init_feed_dict , options=self._local_init_run_options ) logging.info ( `` done running local_init_op . '' ) return true , none else : return false , msg return true , none
__label__0 if files_missing_load : raise runtimeerror ( 'the following files are missing % s : \n % s ' % ( 'load ( `` //tensorflow : tensorflow.bzl '' , `` py_test '' ) .\nthis load statement ' ' is needed because otherwise pip tests will try to use their ' 'dependencies , which are not visible to them . ' , '\n'.join ( files_missing_load ) ) ) else : print ( 'test passed . ' )
__label__0 this is intended to be overridden by subclasses that want to generate different ops .
__label__0 import numpy as np
__label__0 ev = next ( rr ) self.assertequal ( event_pb2.sessionlog.stop , ev.session_log.status )
__label__0 return node
__label__0 # pass name w/ positional arg b = math_ops.add ( x , y , `` b '' ) if not context.executing_eagerly ( ) : # names not defined in eager mode . self.assertregex ( b.values.name , r '' ^b/add . * '' ) self.assertregex ( b.mask.name , r '' ^b/and . * '' )
__label__0 `` ` python lock = grouplock ( num_groups=2 )
__label__0 for name in names_v1 : tf_name = `` tf. % s '' % name if tf_name in function_warnings or tf_name in function_transformers : continue # these require manual change if tf_name in v1_name_exceptions : continue # assert that arg names after converting to v2 are present in # v2 function . # 1. first , create an input of the form : # tf.foo ( arg1=val1 , arg2=val2 , ... ) args = `` , '' .join ( [ `` % s= % d '' % ( from_name , from_index ) for from_index , from_name in enumerate ( arg_names_v1 ) ] ) text_input = `` % s ( % s ) '' % ( tf_name , args ) # 2. convert the input to v2 . _ , _ , _ , text = self._upgrade ( text_input ) new_function_name , new_args = get_func_and_args_from_str ( text ) if `` __internal__ '' in new_function_name : # skip the tf.__internal__ and tf.keras.__internal__ api . continue if new_function_name == `` tf.compat.v1. % s '' % name : if tf_name in keyword_renames : # if we rename arguments , new function must be available in 2.0 . # we should not be using compat.v1 in this case . self.fail ( `` function ' % s ' is not in 2.0 when converting\n % s\nto\n % s '' % ( new_function_name , text_input , text ) ) continue if new_function_name.startswith ( `` tf.compat.v2 '' ) : self.assertin ( new_function_name.replace ( `` tf.compat.v2 . `` , `` tf . `` ) , self.v2_symbols ) continue # 3. verify v2 function and arguments . args_v2 = get_args ( self.v2_symbols [ new_function_name ] ) args_v2.extend ( v2_arg_exceptions ) for new_arg in new_args : self.assertin ( new_arg , args_v2 , `` invalid argument ' % s ' in 2.0 when converting\n % s\nto\n % s.\n '' `` supported arguments : % s '' % ( new_arg , text_input , text , str ( args_v2 ) ) ) # 4. verify that the argument exists in v1 as well . if new_function_name in set ( [ `` tf.nn.ctc_loss '' , `` tf.saved_model.save '' ] ) : continue args_v1 = get_args ( self.v1_symbols [ new_function_name ] ) args_v1.extend ( v2_arg_exceptions ) for new_arg in new_args : self.assertin ( new_arg , args_v1 , `` invalid argument ' % s ' in 1.0 when converting\n % s\nto\n % s.\n '' `` supported arguments : % s '' % ( new_arg , text_input , text , str ( args_v1 ) ) )
__label__0 args : maybe_tf_decorator : any callable object .
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def testtensorflowgoogleimport ( self ) : text = `` import tensorflow.google as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 # tf.raw_ops.add also takes tf.bfloat16 , tf.half , tf.float32 , tf.float64 , # tf.uint8 , tf.int8 , tf.int16 , tf.int32 , tf.int64 , tf.complex64 , # tf.complex128 , but get_random_numeric_tensor only generates tf.float16 , # tf.float32 , tf.float64 , tf.int32 , tf.int64 input_tensor_x = fh.get_random_numeric_tensor ( ) input_tensor_y = fh.get_random_numeric_tensor ( )
__label__0 the type of the slot is determined by the given value .
__label__0 def test_reads_from_cache ( self ) : with ops.graph ( ) .as_default ( ) : training_util.create_global_step ( ) first = training_util._get_or_create_global_step_read ( ) second = training_util._get_or_create_global_step_read ( ) self.assertequal ( first , second )
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 parser.add_argument ( `` -- raw_generate '' , type=str , help= '' generate version_info.cc ( simpler version used for cmake/make ) '' )
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : # initialize with zeros . var = variable_scope.get_variable ( `` v1 '' , shape= [ 10 , 1 ] , initializer=zeros ( ) ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= [ `` v1 '' ] ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started ( init overridden to ones ) . self.assertallequal ( var , prev_int_val )
__label__0 def testrunscodebeforeyield ( self ) : x = [ ] with test_yield_append_before_and_after_yield ( x , 'before ' , `` ) : self.assertequal ( 'before ' , x [ -1 ] )
__label__0 * ` symbols_to_detect ` : maps function names to ` analysisresult ` s * ` imports_to_detect ` : maps imports represented as ( full module name , alias ) tuples to ` analysisresult ` s notifications )
__label__0 class cudadeviceproperties ( ct.structure ) : # see $ cuda_home/include/cuda_runtime_api.h for the definition of # the cudadeviceprop struct . _fields_ = [ ( `` name '' , ct.c_char * 256 ) , ( `` totalglobalmem '' , ct.c_size_t ) , ( `` sharedmemperblock '' , ct.c_size_t ) , ( `` regsperblock '' , ct.c_int ) , ( `` warpsize '' , ct.c_int ) , ( `` mempitch '' , ct.c_size_t ) , ( `` maxthreadsperblock '' , ct.c_int ) , ( `` maxthreadsdim '' , ct.c_int * 3 ) , ( `` maxgridsize '' , ct.c_int * 3 ) , ( `` clockrate '' , ct.c_int ) , ( `` totalconstmem '' , ct.c_size_t ) , ( `` major '' , ct.c_int ) , ( `` minor '' , ct.c_int ) , ( `` texturealignment '' , ct.c_size_t ) , ( `` texturepitchalignment '' , ct.c_size_t ) , ( `` deviceoverlap '' , ct.c_int ) , ( `` multiprocessorcount '' , ct.c_int ) , ( `` kernelexectimeoutenabled '' , ct.c_int ) , ( `` integrated '' , ct.c_int ) , ( `` canmaphostmemory '' , ct.c_int ) , ( `` computemode '' , ct.c_int ) , ( `` maxtexture1d '' , ct.c_int ) , ( `` maxtexture1dmipmap '' , ct.c_int ) , ( `` maxtexture1dlinear '' , ct.c_int ) , ( `` maxtexture2d '' , ct.c_int * 2 ) , ( `` maxtexture2dmipmap '' , ct.c_int * 2 ) , ( `` maxtexture2dlinear '' , ct.c_int * 3 ) , ( `` maxtexture2dgather '' , ct.c_int * 2 ) , ( `` maxtexture3d '' , ct.c_int * 3 ) , ( `` maxtexture3dalt '' , ct.c_int * 3 ) , ( `` maxtexturecubemap '' , ct.c_int ) , ( `` maxtexture1dlayered '' , ct.c_int * 2 ) , ( `` maxtexture2dlayered '' , ct.c_int * 3 ) , ( `` maxtexturecubemaplayered '' , ct.c_int * 2 ) , ( `` maxsurface1d '' , ct.c_int ) , ( `` maxsurface2d '' , ct.c_int * 2 ) , ( `` maxsurface3d '' , ct.c_int * 3 ) , ( `` maxsurface1dlayered '' , ct.c_int * 2 ) , ( `` maxsurface2dlayered '' , ct.c_int * 3 ) , ( `` maxsurfacecubemap '' , ct.c_int ) , ( `` maxsurfacecubemaplayered '' , ct.c_int * 2 ) , ( `` surfacealignment '' , ct.c_size_t ) , ( `` concurrentkernels '' , ct.c_int ) , ( `` eccenabled '' , ct.c_int ) , ( `` pcibusid '' , ct.c_int ) , ( `` pcideviceid '' , ct.c_int ) , ( `` pcidomainid '' , ct.c_int ) , ( `` tccdriver '' , ct.c_int ) , ( `` asyncenginecount '' , ct.c_int ) , ( `` unifiedaddressing '' , ct.c_int ) , ( `` memoryclockrate '' , ct.c_int ) , ( `` memorybuswidth '' , ct.c_int ) , ( `` l2cachesize '' , ct.c_int ) , ( `` maxthreadspermultiprocessor '' , ct.c_int ) , ( `` streamprioritiessupported '' , ct.c_int ) , ( `` globall1cachesupported '' , ct.c_int ) , ( `` locall1cachesupported '' , ct.c_int ) , ( `` sharedmempermultiprocessor '' , ct.c_size_t ) , ( `` regspermultiprocessor '' , ct.c_int ) , ( `` managedmemsupported '' , ct.c_int ) , ( `` ismultigpuboard '' , ct.c_int ) , ( `` multigpuboardgroupid '' , ct.c_int ) , # pad with extra space to avoid dereference crashes if future # versions of cuda extend the size of this struct . ( `` __future_buffer '' , ct.c_char * 4096 ) ]
__label__0 dispatch_target.__name__ = `` elementwise_dispatch_target_for_ '' + api.__name__ dispatch_target.__qualname__ = dispatch_target.__name__ # keep track of what targets we 've registered ( so we can unregister them ) . target_list = _elementwise_api_targets.setdefault ( ( x_type , ) , [ ] ) target_list.append ( ( api , dispatch_target ) )
__label__0 # caution : the google and oss versions of this import are different . import base_dir # pylint : disable=g-import-not-at-top
__label__0 raises : valueerror : if attributes are inconsistent. `` '' '' if parse_example_op.type == `` parseexample '' : return _extract_from_parse_example ( parse_example_op , sess ) elif parse_example_op.type == `` parseexamplev2 '' : return _extract_from_parse_example_v2 ( parse_example_op , sess ) else : raise valueerror ( `` found unexpected type when parsing example . expected ` parseexample ` `` f '' object . received type : { parse_example_op.type } '' )
__label__0 def write_version_info ( filename , git_version ) : `` '' '' write a c file that defines the version functions .
__label__0 def testremovedeprecatedkeywordandreorder ( self ) : `` '' '' test for when a keyword alias is removed and args are reordered . '' '' '' text = `` g ( a , b , kw1=x , c=c ) \n '' acceptable_outputs = [ `` g ( a , b , c=c , kw1=x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliasandreorderrest ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 code_url_prefixes = ( keras_url_prefix , # none - > do n't link to the generated keras api-module files . none , none , f '' https : //github.com/tensorflow/tensorboard/tree/ { tensorboard.__version__ } /tensorboard '' , `` https : //github.com/tensorflow/estimator/tree/master/tensorflow_estimator '' , code_url_prefix , ) elif version.parse ( tf.__version__ ) > = version.parse ( `` 2.9 '' ) : base_dirs = [ base_dir , pathlib.path ( keras.__file__ ) .parent , pathlib.path ( tensorboard.__file__ ) .parent , pathlib.path ( tensorflow_estimator.__file__ ) .parent , ] code_url_prefixes = ( code_url_prefix , keras_url_prefix , f '' https : //github.com/tensorflow/tensorboard/tree/ { tensorboard.__version__ } /tensorboard '' , `` https : //github.com/tensorflow/estimator/tree/master/tensorflow_estimator '' , ) else : raise valueerror ( `` unsupported : version < 2.9 '' )
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated arguments ) '' `` \n '' `` \ndeprecated : some arguments are deprecated : ` ( deprecated ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 def task_indices ( self , job_name ) : `` '' '' returns a list of valid task indices in the given job .
__label__0 returns : train_op : the op to dequeue a token so the replicas can exit this batch and start the next one . this is executed by each replica .
__label__0 def _new__init__ ( self , wrapped_value , tf_should_use_helper ) : # pylint : disable=protected-access self._tf_should_use_helper = tf_should_use_helper self._tf_should_use_wrapped_value = wrapped_value
__label__0 overloadable_operators = { '__add__ ' , '__radd__ ' , '__sub__ ' , '__rsub__ ' , '__mul__ ' , '__rmul__ ' , '__div__ ' , '__rdiv__ ' , '__truediv__ ' , '__rtruediv__ ' , '__floordiv__ ' , '__rfloordiv__ ' , '__mod__ ' , '__rmod__ ' , '__lt__ ' , '__le__ ' , '__gt__ ' , '__ge__ ' , '__ne__ ' , '__eq__ ' , '__and__ ' , '__rand__ ' , '__or__ ' , '__ror__ ' , '__xor__ ' , '__rxor__ ' , '__getitem__ ' , '__pow__ ' , '__rpow__ ' , '__invert__ ' , '__neg__ ' , '__abs__ ' , '__matmul__ ' , '__rmatmul__ ' , }
__label__0 def __call__ ( self , a , b=1 , c='hello ' ) : pass
__label__0 vanilla tf ca n't run ssd-resnet34 on cpu because it does n't support nchw format. `` '' ''
__label__0 tf_decorator.make_decorator ( simple_parametrized_wrapper , wrapped_fn )
__label__0 raises : assertionerror : if last_checkpoints is not a list. `` '' '' assert isinstance ( last_checkpoints , list ) # we use a timestamp of +inf so that this checkpoint will never be # deleted . this is both safe and backwards compatible to a previous # version of the code which used s [ 1 ] as the `` timestamp '' . self._last_checkpoints = [ ( s , np.inf ) for s in last_checkpoints ]
__label__0 @ dispatch.dispatch_for_api ( math_ops.tensor_equals , signature ) def masked_tensor_equals ( self , other ) : del self , other
__label__0 def testwarmstartembeddingcolumnlinearmodel ( self ) : # create old and new vocabs for embedding column `` sc_vocab '' . prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' )
__label__0 def testflattenupto ( self ) : # shallow tree ends at scalar . input_tree = [ [ [ 2 , 2 ] , [ 3 , 3 ] ] , [ [ 4 , 9 ] , [ 5 , 5 ] ] ] shallow_tree = [ [ true , true ] , [ false , true ] ] flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 9 ] , [ 5 , 5 ] ] ) self.assertequal ( flattened_shallow_tree , [ true , true , false , true ] )
__label__0 from tensorflow.python.util import tf_decorator
__label__0 # build another graph with 2 nodes , initialized # differently , and a restore node for them . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0_2 = variable_v1.variablev1 ( 1000.0 , name= '' v0 '' ) v1_2 = variable_v1.variablev1 ( 2000.0 , name= '' v1 '' ) v2_2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) save2 = saver_module.saver ( [ v0_2 , v1_2 , v2_2.saveable ] ) v2_2.insert ( `` k1000 '' , 3000.0 ) .run ( ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def testmultipledecorators ( self ) :
__label__1 class summaryranges :
__label__0 # adding s1 ( s3 should not be deleted because helper is unaware of it ) s1 = save3.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s2 , s1 ] , save3.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s2 , s1 ] , save_dir=save_dir )
__label__0 savers can automatically number checkpoint filenames with a provided counter . this lets you keep multiple checkpoints at different steps while training a model . for example you can number the checkpoint filenames with the training step number . to avoid filling up disks , savers manage checkpoint files automatically . for example , they can keep only the n most recent files , or one checkpoint for every n hours of training .
__label__0 class _object ( object ) :
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import test as test_lib from tensorflow.tools.compatibility import ast_edits
__label__0 @ tf_export ( v1= [ `` train.sessionrunvalues '' ] ) class sessionrunvalues ( collections.namedtuple ( `` sessionrunvalues '' , [ `` results '' , `` options '' , `` run_metadata '' ] ) ) : `` '' '' contains the results of ` session.run ( ) ` .
__label__0 returns : tuple of ( field in the proto or ` none ` if none are found , field descriptor ) `` '' '' field_proto = proto field_desc = none for field_proto , field_desc , _ , _ in _walk_fields ( proto , fields ) : pass return field_proto , field_desc
__label__0 def testunwrapreturnsundecoratedfunctionastarget ( self ) : _ , target = tf_decorator.unwrap ( test_function ) self.assertis ( test_function , target )
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) ws_util._warm_start_var_with_vocab ( fruit_weights , new_vocab_path , 5 , self.get_temp_dir ( ) , prev_vocab_path ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( [ [ 2 . ] , [ 1.5 ] , [ 1 . ] , [ 0.5 ] , [ 0 . ] ] , fruit_weights.eval ( sess ) )
__label__0 # all types supported by tf.random.uniform _tf_random_dtypes = [ tf.float16 , tf.float32 , tf.float64 , tf.int32 , tf.int64 ]
__label__0 return _get_wrapper ( x , tf_should_use_helper )
__label__0 def _tf_core_flatten_up_to ( shallow_tree , input_tree , check_types=true , expand_composites=false ) : is_nested_fn = ( _is_nested_or_composite if expand_composites else _tf_core_is_nested ) _tf_core_assert_shallow_structure ( shallow_tree , input_tree , check_types=check_types , expand_composites=expand_composites , ) # discard paths returned by nest_util._tf_core_yield_flat_up_to . return [ v for _ , v in _tf_core_yield_flat_up_to ( shallow_tree , input_tree , is_nested_fn ) ]
__label__0 # names we already checked for deprecation self._tfmw_deprecated_checked = set ( ) self._tfmw_warning_count = 0
__label__0 # make sure dicts are correctly flattened , yielding values , not keys . input_tree = { `` a '' : 1 , `` b '' : { `` c '' : 2 } , `` d '' : [ 3 , ( 4 , 5 ) ] } shallow_tree = { `` a '' : 0 , `` b '' : 0 , `` d '' : [ 0 , 0 ] } ( input_tree_flattened_as_shallow_tree_paths , input_tree_flattened_as_shallow_tree ) = get_paths_and_values ( shallow_tree , input_tree ) self.assertequal ( input_tree_flattened_as_shallow_tree_paths , [ ( `` a '' , ) , ( `` b '' , ) , ( `` d '' , 0 ) , ( `` d '' , 1 ) ] ) self.assertequal ( input_tree_flattened_as_shallow_tree , [ 1 , { `` c '' : 2 } , 3 , ( 4 , 5 ) ] )
__label__0 def testcannotsetdeprecatedendpointstwice ( self ) : with self.assertraises ( deprecation.deprecatednamesalreadyseterror ) : @ deprecation.deprecated_endpoints ( `` foo1 '' ) @ deprecation.deprecated_endpoints ( `` foo2 '' ) def foo ( ) : # pylint : disable=unused-variable pass
__label__0 `` `` '' utility methods for handling nests .
__label__0 class testupgradefiles ( test_util.tensorflowtestcase ) :
__label__0 # todo ( mdan ) : move these to c++ as well . # moving to c++ can further avoid extra copies made by get_effective_map . _source_mapper_stacks = collections.defaultdict ( lambda : [ sentinelmapper ( ) ] ) _source_filter_stacks = collections.defaultdict ( lambda : [ sentinelfilter ( ) ] )
__label__0 ` tf.distribute.distributeddataset ` generates ` tf.distribute.distributedvalues ` as input to the devices . if you pass the input to a ` tf.function ` and would like to specify the shape and type of each tensor argument to the function , you can pass a ` tf.typespec ` object to the ` input_signature ` argument of the ` tf.function ` . to get the ` tf.typespec ` of the input , you can use the ` element_spec ` property of the ` tf.distribute.distributeddataset ` or ` tf.distribute.distributediterator ` object .
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.range ( 100 ) .batch ( 2 ) > > > dist_dataset = strategy.experimental_distribute_dataset ( dataset ) > > > dist_dataset_iterator = iter ( dist_dataset ) > > > @ tf.function ... def one_step ( input ) : ... return input > > > step_num = 5 > > > for _ in range ( step_num ) : ... strategy.run ( one_step , args= ( dist_dataset_iterator.get_next ( ) , ) ) > > > strategy.experimental_local_results ( dist_dataset_iterator.get_next ( ) ) ( < tf.tensor : shape= ( 1 , ) , dtype=int64 , numpy=array ( [ 10 ] ) > , < tf.tensor : shape= ( 1 , ) , dtype=int64 , numpy=array ( [ 11 ] ) > )
__label__0 raises : runtimeerror : if python version is not at least 3.7. `` '' '' if sys.version_info.major ! = 3 or sys.version_info.minor < 7 : raise runtimeerror ( f'traceback filtering is only available with python 3.7 or higher . ' f'this python version : { sys.version } ' ) global _enable_traceback_filtering _enable_traceback_filtering.value = true
__label__0 @ parameterized.parameters ( [ ' 1.0 , ... , 1.0 ' , ' 1.0 , 1.0 , 1.0 ' ] , [ ' 1.0 , 1.0 ... , 1.0 ' , ' 1.0 , 1.002 , 1.0 ' ] ) def test_wrong_float_counts ( self , want , got ) : output_checker = tf_doctest_lib.tfdoctestoutputchecker ( )
__label__0 example usage :
__label__0 # the function follows the original flow from ` upgrader.process_fil ` with tempfile.namedtemporaryfile ( `` w '' , delete=false ) as temp_file :
__label__0 see the [ ` tf.train.example ` ] ( https : //www.tensorflow.org/tutorials/load_data/tfrecord # tftrainexample ) guide for usage details. `` '' ''
__label__0 def __init__ ( self ) : super ( ) .__init__ ( ) filter_filename = none outer_f = none f = inspect.currentframe ( ) try : if f is not none : # the current frame is __init__ . the first outer frame should be the # caller . outer_f = f.f_back if outer_f is not none : filter_filename = inspect.getsourcefile ( outer_f ) self._filename = filter_filename # this may be called repeatedly : once on entry by the superclass , then by # each child context manager . self._cached_set = none finally : # avoid reference cycles , see : # https : //docs.python.org/3.7/library/inspect.html # the-interpreter-stack del f del outer_f
__label__0 def _assert_global_step ( self , global_step , expected_dtype=dtypes.int64 ) : self.assertequal ( ' % s:0 ' % ops.graphkeys.global_step , global_step.name ) self.assertequal ( expected_dtype , global_step.dtype.base_dtype ) self.assertequal ( [ ] , global_step.get_shape ( ) .as_list ( ) )
__label__0 flags.define_string ( `` site_path '' , `` '' , `` the path prefix ( up to ` ... /api_docs/python ` ) used in the `` `` ` _toc.yaml ` and ` _redirects.yaml ` files '' )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for operator dispatch . '' '' ''
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 with self.assertraiseswithpredicatematch ( errors_impl.operror , lambda e : `` uninitialized value v0 '' in e.message ) : self.evaluate ( v0 ) with self.assertraiseswithpredicatematch ( errors_impl.operror , lambda e : `` uninitialized value v1 '' in e.message ) : self.evaluate ( v1 ) self.assertequal ( 0 , len ( self.evaluate ( v2.keys ( ) ) ) ) self.assertequal ( 0 , len ( self.evaluate ( v2.values ( ) ) ) )
__label__0 text = ( `` tf.nn.conv2d ( input , filter=filter , strides=strides , padding=padding , `` `` use_cudnn_on_gpu=use_cudnn_on_gpu ) '' ) expected_text = ( `` tf.nn.conv2d ( input , filters=filter , strides=strides , `` `` padding=padding ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # print transition details . print ( `` major : % s - > % s '' % ( old_version.major , new_version.major ) ) print ( `` minor : % s - > % s '' % ( old_version.minor , new_version.minor ) ) print ( `` patch : % s - > % s\n '' % ( old_version.patch , new_version.patch ) )
__label__0 goodbye `` '' '' ) , ( 'skip_all ' , [ ] , `` '' '' < ! -- doctest : skip-all -- >
__label__0 returns : result of repeatedly applying ` func ` , with the same structure layout as ` shallow_tree ` . `` '' '' return nest_util.map_structure_up_to ( nest_util.modality.core , shallow_tree , lambda _ , * values : func ( * values ) , # discards the path arg . * inputs , * * kwargs , )
__label__0 from tensorflow.python.platform import test from tensorflow.tools.proto_splitter import chunk_pb2 from tensorflow.tools.proto_splitter import split from tensorflow.tools.proto_splitter.testdata import test_message_pb2
__label__0 def testupdatesdict_doesnotoverridepresententries ( self ) : test_function.foobar = true test_wrapper.foobar = false decorated = tf_decorator.make_decorator ( test_function , test_wrapper ) self.assertfalse ( decorated.foobar ) del test_function.foobar del test_wrapper.foobar
__label__0 if ` root ` is not a module or class , ` visit ` is never called . ` traverse ` never descends into built-in modules .
__label__0 import doctest import re import textwrap
__label__0 @ tf_export ( `` test_op '' ) @ dispatch.add_dispatch_support def test_op ( x , y , z ) : `` '' '' a fake op for testing dispatch of python ops . '' '' '' return x + ( 2 * y ) + ( 3 * z )
__label__0 # using non-iterable elements . input_tree = 0 shallow_tree = [ 9 ] with self.assertraiseswithliteralmatch ( typeerror , nest.if_shallow_is_seq_input_must_be_seq.format ( type ( input_tree ) ) , ) : ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree_paths , [ ( 0 , ) ] ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 def test_invalid_dtype ( self ) : with ops.graph ( ) .as_default ( ) as g : self.assertisnone ( training_util.get_global_step ( ) ) variable_v1.variablev1 ( 0.0 , trainable=false , dtype=dtypes.float32 , name=ops.graphkeys.global_step , collections= [ ops.graphkeys.global_step ] ) self.assertraisesregex ( typeerror , 'does not have integer type ' , training_util.get_global_step ) self.assertraisesregex ( typeerror , 'does not have integer type ' , training_util.get_global_step , g )
__label__0 class saverestorewithvariablenamemap ( test.testcase ) :
__label__0 def __init__ ( self , * args , * * kwargs ) : super ( ) .__init__ ( * args , * * kwargs ) self._fields = [ 1 , 2 , 3 ] # not str , as expected for a namedtuple .
__label__0 s2 = save.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s1 , s2 ] , save.last_checkpoints )
__label__0 example_tuples = [ ] for example in parser.get_examples ( string , name=self._testmethodname ) : source = example.source.rstrip ( '\n ' ) want = example.want if want is not none : want = want.rstrip ( '\n ' ) example_tuples.append ( ( source , want ) )
__label__0 def _testtypesforadam ( self , var , m , v , grad , use_gpu ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var_t = variable_v1.variablev1 ( var ) m_t = variable_v1.variablev1 ( m ) v_t = variable_v1.variablev1 ( v )
__label__0 @ dispatch.add_dispatch_support def foo ( x , * , y ) : return x + y
__label__0 this method is used by other members of the training module , such as ` scaffold ` , or ` checkpointsaverhook ` .
__label__0 # we have initial tokens in the queue so we can call this one by one . after # the token queue becomes empty , they should be called concurrently . # here worker 0 and worker 2 finished first . sessions [ 0 ] .run ( train_ops [ 0 ] ) sessions [ 2 ] .run ( train_ops [ 2 ] )
__label__0 @ tf_export ( `` experimental.dispatch_for_api '' ) def dispatch_for_api ( api , * signatures ) : `` '' '' decorator that overrides the default implementation for a tensorflow api .
__label__0 # flatten each input separately , apply the function to corresponding elements , # then repack based on the structure of the first input . all_flattened_up_to = ( _tf_data_flatten_up_to ( shallow_tree , input_tree ) for input_tree in inputs )
__label__0 `` ` python cluster = tf.train.clusterspec ( { `` worker '' : { 1 : `` worker1.example.com:2222 '' } , `` ps '' : [ `` ps0.example.com:2222 '' , `` ps1.example.com:2222 '' ] } ) `` ` `` '' ''
__label__0 def visit_import ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting an import node in the ast .
__label__0 class configerror ( exception ) : pass
__label__0 def _iterator_transformer ( parent , node , full_name , name , logs ) : `` '' '' transform iterator methods to compat function calls . '' '' '' # first , check that node.func.value is not already something we like # ( tf.compat.v1.data ) , or something which is handled in the rename # ( tf.data ) . this transformer only handles the method call to function call # conversion . if full_name and ( full_name.startswith ( `` tf.compat.v1.data '' ) or full_name.startswith ( `` tf.data '' ) ) : return
__label__0 class generate2test ( googletest.testcase ) :
__label__0 # however , if we restore the checkpoint under scope `` my_scope '' , # import_meta_graph will detect the variable and return a saver for # restoring it . this should happen even when the variable does not # originate from graph_1 . new_saver_3 = saver_module.import_meta_graph ( filename + `` .meta '' , graph=graph_2 , import_scope= '' my_scope '' ) self.assertisinstance ( new_saver_3 , saver_module.saver )
__label__0 from absl.testing import parameterized import tensorflow.compat.v1 as tf # oss tf v2 import placeholder .
__label__0 @ tf_export ( v1= [ 'train.get_global_step ' ] ) def get_global_step ( graph=none ) : `` '' '' get the global step tensor .
__label__0 with self.assertraisesregex ( valueerror , self.unsafe_map_pattern ) : nest.pack_sequence_as ( `` scalar '' , [ 4 , 5 ] )
__label__0 > > > s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > s1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] > > > tf.nest.map_structure ( lambda x , y : none , s1 , s1_list , check_types=false ) ( ( ( none , none ) , none ) , none , ( none , none ) )
__label__0 input_tree = [ ( 1 , ) , ( 2 , ) , 3 ] shallow_tree = [ ( 1 , ) , ( 2 , ) ] expected_message = nest.structures_have_mismatching_lengths.format ( input_length=len ( input_tree ) , shallow_length=len ( shallow_tree ) ) with self.assertraisesregex ( valueerror , expected_message ) : # pylint : disable=g-error-prone-assert-raises nest.assert_shallow_structure ( shallow_tree , input_tree )
__label__0 * empty structures :
__label__0 visitor = public_api.publicapivisitor ( symbol_collector_v1 ) visitor.private_map [ `` tf.compat '' ] = [ `` v1 '' , `` v2 '' ] traverse.traverse ( tf.compat.v1 , visitor )
__label__0 > > > tensor = tf.constant ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] ) > > > tf.nest.flatten ( tensor ) [ < tf.tensor : shape= ( 3 , 3 ) , dtype=float32 , numpy= array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] , dtype=float32 ) > ]
__label__0 def __init__ ( self ) : pass
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' upgrader for python scripts from 1.x tensorflow to 2.0 tensorflow . '' '' ''
__label__0 # make a new class with __init__ wrapped in a warning . class _newclass ( func_or_class ) : # pylint : disable=missing-docstring __doc__ = decorator_utils.add_notice_to_docstring ( func_or_class.__doc__ , 'please use % s instead . ' % name , 'deprecated class ' , ' ( deprecated ) ' , [ ( 'this class is deprecated . ' 'it will be removed in a future version . ' ) ] , notice_type='deprecated ' ) __name__ = func_or_class.__name__ __module__ = _call_location ( outer=true )
__label__0 def testgettraverseshallowstructure ( self ) : scalar_traverse_input = [ 3 , 4 , ( 1 , 2 , [ 0 ] ) , [ 5 , 6 ] , { `` a '' : ( 7 , ) } , [ ] ] scalar_traverse_r = nest.get_traverse_shallow_structure ( lambda s : not isinstance ( s , tuple ) , scalar_traverse_input ) self.assertequal ( scalar_traverse_r , [ true , true , false , [ true , true ] , { `` a '' : false } , [ ] ] ) nest.assert_shallow_structure ( scalar_traverse_r , scalar_traverse_input )
__label__0 where ` api_func ` is a function that takes a single parameter and performs the elementwise operation ( e.g. , ` tf.abs ` ) , and ` x ` is the first argument to the elementwise api .
__label__0 _contrib_warning = ( ast_edits.error , `` < function name > can not be converted automatically . tf.contrib will not '' `` be distributed with tensorflow 2.0 , please consider an alternative in '' `` non-contrib tensorflow , a community-maintained repository such as `` `` tensorflow/addons , or fork the required code . '' )
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import lazy_loader from tensorflow.python.util import tf_inspect
__label__0 @ deprecation.deprecated_args ( date , instructions , `` arg '' , warn_once=true ) def _fn ( arg=0 ) : # pylint : disable=unused-argument pass
__label__0 # sort the build info to ensure deterministic output . sorted_build_info_pairs = sorted ( build_info.items ( ) )
__label__0 class haskwargstest ( test.testcase ) :
__label__0 def testcontribl2 ( self ) : text = `` tf.contrib.layers.l2_regularizer ( scale ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l2 ( 0.5 * ( scale ) ) \n '' , ) self.assertnotin ( `` dropping scope '' , unused_report )
__label__0 references : coursera slide 29 : hinton , 2012 ( [ pdf ] ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) )
__label__0 def getsourcefile ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getsourcefile . '' '' '' return _inspect.getsourcefile ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 if the function definition allows only one specialization , ` args ` and ` kwargs ` may be omitted altogether .
__label__0 # create feature columns . sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=new_vocab_path , vocabulary_size=6 ) emb_vocab = fc.embedding_column ( categorical_column=sc_vocab , dimension=2 ) all_deep_cols = [ emb_vocab ] # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = { } with variable_scope.variable_scope ( `` '' , partitioner=_partitioner ) : # create the variables . fc.linear_model ( features=self._create_dummy_inputs ( ) , feature_columns=all_deep_cols , cols_to_vars=cols_to_vars )
__label__0 def some_op ( x , y ) : return x + y
__label__0 spec = next ( ( d.decorator_argspec for d in decorators if d.decorator_argspec is not none ) , none ) if spec : return spec
__label__0 if decorator_argspec : self.__signature__ = fullargspec_to_signature ( decorator_argspec ) elif callable ( target ) : try : self.__signature__ = inspect.signature ( target ) except ( typeerror , valueerror ) : # certain callables such as builtins can not be inspected for signature . pass
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import ops from tensorflow.python.platform import tf_logging from tensorflow.python.util import tf_decorator
__label__0 return `` \n '' .join ( parts )
__label__0 > > > tf.nest.assert_same_structure ( 1.5 , tf.variable ( 1 , tf.uint32 ) ) > > > tf.nest.assert_same_structure ( `` abc '' , np.array ( [ 1 , 2 ] ) )
__label__0 if not _tf_data_is_nested ( structure ) : if len ( flat_sequence ) ! = 1 : raise valueerror ( `` argument ` structure ` is a scalar but `` f '' ` len ( flat_sequence ) ` = { len ( flat_sequence ) } > 1 '' ) return flat_sequence [ 0 ]
__label__0 self.assertequal ( `` var/part_ % d/slot '' % i , slot.op.name ) self.assertequal ( [ ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float32 , slot.dtype.base_dtype ) self.assertallequal ( 1.0 , slot )
__label__0 returns : if the checkpoint is object-based , this function returns a map from variable names to their corresponding checkpoint keys . if the checkpoint is name-based , this returns an empty dict .
__label__0 def testgetcpucompilablekernelnames ( self ) : `` '' '' tests retrieving compilable op names for cpu . '' '' '' op_names = pywrap_xla_ops.get_cpu_kernel_names ( ) self.assertgreater ( op_names.__len__ ( ) , 0 ) self.assertequal ( op_names.count ( 'max ' ) , 1 ) self.assertequal ( op_names.count ( 'min ' ) , 1 ) self.assertequal ( op_names.count ( 'matmul ' ) , 1 )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 the supervisor is a small wrapper around a ` coordinator ` , a ` saver ` , and a ` sessionmanager ` that takes care of common needs of tensorflow training programs .
__label__0 text = `` import tensorflow.compat.v2 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 def testcreateslotfromscalarvariable ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.test_session ( ) : s = variables.variable ( 1.0 , name= '' var '' ) p_v = variable_scope.get_variable ( `` var '' , shape= [ 2 , 2 ] , partitioner=partitioned_variables.fixed_size_partitioner ( 2 ) ) for i , v in enumerate ( p_v ) : slot = slot_creator.create_slot ( v , initialized_value ( s ) , name= '' slot '' )
__label__0 major , minor , patch = miopen_version_numbers ( rocm_install_path )
__label__0 class value ( tensor ) : `` '' '' tensor that can be associated with a value ( aka `` eager tensor '' ) .
__label__0 def testnestflatten ( self ) : a = object_identity._objectidentitywrapper ( ' a ' ) b = object_identity._objectidentitywrapper ( ' b ' ) c = object_identity._objectidentitywrapper ( ' c ' ) flat = nest.flatten ( [ [ [ ( a , b ) ] ] , c ] ) self.assertequal ( flat , [ a , b , c ] )
__label__0 # save checkpoint from which to warm-start . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : variable_scope.get_variable ( `` input_layer/sc_vocab_embedding/embedding_weights '' , initializer= [ [ 0.5 , 0.4 ] , [ 1. , 1.1 ] , [ 2. , 2.2 ] , [ 3. , 3.3 ] ] ) self._write_checkpoint ( sess )
__label__0 def testinitcapturesdecoratordoc ( self ) : self.assertequal ( 'decorator doc ' , tf_decorator.tfdecorator ( `` , test_function , 'decorator doc ' ) .decorator_doc )
__label__0 nested_list = [ [ 1 ] ] # although ` check_types=false ` is set , this assertion would fail because the # shallow_tree component has a deeper structure than the input_tree # component . with self.assertraisesregex ( # pylint : disable=g-error-prone-assert-raises typeerror , `` if shallow structure is a sequence , input must also be a sequence '' , ) : nest.flatten_up_to ( shallow_tree=nested_list , input_tree=mt , check_types=false )
__label__0 returns : none or a ` sessionrunargs ` object. `` '' '' return none
__label__0 def testtypecheckersarecached ( self ) : checker1 = dispatch.make_type_checker ( int ) checker2 = dispatch.make_type_checker ( int ) self.assertis ( checker1 , checker2 )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for exampleparserconfiguration . '' '' ''
__label__0 @ deprecation.deprecated_args ( date , instructions , `` d1 '' , `` d2 '' ) def _fn ( arg0 , d1=none , arg1=2 , d2=none ) : return arg0 + arg1 if d1 else arg1 + arg0 if d2 else arg0 * arg1
__label__0 def testunwrapreturnsemptyarrayforundecoratedfunction ( self ) : decorators , _ = tf_decorator.unwrap ( test_function ) self.assertequal ( 0 , len ( decorators ) )
__label__0 _inheritable_header = `` _tf_docs_inheritable_header ''
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } }
__label__0 # the ` type ( target ) ` ensures that if a class is received we do n't return # the signature of its __call__ method . return _getargspec ( type ( target ) .__call__ )
__label__0 returns : a decorator .
__label__0 returns : a ` metagraphdef ` proto. `` '' '' # pylint : enable=line-too-long return export_meta_graph ( filename=filename , graph_def=ops.get_default_graph ( ) .as_graph_def ( add_shapes=true , use_pybind11_proto=true ) , saver_def=self.saver_def , collection_list=collection_list , as_text=as_text , export_scope=export_scope , clear_devices=clear_devices , clear_extraneous_savers=clear_extraneous_savers , strip_default_attrs=strip_default_attrs , save_debug_info=save_debug_info , )
__label__0 return tensors
__label__0 def testclusterspecaccessors ( self ) : original_dict = { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : [ `` worker0:2222 '' , `` worker1:2222 '' , `` worker2:2222 '' ] , `` sparse '' : { 0 : `` sparse0:2222 '' , 3 : `` sparse3:2222 '' } } cluster_spec = server_lib.clusterspec ( original_dict )
__label__0 args : new_vocab : [ required ] a path to the new vocabulary file ( used with the model to be trained ) . new_vocab_size : [ required ] an integer indicating how many entries of the new vocabulary will used in training . num_oov_buckets : [ required ] an integer indicating how many oov buckets are associated with the vocabulary . old_vocab : [ required ] a path to the old vocabulary file ( used with the checkpoint to be warm-started from ) . old_vocab_size : [ optional ] an integer indicating how many entries of the old vocabulary were used in the creation of the checkpoint . if not provided , the entire old vocabulary will be used . backup_initializer : [ optional ] a variable initializer used for variables corresponding to new vocabulary entries and oov . if not provided , these entries will be zero-initialized . axis : [ optional ] denotes what axis the vocabulary corresponds to . the default , 0 , corresponds to the most common use case ( embeddings or linear weights for binary classification / regression ) . an axis of 1 could be used for warm-starting output layers with class vocabularies .
__label__0 * type check is set to false :
__label__0 @ classmethod def nested_masked_tensor_with_opposite_masks ( cls , mask , inner_value ) : return nestedmaskedtensor ( mask=mask , value=maskedtensor ( mask=not mask , value=inner_value ) )
__label__0 # generate three metagraphdef protos using different code paths . meta_graph_def_simple = saver_module.export_meta_graph ( ) meta_graph_def_devices_cleared = saver_module.export_meta_graph ( clear_devices=true ) meta_graph_def_from_graph_def = saver_module.export_meta_graph ( clear_devices=true , graph_def=g.as_graph_def ( ) )
__label__0 ` atomicfunction ` does not support gradients . please use the parent ` concretefunction ` if you need gradient support. `` '' ''
__label__0 def testdifference ( self ) :
__label__0 with self.session ( graph=ops_lib.graph ( ) ) as sess : # build a graph with 2 parameter nodes , and save and # restore nodes for them . v0 = variable_op ( 10.0 , name= '' v0 '' ) v1 = variable_op ( 20.0 , name= '' v1 '' ) save = saver_module.saver ( { `` save_prefix/v0 '' : v0 , `` save_prefix/v1 '' : v1 } ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def _addshardedsaveops ( self , filename_tensor , per_device ) : `` '' '' add ops to save the params per shard .
__label__0 def testlearningratedecay ( self ) : for decay in [ `` tf.train.exponential_decay '' , `` tf.train.polynomial_decay '' , `` tf.train.natural_exp_decay '' , `` tf.train.inverse_time_decay '' , `` tf.train.cosine_decay '' , `` tf.train.cosine_decay_restarts '' , `` tf.train.linear_cosine_decay '' , `` tf.train.noisy_linear_cosine_decay '' , `` tf.train.piecewise_constant_decay '' , ] :
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testreadyforlocalinitop ( self ) : server = server_lib.server.create_local_server ( ) logdir = self._test_dir ( `` default_ready_for_local_init_op '' )
__label__0 text = `` from tensorflow.foo import bar '' expected_text = `` from tensorflow.compat.v1.foo import bar '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 returns : ` tuple ` of string argument names .
__label__0 # save checkpoint from which to warm-start . _ , prev_int_val = self._create_prev_run_var ( `` linear_model/sc_int/weights '' , shape= [ 10 , 1 ] , initializer=ones ( ) ) # verify we initialized the values correctly . self.assertallequal ( np.ones ( [ 10 , 1 ] ) , prev_int_val )
__label__0 # tf.train.sequenceexample class sequenceexample ( typing.namedtuple ) : context : dict [ str , feature ] feature_lists : featurelists `` `
__label__0 if repeated_msg_split : # finish writing repeated chunks . start = repeated_msg_split [ 0 ] for end , msg in zip ( itertools.chain.from_iterable ( [ repeated_msg_split [ 1 : ] , [ none ] ] ) , repeated_msg_graphs , ) : _split_repeated_field ( proto , msg , self.repeated_field , start , end ) start = end del field [ repeated_msg_split [ 0 ] : ]
__label__0 * get the recent timings for a given benchmark : select start , timing from entry where test = < test-name > and entry = < entry-name > and start > = < recent-datetime > limit < count >
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities for serializing python objects . '' '' ''
__label__0 def __init__ ( self ) : self._m2 = test_module2.moduleclass2 ( )
__label__0 def _testgraphextensionsave ( self , test_dir ) : filename = os.path.join ( test_dir , `` metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) # creates an inference graph . # hidden 1 images = constant_op.constant ( 1.2 , dtypes.float32 , shape= [ 100 , 28 ] ) with ops_lib.name_scope ( `` hidden1 '' ) : weights = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 28 , 128 ] , stddev=1.0 / math.sqrt ( float ( 28 ) ) ) , name= '' weights '' ) # the use of cond.cond here is purely for adding test coverage # the save and restore of control flow context ( which does n't make any # sense here from a machine learning perspective ) . the typical biases is # a simple variable without the conditions . biases = variable_v1.variablev1 ( cond.cond ( math_ops.less ( random.random ( ) , 0.5 ) , lambda : array_ops.ones ( [ 128 ] ) , lambda : array_ops.zeros ( [ 128 ] ) ) , name= '' biases '' ) hidden1 = nn_ops.relu ( math_ops.matmul ( images , weights ) + biases ) # hidden 2 with ops_lib.name_scope ( `` hidden2 '' ) : weights = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 128 , 32 ] , stddev=1.0 / math.sqrt ( float ( 128 ) ) ) , name= '' weights '' )
__label__0 if not os.path.isdir ( gen_path ) : raise runtimeerror ( `` gen_git_source.py : failed to create dir '' )
__label__0 with riegeli.recordreader ( open ( f '' { path } .cpb '' , `` rb '' ) ) as reader : self.asserttrue ( reader.check_file_format ( ) ) records = list ( reader.read_records ( ) ) self.assertlen ( records , 4 )
__label__0 with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) prev_tensor_name , var = ws_util._get_var_info ( fruit_weights ) checkpoint_utils.init_from_checkpoint ( self.get_temp_dir ( ) , { prev_tensor_name : var } ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( prev_val , fruit_weights.eval ( sess ) )
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import test as test_lib from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import tf_upgrade
__label__0 @ doc_controls.doc_private def _private ( self ) : ... `` `
__label__0 def test_tfn_add ( self ) : @ def_function.function def fn ( ) : x = array_ops.zeros ( ( 2 , 3 ) ) y = array_ops.zeros ( ( 2 , 4 ) ) return x + y
__label__0 def testlisttotuple ( self ) : input_sequence = [ 1 , ( 2 , { 3 : [ 4 , 5 , ( 6 , ) ] } , none , 7 , [ [ [ 8 ] ] ] ) ] expected = ( 1 , ( 2 , { 3 : ( 4 , 5 , ( 6 , ) ) } , none , 7 , ( ( ( 8 , ) , ) , ) ) ) nest.assert_same_structure ( nest.list_to_tuple ( input_sequence ) , expected , )
__label__0 def teardown ( self ) : super ( ) .teardown ( ) for name in self._modules : del sys.modules [ name ] self._modules = [ ]
__label__0 def test_deprecated_illegal_args ( self ) : instructions = `` this is how you update ... '' with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated ( `` '' , instructions ) with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated ( `` 07-04-2016 '' , instructions ) date = `` 2016-07-04 '' with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated ( date , none ) with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated ( date , `` '' )
__label__0 os.chdir ( os.path.abspath ( os.path.join ( os.path.dirname ( __file__ ) , `` .. / .. / .. '' ) ) )
__label__0 note that the queue runners collected in the graph key ` queue_runners ` are already started automatically when you create a session with the supervisor , so unless you have non-collected queue runners to start you do not need to call this explicitly .
__label__0 # functions that were reordered should be changed to the new keyword args # for safety , if positional arguments are used . if you have reversed the # positional arguments yourself , this could do the wrong thing . self.function_reorders = { `` tf.split '' : [ `` axis '' , `` num_or_size_splits '' , `` value '' , `` name '' ] , `` tf.sparse_split '' : [ `` axis '' , `` num_or_size_splits '' , `` value '' , `` name '' ] , `` tf.concat '' : [ `` concat_dim '' , `` values '' , `` name '' ] , `` tf.svd '' : [ `` tensor '' , `` compute_uv '' , `` full_matrices '' , `` name '' ] , `` tf.nn.softmax_cross_entropy_with_logits '' : [ `` logits '' , `` labels '' , `` dim '' , `` name '' ] , `` tf.nn.sparse_softmax_cross_entropy_with_logits '' : [ `` logits '' , `` labels '' , `` name '' ] , `` tf.nn.sigmoid_cross_entropy_with_logits '' : [ `` logits '' , `` labels '' , `` name '' ] , `` tf.op_scope '' : [ `` values '' , `` name '' , `` default_name '' ] , }
__label__0 this simply wraps the get_slot ( ) from the actual optimizer .
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( self , arg0 , arg1 ) : `` '' '' fn doc . '' '' '' return arg0 + arg1
__label__0 # pylint : disable=protected-access # accesses protected members of tf.variable to reset the variable 's internal # state . def _warm_start_var_with_vocab ( var , current_vocab_path , current_vocab_size , prev_ckpt , prev_vocab_path , previous_vocab_size=-1 , current_oov_buckets=0 , prev_tensor_name=none , initializer=none , axis=0 ) : `` '' '' warm-starts given variable from ` prev_tensor_name ` tensor in ` prev_ckpt ` .
__label__0 def contextmanager ( target : callable [ ... , iterator [ _t ] ] , ) - > callable [ ... , contextmanager [ _t ] ] : `` '' '' a tf_decorator-aware wrapper for ` contextlib.contextmanager ` .
__label__0 def _has_tf_decorator_attr ( obj ) : `` '' '' checks if object has _tf_decorator attribute .
__label__0 * restart training from a saved graph and checkpoints .
__label__0 if write_meta_graph : meta_graph_filename = checkpoint_management.meta_graph_filename ( checkpoint_file , meta_graph_suffix=meta_graph_suffix ) if not context.executing_eagerly ( ) : with sess.graph.as_default ( ) : self.export_meta_graph ( meta_graph_filename , strip_default_attrs=strip_default_attrs , save_debug_info=save_debug_info )
__label__0 import argparse import glob import os import shutil import subprocess import sys import tempfile
__label__0 def numpy ( self ) : pass
__label__0 `` ` python with training.monitoredtrainingsession ( master=workers [ worker_id ] .target , is_chief=is_chief , hooks= [ sync_replicas_hook ] ) as mon_sess : while not mon_sess.should_stop ( ) : mon_sess.run ( training_op ) `` ` `` '' ''
__label__0 see readme for details. `` '' ''
__label__0 # break cycles if any ( child is item for item in new_stack ) : # ` in ` , but using ` is ` continue
__label__0 def set_attr ( self , func : any , api_names_attr : str , names : sequence [ str ] ) - > none : setattr ( func , api_names_attr , names )
__label__0 args : nest : the value to produce a flattened paths list for . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 def testmapstructureupto ( self ) : # named tuples . ab_tuple = collections.namedtuple ( `` ab_tuple '' , `` a , b '' ) op_tuple = collections.namedtuple ( `` op_tuple '' , `` add , mul '' ) inp_val = ab_tuple ( a=2 , b=3 ) inp_ops = ab_tuple ( a=op_tuple ( add=1 , mul=2 ) , b=op_tuple ( add=2 , mul=3 ) ) out = nest.map_structure_up_to ( inp_val , lambda val , ops : ( val + ops.add ) * ops.mul , inp_val , inp_ops ) self.assertequal ( out.a , 6 ) self.assertequal ( out.b , 15 )
__label__0 partial_func = functools.partial ( func , n=7 , l=2 ) argspec = tf_inspect.argspec ( args= [ 'm ' , 'n ' , ' l ' , ' k ' ] , varargs=none , keywords=none , defaults= ( 7 , 2 , 4 ) )
__label__0 def testfromimport_nochangeneeded ( self ) : text = `` from bar import a as b '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( text , new_text )
__label__0 else : print ( `` test passed '' )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.dataformatvecpermute . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 args : filename_tensor : string tensor . saveables : a list of basesaverbuilder.saveableobject objects .
__label__0 the output is :
__label__0 def handle ( self , op , args , kwargs ) : # dispatcher only applies if at least one arg is a tensortracer . if not ( any ( self.is_tensor_tracer_arg ( x ) for x in args ) or any ( self.is_tensor_tracer_arg ( x ) for x in kwargs.values ( ) ) ) : return self.not_supported
__label__0 def __hash__ ( self ) : # wrapper id ( ) is also fine for weakrefs . in fact , we rely on # id ( weakref.ref ( a ) ) == id ( weakref.ref ( a ) ) and weakref.ref ( a ) is # weakref.ref ( a ) in _weakobjectidentitywrapper . return id ( self._wrapped )
__label__0 to parse a ` sequenceexample ` in tensorflow refer to the ` tf.io.parse_sequence_example ` function .
__label__0 path = f '' { file_prefix } .cpb '' with riegeli.recordwriter ( file_io.fileio ( path , `` wb '' ) ) as f : metadata = chunk_pb2.chunkmetadata ( message=chunked_message , version=self.version_def ) for chunk in chunks : if isinstance ( chunk , message.message ) : f.write_message ( chunk ) chunk_type = chunk_pb2.chunkinfo.type.message size = chunk.bytesize ( ) else : f.write_record ( chunk ) chunk_type = chunk_pb2.chunkinfo.type.bytes size = len ( chunk ) metadata.chunks.add ( type=chunk_type , size=size , offset=f.last_pos.numeric ) f.write_message ( metadata )
__label__0 def _new__getattribute__ ( self , key ) : if key not in ( '_tf_should_use_helper ' , '_tf_should_use_wrapped_value ' ) : object.__getattribute__ ( self , '_tf_should_use_helper ' ) .sate ( ) if key in ( '_tf_should_use_wrapped_value ' , '_tf_should_use_helper ' , 'mark_used ' , '__setattr__ ' , ) : return object.__getattribute__ ( self , key ) return getattr ( object.__getattribute__ ( self , '_tf_should_use_wrapped_value ' ) , key )
__label__0 partial_func = functools.partial ( func , n=7 ) argspec = tf_inspect.argspec ( args= [ 'm ' , 'n ' ] , varargs=none , keywords=none , defaults= ( 1 , 7 ) )
__label__0 # the rest of the variables . rest_variables = list ( set ( variables.global_variables ( ) ) - set ( var_list.keys ( ) ) ) init_rest_op = variables.variables_initializer ( rest_variables )
__label__0 deprecation.deprecated_argument_lookup ( `` val_new '' , good_value , `` val_old '' , good_value )
__label__1 def factorial ( n ) : if n == 0 or n == 1 : return 1 else : return n * factorial ( n - 1 )
__label__0 args : obj : the class-attribute to hide from the generated docs .
__label__0 define a dummy model and loss :
__label__0 conditionally conformant ` featurelists ` , the parser configuration determines if the feature sizes must match :
__label__0 def _test_function ( unused_arg=0 ) : pass
__label__0 returns : an operation that restores the variables. `` '' '' all_tensors = self.bulk_restore ( filename_tensor , saveables , preferred_shard , restore_sequentially )
__label__0 # make sure the warning comes first , otherwise the name may have changed self._maybe_add_warning ( node , full_name )
__label__0 this is necessary because we have trackable objects ( _listwrapper ) which have behavior identical to built-in python lists ( including being unhashable and comparing based on the equality of their contents by default ) . `` '' ''
__label__0 _byte_units = [ ( 1 , `` b '' ) , ( 1 < < 10 , `` kib '' ) , ( 1 < < 20 , `` mib '' ) , ( 1 < < 30 , `` gib '' ) ]
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testreadyforlocalinitoprestorefromcheckpoint ( self ) : server = server_lib.server.create_local_server ( ) logdir = self._test_dir ( `` ready_for_local_init_op_restore '' )
__label__0 each type signature must contain at least one subclass of ` tf.compositetensor ` ( which includes subclasses of ` tf.extensiontype ` ) , and dispatch will only be triggered if at least one type-annotated parameter contains a ` compositetensor ` value . this rule avoids invoking dispatch in degenerate cases , such as the following examples :
__label__0 returns : the address of the given task in the given job .
__label__0 if want is none : want = ``
__label__0 def __get__ ( self , owner_self , owner_cls ) : return self._func ( owner_cls )
__label__0 for arg_name in arg_names : rename_node = _rename_if_arg_found_transformer ( parent , node , full_name , name , logs , arg_name , arg_ok_predicate , remove_if_ok , message ) node = rename_node if rename_node else node
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( initclass ) )
__label__0 self.assertisinstance ( ct2.component , resource_variable_ops.resourcevariable ) result = variable_utils.convert_variables_to_tensors ( ct2 ) self.assertisinstance ( result.component , tensor.tensor ) self.assertallequal ( result.component , 1 )
__label__0 default settings for doctest are configured to run like a repl : one statement at a time . the doctest source uses ` compile ( ... , mode= '' single '' ) `
__label__0 assert_rank_comment = ( ast_edits.info , `` < function name > has been changed to return none , and '' `` the data and summarize arguments have been removed . '' `` \nthe calls have been converted to compat.v1 for safety ( even though `` `` they may already have been correct ) . '' )
__label__0 def flatten_dict_items ( dictionary ) : `` '' '' returns a dictionary with flattened keys and values .
__label__0 if hasattr ( _inspect , 'getfullargspec ' ) : _getfullargspec = _inspect.getfullargspec # pylint : disable=invalid-name
__label__0 # using non-iterable elements . input_tree = 0 shallow_tree = [ 9 ] expected_message = ( `` if shallow structure is a sequence , input must also `` `` be a sequence . input has type : < ( type|class ) 'int ' > . '' ) with self.assertraisesregex ( typeerror , expected_message ) : flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 partial_func = functools.partial ( func , 7 , 8 ) argspec = tf_inspect.argspec ( args= [ ] , varargs='arg ' , keywords=none , defaults=none )
__label__0 @ property @ deprecation.deprecated ( date , instructions ) def _prop ( self ) : return `` prop_no_doc ''
__label__0 with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : one = variable_v1.variablev1 ( 0.0 ) twos = variable_v1.variablev1 ( [ 0.0 , 0.0 , 0.0 ] ) # saver with no arg , defaults to 'all variables ' . save = saver_module.saver ( ) save.restore ( sess , save_path ) self.assertallclose ( 1.0 , self.evaluate ( one ) ) self.assertallclose ( [ 2.0 , 2.0 , 2.0 ] , self.evaluate ( twos ) )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_instance_fn_with_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 for file in headers : if file.endswith ( `` cc.inc '' ) : continue
__label__0 import code import sys
__label__0 if flags.module : tf_modules = filter_on_submodules ( tf_modules , flags.module )
__label__0 wheel_file = os.path.join ( tmpdir , `` % s- % s.dist-info '' % ( package , version ) , `` wheel '' ) with open ( wheel_file , `` r '' ) as f : content = f.read ( ) with open ( wheel_file , `` w '' ) as f : f.write ( content.replace ( old_py_ver , new_py_ver ) )
__label__0 is_ready , msg = self._model_ready ( sess ) if not is_ready : logging.info ( `` restoring model from % s did not make model ready : % s '' , restoring_file , msg ) return sess , false
__label__0 _wrappers = { }
__label__0 def __ne__ ( self , other ) : return not self.__eq__ ( other )
__label__0 # we now have 2 'last_checkpoints ' : [ s1 , s2 ] . the next call to save ( ) , # would normally delete s1 , because max_to_keep is 2. however , s1 is # older than 0.7s so we must keep it . s3 = save.save ( sess , os.path.join ( save_dir , `` s3 '' ) ) self.assertequal ( [ s2 , s3 ] , save.last_checkpoints )
__label__0 options = [ make_type_checker ( t ) for t in type_args ] return _api_dispatcher.makeunionchecker ( options )
__label__0 returns : a ` saver ` built from saver_def. `` '' '' return saver ( saver_def=saver_def , name=import_scope )
__label__0 def testunboundfuncwithtwoparamspositional ( self ) :
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_deprecated_args_once ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def _assert_subset ( self , expected_subset , actual_set ) : self.asserttrue ( actual_set.issuperset ( expected_subset ) , msg= '' % s is not a superset of % s . '' % ( actual_set , expected_subset ) )
__label__0 args : logdir : a string . directory where event file will be written . graph : a ` graph ` object , such as ` sess.graph ` . max_queue : integer . size of the queue for pending events and summaries . flush_secs : number . how often , in seconds , to flush the pending events and summaries to disk . graph_def : deprecated : use the ` graph ` argument instead. `` '' '' super ( summarywriter , self ) .__init__ ( logdir , graph , max_queue , flush_secs , graph_def )
__label__0 args : x : the instance to wrap . tf_should_use_helper : the object that tracks usage .
__label__0 # check that the parameter nodes have been initialized . if not context.executing_eagerly ( ) : init_all_op = [ variables.global_variables_initializer ( ) , v2_init ] self.evaluate ( init_all_op ) # todo ( xpan ) : why _mutable_hash_table_v2 does n't create empty # table as it claims in eager mode ? self.assertequal ( b '' k1000 '' , self.evaluate ( v2_2.keys ( ) ) ) self.assertequal ( 3000.0 , self.evaluate ( v2_2.values ( ) ) ) self.assertequal ( 1000.0 , self.evaluate ( v0_2 ) ) self.assertequal ( 2000.0 , self.evaluate ( v1_2 ) )
__label__0 def __init__ ( self , a ) : pass
__label__0 x = maskedtensor ( [ 1 , 2 , 3 ] , [ true , false , true ] ) y = some_op ( x ) self.assertallequal ( y.values , [ 2 , 4 , 6 ] ) self.assertallequal ( y.mask , [ true , false , true ] )
__label__0 # we want to get stack frame 3 frames up from current frame , # i.e . above __getattr__ , _tfmw_add_deprecation_warning , # and _call_location calls . for _ in range ( 3 ) : parent = frame.f_back if parent is none : break frame = parent return ' { } : { } '.format ( frame.f_code.co_filename , frame.f_lineno )
__label__0 if self._is_chief : self._logdir = logdir self._save_summaries_secs = save_summaries_secs self._save_model_secs = save_model_secs if self._logdir : self._save_path = os.path.join ( self._logdir , checkpoint_basename ) if summary_writer is supervisor.use_default : if self._logdir : self._summary_writer = _summary.filewriter ( self._logdir ) else : self._summary_writer = summary_writer self._graph_added_to_summary = false
__label__0 self.assertisinstance ( module.foo , lazy_loader.lazyloader )
__label__0 args : input_graph_def : graphdef object containing a model to be transformed . inputs : list of node names for the model inputs . outputs : list of node names for the model outputs . transforms : list of strings containing transform names and parameters .
__label__0 def fn ( ) : x = array_ops.zeros ( ( 2 , 3 ) ) y = array_ops.zeros ( ( 2 , 4 ) ) _ = x + y
__label__0 def uses_star_args_or_kwargs_in_call ( node ) : `` '' '' check if an ast.call node uses arbitrary-length * args or * * kwargs .
__label__0 from tensorflow.core.example import example_parser_configuration_pb2 from tensorflow.python.client import session from tensorflow.python.framework import dtypes from tensorflow.python.ops import array_ops from tensorflow.python.ops import parsing_ops from tensorflow.python.platform import test from tensorflow.python.util.example_parser_configuration import extract_example_parser_configuration
__label__0 if ` session.run ( ) ` raises any exceptions then ` after_run ( ) ` is not called .
__label__0 def _string_split_transformer ( parent , node , full_name , name , logs ) : `` '' '' update tf.string_split arguments : skip_empty , sep , result_type , source . '' '' '' # check the skip_empty parameter : if not false , then use compat.v1 . for i , kw in enumerate ( node.keywords ) : if kw.arg == `` skip_empty '' : if _is_ast_false ( kw.value ) : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` removed argument skip_empty for tf.string_split . '' ) ) node.keywords.pop ( i ) break else : return _rename_to_compat_v1 ( node , full_name , logs , `` tf.string_split 's replacement no longer `` `` takes the skip_empty argument . '' )
__label__0 def testraiseswithnoncallableobject ( self ) : with self.assertraises ( valueerror ) : function_utils.get_func_code ( none )
__label__0 returns : true if this import should not be renamed according to the import_rename_spec. `` '' '' for excluded_prefix in import_rename_spec.excluded_prefixes : if module.startswith ( excluded_prefix ) : return true return false
__label__0 logs = [ self.format_log ( log , none ) for log in ( preprocess_logs + visitor.log ) ] errors = [ self.format_log ( error , in_filename ) for error in ( preprocess_errors + visitor.warnings_and_errors ) ] return 1 , pasta.dump ( t ) , logs , errors
__label__0 at this point graph is finalized and you can not add ops .
__label__0 if os.path.islink ( input_path ) : link_target = os.readlink ( input_path ) link_target_output = os.path.join ( output_root_directory , os.path.relpath ( link_target , root_directory ) ) if ( link_target , link_target_output ) in files_to_process : # create a link to the new location of the target file os.symlink ( link_target_output , output_path ) else : report += `` copying symlink % s without modifying its target % s '' % ( input_path , link_target ) os.symlink ( link_target , output_path ) continue
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.raw_ops.add . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys from python_fuzzing import fuzzinghelper import tensorflow as tf
__label__0 
__label__0 > > > array = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) > > > tf.nest.flatten ( array ) [ array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ]
__label__0 # default name value c = math_ops.add ( x , y ) if not context.executing_eagerly ( ) : # names not defined in eager mode . self.assertregex ( c.values.name , r '' ^add . * '' ) self.assertregex ( c.mask.name , r '' ^and . * '' )
__label__0 def compatible_func ( * args , * * kwargs ) : bound = op_signature.bind ( * args , * * kwargs ) for name , param in func_missing_params.items ( ) : if name not in bound.arguments : continue value = bound.arguments.pop ( name ) if value is not param.default : raise assertionerror ( f '' dispatched op is called with argument ` { name } ` set to a '' `` non-default value , which is not supported by the decorated '' `` function '' ) return func ( * bound.args , * * bound.kwargs )
__label__0 @ parameterized.named_parameters ( [ dict ( testcase_name= '' tuples '' , s1= ( 1 , 2 ) , s2= ( 3 , 4 ) , check_types=true , expected= ( ( ( 0 , ) , 4 ) , ( ( 1 , ) , 6 ) ) ) , dict ( testcase_name= '' dicts '' , s1= { `` a '' : 1 , `` b '' : 2 } , s2= { `` b '' : 4 , `` a '' : 3 } , check_types=true , expected= { `` a '' : ( ( `` a '' , ) , 4 ) , `` b '' : ( ( `` b '' , ) , 6 ) } ) , dict ( testcase_name= '' mixed '' , s1= ( 1 , 2 ) , s2= [ 3 , 4 ] , check_types=false , expected= ( ( ( 0 , ) , 4 ) , ( ( 1 , ) , 6 ) ) ) , dict ( testcase_name= '' nested '' , s1= { `` a '' : [ 2 , 3 ] , `` b '' : [ 1 , 2 , 3 ] } , s2= { `` b '' : [ 5 , 6 , 7 ] , `` a '' : [ 8 , 9 ] } , check_types=true , expected= { `` a '' : [ ( ( `` a '' , 0 ) , 10 ) , ( ( `` a '' , 1 ) , 12 ) ] , `` b '' : [ ( ( `` b '' , 0 ) , 6 ) , ( ( `` b '' , 1 ) , 8 ) , ( ( `` b '' , 2 ) , 10 ) ] } ) , ] ) def testmapwithtuplepathscompatiblestructures ( self , s1 , s2 , check_types , expected ) : def path_and_sum ( path , * values ) : return path , sum ( values ) result = nest.map_structure_with_tuple_paths ( path_and_sum , s1 , s2 , check_types=check_types ) self.assertequal ( expected , result )
__label__0 self.assert_trace_line_count ( fn , count=15 , filtering_enabled=true ) self.assert_trace_line_count ( fn , count=30 , filtering_enabled=false )
__label__0 def stack ( context=1 ) : `` '' '' tfdecorator-aware replacement for inspect.stack . '' '' '' return _inspect.stack ( context ) [ 1 : ]
__label__0 feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { int64_list : { value : [ 4 ] } } feature : { int64_list : { value : [ 5 ] } } feature : { int64_list : { value : [ 2 ] } } } } } `` `
__label__0 text = ( `` tf.image.extract_glimpse ( x , size , off , centered=false , `` `` normalized=false , uniform_noise=true if uniform_noise else `` `` false , name=\ '' foo\ '' ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.image.extract_glimpse ( x , size , off , centered=false , `` `` normalized=false , noise='uniform ' if ( true if uniform_noise else `` `` false ) else 'gaussian ' , name=\ '' foo\ '' ) \n '' , )
__label__0 def set_up ( self , test ) : # enable soft device placement to run distributed doctests . tf.config.set_soft_device_placement ( true ) self.setup ( ) context.async_wait ( )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # xla requires the toolkit path to find ptxas and libdevice . # todo ( csigg ) : pass in both directories instead . cuda_toolkit_paths = ( os.path.normpath ( os.path.join ( cuda_binary_dir , `` .. '' ) ) , os.path.normpath ( os.path.join ( nvvm_library_dir , `` .. / .. '' ) ) , ) if cuda_toolkit_paths [ 0 ] ! = cuda_toolkit_paths [ 1 ] : raise configerror ( `` inconsistent cuda toolkit path : % s vs % s '' % cuda_toolkit_paths )
__label__0 def testwarmstart_multiplecols ( self ) : # create vocab for sparse column `` sc_vocab '' . vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` vocab '' )
__label__0 mean_grad = decay * mean_grad { t-1 } + ( 1-decay ) * gradient mean_square = decay * mean_square { t-1 } + ( 1-decay ) * gradient * * 2 mom = momentum * mom { t-1 } + learning_rate * g_t / sqrt ( mean_square - mean_grad * * 2 + epsilon ) delta = - mom `` '' ''
__label__0 returns : true if traceback filtering is enabled ( e.g . if ` tf.debugging.enable_traceback_filtering ( ) ` was called ) , and false otherwise ( e.g . if ` tf.debugging.disable_traceback_filtering ( ) ` was called ) . `` '' '' value = getattr ( _enable_traceback_filtering , 'value ' , true ) return value
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( newclass ) )
__label__0 args : job_name : the string name of a job in this cluster .
__label__0 def __init__ ( self ) : self.non_dep_variable = variable_scope.get_variable ( name= '' non_dep_variable '' , initializer=6. , use_resource=true )
__label__0 doclines = __doc__.split ( '\n ' )
__label__0 ev = next ( rr ) self.asserttrue ( ev.meta_graph_def )
__label__0 @ test_util.run_v1_only ( `` requires tf v1 variable behavior . '' ) def testpreparesessionwithinsufficientreadyforlocalinitcheck ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=none , local_init_op=w.initializer ) with self.assertraisesregex ( runtimeerror , `` init operations did not make model ready . * '' ) : sm2.prepare_session ( `` '' , init_op=none )
__label__0 @ test_util.run_v1_only ( `` train.saver is v1 only api . '' ) def testint64 ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` int64 '' )
__label__0 - for modality.core : refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 @ parameterized.parameters ( [ false , `` import tensorflow.compat.v2 as tf\ntf.conj ( a ) '' , `` import tensorflow.compat.v2 as tf\ntf.conj ( a ) '' , `` tf.conj ( a ) '' , `` tf.math.conj ( a ) '' ] , [ false , `` import tensorflow.compat.v2 as tf\ntf.to_int32 ( x ) '' , `` import tensorflow.compat.v2 as tf\ntf.to_int32 ( x ) '' , `` tf.to_int32 ( x ) '' , `` tf.cast ( x , dtype=tf.int32 ) '' ] , # verify that upgrade_compat_v1_import option persists between files [ true , `` import tensorflow.compat.v1 as tf\ntf.conj ( a ) '' , `` import tensorflow.compat.v2 as tf\ntf.math.conj ( a ) '' , `` import tensorflow.compat.v1 as tf\ntf.to_int32 ( x ) '' , `` import tensorflow.compat.v2 as tf\ntf.cast ( x , dtype=tf.int32 ) '' ] , ) # pyformat : disable def test_api_spec_reset_between_files ( self , upgrade_compat_v1_import , text_a , expected_text_a , text_b , expected_text_b ) : results = self._upgrade_multiple ( upgrade_compat_v1_import , [ text_a , text_b ] ) result_a , result_b = results [ 0 ] , results [ 1 ] self.assertequal ( result_a [ 3 ] , expected_text_a ) self.assertequal ( result_b [ 3 ] , expected_text_b )
__label__0 def _get_var_info ( var , prev_tensor_name=none ) : `` '' '' helper method for standarizing variable and naming .
__label__0 flags.define_string ( 'code_url_prefix ' , none , ' [ unused ] the url prefix for links to code . ' )
__label__0 return decorator
__label__0 raises : runtimeerror : if multiple items found in collection global_step_read_key. `` '' '' graph = graph or ops.get_default_graph ( ) global_step_read_tensors = graph.get_collection ( global_step_read_key ) if len ( global_step_read_tensors ) > 1 : raise runtimeerror ( 'there are multiple items in collection { } . ' 'there should be only one . '.format ( global_step_read_key ) )
__label__0 ` managed_session ( ) ` launches the `` summary '' and `` checkpoint '' threads which use either the optionally ` summary_op ` and ` saver ` passed to the constructor , or default ones created automatically by the supervisor . if you want to run your own summary and checkpointing logic , disable these services by passing ` none ` to the ` summary_op ` and ` saver ` parameters .
__label__0 @ tf_export ( `` train.clusterspec '' ) class clusterspec : `` '' '' represents a cluster as a set of `` tasks '' , organized into `` jobs '' .
__label__0 class tfinspecttest ( test.testcase ) :
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' run doctests for tensorflow . '' '' ''
__label__0 def _format_log ( self , log , in_filename , out_filename ) : text = `` - '' * 80 + `` \n '' text += `` processing file % r\n outputting to % r\n '' % ( in_filename , out_filename ) text += `` - '' * 80 + `` \n\n '' text += `` \n '' .join ( log ) + `` \n '' text += `` - '' * 80 + `` \n\n '' return text
__label__0 return result
__label__0 from tensorflow.python.util import _pywrap_nest from tensorflow.python.util import _pywrap_utils from tensorflow.python.util import nest_util from tensorflow.python.util.tf_export import tf_export
__label__0 def _split_repeated_field ( proto : message.message , new_proto : message.message , fields : util.fieldtypes , start_index : int , end_index : optional [ int ] = none , ) - > none : `` '' '' generic function for copying a repeated field from one proto to another . '' '' '' util.get_field ( new_proto , fields ) [ 0 ] .mergefrom ( util.get_field ( proto , fields ) [ 0 ] [ start_index : end_index ] )
__label__0 note : ` getfullargspec ` is recommended as the python 2/3 compatible replacement for this function .
__label__0 def get_canonical_name ( api_names : sequence [ str ] , deprecated_api_names : sequence [ str ] ) - > optional [ str ] : `` '' '' get preferred endpoint name .
__label__0 * these examples will raise exceptions :
__label__0 @ tf_export ( '__internal__.register_clear_session_function ' , v1= [ ] ) def register_clear_session_function ( func ) : global _keras_clear_session_function _keras_clear_session_function = func
__label__0 # # input non-list edge-case . # using iterable elements . input_tree = `` input_tree '' shallow_tree = [ `` shallow_tree '' ] expected_message = ( `` if shallow structure is a sequence , input must also `` `` be a sequence . input has type : < ( type|class ) 'str ' > . '' ) with self.assertraisesregex ( typeerror , expected_message ) : flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_shallow_tree , shallow_tree )
__label__0 args : num_groups : the number of groups that will be accessing the resource under consideration . should be a positive number .
__label__0 library_path = _find_library ( base_paths , `` cudnn '' , cudnn_version )
__label__0 @ test_util.run_v1_only ( `` exporting/importing meta graphs is only supported in v1 . '' ) def testslicevariable ( self ) : test_dir = self._get_test_dir ( `` slice_saver '' ) filename = os.path.join ( test_dir , `` metafile '' ) with self.cached_session ( ) : v1 = variable_v1.variablev1 ( [ 20.0 ] , name= '' v1 '' ) v2 = variable_v1.variablev1 ( [ 20.0 ] , name= '' v2 '' ) v2._set_save_slice_info ( variables.variable.savesliceinfo ( `` v1 '' , [ 1 ] , [ 0 ] , [ 1 ] ) )
__label__0 # these values do n't have namedtuple types . self.assertfalse ( nest.is_namedtuple ( 123 ) ) self.assertfalse ( nest.is_namedtuple ( `` abc '' ) ) self.assertfalse ( nest.is_namedtuple ( ( 123 , `` abc '' ) ) )
__label__0 `` ` foo ( tf.constant ( [ 1 , 2 , 3 ] ) ) foo ( [ 1 , 2 , 3 ] ) foo ( np.array ( [ 1 , 2 , 3 ] ) ) `` ` `` '' '' ) , ) tf_export ( `` types.experimental.tensorlike '' ) .export_constant ( __name__ , `` tensorlike '' )
__label__0 this function is mostly for backward compatibility . historically , ` resourcevariable ` s are treated as tf.nest atoms . this is no longer the case after ` resourcevariable ` becoming ` compositetensor ` . unfortunately , tf.nest does n't allow customization of what objects are treated as atoms . calling this function to manually convert ` resourcevariable ` s to atoms to avoid breaking tf.assert_same_structure with inputs of a ` resourcevariable ` and an atom , like a ` tensor ` .
__label__0 def _get_required_param_names ( sig ) : `` '' '' returns a list of required parameter names from a python signature . '' '' '' params = [ ] for p in sig.parameters.values ( ) : if p.kind == p.var_positional : continue if p.kind == p.var_keyword : continue if p.default is not p.empty : continue params.append ( p.name ) return params
__label__0 # check if the dependency is in the pip package , the dependency denylist , # or should be ignored because of its file extension . if not ( ignore or dependency in pip_package_dependencies_list or dependency in dependency_denylist ) : missing_dependencies.append ( dependency )
__label__0 use ` tf.io.parse_example ` to extract tensors from a serialized ` example ` proto :
__label__0 def test_partial_function ( self ) : expected_test_arg = 123
__label__0 # override the behavior of gen_math_ops.atan2 and make it look like add . @ dispatch.dispatch_for_types ( gen_math_ops.atan2 , customtensor ) def custom_atan2 ( y , x , name=none ) : # pylint : disable=unused-variable return customtensor ( gen_math_ops.add ( y.tensor , x.tensor , name ) , ( x.score + y.score ) / 2.0 )
__label__0 `` ` context : { feature : { key : `` locale '' value : { bytes_list : { value : [ `` pt_br '' ] } } } feature : { key : `` age '' value : { float_list : { value : [ 19.0 ] } } } feature : { key : `` favorites '' value : { bytes_list : { value : [ `` majesty rose '' , `` savannah outen '' , `` one direction '' ] } } } } feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } feature_list : { key : `` movie_names '' value : { feature : { bytes_list : { value : [ `` the shawshank redemption '' ] } } feature : { bytes_list : { value : [ `` fight club '' ] } } } } feature_list : { key : `` actors '' value : { feature : { bytes_list : { value : [ `` tim robbins '' , `` morgan freeman '' ] } } feature : { bytes_list : { value : [ `` brad pitt '' , `` edward norton '' , `` helena bonham carter '' ] } } } } } `` `
__label__0 self._safe_close ( sess )
__label__0 self.assertequal ( `` const/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , array_ops.shape ( slot ) .eval ( ) ) self.assertequal ( dtypes.float64 , slot.dtype.base_dtype ) self.assertallequal ( [ 0.0 , 0.0 ] , self.evaluate ( slot ) )
__label__0 if ` structure ` is an atom , ` flat_sequence ` must be a single-item list ; in this case the return value is ` flat_sequence [ 0 ] ` .
__label__0 def testsomeerrors ( self ) : with ops_lib.graph ( ) .as_default ( ) : v0 = variable_v1.variablev1 ( [ 10.0 ] , name= '' v0 '' ) v1 = variable_v1.variablev1 ( [ 20.0 ] , name= '' v1 '' ) v2 = variable_v1.variablev1 ( [ 20.0 ] , name= '' v2 '' ) v2._set_save_slice_info ( variables.variable.savesliceinfo ( `` v1 '' , [ 1 ] , [ 0 ] , [ 1 ] ) )
__label__0 args : num_tokens : number of tokens to add to the queue .
__label__0 # start a second session . in that session the parameter nodes # have not been initialized either . with self.cached_session ( ) as sess : v0 = variable_v1.variablev1 ( -1.0 , name= '' v0 '' ) v1 = variable_v1.variablev1 ( -1.0 , name= '' v1 '' ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } )
__label__0 [ ! [ python ] ( https : //img.shields.io/pypi/pyversions/tensorflow.svg ? style=plastic ) ] ( https : //badge.fury.io/py/tensorflow ) [ ! [ pypi ] ( https : //badge.fury.io/py/tensorflow.svg ) ] ( https : //badge.fury.io/py/tensorflow )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow.ops.gen_training_ops . '' '' ''
__label__0 ` managed_session ( ) ` only supports initializing the model by running an ` init_op ` or restoring from the latest checkpoint . if you have special initialization needs , see how to specify a ` local_init_op ` when creating the supervisor . you can also use the ` sessionmanager ` directly to create a session and check if it could be initialized automatically. `` '' ''
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import test as test_lib from tensorflow.tools.compatibility import all_renames_v2
__label__0 # todo ( b/225045380 ) : move to a `` leaf '' library to use in trace_type . @ tf_export ( `` __internal__.nest.is_attrs '' , v1= [ ] ) def is_attrs ( obj ) : `` '' '' returns a true if its input is an instance of an attr.s decorated class . '' '' '' return _is_attrs ( obj )
__label__0 # note : this module should contain * * type definitions only * * .
__label__0 this map can be edited , but it should not be edited once traversal has begun .
__label__0 @ dispatch.dispatch_for_api ( math_ops.add ) def masked_add ( x : maskedtensor , y : maskedtensor , name=none ) : with ops.name_scope ( name ) : return maskedtensor ( x.values + y.values , x.mask & y.mask )
__label__0 lineno = node.func.value.lineno col_offset = node.func.value.col_offset node.func.value = ast_edits.full_name_node ( `` tf.keras.regularizers '' ) node.func.value.lineno = lineno node.func.value.col_offset = col_offset node.func.attr = `` l1 ''
__label__0 # we should be done . self.assertraises ( stopiteration , lambda : next ( rr ) )
__label__0 def testpreparesessionsucceeds ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' deprecation tests . '' '' ''
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` cusparse_ver_major '' , `` cusparse_ver_minor '' , `` cusparse_ver_patch '' ) ) return `` . `` .join ( version )
__label__0 raises : valueerror : if ` dispatch_target ` was not registered using ` @ dispatch_for ` , ` @ dispatch_for_unary_elementwise_apis ` , or ` @ dispatch_for_binary_elementwise_apis ` . `` '' '' found = false
__label__0 if linger_strs : print ( `` warning : below are potentially instances of lingering old version `` `` string \ '' % s\ '' in source directory \ '' % s/\ '' that are not `` `` updated by this script . please check them manually ! '' % ( lingering_string , tf_src_dir ) ) for linger_str in linger_strs : print ( linger_str ) else : print ( `` no lingering old version strings \ '' % s\ '' found in source directory '' `` \ '' % s/\ '' . good . '' % ( lingering_string , tf_src_dir ) )
__label__0 def testmetrics ( self ) : metrics = [ `` accuracy '' , `` auc '' , `` average_precision_at_k '' , `` false_negatives '' , `` false_negatives_at_thresholds '' , `` false_positives '' , `` false_positives_at_thresholds '' , `` mean '' , `` mean_absolute_error '' , `` mean_cosine_distance '' , `` mean_iou '' , `` mean_per_class_accuracy '' , `` mean_relative_error '' , `` mean_squared_error '' , `` mean_tensor '' , `` percentage_below '' , `` precision '' , `` precision_at_k '' , `` precision_at_thresholds '' , `` precision_at_top_k '' , `` recall '' , `` recall_at_k '' , `` recall_at_thresholds '' , `` recall_at_top_k '' , `` root_mean_squared_error '' , `` sensitivity_at_specificity '' , `` sparse_average_precision_at_k '' , `` sparse_precision_at_k '' , `` specificity_at_sensitivity '' , `` true_negatives '' , `` true_negatives_at_thresholds '' , `` true_positives '' , `` true_positives_at_thresholds '' , ] for m in metrics : text = `` tf.metrics . '' + m + `` ( a , b ) '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` tf.compat.v1.metrics . '' + m + `` ( a , b ) '' , new_text ) self.assertin ( `` tf.metrics have been replaced with object oriented versions '' , report )
__label__0 # set max_wait_secs to allow us to try a few times . with self.assertraises ( errors.deadlineexceedederror ) : sm.wait_for_session ( master= '' '' , max_wait_secs=3 )
__label__0 if no_change_to_outfile_on_error and ret [ 0 ] == 0 : os.remove ( temp_file.name ) else : shutil.move ( temp_file.name , out_filename ) return ret
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utility to manipulate resource variables . '' '' ''
__label__0 def testmanagedendofinputonequeue ( self ) : # tests that the supervisor finishes without an error when using # a fixed number of epochs , reading from a single queue . logdir = self._test_dir ( `` managed_end_of_input_one_queue '' ) os.makedirs ( logdir ) data_path = self._csv_data ( logdir ) with ops.graph ( ) .as_default ( ) : # create an input pipeline that reads the file 3 times . filename_queue = input_lib.string_input_producer ( [ data_path ] , num_epochs=3 ) reader = io_ops.textlinereader ( ) _ , csv = reader.read ( filename_queue ) rec = parsing_ops.decode_csv ( csv , record_defaults= [ [ 1 ] , [ 1 ] , [ 1 ] ] ) sv = supervisor.supervisor ( logdir=logdir ) with sv.managed_session ( `` '' ) as sess : while not sv.should_stop ( ) : sess.run ( rec )
__label__0 @ dispatch.dispatch_for_api ( array_ops.concat ) def masked_concat ( values : maskedtensorlist , axis ) : del values , axis
__label__0 del another_handler
__label__0 def test_partial ( self ) : partial = functools.partial ( _test_function , unused_arg=7 ) decorator_utils.validate_callable ( partial , `` test '' )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 default_builder_class = base_page.templatepagebuilder
__label__0 returns : a fullargspec with empty kwonlyargs , kwonlydefaults and annotations. `` '' '' return _convert_maybe_argspec_to_fullargspec ( getargspec ( target ) )
__label__0 tf_modules = [ ] for name , module in sys.modules.items ( ) : # the below for loop is a constant time loop . for package in packages : if name.startswith ( package ) : tf_modules.append ( module )
__label__0 - for modality.data : if ` structure ` is a scalar , ` flat_sequence ` must be a single-element list ; in this case the return value is ` flat_sequence [ 0 ] ` .
__label__0 # generate a metagraphdef containing the while loop . with session.session ( ) as sess : self.evaluate ( init_op ) self.evaluate ( output ) saver = saver_module.saver ( ) saver.save ( sess , saver_ckpt ) saver.export_meta_graph ( filename )
__label__0 def xla_src_root ( ) - > pathlib.path : `` '' '' gets the path to the root of the xla source tree . '' '' '' is_oss = `` bazel_test '' in os.environ test_srcdir = os.environ [ `` test_srcdir '' ] test_workspace = os.environ [ `` test_workspace '' ] if is_oss : return pathlib.path ( test_srcdir ) / test_workspace else : return pathlib.path ( test_srcdir ) / test_workspace / `` third_party '' / `` xla ''
__label__0 # do we have enough time left to try again ? remaining_ms_after_wait = ( timer.secs_remaining ( ) - self._recovery_wait_secs ) if remaining_ms_after_wait < 0 : raise errors.deadlineexceedederror ( none , none , `` session was not ready after waiting % d secs . '' % ( max_wait_secs , ) )
__label__0 self.size_check = size_check super ( ) .__init__ ( proto , proto_size , * * kwargs )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add_n )
__label__0 t = typevar ( `` t '' )
__label__0 returns : the wrapped function ( or the original function if no wrapping is needed ) . `` '' '' if `` name '' not in api_signature.parameters : return func # no wrapping needed ( api has no name parameter ) .
__label__0 args : values : a nested structure of ` resourcevariable ` s , or any other objects .
__label__0 def testaddshouldusewarningwhennotused ( self ) : c = constant_op.constant ( 0 , name='blah0 ' ) def in_this_function ( ) : h = tf_should_use._add_should_use_warning ( c , warn_in_eager=true ) del h with reroute_error ( ) as error : in_this_function ( ) msg = '\n'.join ( error.call_args [ 0 ] ) self.assertin ( 'object was never used ' , msg ) if not context.executing_eagerly ( ) : self.assertin ( 'blah0:0 ' , msg ) self.assertin ( 'in_this_function ' , msg ) self.assertfalse ( gc.garbage )
__label__0 @ tf_export ( `` types.experimental.callable '' , v1= [ ] ) class callable ( metaclass=abc.abcmeta ) : `` '' '' base class for tf callables like those created by tf.function .
__label__0 def testdenseandsparsejobs ( self ) : cluster_def = server_lib.clusterspec ( { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : { 0 : `` worker0:2222 '' , 2 : `` worker2:2222 '' } } ) .as_cluster_def ( ) server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_def , job_name= '' worker '' , task_index=2 , protocol= '' grpc '' )
__label__0 @ _wrap_decorator ( func , 'deprecated ' ) def new_func ( * args , * * kwargs ) : # pylint : disable=missing-docstring if _print_deprecation_warnings : if func not in _printed_warning and cls not in _printed_warning : if warn_once : _printed_warning [ func ] = true if cls : _printed_warning [ cls ] = true _log_deprecation ( 'from % s : % s ( from % s ) is deprecated and will be removed % s.\n ' 'instructions for updating : \n % s ' , _call_location ( ) , decorator_utils.get_qualified_name ( func ) , func_or_class.__module__ , 'in a future version ' if date is none else ( 'after % s ' % date ) , instructions ) return func ( * args , * * kwargs )
__label__0 def _try_run_local_init_op ( self , sess : session.session ) - > tuple [ bool , optional [ str ] ] : `` '' '' tries to run _local_init_op , if not none , and is ready for local init .
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_concat )
__label__0 returns : a string describing the caller source location. `` '' '' frame = tf_inspect.currentframe ( ) assert frame.f_back.f_code.co_name == '_tfmw_add_deprecation_warning ' , ( 'this function should be called directly from ' '_tfmw_add_deprecation_warning , as the caller is identified ' 'heuristically by chopping off the top stack frames . ' )
__label__0 traversing system modules can take a long time , it is advisable to pass a ` visit ` callable which denylists such modules .
__label__0 `` ` python @ dispatch_for_types ( math_ops.add , raggedtensor , raggedtensorvalue ) def ragged_add ( x , y , name=none ) : ... `` `
__label__0 def __delitem__ ( self , key ) : del self._storage [ self._wrap_key ( key ) ]
__label__0 def testadddispatchfortypes_with_cppop ( self ) : original_handlers = gen_math_ops.atan2._tf_fallback_dispatchers [ : ]
__label__0 test_obj = callable ( ) self.assertequal ( argspec , tf_inspect.getargspec ( test_obj ) )
__label__0 after :
__label__0 result [ `` rocm_toolkit_path '' ] = rocm_install_path result.update ( _find_rocm_config ( rocm_install_path ) ) result.update ( _find_hipruntime_config ( rocm_install_path ) ) result.update ( _find_miopen_config ( rocm_install_path ) ) result.update ( _find_rocblas_config ( rocm_install_path ) ) result.update ( _find_rocrand_config ( rocm_install_path ) ) result.update ( _find_rocfft_config ( rocm_install_path ) ) if result [ `` rocm_version_number '' ] > = 40100 : result.update ( _find_hipfft_config ( rocm_install_path ) ) result.update ( _find_roctracer_config ( rocm_install_path ) ) result.update ( _find_hipsparse_config ( rocm_install_path ) ) if result [ `` rocm_version_number '' ] > = 40500 : result.update ( _find_hipsolver_config ( rocm_install_path ) ) result.update ( _find_rocsolver_config ( rocm_install_path ) )
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' lazy loader tests . '' '' ''
__label__0 def testgetargspeconpartialvalidargspec ( self ) : `` '' '' tests getargspec on partial function with valid argspec . '' '' ''
__label__0 class deprecatedargstest ( test.testcase ) :
__label__0 # the use of while_loop.while_loop here is purely for adding test # coverage the save and restore of control flow context ( which does n't # make any sense here from a machine learning perspective ) . the typical # biases is a simple variable without the conditions . def loop_cond ( it , _ ) : return it < 2
__label__0 to implement this as protos it 's broken up into sub-messages as follows :
__label__0 class classproperty ( object ) : # pylint : disable=invalid-name `` '' '' class property decorator .
__label__0 > > > tf.nest.map_structure ( lambda x : x + 1 , ( ) ) ( )
__label__0 @ parameterized.parameters ( [ dict ( inputs= [ ] , expected= [ ] ) , dict ( inputs= [ 23 , `` 42 '' ] , expected= [ ( `` 0 '' , 23 ) , ( `` 1 '' , `` 42 '' ) ] ) , dict ( inputs= [ [ [ [ 108 ] ] ] ] , expected= [ ( `` 0/0/0/0 '' , 108 ) ] ) , dict ( inputs=foo ( a=3 , b=bar ( c=23 , d=42 ) ) , expected= [ ( `` a '' , 3 ) , ( `` b/c '' , 23 ) , ( `` b/d '' , 42 ) ] ) , dict ( inputs=foo ( a=bar ( c=23 , d=42 ) , b=bar ( c=0 , d= '' thing '' ) ) , expected= [ ( `` a/c '' , 23 ) , ( `` a/d '' , 42 ) , ( `` b/c '' , 0 ) , ( `` b/d '' , `` thing '' ) ] ) , dict ( inputs=bar ( c=42 , d=43 ) , expected= [ ( `` c '' , 42 ) , ( `` d '' , 43 ) ] ) , dict ( inputs=bar ( c= [ 42 ] , d=43 ) , expected= [ ( `` c/0 '' , 42 ) , ( `` d '' , 43 ) ] ) , ] ) def testflattenwithstringpaths ( self , inputs , expected ) : self.assertequal ( nest.flatten_with_joined_string_paths ( inputs , separator= '' / '' ) , expected )
__label__0 class child2 ( parent ) : def method1 ( self ) : pass def method2 ( self ) : pass `` `
__label__0 logging.info ( `` restoring parameters from % s '' , checkpoint_prefix ) try : if context.executing_eagerly ( ) : self._build_eager ( save_path , build_save=false , build_restore=true ) else : sess.run ( self.saver_def.restore_op_name , { self.saver_def.filename_tensor_name : save_path } ) except errors.notfounderror as err : # there are three common conditions that might cause this error : # 0. the file is missing . we ignore here , as this is checked above . # 1. this is an object-based checkpoint trying name-based loading . # 2. the graph has been altered and a variable or other name is missing .
__label__0 self.generic_visit ( node )
__label__0 text = `` tf.nn.dropout ( x , keep_prob=.4 , name=\ '' foo\ '' ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.nn.dropout ( x , rate=1 - ( .4 ) , name=\ '' foo\ '' ) \n '' , )
__label__0 # pylint : disable=g-import-not-at-top , g-importing-member try : from shutil import which except importerror : from distutils.spawn import find_executable as which # pylint : enable=g-import-not-at-top , g-importing-member
__label__0 def __len__ ( self ) : return len ( self._storage )
__label__0 # # # # use for multiple replicas
__label__0 def find_files ( pattern , root ) : `` '' '' return all the files matching pattern below root dir . '' '' '' for dirpath , _ , files in os.walk ( root ) : for filename in fnmatch.filter ( files , pattern ) : yield os.path.join ( dirpath , filename )
__label__0 we also test whether a converted file is executable . test_file_v0_11.py aims to exhaustively test that api changes are convertible and actually work when run with current tensorflow. `` '' ''
__label__0 # restores into a different number of partitions . restored_full = _restore ( partitioner=partitioned_variables.fixed_size_partitioner ( num_shards=3 ) ) self.assertallequal ( saved_full , restored_full )
__label__0 def main ( ) : parser = argparse.argumentparser ( formatter_class=argparse.rawdescriptionhelpformatter , description= '' '' '' convert a tensorflow python file from 1.x to 2.0
__label__0 class _customsequencethatraisesexception ( collections.abc.sequence ) :
__label__0 args : min_length : the minimum length of the list . max_length : the maximum length of the list .
__label__0 upgrade_dir = os.path.join ( self.get_temp_dir ( ) , `` foo '' ) output_dir = os.path.join ( self.get_temp_dir ( ) , `` bar '' ) os.mkdir ( upgrade_dir ) file_a = os.path.join ( upgrade_dir , `` a.py '' ) file_b = os.path.join ( upgrade_dir , `` b.py '' )
__label__0 partial_func = functools.partial ( func , 7 ) argspec = tf_inspect.argspec ( args= [ 'n ' ] , varargs=none , keywords='kwarg ' , defaults=none )
__label__0 calls the ` handle ` method of each ` opdispatcher ` that has been registered to handle ` op ` , and returns the value from the first successful handler .
__label__0 miopen_config = { `` miopen_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 @ tf_export ( `` types.experimental.tracetype '' , v1= [ ] ) class tracetype ( metaclass=abc.abcmeta ) : `` '' '' represents the type of object ( s ) for tf.function tracing purposes .
__label__0 returns : true if the node uses starred variadic positional args or keyword args . false if it does not. `` '' '' if sys.version_info [ :2 ] > = ( 3 , 5 ) : # check for a * * kwarg usage in python 3.5+ for keyword in node.keywords : if keyword.arg is none : return true else : if node.kwargs : return true return false
__label__0 > > > import collections > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( `` bar '' , `` a b '' ) ( 1 , 2 ) , ... collections.namedtuple ( `` foo '' , `` a b '' ) ( 2 , 3 ) , ... check_types=false )
__label__0 self.assertequal ( ( ' a ' , ' b ' ) , function_utils.fn_args ( foo ( ) ) )
__label__0 examples :
__label__0 # # both non-list edge-case . # using iterable elements . input_tree = `` input_tree '' shallow_tree = `` shallow_tree '' flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 returns : all the modules in the submodule. `` '' ''
__label__0 def build_chunks ( self ) - > none : `` '' '' builds the splitter object by generating chunks from the proto .
__label__0 # todo ( mdan ) : consider adding abc once the dependence on isinstance is reduced . # todo ( mdan ) : add type annotations .
__label__0 returns : bytes representation of the constant tensor content. `` '' '' if node.op == _const_op : tensor_proto = node.attr [ `` value '' ] .tensor if tensor_proto.tensor_content : b = tensor_proto.tensor_content else : # the raw tensor value could be stored in one of the `` xxx_val '' attributes . # extract it here , and convert to bytes . b = tensor_util.makendarray ( tensor_proto ) .tobytes ( )
__label__0 def func ( a=13 ) : return a
__label__0 def _init_fn ( sess ) : sess.run ( v.initializer )
__label__0 with self.assertraisesregex ( valueerror , ( `` do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\nsecond structure : `` ) ) : nest.assert_same_structure ( [ [ 3 ] , 4 ] , [ 3 , [ 4 ] ] )
__label__0 if checkpoint_filename_with_path : _restore_checkpoint_and_maybe_run_saved_model_initializers ( sess , saver , checkpoint_filename_with_path ) return sess , true
__label__0 def _fn ( arg0 , arg1 , deprecated=none ) : return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 def testrandompoissonconversion ( self ) : text1 = `` tf.random_poisson ( lam , shape , dtype ) '' text2 = `` tf.random.poisson ( lam , shape , dtype ) '' expected_text = `` tf.random.poisson ( lam=lam , shape=shape , dtype=dtype ) '' _ , unused_report , unused_errors , new_text1 = self._upgrade ( text1 ) self.assertequal ( new_text1 , expected_text ) _ , unused_report , unused_errors , new_text2 = self._upgrade ( text2 ) self.assertequal ( new_text2 , expected_text )
__label__0 for signature_checker in signature_checkers : dispatcher.register ( signature_checker , dispatch_target ) _type_based_dispatch_signatures [ api ] [ dispatch_target ] .extend ( signatures )
__label__0 class myclass ( object ) :
__label__0 self.change_to_function = { `` tf.ones_initializer '' , `` tf.zeros_initializer '' , }
__label__0 self.assertequal ( ( ' a ' , ) , function_utils.fn_args ( double_wrapped_fn ) )
__label__0 # save the initialized values in the file at `` save_path '' # use a variable name map to set the saved tensor names val = save.save ( sess , save_path ) self.assertisinstance ( val , str ) self.assertequal ( save_path , val )
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( 1 , 2 , true , false ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 sparse_types = parse_example_op.get_attr ( `` sparse_types '' ) dense_types = parse_example_op.get_attr ( `` tdense '' ) dense_shapes = parse_example_op.get_attr ( `` dense_shapes '' )
__label__0 def _testgradientserdes ( self , graph_fn ) : `` '' '' tests that gradients can be computed after exporting and importing .
__label__0 later this model can be restored and contents loaded .
__label__0 # clean up gen_math_ops.atan2._tf_fallback_dispatchers = original_handlers
__label__0 import glob import os import shutil import time import uuid
__label__0 * ` log_level ` : the log level to which this detection should be logged * ` log_message ` : the message that should be logged for this detection
__label__0 # the saver sorts by name before parsing , so we need a name property . @ property def name ( self ) : return self.non_dep_variable.name
__label__0 with self.assertraisesregex ( valueerror , `` dispatch currently only supports type `` `` annotations for positional parameters '' ) :
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflatten ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) leaves = nest.flatten ( mt ) self.assertlen ( leaves , 1 ) self.assertallequal ( leaves [ 0 ] , [ 1 ] )
__label__0 s2 = save.save ( none , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s1 , s2 ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s1 , s2 ] , save_dir=save_dir )
__label__0 def testmultiplesessions ( self ) : with self.cached_session ( ) as sess : with session.session ( ) as other_sess : zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) self.evaluate ( variables.global_variables_initializer ( ) ) coord = coordinator.coordinator ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) # note that this test does not actually start the threads . threads = qr.create_threads ( sess , coord=coord ) other_threads = qr.create_threads ( other_sess , coord=coord ) self.assertequal ( len ( threads ) , len ( other_threads ) )
__label__0 raises : runtimeerror : if called with eager execution enabled .
__label__0 < < api_list > > `` '' ''
__label__0 1. start a step : fetch variables and compute gradients . 2. once the gradients have been computed , push them into gradient accumulators . each accumulator will check the staleness and drop the stale . 3. after pushing all the gradients , dequeue an updated value of global_step from the token queue and record that step to its local_step variable . note that this is effectively a barrier . 4. start the next batch .
__label__0 if not op_name.startswith ( `` _ '' ) : path = pathlib.path ( `` / '' ) / flags.site_path / `` tf/raw_ops '' / op_name path = path.with_suffix ( `` .md '' ) link = ( ' < a id= { op_name } href= '' { path } '' > { op_name } < /a > ' ) .format ( op_name=op_name , path=str ( path ) ) parts.append ( `` | { link } | { has_gradient } | { is_xla_compilable } | '' .format ( link=link , has_gradient=has_gradient , is_xla_compilable=is_xla_compilable , ) )
__label__0 major , minor , patch = rocsolver_version_numbers ( rocm_install_path )
__label__0 # their types are not namedtuple values themselves . self.assertfalse ( nest.is_namedtuple ( foo ) ) self.assertfalse ( nest.is_namedtuple ( subfoo ) ) self.assertfalse ( nest.is_namedtuple ( typedfoo ) )
__label__0 from tensorflow.python.framework import _pywrap_python_api_dispatcher as _api_dispatcher from tensorflow.python.framework import ops from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_export as tf_export_lib from tensorflow.python.util import tf_inspect from tensorflow.python.util import traceback_utils from tensorflow.python.util import type_annotations from tensorflow.python.util.tf_export import tf_export
__label__0 chunk_info = proto.chunks [ i ] self.assertequal ( chunk_pb2.chunkinfo.type.bytes , chunk_info.type ) self.assertequal ( len ( expected_data ) , chunk_info.size ) reader.seek_numeric ( chunk_info.offset ) self.assertequal ( expected_data , reader.read_record ( ) )
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_int ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_int : [ np.zeros ( [ 10 , 1 ] ) ] } , sess )
__label__0 save = saver_module.saver ( { `` real_num '' : real_num , `` imag_num '' : imag_num } ) variables.global_variables_initializer ( )
__label__0 # property is computed first time it is accessed . self.assertlen ( log , 0 ) self.assertequal ( myclass.value , `` myclass '' ) self.assertequal ( log , [ myclass ] )
__label__0 if attr : class badattr ( object ) : `` '' '' class that has a non-iterable __attrs_attrs__ . '' '' '' __attrs_attrs__ = none
__label__0 return config
__label__0 new_line_node = pasta.parse ( line_to_insert ) ast.copy_location ( new_line_node , node ) parent.body.insert ( parent.body.index ( node ) + insert_offset , new_line_node ) insert_offset += 1
__label__0 with self.assertraisesregex ( valueerror , `` only valid keyword argument . * foo '' ) : nest.map_structure ( lambda x : none , structure1 , check_types=false , foo= '' a '' )
__label__0 args : path : an object that can be converted to path representation .
__label__0 flat_structure = ( _tf_data_flatten ( s ) for s in structure ) entries = zip ( * flat_structure )
__label__0 def teststringconversion ( self ) : cluster_spec = server_lib.clusterspec ( { `` ps '' : [ `` ps0:1111 '' ] , `` worker '' : [ `` worker0:3333 '' , `` worker1:4444 '' ] } )
__label__0 ` traverse_fn ` must accept any possible subtree of ` structure ` and return a depth=1 structure containing ` true ` or ` false ` values , describing which of the top-level subtrees may be traversed . it may also return scalar ` true ` or ` false ` `` traversal is ok / not ok for all subtrees . ''
__label__0 from tensorflow.python.framework import test_util from tensorflow.python.platform import googletest from tensorflow.python.util import type_annotations
__label__0 # # everything below here is legitimate . # it 'll stay , but it 's not officially part of the api . 'tf.app ' : [ 'flags ' ] , # imported for compatibility between py2/3 . 'tf.test ' : [ 'mock ' ] , }
__label__0 returns : consumed float based on input bytes and constraints. `` '' '' return self.fdp.consumefloatinrange ( min_float , max_float )
__label__0 def visit_importfrom ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting an import-from node in the ast .
__label__0 def test_raises_type_error ( self ) : with self.assertraisesregex ( typeerror , 'should be a callable ' ) : function_utils.has_kwargs ( 'not a function ' )
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( arg0 , arg1 ) : `` '' '' fn doc . '' '' '' return arg0 + arg1
__label__0 class mymodule2 : pass module = mymodule2 ( ) self.assertfalse ( tf_inspect.isanytargetmethod ( module ) ) def f2 ( x ) : del x self.assertfalse ( tf_inspect.isanytargetmethod ( f2 ) ) f2 = functools.partial ( f2 , 1 ) self.assertfalse ( tf_inspect.isanytargetmethod ( f2 ) ) f2 = test_decorator ( 'tf_decorator1 ' ) ( f2 ) self.assertfalse ( tf_inspect.isanytargetmethod ( f2 ) ) f2 = test_decorator ( 'tf_decorator2 ' ) ( f2 ) self.assertfalse ( tf_inspect.isanytargetmethod ( f2 ) ) self.assertfalse ( tf_inspect.isanytargetmethod ( lambda : none ) ) self.assertfalse ( tf_inspect.isanytargetmethod ( none ) ) self.assertfalse ( tf_inspect.isanytargetmethod ( 1 ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassmapstructureupto ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt2 = maskedtensor ( mask=true , value=constant_op.constant ( [ 2 ] ) ) mt3 = maskedtensor ( mask=true , value=constant_op.constant ( [ 3 ] ) ) mt_out_template = maskedtensor ( mask=false , value=constant_op.constant ( [ 4 ] ) )
__label__0 def testusesessionmanager ( self ) : with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] ) sm = session_manager_lib.sessionmanager ( ) # pass in session_manager . the additional init_op is ignored . sv = supervisor.supervisor ( logdir= '' '' , session_manager=sm ) sv.prepare_or_wait_for_session ( `` '' )
__label__0 uses utf-8 encoding for text by default .
__label__0 4. numpy array ( will not flatten ) :
__label__0 def __call__ ( self , string ) : `` '' '' extracts floats from a string .
__label__0 # first check if the entire functiondef can be split into a separate chunk . # we do this before the ` repeatedmessagesplitter ` , which is costly because # it iterates through every ` node_def ` . if _greedy_split ( self.proto_size ) and not _above_max_size ( self.proto_size ) : size_diff += largemessagesplitter ( self._proto , self.proto_size , parent_splitter=self , fields_in_parent= [ ] , ) .build_chunks ( )
__label__0 class _syncreplicasoptimizerhook ( session_run_hook.sessionrunhook ) : `` '' '' a sessionrunhook handles ops related to syncreplicasoptimizer . '' '' ''
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops as ops_lib from tensorflow.python.ops import gen_lookup_ops from tensorflow.python.training import saver as saver_module
__label__0 # providing non-keyword args should fail . with self.assertraisesregex ( valueerror , `` only accepts keyword arguments '' ) : self.assertequal ( 3 , func_with_decorator ( 1 , 2 ) )
__label__0 def testcontribxavierinitializer ( self ) : for contrib_alias in [ `` tf.contrib . `` , `` contrib_ '' ] : text = contrib_alias + `` layers.xavier_initializer ( ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution=\ '' uniform\ '' ) \n '' , )
__label__0 a ` tf.train.clusterspec ` represents the set of processes that participate in a distributed tensorflow computation . every ` tf.distribute.server ` is constructed in a particular cluster .
__label__0 def print_hello_before_calling ( target ) : def wrapper ( * args , * * kwargs ) : print ( 'hello ' ) return target ( * args , * * kwargs ) return tf_decorator.make_decorator ( target , wrapper )
__label__0 text = `` tf.debugging. % s ( a ) '' % name expected_text = `` tf.compat.v1.debugging. % s ( a ) '' % name _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` % s has been '' % name , report )
__label__0 def testremovemultiplekeywords ( self ) : `` '' '' remove multiple keywords at once . '' '' '' # not using deprecated keywords - > no rename text = `` h ( a , kw1=x , kw2=y ) \n '' _ , new_text = self._upgrade ( removemultiplekeywordarguments ( ) , text ) self.assertequal ( new_text , text )
__label__0 self.saver_def = self._builder._build_internal ( # pylint : disable=protected-access self._var_list , reshape=self._reshape , sharded=self._sharded , max_to_keep=self._max_to_keep , keep_checkpoint_every_n_hours=self._keep_checkpoint_every_n_hours , name=self._name , restore_sequentially=self._restore_sequentially , filename=checkpoint_path , build_save=build_save , build_restore=build_restore ) elif self.saver_def and self._name : # since self._name is used as a name_scope by builder ( ) , we are # overloading the use of this field to represent the `` import_scope '' as # well . self.saver_def.filename_tensor_name = ops.prepend_name_scope ( self.saver_def.filename_tensor_name , self._name ) self.saver_def.save_tensor_name = ops.prepend_name_scope ( self.saver_def.save_tensor_name , self._name ) self.saver_def.restore_op_name = ops.prepend_name_scope ( self.saver_def.restore_op_name , self._name )
__label__0 def test_deprecated_illegal_args ( self ) : instructions = `` this is how you update ... '' with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated_arg_values ( `` '' , instructions , deprecated=true ) with self.assertraisesregex ( valueerror , `` yyyy-mm-dd '' ) : deprecation.deprecated_arg_values ( `` 07-04-2016 '' , instructions , deprecated=true ) date = `` 2016-07-04 '' with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated_arg_values ( date , none , deprecated=true ) with self.assertraisesregex ( valueerror , `` instructions '' ) : deprecation.deprecated_arg_values ( date , `` '' , deprecated=true ) with self.assertraisesregex ( valueerror , `` argument '' ) : deprecation.deprecated_arg_values ( date , instructions )
__label__0 def _add_dispatch_for_binary_elementwise_api ( api , x_type , y_type , elementwise_api_handler ) : `` '' '' registers a binary elementwise handler as a dispatcher for a given api . '' '' '' api_signature = tf_inspect.signature ( api ) x_name , y_name = list ( api_signature.parameters ) [ :2 ] name_index = _find_name_index ( api_signature )
__label__0 return decorator_func
__label__0 if you pass a ` graph ` to the constructor it is added to the event file . ( this is equivalent to calling ` add_graph ( ) ` later ) .
__label__0 def blocking_dequeue ( ) : with self.assertraisesregex ( errors_impl.cancellederror , `` session : :close '' ) : sess.run ( dequeue_t )
__label__0 # assert calls with the deprecated arguments log warnings . self.assertequal ( 2 , _fn ( 1 , none , 2 , d2=false ) ) self.assertequal ( 2 , mock_warning.call_count ) ( args1 , _ ) = mock_warning.call_args_list [ 0 ] self.assertregex ( args1 [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions , `` d1 '' ] ) , set ( args1 [ 1 : ] ) ) ( args2 , _ ) = mock_warning.call_args_list [ 1 ] self.assertregex ( args2 [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions , `` d2 '' ] ) , set ( args2 [ 1 : ] ) )
__label__0 # you can create the hook which handles initialization and queues . sync_replicas_hook = opt.make_session_run_hook ( is_chief ) `` `
__label__0 args : docstring : the docstring to normalize
__label__0 args : seq : the value to test .
__label__0 def get_git_version ( git_base_path , git_tag_override ) : `` '' '' get the git version from the repository .
__label__0 def testdispatcherrorforbadapi ( self ) :
__label__0 for each file , the uploader reads the `` testresults '' data , transforms it into the schema used in the datastore ( see below ) , and upload it to the datastore . after processing a file , the uploader moves it to a specified archive directory for safe-keeping .
__label__0 # it should get renamed and reordered even if it 's not in order text = `` g2 ( a , b , d=d , c=c , kw1_alias=x ) \n '' acceptable_outputs = [ `` g2 ( a , b , kw1=x , c=c , d=d ) \n '' , `` g2 ( a , b , c=c , d=d , kw1=x ) \n '' , `` g2 ( a , b , d=d , c=c , kw1=x ) \n '' , `` g2 ( a=a , b=b , kw1=x , c=c , d=d ) \n '' , `` g2 ( a=a , b=b , c=c , d=d , kw1=x ) \n '' , `` g2 ( a=a , b=b , d=d , c=c , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 `` ` python code `` `
__label__0 # use only the old names , in reverse order , should give one of same outputs text = `` h ( a , kw2_alias=y , kw1_alias=x ) \n '' _ , new_text = self._upgrade ( removemultiplekeywordarguments ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 def testwarmstartvarwithcolumnvocabbothvarspartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_output_layer '' , shape= [ 4 , 2 ] , initializer= [ [ 0.5 , 0.3 ] , [ 1. , 0.8 ] , [ 1.5 , 1.2 ] , [ 2. , 2.3 ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] )
__label__0 for input_tree in inputs : _tf_core_assert_shallow_structure ( shallow_tree , input_tree , check_types=check_types , expand_composites=expand_composites , )
__label__0 def _find_cufft_config ( base_paths , required_version , cuda_version ) :
__label__0 if dtype is none : dtype = self.get_tf_dtype ( allowed_set=_tf_random_dtypes ) elif dtype not in _tf_random_dtypes : raise tf.errors.invalidargumenterror ( none , none , 'given dtype { } is not accepted in get_random_numeric_tensor'.format ( dtype ) )
__label__0 args : opt : the actual optimizer that will be used to compute and apply the gradients . must be one of the optimizer classes . replicas_to_aggregate : number of replicas to aggregate for each variable update . total_num_replicas : total number of tasks/workers/replicas , could be different from replicas_to_aggregate . if total_num_replicas > replicas_to_aggregate : it is backup_replicas + replicas_to_aggregate . if total_num_replicas < replicas_to_aggregate : replicas compute multiple batches per update to variables . variable_averages : optional ` exponentialmovingaverage ` object , used to maintain moving averages for the variables passed in ` variables_to_average ` . variables_to_average : a list of variables that need to be averaged . only needed if variable_averages is passed in . use_locking : if true use locks for update operation . name : string . optional name of the returned operation. `` '' '' if total_num_replicas is none : total_num_replicas = replicas_to_aggregate
__label__0 from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_export
__label__0 import_header = ( `` import tensorflow as tf\n '' `` import tensorflow.compat.v1 as tf_v1\n '' `` import tensorflow.compat.v2 as tf_v2\n '' ) text = import_header + old_symbol expected_text = import_header + new_symbol _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 args : metadata : a custom python objet that stands for the static config for reconstructing a new object of the current class . components : a ` tuple ` that contains the dynamic data fields of the current class , for object reconstruction .
__label__0 returns : obj `` '' ''
__label__0 example usage :
__label__0 the dispatch target 's signature must match the signature of the api that is being overridden . in particular , parameters must have the same names , and must occur in the same order . the dispatch target may optionally elide the `` name '' parameter , in which case it will be wrapped with a call to ` tf.name_scope ` when appropraite .
__label__0 # make sure 0 is a tf.nest atom with expand_composites=true . flat_results = nest.flatten ( results , expand_composites=true ) expected_flat_results = [ 0 , 0 , 3 , 4 , 5 ] self.assertallequal ( flat_results , expected_flat_results )
__label__0 for i in range ( device_count.value ) : properties = cudadeviceproperties ( ) rc = libcudart.cudagetdeviceproperties ( ct.byref ( properties ) , i ) if rc ! = 0 : raise valueerror ( `` could not get device properties '' ) pci_bus_id = `` `` * 13 rc = libcudart.cudadevicegetpcibusid ( ct.c_char_p ( pci_bus_id ) , 13 , i ) if rc ! = 0 : raise valueerror ( `` could not get device pci bus id '' )
__label__0 the decorated function ( known as the `` elementwise assert handler '' ) overrides the default implementation for any binary elementwise assert api whenever the value for the first two arguments ( typically named ` x ` and ` y ` ) match the specified type annotations . the handler is called with two arguments :
__label__0 class tfinspectgetcallargstest ( test.testcase ) :
__label__0 # it should get renamed even if it 's last text = `` g ( a , b , c=c , kw1_alias=x ) \n '' acceptable_outputs = [ `` g ( a , b , kw1=x , c=c ) \n '' , `` g ( a , b , c=c , kw1=x ) \n '' , `` g ( a=a , b=b , kw1=x , c=c ) \n '' , `` g ( a=a , b=b , c=c , kw1=x ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 from tensorflow.python import pywrap_tensorflow # pylint : disable=unused-import , g-bad-import-order from tensorflow.python.util.tf_export import tf_export
__label__0 def testclassmethod ( self ) :
__label__0 import collections import typing import numpy as np
__label__0 def get_effective_source_map ( self ) : return empty_dict
__label__0 import ctypes as ct import platform
__label__0 @ parameterized.parameters ( { `` mapping_type '' : collections.ordereddict } , { `` mapping_type '' : _custommapping } ) @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testflattendictorder ( self , mapping_type ) : `` '' '' ` flatten ` orders dicts by key , including ordereddicts . '' '' '' ordered = mapping_type ( [ ( `` d '' , 3 ) , ( `` b '' , 1 ) , ( `` a '' , 0 ) , ( `` c '' , 2 ) ] ) plain = { `` d '' : 3 , `` b '' : 1 , `` a '' : 0 , `` c '' : 2 } ordered_flat = nest.flatten ( ordered ) plain_flat = nest.flatten ( plain ) self.assertequal ( [ 0 , 1 , 2 , 3 ] , ordered_flat ) self.assertequal ( [ 0 , 1 , 2 , 3 ] , plain_flat )
__label__0 # fetch indices . sparse_keys_start = 0 dense_keys_start = sparse_keys_start + num_sparse dense_def_start = dense_keys_start + num_dense
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 self.assertequal ( { ' a ' : 1 , ' b ' : 4 } , tf_inspect.getcallargs ( func , b=4 ) )
__label__0 class getfunccodetest ( test.testcase ) :
__label__0 this is supposed to be executed in the beginning of the chief/sync thread so that even if the total_num_replicas is less than replicas_to_aggregate , the model can still proceed as the replicas can compute multiple steps per variable update . make sure : ` num_tokens > = replicas_to_aggregate - total_num_replicas ` .
__label__0 def _tf_core_yield_sorted_items ( iterable ) : `` '' '' yield ( key , value ) pairs for ` iterable ` in a deterministic order .
__label__0 def testadddispatchfortypes_with_pythonop ( self ) : original_handlers = test_op._tf_fallback_dispatchers [ : ]
__label__0 returns : an ` tf.distribute.distributediterator ` instance for the given ` tf.distribute.distributeddataset ` object to enumerate over the distributed data. `` '' '' raise notimplementederror ( `` must be implemented in descendants '' )
__label__0 # pip_package_dependencies_list is the list of included files in pip packages pip_package_dependencies = subprocess.check_output ( [ `` bazel '' , `` cquery '' , `` -- experimental_cc_shared_library '' , pip_package_query_expression ] ) if isinstance ( pip_package_dependencies , bytes ) : pip_package_dependencies = pip_package_dependencies.decode ( `` utf-8 '' ) pip_package_dependencies_list = pip_package_dependencies.strip ( ) .split ( `` \n '' ) pip_package_dependencies_list = [ x.split ( ) [ 0 ] for x in pip_package_dependencies_list ] print ( `` pip package superset size : % d '' % len ( pip_package_dependencies_list ) )
__label__0 parser.add_argument ( `` -- generate '' , type=str , help= '' generate given spec-file , head-symlink-file , ref-symlink-file '' , nargs= '' + '' )
__label__0 # restore different ops from shard 1 of the saved files . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : v1 = variable_v1.variablev1 ( 222 ) t1 = saver_test_utils.checkpointedop ( name= '' t1 '' ) save = saver_module.saver ( { `` v1 '' : v1 , `` t1 '' : t1.saveable } , write_version=self._write_version , sharded=true ) self.evaluate ( variables.global_variables_initializer ( ) ) t1.insert ( `` k22 '' , 44.0 ) .run ( ) self.assertequal ( 222 , self.evaluate ( v1 ) ) self.assertequal ( b '' k22 '' , self.evaluate ( t1.keys ( ) ) ) self.assertequal ( 44.0 , self.evaluate ( t1.values ( ) ) ) save.restore ( sess , save_path + `` -00001-of-00002 '' ) self.assertequal ( 20 , self.evaluate ( v1 ) ) self.assertequal ( b '' k2 '' , self.evaluate ( t1.keys ( ) ) ) self.assertequal ( 40.0 , self.evaluate ( t1.values ( ) ) )
__label__0 def stop ( self , threads=none , close_summary_writer=true , ignore_live_threads=false ) : `` '' '' stop the services and the coordinator .
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_export
__label__0 with self.assertraisesregex ( valueerror , ( `` the two structures do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\n '' `` second structure : . * \n\n '' `` more specifically : substructure `` r ' '' type=tuple str=\ ( \ ( 1 , 2\ ) , 3\ ) '' is a sequence , while ' 'substructure `` type=str str=spam '' is not\n ' `` entire first structure : \n '' r '' \ ( \ ( \ ( \. , \.\ ) , \.\ ) , \. , \ ( \. , \.\ ) \ ) \n '' `` entire second structure : \n '' r '' \ ( \. , \.\ ) '' ) ) : nest.assert_same_structure ( structure1 , structure_different_num_elements )
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================= `` '' '' tests for variable_utils . '' '' ''
__label__0 def extract_example_parser_configuration ( parse_example_op , sess ) : `` '' '' returns an exampleparserconfig proto .
__label__0 def testgetfullargspecondecoratorthatchangesfullargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def testmultiplecallsofseparateinstances ( self ) : x = [ ] with test_yield_append_before_and_after_yield ( x , 1 , 2 ) : pass with test_yield_append_before_and_after_yield ( x , 3 , 4 ) : pass self.assertequal ( [ 1 , 2 , 3 , 4 ] , x )
__label__0 class mockmodule ( types.moduletype ) : pass
__label__0 # start a second session . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0_wrong_dtype = variable_op ( 1 , name= '' v0 '' , dtype=dtypes.int32 ) # restore the saved value with different dtype # in the parameter nodes . save = saver_module.saver ( { `` v0 '' : v0_wrong_dtype } ) with self.assertraisesregex ( errors.invalidargumenterror , `` original dtype '' ) : save.restore ( sess , save_path )
__label__0 @ tf_export ( 'debugging.enable_traceback_filtering ' ) def enable_traceback_filtering ( ) : `` '' '' enable filtering out tensorflow-internal frames in exception stack traces .
__label__0 def testgetuserframes ( self ) :
__label__0 @ deprecation.deprecated_arg_values ( date , instructions , warn_once=false , deprecated=true ) def _fn ( arg0 , arg1 , deprecated=true ) : `` '' '' fn doc . '' '' '' return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 # the names are different and will work . saver_module.saver ( { `` vee1 '' : v1 , `` other '' : [ v2 ] } )
__label__0 if a function is assigned to a variable , you can export it by calling tf_export explicitly . for e.g . : `` ` python foo = get_foo ( ... ) tf_export ( 'foo ' , 'bar.foo ' ) ( foo ) `` `
__label__0 self.assertequal ( [ 0 , 1 ] , cluster_spec.task_indices ( `` ps '' ) ) self.assertequal ( [ 0 , 1 , 2 ] , cluster_spec.task_indices ( `` worker '' ) ) self.assertequal ( [ 0 , 3 ] , cluster_spec.task_indices ( `` sparse '' ) ) with self.assertraises ( valueerror ) : cluster_spec.task_indices ( `` unknown '' )
__label__0 returns : a ` queuerunner ` for chief to execute .
__label__0 def testclosecancelsblockingoperation ( self ) : server = self._cached_server with ops.graph ( ) .as_default ( ) : sess = session.session ( server.target , config=self._userpcconfig ( ) )
__label__0 define a dummy model and loss :
__label__0 args : master : ` string ` representation of the tensorflow master to use . config : optional configproto proto used to configure the session . max_wait_secs : maximum time to wait for the session to become available .
__label__0 def _assert_subset ( self , expected_subset , actual_set ) : self.asserttrue ( actual_set.issuperset ( expected_subset ) , msg= '' % s is not a superset of % s . '' % ( actual_set , expected_subset ) )
__label__0 def _get_code ( input_file ) : `` '' '' loads the ipynb file and returns a list of codelines . '' '' ''
__label__0 def testthreads ( self ) : with self.cached_session ( ) as sess : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) self.evaluate ( variables.global_variables_initializer ( ) ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to , _mockop ( `` bad_op '' ) ] ) threads = qr.create_threads ( sess , start=true ) self.assertequal ( sorted ( t.name for t in threads ) , [ `` queuerunnerthread-fifo_queue-countupto:0 '' , `` queuerunnerthread-fifo_queue-bad_op '' ] ) for t in threads : t.join ( ) exceptions = qr.exceptions_raised self.assertequal ( 1 , len ( exceptions ) ) self.asserttrue ( `` operation not in the graph '' in str ( exceptions [ 0 ] ) )
__label__0 def testnestedwhileloopsserdes ( self ) : # test two simple nested while loops . def body ( i , x ) : _ , r = while_loop.while_loop ( lambda j , y : j < 3 , lambda j , y : ( j + 1 , y + x ) , [ 0 , 0.0 ] ) return i + 1 , x + r self._testwhileloopandgradientserdes ( body )
__label__0 upload_benchmark_files ( options )
__label__0 self.module_deprecations = module_deprecations_v2.module_deprecations
__label__0 _stack_dict = none # subclasses should override _thread_key = none
__label__0 @ test.mock.patch.object ( logging , 'warning ' , autospec=true ) def testdeprecationwarnings ( self , mock_warning ) : module = mockmodule ( 'test ' ) module.foo = 1 module.bar = 2 module.baz = 3 all_renames_v2.symbol_renames [ 'tf.test.bar ' ] = 'tf.bar2 ' all_renames_v2.symbol_renames [ 'tf.test.baz ' ] = 'tf.compat.v1.baz '
__label__0 def testupgradeinplacewithsymlink ( self ) : if os.name == `` nt '' : self.skiptest ( `` os.symlink does n't work uniformly on windows . '' )
__label__0 def __init__ ( self ) : self.non_dep_variable = variable_scope.get_variable ( name= '' non_dep_variable '' , initializer=6. , use_resource=true ) self.mirrored = variable_scope.get_variable ( name= '' mirrored '' , initializer=15. , use_resource=true )
__label__0 def teststartqueuerunners ( self ) : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) init_op = variables.global_variables_initializer ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) queue_runner_impl.add_queue_runner ( qr ) with self.cached_session ( ) as sess : init_op.run ( ) threads = queue_runner_impl.start_queue_runners ( sess ) for t in threads : t.join ( ) self.assertequal ( 0 , len ( qr.exceptions_raised ) ) # the variable should be 3. self.assertequal ( 3 , self.evaluate ( var ) )
__label__0 args : symbol : symbol to get api names for .
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 def main ( unused_args ) : name = flags.name test_name = flags.test_name test_args = `` `` .join ( flags.test_args ) benchmark_type = flags.benchmark_type test_results , _ = run_and_gather_logs_lib.run_and_gather_logs ( name , test_name=test_name , test_args=test_args , benchmark_type=benchmark_type , skip_processing_logs=flags.skip_export ) if flags.skip_export : return
__label__0 return roctracer_config
__label__0 # adding s1 ( s3 should now be deleted as oldest in list ) s1 = save2.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s2 , s1 ] , save2.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) ) self.assertfalse ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s3 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s2 ) ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( checkpoint_management.meta_graph_filename ( s1 ) ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s2 , s1 ] , save_dir=save_dir )
__label__0 def __lt__ ( self , other ) : self._assert_type ( other ) return id ( self._wrapped ) < id ( other._wrapped ) # pylint : disable=protected-access
__label__0 > > > s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > s1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] > > > tf.nest.map_structure ( lambda x , y : none , s1 , s1_list , check_types=false ) ( ( ( none , none ) , none ) , none , ( none , none ) )
__label__0 def find_modules ( ) : `` '' '' finds all the modules in the core package imported .
__label__0 @ abc.abstractmethod def write ( self , file_prefix : str ) - > str : `` '' '' serializes proto to disk .
__label__0 # every directory that contains a .py file gets an empty __init__.py file . create_init_files ( os.path.join ( srcs_dir , `` tensorflow '' ) )
__label__0 raise runtimeerror ( `` '' '' one or more added test dependencies are not in the pip package . if these test dependencies need to be in tensorflow pip package , please add them to //tensorflow/tools/pip_package/build . else add no_pip tag to the test . '' '' '' )
__label__0 self.assertequal ( input_tree_flattened_paths , [ ( 0 , 0 , 0 ) , ( 0 , 0 , 1 ) , ( 0 , 1 , 0 , 0 ) , ( 0 , 1 , 0 , 1 ) , ( 0 , 1 , 1 , 0 , 0 ) , ( 0 , 1 , 1 , 0 , 1 ) , ( 0 , 1 , 1 , 1 , 0 , 0 ) , ( 0 , 1 , 1 , 1 , 0 , 1 ) ] ) self.assertequal ( input_tree_flattened , [ `` a '' , 1 , `` b '' , 2 , `` c '' , 3 , `` d '' , 4 ] )
__label__0 # set up extra packages , which are optional sets of other python package deps . # e.g . `` pip install tensorflow [ and-cuda ] '' below installs the normal tf deps # plus the cuda libraries listed . extra_packages = { } extra_packages [ 'and-cuda ' ] = [ # todo ( nluehr ) : set nvidia- * versions based on build components . 'nvidia-cublas-cu12 == 12.3.4.1 ' , 'nvidia-cuda-cupti-cu12 == 12.3.101 ' , 'nvidia-cuda-nvcc-cu12 == 12.3.107 ' , 'nvidia-cuda-nvrtc-cu12 == 12.3.107 ' , 'nvidia-cuda-runtime-cu12 == 12.3.101 ' , 'nvidia-cudnn-cu12 == 8.9.7.29 ' , 'nvidia-cufft-cu12 == 11.0.12.1 ' , 'nvidia-curand-cu12 == 10.3.4.107 ' , 'nvidia-cusolver-cu12 == 11.5.4.101 ' , 'nvidia-cusparse-cu12 == 12.2.0.103 ' , 'nvidia-nccl-cu12 == 2.19.3 ' , 'nvidia-nvjitlink-cu12 == 12.3.101 ' , ]
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 args : server_or_cluster_def : a ` tf.train.serverdef ` or ` tf.train.clusterdef ` protocol buffer , or a ` tf.train.clusterspec ` object , describing the server to be created and/or the cluster of which it is a member . job_name : ( optional . ) specifies the name of the job of which the server is a member . defaults to the value in ` server_or_cluster_def ` , if specified . task_index : ( optional . ) specifies the task index of the server in its job . defaults to the value in ` server_or_cluster_def ` , if specified . otherwise defaults to 0 if the server 's job has only one task . protocol : ( optional . ) specifies the protocol to be used by the server . acceptable values include ` `` grpc '' , `` grpc+verbs '' ` . defaults to the value in ` server_or_cluster_def ` , if specified . otherwise defaults to ` `` grpc '' ` . config : ( options . ) a ` tf.compat.v1.configproto ` that specifies default configuration options for all sessions that run on this server . start : ( optional . ) boolean , indicating whether to start the server after creating it . defaults to ` true ` .
__label__0 # checks if all arguments have default value set after first one . invalid_default_values = [ args [ i ] for i , j in enumerate ( all_defaults ) if j is no_default and i > first_default ]
__label__0 def _convert_variables_to_tensors ( self ) : return ct2 ( ops.convert_to_tensor ( self.component ) )
__label__0 with self.assertraises ( indexerror ) : _ = trace [ len ( trace ) ]
__label__1 def greet ( name ) : return f '' hello , { name } ! ''
__label__0 returns : a decorator .
__label__0 return warned
__label__0 @ tf_export ( `` test_op_with_kwonly '' ) @ dispatch.add_dispatch_support def test_op_with_kwonly ( * , x , y , z , optional=none ) : `` '' '' a fake op for testing dispatch of python ops . '' '' '' del optional return x + ( 2 * y ) + ( 3 * z )
__label__0 def test_criticalsection_upgrade ( self ) : text = `` tf.contrib.framework.criticalsection ( shared_name='blah ' ) '' expected = `` tf.criticalsection ( shared_name='blah ' ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 class splitbasedonsize ( split.composablesplitter ) : `` '' '' a splitter that 's based on the size of the input proto . '' '' ''
__label__0 return deprecated_wrapper
__label__0 def add_notice_to_docstring ( doc , instructions , no_doc_str , suffix_str , notice , notice_type='warning ' ) : `` '' '' adds a deprecation notice to a docstring .
__label__0 # list of indices at which to split the repeated field . for example , [ 3 , 5 ] # means that the field list is split into : [ :3 ] , [ 3:5 ] , [ 5 : ] repeated_msg_split = [ ] # should be the same length as the list above . contains new protos to hold # the elements that are split from the original proto . # from the [ 3 , 5 ] example above , the messages in this list contain nodes # [ 3:5 ] and [ 5 : ] repeated_msg_graphs = [ ] # track the total size of the current node split . total_size = 0
__label__0 from tensorflow.core.framework import graph_debug_info_pb2 from tensorflow.python.util import _tf_stack
__label__0 # output is : # [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 9 ] , [ 5 , 5 ] ] # [ true , true , false , true ] `` `
__label__0 inner_decorator = tf_decorator.tfdecorator ( `` , test_undecorated_function , `` , inner_argspec ) outer_decorator = tf_decorator.tfdecorator ( `` , inner_decorator , `` , outer_argspec ) self.assertequal ( outer_argspec , tf_inspect.getfullargspec ( outer_decorator ) )
__label__0 raises : invalidsymbolnameerror : if you try to export symbol under disallowed name. `` '' '' all_symbol_names = set ( self._names ) | set ( self._names_v1 ) if self._api_name == tensorflow_api_name : for subpackage in subpackage_namespaces : if any ( n.startswith ( subpackage ) for n in all_symbol_names ) : raise invalidsymbolnameerror ( ' @ tf_export is not allowed to export symbols under % s . * ' % ( subpackage ) ) else : if not all ( n.startswith ( self._api_name ) for n in all_symbol_names ) : raise invalidsymbolnameerror ( 'can only export symbols under package name of component . ' )
__label__0 # 3 workers and one of them is backup . @ test_util.run_v1_only ( `` this exercises tensor lookup via names which is not supported in v2 . '' ) def test3workers1backup ( self ) : num_workers = 3 replicas_to_aggregate = 2 num_ps = 2 workers , _ = create_local_cluster ( num_workers=num_workers , num_ps=num_ps )
__label__0 text = `` tf.name_scope ( name=n , default_name=d , values=s ) '' expected_text = `` tf.compat.v1.name_scope ( name=n , default_name=d , values=s ) '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` ` name ` passed to ` name_scope ` `` , report )
__label__0 from absl import app from absl import flags from packaging import version import tensorflow as tf from tensorflow_docs.api_generator import doc_controls from tensorflow_docs.api_generator import doc_generator_visitor from tensorflow_docs.api_generator import generate_lib from tensorflow_docs.api_generator.pretty_docs import base_page from tensorflow_docs.api_generator.pretty_docs import module_page import yaml
__label__0 def testxlaexperimental ( self ) : text = `` tf.xla.experimental.jit_scope ( 0 ) '' expected_text = `` tf.xla.experimental.jit_scope ( 0 ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 added_names_message = `` '' '' some function names in self.reordered_function_names are not in reorders_v2.py . please run the following commands to update reorders_v2.py : bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map `` '' '' removed_names_message = `` '' '' % s in self.reorders_v2 does not match any name in self.reordered_function_names . please run the following commands to update reorders_v2.py : bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map `` '' '' self.asserttrue ( reordered_function_names.issubset ( function_reorders ) , added_names_message ) # function_reorders should contain reordered_function_names # and their tensorflow v1 aliases . for name in function_reorders : if name in manual_function_reorders : continue # get other names for this function attr = get_symbol_for_name ( tf.compat.v1 , name ) _ , attr = tf_decorator.unwrap ( attr ) v1_names = tf_export.get_v1_names ( attr ) self.asserttrue ( v1_names ) v1_names = [ `` tf. % s '' % n for n in v1_names ] # check if any other name is in self.asserttrue ( any ( n in reordered_function_names for n in v1_names ) , removed_names_message % name )
__label__0 creates a new ` session ` on 'master ' . waits for the model to be initialized or recovered from a checkpoint . it 's expected that another thread or process will make the model ready , and that this is intended to be used by threads/processes that participate in a distributed training configuration where a different thread/process is responsible for initializing or recovering the model being trained .
__label__0 def testdispatcherrorforunsupportedtypeannotation ( self ) : with self.assertraisesregex ( valueerror , `` type annotation . * is not currently supported by dispatch . `` ) :
__label__0 class saverestoreshardedtest ( test.testcase ) :
__label__0 > > > ragged_tensor1 = tf.raggedtensor.from_row_splits ( ... values= [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ] , ... row_splits= [ 0 , 4 , 4 , 7 , 8 , 8 ] ) > > > ragged_tensor2 = tf.raggedtensor.from_row_splits ( ... values= [ 3 , 1 , 4 ] , ... row_splits= [ 0 , 3 ] ) > > > tf.nest.assert_same_structure ( ... ragged_tensor1 , ... ragged_tensor2 , ... expand_composites=true )
__label__0 this is the inverse of the ` nest.pack_sequence_as ` method that takes in a flattened list and re-packs it into the nested structure .
__label__0 # # shallow non-list edge-case . # using iterable elements . input_tree = [ `` input_tree '' ] shallow_tree = `` shallow_tree '' ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 defined as part of the test because the mutablehashtable python code is currently in contrib. `` '' ''
__label__0 @ tf_export ( v1= [ `` train.vocabinfo '' ] ) class vocabinfo ( collections.namedtuple ( `` vocabinfo '' , [ `` new_vocab '' , `` new_vocab_size '' , `` num_oov_buckets '' , `` old_vocab '' , `` old_vocab_size '' , `` backup_initializer '' , `` axis '' , ] ) ) : `` '' '' vocabulary information for warm-starting .
__label__0 # assert function docs are properly updated . self.assertequal ( `` prop doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' `` \n '' `` \nreturns : '' `` \n string . '' % ( date , instructions ) , getattr ( _object , `` _prop '' ) .__doc__ )
__label__0 upgrader = ast_edits.astcodeupgrader ( renameimports ( ) ) upgrader.process_tree ( upgrade_dir , output_dir , copy_other_files=true )
__label__0 def testgetargspecignoresdecoratorsthatdontprovideargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } , )
__label__0 def testframesummaryequality ( self ) : frames1 = tf_stack.extract_stack ( ) frames2 = tf_stack.extract_stack ( )
__label__0 returns : a tuple ( is_ready , msg ) , where is_ready is true if ready to run local_init_op and false otherwise , and msg is ` none ` if the model is ready to run local_init_op , a ` string ` with the reason why it is not ready otherwise. `` '' '' return _ready ( self._ready_for_local_init_op , sess , `` model not ready for local init '' )
__label__0 this will produce the following docs :
__label__0 def testreport ( self ) : text = `` tf.mul ( a , b ) \n '' _ , report , unused_errors , unused_new_text = self._upgrade ( text ) # this is not a complete test , but it is a sanity test that a report # is generating information . self.asserttrue ( report.find ( `` renamed function ` tf.mul ` to ` tf.multiply ` `` ) )
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import errors_impl from tensorflow.python.framework import test_util from tensorflow.python.ops import math_ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import server_lib
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_init_class ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 class objectidentityweakkeydictionary ( objectidentitydictionary ) : `` '' '' like weakref.weakkeydictionary , but compares objects with `` is '' . '' '' ''
__label__0 return compile ( source , filename='dummy.py ' , mode='exec ' , flags=flags , dont_inherit=dont_inherit , optimize=optimize )
__label__0 if ` shallow_tree ` and ` input_tree ` are atoms , this returns a single-item list : ` [ input_tree ] ` .
__label__0 def cast ( self , value , cast_context ) - > any : `` '' '' cast value to this type .
__label__0 with self.cached_session ( ) as sess : output_np = sess.run ( output , feed_dict=feed_dict ) self.assertallclose ( output_np [ 0 ] , feed_dict [ inp_a ] [ 0 ] + feed_dict [ inp_b ] [ 0 ] ) self.assertallclose ( output_np [ 1 ] , feed_dict [ inp_a ] [ 1 ] + feed_dict [ inp_b ] [ 1 ] )
__label__0 returns : the input function with _tf_api_names attribute set. `` '' '' api_names_attr = api_attrs [ self._api_name ] .names api_names_attr_v1 = api_attrs_v1 [ self._api_name ] .names
__label__0 input_tree = [ 0 , 1 ] shallow_tree = 9 flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 > > > @ dispatch_for_api ( tf.math.add , { ' x ' : maskedtensor , ' y ' : maskedtensor } ) ... def masked_add ( x , y , name=none ) : ... return maskedtensor ( x.values + y.values , x.mask & y.mask )
__label__0 def full_name_node ( name , ctx=ast.load ( ) ) : `` '' '' make an attribute or name node for name .
__label__0 def rename_libtensorflow ( srcs_dir : str , version : str ) : `` '' '' update libtensorflow_cc file name . bazel sets full tf version in name but libtensorflow_cc must contain only major . update accordingly to the platform : e.g . libtensorflow_cc.so.2.15.0 - > libtensorflow_cc.2 args : srcs_dir : target directory with files . version : major version to be set. `` '' '' major_version = version.split ( `` . `` ) [ 0 ] if is_macos ( ) : shutil.move ( os.path.join ( srcs_dir , `` libtensorflow_cc . { } .dylib '' .format ( version ) ) , os.path.join ( srcs_dir , `` libtensorflow_cc . { } .dylib '' .format ( major_version ) ) , ) shutil.move ( os.path.join ( srcs_dir , `` libtensorflow_framework . { } .dylib '' .format ( version ) ) , os.path.join ( srcs_dir , `` libtensorflow_framework . { } .dylib '' .format ( major_version ) ) , ) else : shutil.move ( os.path.join ( srcs_dir , `` libtensorflow_cc.so . { } '' .format ( version ) ) , os.path.join ( srcs_dir , `` libtensorflow_cc.so . { } '' .format ( major_version ) ) , ) shutil.move ( os.path.join ( srcs_dir , `` libtensorflow_framework.so . { } '' .format ( version ) ) , os.path.join ( srcs_dir , `` libtensorflow_framework.so . { } '' .format ( major_version ) ) , )
__label__0 # restore the saved values in the parameter nodes . save.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 # either bytes or text . bytes_or_text_types = ( bytes , str ) tf_export ( 'compat.bytes_or_text_types ' ) .export_constant ( __name__ , 'bytes_or_text_types ' )
__label__0 class callable ( object ) :
__label__0 conformant pair of sequenceexample :
__label__0 def __init__ ( self ) : # maps from a function name to a dictionary that describes how to # map from an old argument keyword to the new argument keyword . self.function_keyword_renames = { `` tf.batch_matmul '' : { `` adj_x '' : `` adjoint_a '' , `` adj_y '' : `` adjoint_b '' , } , `` tf.count_nonzero '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_all '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_any '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_max '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_mean '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_min '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_prod '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_sum '' : { `` reduction_indices '' : `` axis '' } , `` tf.reduce_logsumexp '' : { `` reduction_indices '' : `` axis '' } , `` tf.expand_dims '' : { `` dim '' : `` axis '' } , `` tf.argmax '' : { `` dimension '' : `` axis '' } , `` tf.argmin '' : { `` dimension '' : `` axis '' } , `` tf.reduce_join '' : { `` reduction_indices '' : `` axis '' } , `` tf.sparse_concat '' : { `` concat_dim '' : `` axis '' } , `` tf.sparse_split '' : { `` split_dim '' : `` axis '' } , `` tf.sparse_reduce_sum '' : { `` reduction_axes '' : `` axis '' } , `` tf.reverse_sequence '' : { `` seq_dim '' : `` seq_axis '' , `` batch_dim '' : `` batch_axis '' } , `` tf.sparse_reduce_sum_sparse '' : { `` reduction_axes '' : `` axis '' } , `` tf.squeeze '' : { `` squeeze_dims '' : `` axis '' } , `` tf.split '' : { `` split_dim '' : `` axis '' , `` num_split '' : `` num_or_size_splits '' } , `` tf.concat '' : { `` concat_dim '' : `` axis '' } , }
__label__0 def testbuildshouldbecalledbeforesaveincaseofdeferbuild ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` error_deferred_build '' ) with ops_lib.graph ( ) .as_default ( ) , session.session ( ) as sess : variable_v1.variablev1 ( 1.0 ) saver = saver_module.saver ( defer_build=true ) with self.assertraisesregex ( runtimeerror , `` build '' ) : saver.save ( sess , save_path )
__label__0 @ deprecation.deprecated_args ( date , instructions , `` deprecated '' ) def _fn ( arg0 , arg1 , * deprecated ) : return arg0 + arg1 if deprecated else arg1 + arg0
__label__0 return file_count , report , tree_errors
__label__0 def test_cycle ( self ) :
__label__0 import riegeli
__label__0 args : structure : tuple or list constructed of scalars and/or other tuples/lists , or a scalar . note : numpy arrays are considered scalars . flat_sequence : flat sequence to pack .
__label__0 def testdispatchfortypes_signaturemismatchnames ( self ) : with self.assertraisesregex ( assertionerror , `` the decorated function 's non-default arguments must be identical to '' `` that of the overridden op . `` , ) : @ dispatch.dispatch_for_types ( test_op , customtensor ) def override_for_test_op ( a , b , c ) : # pylint : disable=unused-variable return customtensor ( test_op ( a.tensor , b.tensor , c.tensor ) , ( a.score + b.score + c.score ) / 3.0 )
__label__0 def _testscopedsave ( self , test_dir , exported_filename , ckpt_filename ) : graph = ops_lib.graph ( ) with graph.as_default ( ) : # creates an inference graph . # hidden 1 images = constant_op.constant ( 1.2 , dtypes.float32 , shape= [ 100 , 28 ] , name= '' images '' ) with ops_lib.name_scope ( `` hidden1 '' ) : weights1 = variable_v1.variablev1 ( random_ops.truncated_normal ( [ 28 , 128 ] , stddev=1.0 / math.sqrt ( float ( 28 ) ) ) , name= '' weights '' ) # the use of cond.cond here is purely for adding test # coverage the save and restore of control flow context ( which does n't # make any sense here from a machine learning perspective ) . the typical # biases is a simple variable without the conditions . biases1 = variable_v1.variablev1 ( cond.cond ( math_ops.less ( random.random ( ) , 0.5 ) , lambda : array_ops.ones ( [ 128 ] ) , lambda : array_ops.zeros ( [ 128 ] ) ) , name= '' biases '' ) hidden1 = nn_ops.relu ( math_ops.matmul ( images , weights1 ) + biases1 )
__label__0 def testflatten_numpyisnotflattened ( self ) : structure = np.array ( [ 1 , 2 , 3 ] ) flattened = nest.flatten ( structure ) self.assertlen ( flattened , 1 )
__label__0 args : local_init_op : ` operation ` run for every new supervisor instance . if set to use_default , use the first op from the graphkeys.local_init_op collection . if the collection is empty , create an op that initializes all local variables and all tables. `` '' '' if local_init_op is supervisor.use_default : local_init_op = self._get_first_op_from_collection ( ops.graphkeys.local_init_op ) if local_init_op is none : op_list = [ variables.local_variables_initializer ( ) , lookup_ops.tables_initializer ( ) ] if op_list : local_init_op = control_flow_ops.group ( * op_list ) ops.add_to_collection ( ops.graphkeys.local_init_op , local_init_op ) self._local_init_op = local_init_op
__label__0 def __call__ ( self ) : pass
__label__0 `` '' '' super ( rmspropoptimizer , self ) .__init__ ( use_locking , name ) self._learning_rate = learning_rate self._decay = decay self._momentum = momentum self._epsilon = epsilon self._centered = centered
__label__0 def _make_signature_checker ( api_signature , signature ) : `` '' '' builds a pysignaturechecker for the given type signature .
__label__0 def make_session_run_hook ( self , is_chief , num_tokens=-1 ) : `` '' '' creates a hook to handle syncreplicashook ops such as initialization . '' '' '' return _syncreplicasoptimizerhook ( self , is_chief , num_tokens )
__label__0 each operation dispatcher acts as an override handler for a single tensorflow operation , and its results are used when the handler indicates that it can handle the operation 's arguments ( by returning any value other than ` opdispatcher.not_supported ` ) . `` '' ''
__label__0 def rocsolver_version_numbers ( path ) : possible_version_files = [ `` include/rocsolver/rocsolver-version.h '' , # rocm 5.2 `` rocsolver/include/rocsolver-version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` rocsolver version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` rocsolver_version_major '' ) minor = _get_header_version ( version_file , `` rocsolver_version_minor '' ) patch = _get_header_version ( version_file , `` rocsolver_version_patch '' ) return major , minor , patch
__label__0 def testunwrapreturnsfinalfunctionastarget ( self ) : self.assertequal ( ( 4 + 1 ) * 2 , test_decorated_function ( 4 ) ) _ , target = tf_decorator.unwrap ( test_decorated_function ) self.asserttrue ( tf_inspect.isfunction ( target ) ) self.assertequal ( 4 * 2 , target ( 4 ) )
__label__0 def flatten_with_tuple_paths_up_to ( shallow_tree , input_tree , check_types=true , expand_composites=false ) : `` '' '' flattens ` input_tree ` up to ` shallow_tree ` .
__label__0 args : arg0 : arg 0. arg1 : arg 1 .
__label__0 @ def_function.function def fn_resource_sparse_apply_adagrad_v2 ( ) : ret = constant_op.constant ( 0 , dtypes.int32 ) for i in math_ops.range ( num_iter ) : adagrad_op = gen_training_ops.resource_sparse_apply_adagrad_v2 ( var.handle , accum.handle , lr , epsilon , grad , constant_op.constant ( indices , dtypes.int32 ) ) with ops.control_dependencies ( [ adagrad_op ] ) : ret += i return ret
__label__0 # tensorrt.tensorrt is only valid in oss try : from tensorrt.tensorrt import tensorrt_config # pylint : disable=g-import-not-at-top except importerror : tensorrt_config = none
__label__0 save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } , restore_sequentially=true ) ckpt_prefix = save.save ( sess , save_path ) filesize = saver_module._get_checkpoint_size ( ckpt_prefix ) count_after_one_save = metrics.getcheckpointsize ( api_label=api_label , filesize=filesize )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a tool to generate api_docs for tensorflow2 .
__label__0 # no keywords specified , no reordering , so we should get input as output text = `` f ( a , b , c , d ) \n '' ( _ , report , _ ) , new_text = self._upgrade ( renamekeywordspec ( ) , text ) self.assertequal ( new_text , text ) self.assertnotin ( `` manual check required '' , report )
__label__0 it can be thought of as a proto-implementation of the following python type :
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , shape= [ 6 , 1 ] , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) ws_util._warm_start_var_with_vocab ( fruit_weights , new_vocab_path , 5 , self.get_temp_dir ( ) , prev_vocab_path , current_oov_buckets=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.asserttrue ( isinstance ( fruit_weights , variables.partitionedvariable ) ) fruit_weights_vars = fruit_weights._get_variable_list ( ) self.assertallclose ( [ [ 2 . ] , [ 1.5 ] , [ 1 . ] ] , fruit_weights_vars [ 0 ] .eval ( sess ) ) self.assertallclose ( [ [ 0.5 ] , [ 0 . ] , [ 0 . ] ] , fruit_weights_vars [ 1 ] .eval ( sess ) )
__label__0 rmsprop algorithm [ tieleman2012rmsprop ]
__label__0 module._fastdict_insert ( `` bar '' , 1 ) # after _fastdict_insert ( ) the attribute is added . self.asserttrue ( module._fastdict_key_in ( `` bar '' ) ) self.assertequal ( 1 , module.bar )
__label__0 @ dispatch.dispatch_for_unary_elementwise_apis ( maskedtensor ) def handler ( api_func , x ) : return maskedtensor ( api_func ( x.values ) , x.mask )
__label__0 with open ( trampoline_path , ' r ' ) as t : tramp_tpl = string.template ( t.read ( ) )
__label__0 node.keywords.append ( new_arg ) if isinstance ( node.func , ast.attribute ) : node.func.attr = `` cast '' else : assert isinstance ( node.func , ast.name ) node.func.id = `` cast ''
__label__0 args : cls : the class to search for . type signatures are searched recursively , so e.g. , if ` cls=raggedtensor ` , then information will be returned for all dispatch targets that have ` raggedtensor ` anywhere in their type annotations ( including nested in ` typing.union ` or ` typing.list ` . )
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import math_ops from tensorflow.python.ops import variable_v1 from tensorflow.python.platform import test from tensorflow.python.training import server_lib
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' helper functions to add documentation to type aliases . '' '' ''
__label__0 var0_np , mg0_np , rms0_np , mom0_np = self._sparse_rmsprop_update_numpy ( var0_np , grads0_np_indices , grads0_np , mg0_np , rms0_np , mom0_np , learning_rate , decay , momentum , epsilon , centered ) var1_np , mg1_np , rms1_np , mom1_np = self._sparse_rmsprop_update_numpy ( var1_np , grads1_np_indices , grads1_np , mg1_np , rms1_np , mom1_np , learning_rate , decay , momentum , epsilon , centered )
__label__0 returns : a pair ( sess , initialized ) where 'initialized ' is ` true ` if the session could be recovered and initialized , ` false ` otherwise .
__label__0 def func ( a=1 , b=2 ) : return ( a , b )
__label__0 # verifies savers collection is saved as bytes_list for meta_graph_def0 . collection_def = meta_graph_def0.collection_def [ `` savers '' ] kind = collection_def.whichoneof ( `` kind '' ) self.assertequal ( kind , `` bytes_list '' ) # verifies that there are 2 entries in savers collection . savers = getattr ( collection_def , kind ) self.assertequal ( 2 , len ( savers.value ) )
__label__0 `` `` '' command to upload benchmark test results to a cloud datastore .
__label__0 # build a graph with 2 parameter nodes on different devices and save . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : ds0 = dataset_ops.dataset.range ( 10 ) it0 = dataset_ops.make_initializable_iterator ( ds0 ) get_next0 = it0.get_next ( ) saveable0 = iterator_ops._iteratorsaveable ( it0._iterator_resource , name= '' saveable_it0 '' )
__label__0 def _tf_core_packed_nest_with_indices ( structure , flat , index , is_nested_fn , sequence_fn=none ) : `` '' '' helper function for pack_sequence_as .
__label__0 checkpoints are binary files in a proprietary format which map variable names to tensor values . the best way to examine the contents of a checkpoint is to load it using a ` saver ` .
__label__0 def _print_if_not_none ( obj ) : `` '' '' print like a notebook : show the repr if the object is not none .
__label__0 raises : valueerror : if partial function has positionally bound arguments `` '' '' if isinstance ( fn , functools.partial ) : args = fn_args ( fn.func ) args = [ a for a in args [ len ( fn.args ) : ] if a not in ( fn.keywords or [ ] ) ] else : if _is_callable_object ( fn ) : fn = fn.__call__ args = tf_inspect.getfullargspec ( fn ) .args if _is_bound_method ( fn ) and args : # if it 's a bound method , it may or may not have a self/cls first # argument ; for example , self could be captured in * args . # if it does have a positional argument , it is self/cls . args.pop ( 0 ) return tuple ( args )
__label__0 # these should not be visible in the main tf module . try : del core except nameerror : pass
__label__0 self.assertprotoequals ( `` '' '' cluster { job { name : 'ps ' tasks { key : 0 value : 'ps0:2222 ' } tasks { key : 1 value : 'ps1:2222 ' } } job { name : 'worker ' tasks { key : 0 value : 'worker0:2222 ' } tasks { key : 1 value : 'worker1:2222 ' } tasks { key : 2 value : 'worker2:2222 ' } } } job_name : 'worker ' task_index : 2 protocol : 'grpc ' `` '' '' , server_def )
__label__0 def _get_default_cuda_paths ( cuda_version ) : if not cuda_version : cuda_version = `` * '' elif not `` . '' in cuda_version : cuda_version = cuda_version + `` . * ''
__label__0 def _get_legacy_path ( env_name , default= [ ] ) : `` '' '' returns a path specified by a legacy environment variable .
__label__0 def _add_deprecated_function_notice_to_docstring ( doc , date , instructions ) : `` '' '' adds a deprecation notice to a docstring for deprecated functions . '' '' '' main_text = [ 'this function is deprecated . it will be removed % s . ' % ( 'in a future version ' if date is none else ( 'after % s ' % date ) ) ] if instructions : main_text.append ( 'instructions for updating : ' ) return decorator_utils.add_notice_to_docstring ( doc , instructions , 'deprecated function ' , ' ( deprecated ) ' , main_text , notice_type='deprecated ' )
__label__0 # otherwise return the line unchanged . return _expand_variables ( line , cmake_vars )
__label__0 expected_proto = `` '' '' job { name : 'ps ' tasks { key : 0 value : 'ps0:2222 ' } tasks { key : 1 value : 'ps1:2222 ' } } job { name : 'worker ' tasks { key : 0 value : 'worker0:2222 ' } tasks { key : 1 value : 'worker1:2222 ' } tasks { key : 2 value : 'worker2:2222 ' } } `` '' ''
__label__0 def test_method ( self ) : self.assertequal ( `` getqualifiednametest.test_method '' , decorator_utils.get_qualified_name ( getqualifiednametest.test_method ) )
__label__0 raises : valueerror : ( 1 ) if an invalid ` stage ` is selected ( 2 ) or if applied to a function which is not compiled ( ` jit_compile=true ` is not set ) . ( 3 ) or if input shapes are not fully defined for tf.tensorspec inputs typeerror : when called with input in graph mode. `` '' '' pass
__label__0 def after_create_session ( self , session , coord ) : # pylint : disable=unused-argument `` '' '' called when new tensorflow session is created .
__label__1 def is_valid_email ( email ) : return `` @ '' in email and `` . '' in email
__label__0 class customtensor ( object ) : `` '' '' a fake composite tensor class , for testing type-based dispatching . '' '' ''
__label__0 if __name__ == '__main__ ' : main ( )
__label__0 @ property def decorator_name ( self ) : return self._decorator_name
__label__0 if not hasattr ( symbol , '__dict__ ' ) : return names_v1 if tensorflow_api_attr_v1 in symbol.__dict__ : names_v1.extend ( getattr ( symbol , tensorflow_api_attr_v1 ) ) if keras_api_attr_v1 in symbol.__dict__ : names_v1.extend ( getattr ( symbol , keras_api_attr_v1 ) ) return names_v1
__label__0 with self.session ( graph=graph ) as sess : self.evaluate ( variables.global_variables_initializer ( ) ) saver1.save ( sess , saver1_ckpt , write_state=false ) saver2.save ( sess , saver2_ckpt , write_state=false )
__label__0 # # # # registered apis
__label__0 def _find_nccl_config ( base_paths , required_version ) :
__label__0 called = [ parent for _ , parent , _ in visitor.call_log ]
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # returns standard if a tensorflow- * package is being built , and nightly if a # tf_nightly- * package is being built . def standard_or_nightly ( standard , nightly ) : return nightly if 'tf_nightly ' in project_name else standard
__label__0 def test_contrib_summary_image_nostep ( self ) : text = `` tf.contrib.summary.image ( 'foo ' , myval ) '' expected = ( `` tf.compat.v2.summary.image ( name='foo ' , data=myval , `` `` step=tf.compat.v1.train.get_or_create_global_step ( ) ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'step ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 def getsourcelines ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getsourcelines . '' '' '' return _inspect.getsourcelines ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 raises : runtimeerror : ( implementation detail ) if `` self._write_version '' is an unexpected value. `` '' '' # pylint : disable=protected-access tensor_names = [ ] tensors = [ ] tensor_slices = [ ] for saveable in saveables : for spec in saveable.specs : tensor_names.append ( spec.name ) tensors.append ( spec.tensor ) tensor_slices.append ( spec.slice_spec ) if self._write_version == saver_pb2.saverdef.v1 : return io_ops._save ( filename=filename_tensor , tensor_names=tensor_names , tensors=tensors , tensor_slices=tensor_slices ) elif self._write_version == saver_pb2.saverdef.v2 : # `` filename_tensor '' is interpreted * not as a filename * , but as a prefix # of a v2 checkpoint : e.g . `` /fs/train/ckpt- < step > /tmp/worker < i > - < step > '' . return io_ops.save_v2 ( filename_tensor , tensor_names , tensor_slices , tensors ) else : raise runtimeerror ( `` unexpected write_version : `` + self._write_version )
__label__0 this entails renaming the 'scale ' arg to ' l ' and dropping any provided scope arg. `` '' '' # check if we have a scale or scope keyword arg scope_keyword = none for keyword in node.keywords : if keyword.arg == `` scale '' : logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` renaming scale arg of regularizer\n '' ) ) keyword.arg = `` l '' if keyword.arg == `` scope '' : scope_keyword = keyword
__label__0 fetched = sess.run ( fetch_list )
__label__0 text = `` from foo import * '' expected_text = `` from bar import * '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 # python 2 if hasattr ( function , 'im_class ' ) : return function.im_class.__name__ + ' . ' + function.__name__ return function.__name__
__label__0 restore_graph = ops_lib.graph ( ) with restore_graph.as_default ( ) , self.session ( graph=restore_graph ) as sess : restored_vars = _model ( ) save = saver_module.saver ( max_to_keep=1 ) save.restore ( sess , save_dir ) restored_vals = self.evaluate ( restored_vars )
__label__0 def test_simple_function ( self ) : def fn ( a , b ) : return a + b self.assertequal ( ( ' a ' , ' b ' ) , function_utils.fn_args ( fn ) )
__label__0 def testsparseconcat ( self ) : text = `` tf.sparse.concat ( ax , inp , name , exp , concat ) '' expected_text = ( `` tf.sparse.concat ( ax , inp , name=name , expand_nonconcat_dims=exp , `` `` axis=concat ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 lineno = node.func.value.lineno col_offset = node.func.value.col_offset node.func.value = ast_edits.full_name_node ( `` tf.keras.regularizers '' ) node.func.value.lineno = lineno node.func.value.col_offset = col_offset node.func.attr = `` l2 ''
__label__0 import time
__label__0 in particular , if git is in detached head state , this will return none . if git is in attached head , it will return the branch reference . e.g . if on 'master ' , the head will contain 'ref : refs/heads/master ' so 'refs/heads/master ' will be returned .
__label__0 # changing the shape works because ` validate_shape ` is false . sharing_sess_1.run ( v.initializer , feed_dict= { init_value : [ 86 , 99 ] } ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_0.run ( v ) ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_1.run ( v ) ) with self.assertraises ( errors_impl.failedpreconditionerror ) : isolate_sess_0.run ( v ) with self.assertraises ( errors_impl.failedpreconditionerror ) : isolate_sess_1.run ( v )
__label__0 class foo ( object ) :
__label__0 def test_serialize_shape ( self ) : round_trip = json.loads ( json.dumps ( tensor_shape.tensorshape ( [ none , 2 , 3 ] ) , default=serialization.get_json_type ) ) self.assertis ( round_trip [ 0 ] , none ) self.assertequal ( round_trip [ 1 ] , 2 )
__label__0 from tensorflow.python.client import session from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import server_lib
__label__0 with open ( os.path.join ( args.outdir , f ' { lib_name } .tramp.s ' ) , ' w ' ) as f : with open ( table_path , ' r ' ) as t : table_text = string.template ( t.read ( ) ) .substitute ( lib_suffix=lib_name , table_size=ptr_size * ( len ( funs ) + 1 ) ) f.write ( table_text )
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` fn doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated . it will be removed after % s . '' `` \ninstructions for updating : \n % s '' `` \n '' `` \nargs : '' `` \n arg0 : arg 0 . '' `` \n arg1 : arg 1 . '' `` \n '' `` \nreturns : '' `` \n sum of args . '' % ( date , instructions ) , _fn.__doc__ )
__label__0 cusolver_paths = base_paths if tuple ( int ( v ) for v in cuda_version.split ( `` . '' ) ) < ( 11 , 0 ) : cusolver_paths = cuda_paths cusolver_version = os.environ.get ( `` tf_cusolver_version '' , `` '' ) result.update ( _find_cusolver_config ( cusolver_paths , cusolver_version , cuda_version ) )
__label__0 # attribute values must be unique to each api . api_attrs = { tensorflow_api_name : _attributes ( '_tf_api_names ' , '_tf_api_constants ' ) , keras_api_name : _attributes ( '_keras_api_names ' , '_keras_api_constants ' ) , }
__label__0 try : del tools except nameerror : pass
__label__0 def testpreparesessionsucceedswithinitfeeddict ( self ) : with ops.graph ( ) .as_default ( ) : p = array_ops.placeholder ( dtypes.float32 , shape= ( 3 , ) ) v = variable_v1.variablev1 ( p , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) , init_feed_dict= { p : [ 1.0 , 2.0 , 3.0 ] } ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) )
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for traceback_utils . '' '' ''
__label__0 def check_for_lingering_string ( lingering_string ) : `` '' '' check for given lingering strings . '' '' '' formatted_string = lingering_string.replace ( `` . `` , r '' \ . '' ) try : linger_str_output = subprocess.check_output ( [ `` grep '' , `` -rnoh '' , formatted_string , tf_src_dir ] ) linger_strs = linger_str_output.decode ( `` utf8 '' ) .split ( `` \n '' ) except subprocess.calledprocesserror : linger_strs = [ ]
__label__0 from absl import app from tensorflow.tools.test import system_info_lib
__label__0 def testmanagedsessionusererror ( self ) : logdir = self._test_dir ( `` managed_user_error '' ) with ops.graph ( ) .as_default ( ) : my_op = constant_op.constant ( 1.0 ) sv = supervisor.supervisor ( logdir=logdir ) last_step = none with self.assertraisesregex ( runtimeerror , `` failing here '' ) : with sv.managed_session ( `` '' ) as sess : for step in range ( 10 ) : last_step = step if step == 1 : raise runtimeerror ( `` failing here '' ) else : self.evaluate ( my_op ) # supervisor has been stopped . self.asserttrue ( sv.should_stop ( ) ) self.assertequal ( 1 , last_step )
__label__0 with self.assertraiseswithliteralmatch ( # pylint : disable=g-error-prone-assert-raises typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( mt ) , input_type=type ( simple_list ) ) , ) : nest.flatten_with_tuple_paths_up_to ( shallow_tree=mt , input_tree=simple_list , check_types=true )
__label__0 major , minor , patch = rocfft_version_numbers ( rocm_install_path )
__label__0 args : primary : the primary ` variable ` or ` tensor ` . name : name to use for the slot variable . dtype : type of the slot variable . defaults to the type of ` primary ` . colocate_with_primary : boolean . if true the slot is located on the same device as ` primary ` . copy_xla_sharding : boolean . if true also copies xla sharding from primary .
__label__0 else : header_version = cuda_version header_path = _find_file ( base_paths , _header_paths ( ) , `` cusparse.h '' ) cusparse_version = required_version
__label__0 # copyright 2023 the tensorflow authors . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # https : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . `` `` '' tool to rearrange files and build the wheel .
__label__0 if not renamed_keywords : return false
__label__0 def _resource_apply_dense ( self , grad , var ) : rms = self.get_slot ( var , `` rms '' ) mom = self.get_slot ( var , `` momentum '' ) if self._centered : mg = self.get_slot ( var , `` mg '' ) return gen_training_ops.resource_apply_centered_rms_prop ( var.handle , mg.handle , rms.handle , mom.handle , math_ops.cast ( self._learning_rate_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , grad.dtype.base_dtype ) , grad , use_locking=self._use_locking ) else : return gen_training_ops.resource_apply_rms_prop ( var.handle , rms.handle , mom.handle , math_ops.cast ( self._learning_rate_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , grad.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , grad.dtype.base_dtype ) , grad , use_locking=self._use_locking )
__label__0 # pylint does not correctly recognize these as class names and # suggests to use variable style under_score naming . # pylint : disable=invalid-name named0ab = collections.namedtuple ( `` named_0 '' , ( `` a '' , `` b '' ) ) named1ab = collections.namedtuple ( `` named_1 '' , ( `` a '' , `` b '' ) ) samenameab = collections.namedtuple ( `` same_name '' , ( `` a '' , `` b '' ) ) samenameab2 = collections.namedtuple ( `` same_name '' , ( `` a '' , `` b '' ) ) samenamexy = collections.namedtuple ( `` same_name '' , ( `` x '' , `` y '' ) ) samename1xy = collections.namedtuple ( `` same_name_1 '' , ( `` x '' , `` y '' ) ) samename1xy2 = collections.namedtuple ( `` same_name_1 '' , ( `` x '' , `` y '' ) ) notsamename = collections.namedtuple ( `` not_same_name '' , ( `` a '' , `` b '' ) ) # pylint : enable=invalid-name
__label__0 for path in files_to_process : if os.path.islink ( path ) : report += `` skipping symlink % s.\n '' % path continue file_count += 1 _ , l_report , l_errors = self.process_file ( path , path ) tree_errors [ path ] = l_errors report += l_report
__label__0 # verifies behavior of tf.session.reset ( ) with multiple containers using # tf.container . # todo ( b/34465411 ) : starting multiple servers with different configurations # in the same test is flaky . move this test case back into # `` server_lib_test.py '' when this is no longer the case . @ test_util.run_deprecated_v1 def testmultiplecontainers ( self ) : with ops.container ( `` test0 '' ) : v0 = variables.variable ( 1.0 , name= '' v0 '' ) with ops.container ( `` test1 '' ) : v1 = variables.variable ( 2.0 , name= '' v0 '' ) server = server_lib.server.create_local_server ( ) sess = session.session ( server.target ) sess.run ( variables.global_variables_initializer ( ) ) self.assertallequal ( 1.0 , sess.run ( v0 ) ) self.assertallequal ( 2.0 , sess.run ( v1 ) )
__label__0 > > > structure = ( ( ' a ' , ' b ' ) , ( ' c ' , 'd ' , ' e ' ) , ' f ' ) > > > flat_sequence = [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) ( ( 1.0 , 2.0 ) , ( 3.0 , 4.0 , 5.0 ) , 6.0 )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_where )
__label__0 class deprecationargumentstest ( test.testcase ) :
__label__0 `` ` python example_dictionary = { ( 4 , 5 , ( 6 , 8 ) ) : ( `` a '' , `` b '' , ( `` c '' , `` d '' ) ) } result = { 4 : `` a '' , 5 : `` b '' , 6 : `` c '' , 8 : `` d '' } flatten_dict_items ( example_dictionary ) == result `` `
__label__0 def _tf_data_flatten_up_to ( shallow_tree , input_tree ) : _tf_data_assert_shallow_structure ( shallow_tree , input_tree ) return list ( _tf_data_yield_flat_up_to ( shallow_tree , input_tree ) )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 def testinplace ( self ) : `` '' '' check to make sure we do n't have a file system race . '' '' '' temp_file = tempfile.namedtemporaryfile ( `` w '' , delete=false ) original = `` tf.mul ( a , b ) \n '' upgraded = `` tf.multiply ( a , b ) \n '' temp_file.write ( original ) temp_file.close ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade.tfapichangespec ( ) ) upgrader.process_file ( temp_file.name , temp_file.name ) self.assertallequal ( open ( temp_file.name ) .read ( ) , upgraded ) os.unlink ( temp_file.name )
__label__0 returns : a decorator that overrides the default implementation for ` api ` .
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # pylint : disable=line-too-long `` '' '' list of renames to apply when converting from tf 1.0 to tf 2.0 .
__label__0 the output is :
__label__0 these share most of their implementation with sum , so we only test basic functionality. `` '' '' for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 8 , 3 , 1 , 0 ] , dtype=dtype ) , self._unsortedsegmentprod ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] , dtype=dtype ) , np.array ( [ 3 , -1 , 0 , 1 , 0 , -1 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 the script searches for sycl library and header files on the system , inspects them to determine their version and prints the configuration to stdout . the path to inspect is specified through an environment variable ( sycl_path ) . if no valid configuration is found , the script prints to stderr and returns an error code . the script takes the directory specified by the sycl_path environment variable . the script looks for headers and library files in a hard-coded set of subdirectories from base path of the specified directory . if sycl_path is not specified , then `` /opt/sycl '' is used as it default value `` '' ''
__label__0 for testing ` deprecation.deprecate_moved_module ` . `` '' ''
__label__0 message = mock_warning.call_args [ 0 ] [ 0 ] % mock_warning.call_args [ 0 ] [ 1 : ] self.assertregex ( message , r '' . * some_op \ ( from __main__\ ) is deprecated and will be `` `` removed in a future version . * '' )
__label__0 def _addrestoreops ( self , filename_tensor , saveables , restore_sequentially , reshape , preferred_shard=-1 , name= '' restore_all '' ) : `` '' '' add operations to restore saveables .
__label__0 `` ` shell nest.flatten ( value ) [ 3 , 23 , 42 ] list ( nest.yield_flat_paths ( value ) ) [ ( ' a ' , ) , ( ' b ' , ' c ' ) , ( ' b ' , 'd ' ) ] `` `
__label__0 s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) * 10 s2 = ( ( ( `` foo1 '' , `` foo2 '' ) , `` foo3 '' ) , `` foo4 '' , ( `` foo5 '' , `` foo6 '' ) ) * 10 self.run_and_report ( s1 , s2 , `` assert_same_structure_60_elem '' )
__label__0 def start_standard_services ( self , sess ) : `` '' '' start the standard services for 'sess ' .
__label__0 returns : the map marking symbols to not include. `` '' '' return self._private_map
__label__0 if global_step is none : raise valueerror ( `` global step is required to check staleness '' )
__label__0 def testcallingclassmethodondecoratedclass ( self ) : self.assertequal ( 2 , testdecoratedclass ( ) .two_func ( ) )
__label__0 def testunwrapreturnsdecoratorlistfromoutermosttoinnermost ( self ) : decorators , _ = tf_decorator.unwrap ( test_decorated_function ) self.assertequal ( 'decorator 1 ' , decorators [ 0 ] .decorator_name ) self.assertequal ( 'test_decorator_increment_first_int_arg ' , decorators [ 1 ] .decorator_name ) self.assertequal ( 'decorator 3 ' , decorators [ 2 ] .decorator_name ) self.assertequal ( 'decorator 3 documentation ' , decorators [ 2 ] .decorator_doc )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassmapstructure ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt_doubled = nest.map_structure ( lambda x : x * 2 , mt ) self.assertisinstance ( mt_doubled , maskedtensor ) self.assertequal ( mt_doubled.mask , true ) self.assertallequal ( mt_doubled.value , [ 2 ] )
__label__0 with ops.device ( `` /job : worker/task : '' + str ( worker_id ) ) : grads_0 = constant_op.constant ( 0.1 + worker_id * 0.2 ) grads_1 = constant_op.constant ( 0.9 + worker_id * 0.2 ) # this is to test against sparse gradients . grads_sparse = indexed_slices.indexedslices ( constant_op.constant ( [ 0.1 + worker_id * 0.2 ] , shape= [ 1 , 1 ] ) , constant_op.constant ( [ 1 ] ) , constant_op.constant ( [ 2 , 1 ] ) ) sgd_opt = gradient_descent.gradientdescentoptimizer ( 2.0 ) sync_rep_opt = training.syncreplicasoptimizer ( sgd_opt , replicas_to_aggregate=replicas_to_aggregate , total_num_replicas=num_workers ) train_op = [ sync_rep_opt.apply_gradients ( zip ( [ grads_0 , grads_1 , grads_sparse ] , [ var_0 , var_1 , var_sparse ] ) , global_step=global_step ) ] sync_replicas_hook = sync_rep_opt.make_session_run_hook ( is_chief , num_tokens=num_workers )
__label__0 def testglobaldispatchergetitem ( self ) : original_global_dispatchers = dispatch._global_dispatchers try : tensortraceropdispatcher ( ) .register ( )
__label__0 import argparse
__label__0 def testrenameargs ( self ) : text = ( `` tf.nn.pool ( input_a , window_shape_a , pooling_type_a , padding_a , `` `` dilation_rate_a , strides_a , name_a , data_format_a ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , ( `` tf.nn.pool ( input_a , window_shape_a , pooling_type_a , `` `` padding=padding_a , dilations=dilation_rate_a , `` `` strides=strides_a , name=name_a , `` `` data_format=data_format_a ) \n '' ) )
__label__0 def __iter__ ( self ) : keys = self._storage.keys ( ) for key in keys : unwrapped = key.unwrapped if unwrapped is none : del self [ key ] else : yield unwrapped
__label__0 while true : sess = session.session ( self._target , graph=self._graph , config=config ) not_ready_msg = none not_ready_local_msg = none local_init_success , not_ready_local_msg = self._try_run_local_init_op ( sess ) if local_init_success : # successful if local_init_op is none , or ready_for_local_init_op passes is_ready , not_ready_msg = self._model_ready ( sess ) if is_ready : return sess
__label__0 @ tf_export ( `` distribute.distributediterator '' , v1= [ ] ) class distributediteratorinterface ( iterator ) : `` '' '' an iterator over ` tf.distribute.distributeddataset ` .
__label__0 def prepare_srcs ( deps : list [ str ] , srcs_dir : str ) - > none : `` '' '' rearrange source files in target the target directory .
__label__0 raises : valueerror : if this is called before apply_gradients ( ) . valueerror : if num_tokens are smaller than replicas_to_aggregate - total_num_replicas. `` '' '' if self._gradients_applied is false : raise valueerror ( `` get_init_tokens_op ( ) should be called after apply_gradients ( ) . '' )
__label__0 # clean up test_op_with_optional._tf_fallback_dispatchers = original_handlers
__label__0 def testwarmstartvarwithcolumnvocab ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_output_layer '' , initializer= [ [ 0.5 , 0.3 ] , [ 1. , 0.8 ] , [ 1.5 , 1.2 ] , [ 2. , 2.3 ] ] )
__label__0 __slots__ = ( `` size_check '' , )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testmapstructure ( self ) : structure1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) structure2 = ( ( ( 7 , 8 ) , 9 ) , 10 , ( 11 , 12 ) ) structure1_plus1 = nest.map_structure ( lambda x : x + 1 , structure1 ) nest.assert_same_structure ( structure1 , structure1_plus1 ) self.assertallequal ( [ 2 , 3 , 4 , 5 , 6 , 7 ] , nest.flatten ( structure1_plus1 ) ) structure1_plus_structure2 = nest.map_structure ( lambda x , y : x + y , structure1 , structure2 ) self.assertequal ( ( ( ( 1 + 7 , 2 + 8 ) , 3 + 9 ) , 4 + 10 , ( 5 + 11 , 6 + 12 ) ) , structure1_plus_structure2 )
__label__0 library_path = _find_library ( base_paths , `` cusolver '' , cusolver_version )
__label__0 # get the name & index for each iterable parameter . if iterable_parameters is none : iterable_params = none else : arg_names = tf_inspect.getargspec ( dispatch_target ) .args iterable_params = [ ( name , arg_names.index ( name ) ) for name in iterable_parameters ]
__label__0 args : obj : the object to serialize
__label__0 _is_nested_or_composite = _pywrap_utils.isnestedorcomposite
__label__0 def testimageresizeextrapositionalargs ( self ) : for method in [ `` bilinear '' , `` area '' , `` bicubic '' , `` nearest_neighbor '' ] : text = `` tf.image.resize_ % s ( i , s , a , p ) '' % method expected_text = [ `` tf.image.resize ( i , s , `` , `` preserve_aspect_ratio=p , `` , `` method=tf.image.resizemethod. % s ) '' % method.upper ( ) ] _ , unused_report , unused_errors , new_text = self._upgrade ( text ) for s in expected_text : self.assertin ( s , new_text )
__label__0 @ atheris.instrument_func def testoneinput ( input_bytes ) : `` '' '' test randomized integer fuzzing input for v1 vs v2 apis . '' '' '' fh = fuzzinghelper ( input_bytes )
__label__0 def mkdir_and_copy_file ( self , header ) : install_dir = os.path.join ( self.install_dir , os.path.dirname ( header ) ) # windows platform uses `` \ '' in path strings , the external header location # expects `` / '' in paths . hence , we replaced `` \ '' with `` / '' for this reason install_dir = install_dir.replace ( '\\ ' , '/ ' ) # get rid of some extra intervening directories so we can have fewer # directories for -i install_dir = re.sub ( '/google/protobuf_archive/src ' , `` , install_dir )
__label__0 def testwarmstartvarprevvarpartitioned ( self ) : _ , weights = self._create_prev_run_var ( `` fruit_weights '' , shape= [ 4 , 1 ] , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) prev_val = np.concatenate ( [ weights [ 0 ] , weights [ 1 ] ] , axis=0 )
__label__0 def testgetargspeconcallableobject ( self ) :
__label__0 def __init__ ( self , wrapped , module_name , public_apis=none , deprecation=true , has_lite=false ) : super ( tfmodulewrapper , self ) .__init__ ( wrapped.__name__ ) fastmoduletype.set_getattr_callback ( self , tfmodulewrapper._getattr ) fastmoduletype.set_getattribute_callback ( self , tfmodulewrapper._getattribute ) self.__dict__.update ( wrapped.__dict__ ) # prefix all local attributes with _tfmw_ so that we can # handle them differently in attribute access methods . self._tfmw_wrapped_module = wrapped self._tfmw_module_name = module_name self._tfmw_public_apis = public_apis self._tfmw_print_deprecation_warnings = deprecation self._tfmw_has_lite = has_lite self._tfmw_is_compat_v1 = ( wrapped.__name__.endswith ( '.compat.v1 ' ) ) # set __all__ so that import * work for lazy loaded modules if self._tfmw_public_apis : self._tfmw_wrapped_module.__all__ = list ( self._tfmw_public_apis.keys ( ) ) self.__all__ = list ( self._tfmw_public_apis.keys ( ) ) else : if hasattr ( self._tfmw_wrapped_module , '__all__ ' ) : self.__all__ = self._tfmw_wrapped_module.__all__ else : self._tfmw_wrapped_module.__all__ = [ attr for attr in dir ( self._tfmw_wrapped_module ) if not attr.startswith ( ' _ ' ) ] self.__all__ = self._tfmw_wrapped_module.__all__
__label__0 def rocrand_version_number ( path ) : possible_version_files = [ `` include/rocrand/rocrand_version.h '' , # rocm 5.1 `` rocrand/include/rocrand_version.h '' , # rocm 5.0 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` rocrand version file not found in { } '' .format ( possible_version_files ) ) version_number = _get_header_version ( version_file , `` rocrand_version '' ) return version_number
__label__0 def testnonsharded ( self ) : save_dir = self._get_test_dir ( `` max_to_keep_non_sharded '' )
__label__0 import_header = `` import tensorflow.compat.v2 as tf\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` ` tensorflow.compat.v2 ` was directly imported as ` tf ` `` , report ) self.assertempty ( errors )
__label__0 def __reduce__ ( self ) : return __import__ , ( self.__name__ , )
__label__0 def testunrestrictedfunctionwarnings ( self ) : class foowarningspec ( ast_edits.noupdatespec ) : `` '' '' usages of function attribute foo ( ) prints out a warning . '' '' ''
__label__0 todo ( touts ) : make sure that all the devices found are on different job/replica/task/cpu|gpu . it would be bad if 2 were on the same device . it can happen if the devices are unspecified .
__label__0 new_node = ast.import ( new_aliases ) ast.copy_location ( new_node , node ) pasta.ast_utils.replace_child ( parent , node , new_node ) self.add_log ( info , node.lineno , node.col_offset , `` changed import from % r to % r . '' % ( pasta.dump ( node ) , pasta.dump ( new_node ) ) )
__label__0 fixed_config.default_value.copyfrom ( tensor_util.make_tensor_proto ( dense_defaults [ i ] ) ) # convert the shape from the attributes # into a tensorshapeproto . fixed_config.shape.copyfrom ( tensor_shape.tensorshape ( dense_shapes [ i ] ) .as_proto ( ) )
__label__0 def testpacksequenceas_compositetensor ( self ) : val = ragged_tensor.raggedtensor.from_row_splits ( values= [ 1 ] , row_splits= [ 0 , 1 ] ) with self.assertraisesregex ( valueerror , `` structure had 2 atoms , but flat_sequence had 1 items . `` ) : nest.pack_sequence_as ( val , [ val ] , expand_composites=true )
__label__0 # modules/classes we want to suppress entirely . self._private_map = { 'tf ' : [ 'compiler ' , 'core ' , # todo ( scottzhu ) : see b/227410870 for more details . currently # dtensor api is exposed under tf.experimental.dtensor , but in the # meantime , we have tensorflow/dtensor directory which will be treat # as a python package . we want to avoid step into the # tensorflow/dtensor directory when visit the api . # when the tf.dtensor becomes the public api , it will actually pick # up from tf.compat.v2.dtensor as priority and hide the # tensorflow/dtensor package . 'security ' , 'dtensor ' , 'python ' , 'tsl ' , # todo ( tlongeri ) : remove after tsl is moved out of tf . ] , # some implementations have this internal module that we should n't # expose . 'tf.flags ' : [ 'cpp_flags ' ] , }
__label__0 def testrenameconstant ( self ) : text = `` tf.monolithic_build\n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.sysconfig.monolithic_build\n '' ) text = `` some_call ( tf.monolithic_build ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` some_call ( tf.sysconfig.monolithic_build ) \n '' )
__label__0 saved_model_load_warning = ( ast_edits.warning , `` tf.saved_model.load works differently in 2.0 compared to 1.0. see `` `` migration information in the documentation of `` `` tf.compat.v1.saved_model.load . '' `` \nthe calls have been converted to compat.v1 . '' )
__label__0 args : obj : the class-attribute to hide from the generated docs .
__label__0 # tf.train.featurelists featurelists = dict [ str , featurelist ]
__label__0 api_attrs_v1 = { tensorflow_api_name : _attributes ( '_tf_api_names_v1 ' , '_tf_api_constants_v1 ' ) , keras_api_name : _attributes ( '_keras_api_names_v1 ' , '_keras_api_constants_v1 ' ) , }
__label__0 # handles # cmakedefine lines match = _cmake_define_regex.match ( line ) if match : name = match.group ( 1 ) suffix = match.group ( 2 ) or `` '' if name in cmake_vars : return `` # define { } { } \n '' .format ( name , _expand_variables ( suffix , cmake_vars ) ) else : return `` / * # undef { } * /\n '' .format ( name )
__label__0 for details , see https : //github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md. `` '' ''
__label__0 `` ` python c `` `
__label__0 def visit_import ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting an import node in the ast .
__label__0 if len ( global_step_read_tensors ) == 1 : return global_step_read_tensors [ 0 ] return none
__label__0 returns : a ` dict ` mapping ` api ` - > ` signatures ` , where ` api ` is a tensorflow api function ; and ` signatures ` is a list of dispatch signatures for ` api ` that include ` cls ` . ( each signature is a dict mapping argument names to type annotations ; see ` dispatch_for_api ` for more info . ) `` `` ''
__label__0 with context.graph_mode ( ) : with self.session ( graph=ops_lib.graph ( ) ) as sess : w3 = resource_variable_ops.resourcevariable ( 0.0 , name= '' w3 '' ) w4 = resource_variable_ops.resourcevariable ( 0.0 , name= '' w4 '' ) graph_saver = saver_module.saver ( [ w3 , w4 ] ) self.evaluate ( variables.global_variables_initializer ( ) ) graph_saver.restore ( sess , eager_ckpt_prefix ) self.assertallequal ( w3 , 3.0 ) self.assertallequal ( w4 , 4.0 )
__label__0 feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } feature : { float_list : { value : [ 2.0 ] } } } } } `` `
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 def handle ( self , op , args , kwargs ) : `` '' '' handle the specified operation with the specified arguments . '' '' ''
__label__0 def testdispatchforlist ( self ) :
__label__0 # # # # before & after usage example before :
__label__0 def get_arg_value ( node , arg_name , arg_pos=none ) : `` '' '' get the value of an argument from a ast.call node .
__label__0 raises : valueerror : ` axis ` is neither 0 or 1 .
__label__0 with tf.graph ( ) .as_default ( ) : ... add operations to the graph ... # create a supervisor that uses log directory on a shared file system . # indicate if you are the 'chief ' sv = supervisor ( logdir='/shared_directory/ ... ' , is_chief=is_chief ) # get a session in a tensorflow server on the cluster . with sv.managed_session ( server.target ) as sess : # use the session to train the graph . while not sv.should_stop ( ) : sess.run ( < my_train_op > ) `` `
__label__0 def _wrapped ( cls ) : setattr ( cls , _inheritable_header , text ) return cls
__label__0 contrib_summary_family_arg_comment = ( ast_edits.warning , `` < function name > replacement does not accept a 'family ' argument ; `` `` instead regular name scoping should be used . this call site specifies `` `` a family argument that has been removed on conversion , so the emitted `` `` tag names may be incorrect without manual editing . '' )
__label__0 the writer writes all chunks into a riegeli file . the chunk metadata ( chunkmetadata ) is written at the very end .
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 # remove a checkpoint and check that last checkpoints are # restored correctly . for fname in gfile.glob ( `` { } * '' .format ( s1 ) ) : gfile.remove ( fname ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s1 ) )
__label__0 class sequenceexample ( typing.namedtuple ) : context : dict [ str , feature ] feature_lists : featurelists `` `
__label__0 > > > class myclass ( object ) : ... @ cached_classproperty ... def value ( cls ) : ... print ( `` computing value '' ) ... return ' < property of % s > ' % cls.__name__ > > > class mysubclass ( myclass ) : ... pass > > > myclass.value computing value ' < property of myclass > ' > > > myclass.value # uses cached value ' < property of myclass > ' > > > mysubclass.value computing value ' < property of mysubclass > '
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import _function_parameter_canonicalizer_binding_for_test
__label__0 returns : the tuple ( new_index , child ) , where : * new_index - the updated index into ` flat ` having processed ` structure ` . * packed - the subset of ` flat ` corresponding to ` structure ` , having started at ` index ` , and packed into the same nested format .
__label__0 def wait_for_session ( self , master : str , config=none , max_wait_secs=float ( `` inf '' ) ) - > optional [ session.session ] : `` '' '' creates a new ` session ` and waits for model to be ready .
__label__0 @ dispatch.dispatch_for_api ( math_ops.multiply , { none : maskedtensor } ) def my_multiply ( x , y , name=none ) : # pylint : disable=unused-variable del x , y , name
__label__0 # restore with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : ds0 = dataset_ops.dataset.range ( 10 ) it0 = dataset_ops.make_initializable_iterator ( ds0 ) get_next0 = it0.get_next ( ) saveable0 = iterator_ops._iteratorsaveable ( it0._iterator_resource , name= '' saveable_it0 '' )
__label__0 # visitor that converts to v2 and checks v2 argument names . def conversion_visitor ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) if not tf_inspect.isfunction ( attr ) : continue names_v1 = tf_export.get_v1_names ( attr ) arg_names_v1 = get_args ( attr )
__label__0 data = open ( filename ) .read ( ) .strip ( ) items = data.split ( `` `` ) if len ( items ) == 1 : return none elif len ( items ) == 2 and items [ 0 ] == `` ref : '' : return items [ 1 ] .strip ( ) else : raise runtimeerror ( `` git directory has unparseable head '' )
__label__0 self.assertisinstance ( module.foo , lazy_loader.lazyloader )
__label__0 below is an example of migrating away from using a global step to using a keras optimizer :
__label__0 if library_size > approx_node_size : library_size -= function_splitter.build_chunks ( ) if library_size + approx_node_size > constants.max_size ( ) : approx_node_size -= node_splitter.build_chunks ( )
__label__0 rms0 = opt.get_slot ( var0 , `` rms '' ) self.asserttrue ( rms0 is not none ) rms1 = opt.get_slot ( var1 , `` rms '' ) self.asserttrue ( rms1 is not none ) mom0 = opt.get_slot ( var0 , `` momentum '' ) self.asserttrue ( mom0 is not none ) mom1 = opt.get_slot ( var1 , `` momentum '' ) self.asserttrue ( mom1 is not none )
__label__0 func_missing_params = { }
__label__0 ` tracetype ` is an abstract class that other classes might inherit from to provide information regarding associated class ( es ) for the purposes of tf.function tracing . the typing logic provided through this mechanism will be used to make decisions regarding usage of cached concrete functions and retracing .
__label__0 def _tf_data_yield_flat_up_to ( shallow_tree , input_tree ) : `` '' '' yields elements ` input_tree ` partially flattened up to ` shallow_tree ` . '' '' '' if _tf_data_is_nested ( shallow_tree ) : for shallow_branch , input_branch in zip ( _tf_data_yield_value ( shallow_tree ) , _tf_data_yield_value ( input_tree ) ) : for input_leaf in _tf_data_yield_flat_up_to ( shallow_branch , input_branch ) : yield input_leaf else : yield input_tree
__label__0 with session.session ( graph=ops_lib.graph ( ) ) as sess : saver_module.import_meta_graph ( meta_graph_def , clear_devices=true , import_scope= '' new_model '' ) self.evaluate ( variables.global_variables_initializer ( ) ) sess.run ( [ `` new_model/optimize '' ] , { `` new_model/image:0 '' : np.random.random ( [ 1 , 784 ] ) , `` new_model/label:0 '' : np.random.randint ( 10 , size= [ 1 , 10 ] ) } )
__label__0 __slots__ = [ `` __weakref__ '' ]
__label__0 * ` set ` ; ` { `` a '' , `` b '' } ` is an atom , while ` [ `` a '' , `` b '' ] ` is a nested structure . * [ ` dataclass ` classes ] ( https : //docs.python.org/library/dataclasses.html ) * ` tf.tensor ` * ` numpy.array `
__label__0 def testdispatcherrorfornosignature ( self ) : with self.assertraisesregex ( valueerror , `` must be called with at least one signature '' ) :
__label__0 self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) - ( 0.5 * ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) + ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 2e-5 ) ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) - ( 0.5 * ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) + ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 2e-5 ) ) ) ] ) , self.evaluate ( var1 ) )
__label__0 def testwithlambda ( self ) : anon_fn = lambda x : x code = function_utils.get_func_code ( anon_fn ) self.assertisnotnone ( code ) self.assertregex ( code.co_filename , 'function_utils_test.py ' )
__label__0 def testgetargspecthatcontainsvarargsandkwonlyargs ( self ) : argspec = tf_inspect.getargspec ( test_decorated_function_with_varargs_and_kwonlyargs ) self.assertequal ( [ ' b ' , ' c ' ] , argspec.args ) self.assertequal ( ( 2 , 'hello ' ) , argspec.defaults )
__label__0 text = ( `` tf.sparse.split ( sp_input=sp_input , num_split=num_split , `` `` name=name , split_dim=axis ) '' ) expected_text = ( `` tf.sparse.split ( sp_input=sp_input , num_split=num_split , `` `` name=name , axis=axis ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 # modules/classes we do not want to descend into if we hit them . usually , # system modules exposed through platforms for compatibility reasons . # each entry maps a module path to a name to ignore in traversal . self._do_not_descend_map = { 'tf ' : [ 'examples ' , 'flags ' , # do n't add flags # todo ( drpng ) : this can be removed once sealed off . 'platform ' , # todo ( drpng ) : this can be removed once sealed . 'pywrap_tensorflow ' , # todo ( drpng ) : this can be removed once sealed . 'user_ops ' , 'tools ' , 'tensorboard ' , ] ,
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # make sure output directory does not overlap with root_directory norm_root = os.path.split ( os.path.normpath ( root_directory ) ) norm_output = os.path.split ( os.path.normpath ( output_root_directory ) ) if norm_root == norm_output : print ( `` output directory % r same as input directory % r '' % ( root_directory , output_root_directory ) ) sys.exit ( 1 )
__label__0 class _object ( object ) :
__label__0 # we have initial tokens in the queue so we can call this one by one . after # the first step , this will no longer work as there will be no more extra # tokens in the queue . sessions [ 0 ] .run ( train_ops [ 0 ] ) sessions [ 1 ] .run ( train_ops [ 1 ] )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' functions used to extract and analyze stacks . faster than python libs . '' '' '' # pylint : disable=g-bad-name import collections import inspect import threading
__label__0 arguments : fn : the function or method to decorate . any exception raised within the function will be reraised with its internal stack trace frames filtered out .
__label__0 def visit_attribute ( self , node ) : # pylint : disable=invalid-name `` '' '' handle bare attributes i.e . [ tf.foo , tf.bar ] . '' '' '' full_name = self._get_full_name ( node ) if full_name : detection = self._api_analysis_spec.symbols_to_detect.get ( full_name , none ) if detection : self.add_result ( detection ) self.add_log ( detection.log_level , node.lineno , node.col_offset , detection.log_message )
__label__0 fn_has_kwargs = lambda * * x : x self.asserttrue ( function_utils.has_kwargs ( fn_has_kwargs ) )
__label__0 # these flags are required by infrastructure , not all of them are used . flags.define_string ( 'output_dir ' , none , ( `` use this branch as the root version and do n't '' ' create in version directory ' ) )
__label__0 all elements of the required_version need to be present in the actual_version .
__label__0 # positional * args passed in that we can not inspect , should warn text = `` f ( a , * args ) \n '' ( _ , report , _ ) , _ = self._upgrade ( renamekeywordspec ( ) , text ) self.assertnotin ( `` manual check required '' , report )
__label__0 # pylint : disable=protected-access def __init__ ( self , name , table_ref=none ) : if table_ref is none : self.table_ref = gen_lookup_ops.mutable_hash_table_v2 ( key_dtype=dtypes.string , value_dtype=dtypes.float32 , name=name ) else : self.table_ref = table_ref self._name = name if not context.executing_eagerly ( ) : self._saveable = checkpointedop.customsaveable ( self , name ) ops_lib.add_to_collection ( ops_lib.graphkeys.saveable_objects , self._saveable )
__label__0 the function _print_warning_for_function matches the full name of the called function , e.g. , tf.foo.bar ( ) . this function matches the function name that is called , as long as the function is an attribute . for example , ` tf.foo.bar ( ) ` and ` foo.bar ( ) ` are matched , but not ` bar ( ) ` .
__label__0 self.assertfalse ( errors )
__label__0 class invalidsymbolnameerror ( exception ) : `` '' '' raised when trying to export symbol as an invalid or unallowed name . '' '' ''
__label__0 with self.assertraisesregex ( typeerror , `` callable '' ) : nest.map_structure ( `` bad '' , structure1_plus1 )
__label__0 this behaves like a regular op - in eager mode , it immediately starts execution , returning results . in graph mode , it creates ops which return symbolic tensorflow values ( like ` tf.tensor ` , ` tf.data.dataset ` , etc. ) . for example , ` tf.function ` callables typically generate a ` tf.raw_ops.partitionedcall ` op , but not always - the exact operations being generated are an internal implementation detail .
__label__0 # verifies savers is saved as bytes_list for meta_graph_def . collection_def = meta_graph_def.collection_def [ `` savers '' ] kind = collection_def.whichoneof ( `` kind '' ) self.assertequal ( kind , `` bytes_list '' ) # verifies that there are 2 entries in savers collection . savers = getattr ( collection_def , kind ) self.assertequal ( 2 , len ( savers.value ) )
__label__0 hipsolver_config = { `` hipsolver_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 note : this function is intended to support internal legacy use cases ( such as raggedtensorvalue ) , and will probably not be exposed as a public api .
__label__0 a structure is either :
__label__0 super ( syncreplicasoptimizer , self ) .__init__ ( use_locking , name ) logging.info ( `` syncreplicasv2 : replicas_to_aggregate= % s ; total_num_replicas= % s '' , replicas_to_aggregate , total_num_replicas ) self._opt = opt self._replicas_to_aggregate = replicas_to_aggregate self._gradients_applied = false self._variable_averages = variable_averages self._variables_to_average = variables_to_average self._total_num_replicas = total_num_replicas self._tokens_per_step = max ( total_num_replicas , replicas_to_aggregate ) self._global_step = none self._sync_token_queue = none
__label__0 field , _ = util.get_field ( proto , [ `` field_one '' , 2 , 1 , `` string_field '' ] ) self.assertequal ( `` inner_inner_string '' , field )
__label__0 self.assertequal ( 3 , wrapped_fn ( 3 ) ) self.assertequal ( 3 , wrapped_fn ( a=3 ) )
__label__0 def testdoctestdoesnotload ( self ) : module = types.moduletype ( `` mytestmodule '' ) module.foo = lazy_loader.lazyloader ( `` foo '' , module.__dict__ , `` os.path '' )
__label__0 def create_slot ( primary , val , name , colocate_with_primary=true , * , copy_xla_sharding=false ) : `` '' '' create a slot initialized to the given value .
__label__0 return tf_decorator.make_decorator ( target , wrapper )
__label__0 for example :
__label__0 class allrenamesv2test ( test_util.tensorflowtestcase ) :
__label__0 conditionally conformant pair of ` sequenceexample ` s ; the parser configuration determines if the feature sizes must match :
__label__0 import os import sys import threading import traceback import types from tensorflow.python.util import tf_decorator from tensorflow.python.util.tf_export import tf_export
__label__0 > > > tf.nest.assert_same_structure ( 1.5 , tf.variable ( 1 , tf.uint32 ) ) > > > tf.nest.assert_same_structure ( `` abc '' , np.array ( [ 1 , 2 ] ) )
__label__0 # note : ` expeced ` does not contain keys or values from sillytensor . targets = dispatch.type_based_dispatch_signatures_for ( maskedtensor ) expected = { math_ops.add : [ { `` x '' : maskedtensor , `` y '' : maskedtensor } ] , array_ops.concat : [ { `` values '' : maskedtensorlist } ] } self.assertequal ( targets , expected )
__label__0 raise configerror ( ' # define `` { } '' is either\n'.format ( name ) + `` not present in file { } or\n '' .format ( path ) + `` its value is not an integer literal '' )
__label__0 # insert a newline after the import if necessary old_suffix = pasta.base.formatting.get ( node , `` suffix '' ) if old_suffix is none : old_suffix = os.linesep if os.linesep not in old_suffix : pasta.base.formatting.set ( node , `` suffix '' , old_suffix + os.linesep )
__label__0 > > > tuple = ( ( 1.0 , 2.0 ) , ( 3.0 , 4.0 , 5.0 ) , 6.0 ) > > > tf.nest.flatten ( tuple ) [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ]
__label__0 def test_bound_method_no_self ( self ) :
__label__0 returns : a tuple of the tensor name and var. `` '' '' if checkpoint_utils._is_variable ( var ) : # pylint : disable=protected-access current_var_name = _infer_var_name ( [ var ] ) elif ( isinstance ( var , list ) and all ( checkpoint_utils._is_variable ( v ) for v in var ) ) : # pylint : disable=protected-access current_var_name = _infer_var_name ( var ) elif isinstance ( var , variables_lib.partitionedvariable ) : current_var_name = _infer_var_name ( [ var ] ) var = var._get_variable_list ( ) # pylint : disable=protected-access else : raise typeerror ( `` var must be one of the following : a variable , list of variable or `` `` partitionedvariable , but is { } '' .format ( type ( var ) ) ) if not prev_tensor_name : # assume tensor name remains the same . prev_tensor_name = current_var_name
__label__0 @ test_util.run_in_graph_and_eager_modes @ test.mock.patch.object ( saver_module , `` time '' ) def testnonsharded ( self , mock_time ) : save_dir = self._get_test_dir ( `` keep_checkpoint_every_n_hours '' )
__label__0 args : func : decorated symbol ( function or class ) .
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 def _find_header ( base_paths , header_name , required_version , get_version ) : `` '' '' returns first valid path to a header that matches the requested version . '' '' '' return _find_versioned_file ( base_paths , _header_paths ( ) , header_name , required_version , get_version )
__label__0 5 . ` tf.tensor ` ( will not flatten ) :
__label__0 - for modality.core : refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 args : symbol : api function or class . api_name : api name . currently , only ` tensorflow ` . add_prefix_to_v1_names : specifies whether a name available only in v1 should be prefixed with compat.v1 .
__label__0 class publicapivisitor : `` '' '' visitor to use with ` traverse ` to visit exactly the public tf api . '' '' ''
__label__0 def _traverse_internal ( root , visit , stack , path ) : `` '' '' internal helper for traverse . '' '' ''
__label__0 # verifies savers collection is saved as bytes_list for meta_graph_def1 . collection_def = meta_graph_def1.collection_def [ `` savers '' ] kind = collection_def.whichoneof ( `` kind '' ) self.assertequal ( kind , `` bytes_list '' )
__label__0 slots are typically used as accumulators to track values associated with the primary object :
__label__0 # positional * args passed in that we can not inspect , should warn text = `` f ( a , * args , kw1=c ) \n '' ( _ , report , _ ) , _ = self._upgrade ( reorderandrenamekeywordspec ( ) , text ) self.assertin ( `` manual check required '' , report )
__label__0 self.assertequal ( { ' a ' : 5 } , tf_inspect.getcallargs ( func , a=5 ) )
__label__0 attributes in public classes can be hidden from the api by having an ' _ ' in front of the name ( e.g . classname._variables ) . this does n't work when attributes or methods are inherited from a parent class . to hide inherited attributes , set their values to be ` deprecation.hide_attribute_from_api ` . `` '' ''
__label__0 from tensorflow.python.platform import googletest from tensorflow.tools.docs import generate2
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenwithtuplepaths ( self ) : mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) ) mt_flat_paths = nest.flatten_with_tuple_paths ( mt ) self.assertequal ( mt_flat_paths [ 0 ] [ 0 ] , ( 0 , ) ) self.assertallequal ( mt_flat_paths [ 0 ] [ 1 ] , [ 1 ] )
__label__0 class slotcreatortest ( test.testcase ) :
__label__0 raises : runtimeerror : if the version string is not valid. `` '' '' # check validity of new version string . if not re.search ( r '' [ 0-9 ] +\. [ 0-9 ] +\ . [ a-za-z0-9 ] + '' , string ) : raise runtimeerror ( `` invalid version string : % s '' % string )
__label__0 return version ( old_major , old_minor , old_patch_num , old_extension , version_type )
__label__0 * get all test names uniquified ( e.g . display a list of available tests ) : select distinct on ( test ) test from test
__label__0 ` mirrored ` values are like ` perreplica ` values , except we know that the value on all replicas are the same . ` mirrored ` values are kept synchronized by the distribution strategy in use , while ` perreplica ` values are left unsynchronized . ` mirrored ` values typically represent model weights . we can safely read a ` mirrored ` value in a cross-replica context by using the value on any replica , while perreplica values should not be read or manipulated in a cross-replica context . ''
__label__0 @ dispatch.dispatch_for_api ( math_ops.add ) def silly_add ( x : sillytensor , y : sillytensor ) : del x , y
__label__0 class testvisitor ( object ) :
__label__0 ones = init_ops.ones_initializer norms = init_ops.truncated_normal_initializer rand = init_ops.random_uniform_initializer zeros = init_ops.zeros_initializer
__label__0 def testmanagedsessiondonotkeepsummarywriter ( self ) : logdir = self._test_dir ( `` managed_not_keep_summary_writer '' ) with ops.graph ( ) .as_default ( ) : summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sv = supervisor.supervisor ( logdir=logdir , summary_op=none ) with sv.managed_session ( `` '' , close_summary_writer=true , start_standard_services=false ) as sess : sv.summary_computed ( sess , sess.run ( summ ) ) # sleep 1.2s to make sure that the next event file has a different name # than the current one . time.sleep ( 1.2 ) with sv.managed_session ( `` '' , close_summary_writer=true , start_standard_services=false ) as sess : sv.summary_computed ( sess , sess.run ( summ ) ) event_paths = sorted ( glob.glob ( os.path.join ( logdir , `` event * '' ) ) ) self.assertequal ( 2 , len ( event_paths ) ) # the two event files should have the same contents . for path in event_paths : # the summary iterator should report the summary once as we closed the # summary writer across the 2 sessions . rr = summary_iterator.summary_iterator ( path ) # the first event should list the file_version . ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version )
__label__0 note that this can only be called after calling apply_gradients ( ) which actually generates this queuerunner .
__label__0 contents = `` '' '' # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== \ '' \ '' \ '' auto-generated module providing information about the build.\ '' \ '' \ '' import collections
__label__0 def _find_hipfft_config ( rocm_install_path ) :
__label__0 def main ( ) : try : for key , value in sorted ( find_rocm_config ( ) .items ( ) ) : print ( `` % s : % s '' % ( key , value ) ) except configerror as e : sys.stderr.write ( `` \nerror : { } \n\n '' .format ( str ( e ) ) ) sys.exit ( 1 )
__label__0 for i in range ( num_threads ) : threads [ i ] .start ( ) for i in range ( num_threads ) : threads [ i ] .join ( )
__label__0 # replace first component of from-import with new name . new_from_import = ( import_rename_spec.new_name + from_import [ len ( from_import_first_component ) : ] ) updated_node = ast.importfrom ( new_from_import , updated_aliases , node.level ) ast.copy_location ( updated_node , node ) pasta.ast_utils.replace_child ( parent , node , updated_node )
__label__0 if the output is not used , a ` logging.error ` is logged . if ` error_in_function ` is set , then a ` runtimeerror ` will be raised at the end of function tracing if the output is not used by that point .
__label__0 def getsource ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getsource . '' '' '' return _inspect.getsource ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 print ( deprecated_cls.__name__ ) print ( deprecated_cls.__module__ ) print ( deprecated_cls.__doc__ )
__label__0 ` elementwise_assert_handler ( assert_func , x , y ) `
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testsetupfail ( self ) : logdir = self._test_dir ( `` setup_fail '' ) with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) with self.assertraisesregex ( valueerror , `` must have their device set '' ) : supervisor.supervisor ( logdir=logdir , is_chief=false ) with ops.graph ( ) .as_default ( ) , ops.device ( `` /job : ps '' ) : variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) supervisor.supervisor ( logdir=logdir , is_chief=false )
__label__0 goodbye `` '' '' ) ]
__label__0 self.assertequal ( `` fn doc . ( deprecated ) '' `` \n '' `` \ndeprecated : this function is deprecated. `` `` it will be removed in a future version . '' `` \ninstructions for updating : \n % s '' `` \n '' `` \nargs : '' `` \n arg0 : arg 0 . '' `` \n arg1 : arg 1 . '' `` \n '' `` \nreturns : '' `` \n sum of args . '' % instructions , _fn.__doc__ )
__label__0 calling < function > ( from < module > ) with < arg > is deprecated and will be removed after < date > . instructions for updating : < instructions >
__label__0 self.asserttrue ( os.path.islink ( file_d ) ) self.assertequal ( file_c , os.readlink ( file_d ) ) # file pointed to by symlink is in a different directory . # therefore , it should not be upgraded . with open ( file_c , `` r '' ) as f : self.assertequal ( `` import foo as f '' , f.read ( ) )
__label__0 self.assertis ( override , override_for_test_op )
__label__0 def testduplicatedispatchforunaryelementwiseapiserror ( self ) :
__label__0 # worker 1 finished later and its gradients will now be dropped as it is # stale . sessions [ 1 ] .run ( train_ops [ 1 ] )
__label__0 def is_generic_union ( tp ) : `` '' '' returns true if ` tp ` is a parameterized typing.union value . '' '' '' return ( tp is not typing.union and getattr ( tp , '__origin__ ' , none ) is typing.union )
__label__0 goodbye `` '' '' [ 1 : ] ) , ( 'last ' , [ ( 'code ' , none ) ] , `` '' '' hello
__label__0 def testtwojobs ( self ) : cluster_def = server_lib.clusterspec ( { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : [ `` worker0:2222 '' , `` worker1:2222 '' , `` worker2:2222 '' ] } ) .as_cluster_def ( ) server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_def , job_name= '' worker '' , task_index=2 , protocol= '' grpc '' )
__label__0 def __init__ ( self , decorator_name , target , decorator_doc= '' , decorator_argspec=none ) : self._decorated_target = target self._decorator_name = decorator_name self._decorator_doc = decorator_doc self._decorator_argspec = decorator_argspec if hasattr ( target , '__name__ ' ) : self.__name__ = target.__name__ if hasattr ( target , '__qualname__ ' ) : self.__qualname__ = target.__qualname__ if self._decorator_doc : self.__doc__ = self._decorator_doc elif hasattr ( target , '__doc__ ' ) and target.__doc__ : self.__doc__ = target.__doc__ else : self.__doc__ = ``
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 def get_init_tokens_op ( self , num_tokens=-1 ) : `` '' '' returns the op to fill the sync_token_queue with the tokens .
__label__0 def parse_branch_ref ( filename ) : `` '' '' given a filename of a .git/head file return ref path .
__label__0 def testseparableconv2d ( self ) : text = `` tf.nn.separable_conv2d ( inp , d , pt , strides , pad , rate , name , fmt ) '' expected_text = ( `` tf.nn.separable_conv2d ( inp , d , pt , strides , pad , dilations=rate , `` `` name=name , data_format=fmt ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 class fencedcelloutputchecker ( tf_doctest_lib.tfdoctestoutputchecker ) : `` '' '' tfdoctestchecker with a different warning message . '' '' '' message = textwrap.dedent ( `` '' '' \n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # check the documentation ( go/g3doctest ) on how to write # testable g3docs . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # `` '' '' )
__label__0 this proto implements the ` list [ float ] ` portion .
__label__0 import pasta
__label__0 # non-equivalent namedtuples . bar = collections.namedtuple ( `` bar '' , [ `` a '' , `` b '' ] ) self.assertfalse ( nest.same_namedtuples ( foo1 ( 1 , 2 ) , bar ( 1 , 2 ) ) ) fooxy = collections.namedtuple ( `` foo '' , [ `` x '' , `` y '' ] ) self.assertfalse ( nest.same_namedtuples ( foo1 ( 1 , 2 ) , fooxy ( 1 , 2 ) ) )
__label__0 class _tfshouldusehelper ( object ) : `` '' '' object stored in tfshoulduse-wrapped objects .
__label__0 for orig , restored in zip ( orig_vals , restored_vals ) : self.assertallequal ( orig , restored )
__label__0 test_name = str ( test_result [ `` name '' ] ) start_time = datetime.datetime.utcfromtimestamp ( float ( test_result [ `` starttime '' ] ) ) batch = [ ]
__label__0 args : arg0 : arg 0. arg1 : arg 1. deprecated : deprecated !
__label__0 def __init__ ( self , table , name ) : tensors = table._export ( ) specs = [ saver_module.basesaverbuilder.savespec ( tensors [ 0 ] , `` '' , name + `` -keys '' ) , saver_module.basesaverbuilder.savespec ( tensors [ 1 ] , `` '' , name + `` -values '' ) ] super ( checkpointedop.customsaveable , self ) .__init__ ( table , specs , name )
__label__0 args : sess : a ` session ` object . summary : a summary proto , or a string holding a serialized summary proto . global_step : int . global step this summary is associated with . if ` none ` , it will try to fetch the current step .
__label__0 the install_headers that comes with setuptools copies all files to the same directory . but we need the files to be in a specific directory hierarchy for -i < include_dir > to work correctly. `` '' '' description = 'install c/c++ header files '
__label__0 module_deprecations = { `` tf.contrib '' : _contrib_warning , `` tf.contrib.cudnn_rnn '' : _contrib_cudnn_rnn_warning , `` tf.contrib.rnn '' : _contrib_rnn_warning , `` tf.flags '' : _flags_warning , `` tf.app.flags '' : _flags_warning , `` tf.contrib.distribute '' : _contrib_dist_strat_warning , `` tf.contrib.seq2seq '' : _contrib_seq2seq_warning }
__label__0 def set_body ( ns ) : ns.update ( tx.__dict__ ) return ns
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 def _wrap_key ( self , key ) : return _objectidentitywrapper ( key )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' exposes the python wrapper for graph transforms . '' '' '' # pylint : disable=unused-import , wildcard-import , line-too-long from tensorflow.core.framework import graph_pb2 from tensorflow.python.util import compat from tensorflow.python.util._pywrap_transform_graph import transformgraphwithstringinputs
__label__0 raises : typeerror : if the arguments do not have the appropriate type . valueerror : if an argument is not specified and can not be inferred. `` '' '' server_def = tensorflow_server_pb2.serverdef ( ) if isinstance ( server_or_cluster_def , tensorflow_server_pb2.serverdef ) : server_def.mergefrom ( server_or_cluster_def ) if job_name is not none : server_def.job_name = job_name if task_index is not none : server_def.task_index = task_index if protocol is not none : server_def.protocol = protocol if config is not none : server_def.default_session_config.mergefrom ( config ) else : try : cluster_spec = clusterspec ( server_or_cluster_def ) except typeerror : raise typeerror ( `` could not convert ` server_or_cluster_def ` to a `` `` ` tf.train.serverdef ` or ` tf.train.clusterspec ` . '' ) if job_name is none : if len ( cluster_spec.jobs ) == 1 : job_name = cluster_spec.jobs [ 0 ] else : raise valueerror ( `` must specify an explicit ` job_name ` . '' ) if task_index is none : task_indices = cluster_spec.task_indices ( job_name ) if len ( task_indices ) == 1 : task_index = task_indices [ 0 ] else : raise valueerror ( `` must specify an explicit ` task_index ` . '' ) if protocol is none : protocol = `` grpc ''
__label__0 def __iter__ ( self ) : for key in self._storage : yield key.unwrapped
__label__0 args : filename : filename to write to . git_version : the result of a git describe. `` '' '' if b '' \ '' '' in git_version or b '' \\ '' in git_version : git_version = b '' git_version_is_invalid '' # do not cause build to fail ! contents = `` '' '' / * generated by gen_git_source.py * /
__label__0 def testfilenametensor ( self ) : # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) : v0 = variable_v1.variablev1 ( 0 , name= '' v0 '' ) filename = b '' somerandomfilename '' save = saver_module.saver ( { `` v0 '' : v0 } , filename=filename ) with self.cached_session ( ) as sess : tensor = sess.graph.get_tensor_by_name ( save.saver_def.filename_tensor_name ) self.assertequal ( self.evaluate ( tensor ) , filename )
__label__0 * entry : - test : unique name of this test ( string ) - entry : unique name of this benchmark entry within this test ( string ) - start : start time of this test run ( datetime ) - timing : average time ( usec ) per iteration of this test/entry run ( float ) - info : json-encoded entry metadata ( string , not indexed )
__label__0 # running on worker with the new session should work as expected v = sess_new.run ( b ) self.assertallequal ( v , [ 2.0 ] )
__label__0 examples :
__label__0 init_args = [ ]
__label__0 child_path = path + ' . ' + name if path else name _traverse_internal ( child , visit , new_stack , child_path )
__label__0 return self.tfnamescore ( canonical_score , super ( ) ._score_name ( path ) )
__label__0 self.assertequal ( { } , tf_inspect.getcallargs ( empty ) )
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testinitfn ( self ) : logdir = self._test_dir ( `` default_init_op '' ) with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] )
__label__0 returns : string. `` '' '' return `` prop_with_doc ''
__label__0 self.module_deprecations = { }
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_with_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 text = `` from tensorflow.compat import v2 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 # verifies copy to the same graph . with graph1.as_default ( ) : var_list_2 = meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden1 '' , to_scope= '' hidden2 '' )
__label__0 def __init__ ( self ) : ast_edits.noupdatespec.__init__ ( self ) self.import_renames = { `` foo '' : ast_edits.importrename ( `` bar '' , excluded_prefixes= [ `` foo.baz '' ] ) }
__label__0 def _tfmw_add_deprecation_warning ( self , name , attr ) : `` '' '' print deprecation warning for attr with given name if necessary . '' '' '' if ( self._tfmw_warning_count < _per_module_warning_limit and name not in self._tfmw_deprecated_checked ) :
__label__0 # numpy 1.8 scalars do n't inherit from numbers.integral in python 3 , so we # need to check them specifically . the same goes from real and complex . integral_types = ( _numbers.integral , _np.integer ) tf_export ( 'compat.integral_types ' ) .export_constant ( __name__ , 'integral_types ' ) real_types = ( _numbers.real , _np.integer , _np.floating ) tf_export ( 'compat.real_types ' ) .export_constant ( __name__ , 'real_types ' ) complex_types = ( _numbers.complex , _np.number ) tf_export ( 'compat.complex_types ' ) .export_constant ( __name__ , 'complex_types ' )
__label__0 def _make_default_values ( fullargspec : inspect.fullargspec ) - > dict [ str , any ] : `` '' '' returns default values from the function 's fullargspec . '' '' '' if fullargspec.defaults is not none : defaults = { name : value for name , value in zip ( fullargspec.args [ -len ( fullargspec.defaults ) : ] , fullargspec.defaults ) } else : defaults = { }
__label__0 def testsplit ( self ) : s = repeatedstringsplitter ( test_message_pb2.repeatedstring ( strings= [ ] ) ) chunks = s.split ( ) [ 0 ] self.assertlen ( chunks , 1 ) self.assertisinstance ( chunks [ 0 ] , test_message_pb2.repeatedstring )
__label__0 args : target : the tensorflow api that should support dispatch . iterable_parameters : optional list of parameter names that may be called with iterables ( such as the ` inputs ` parameter for ` tf.add_n ` ) .
__label__0 from absl import logging import riegeli
__label__0 returns : the ` fullargspec ` that describes the signature of the outermost decorator that changes the callable 's signature . if the callable is not decorated , ` inspect.getfullargspec ( ) ` will be called directly on the callable. `` '' '' decorators , target = tf_decorator.unwrap ( obj )
__label__0 cell_index += 1
__label__0 # step 2 : the root mean square accumulators contain the previous update . self.evaluate ( update ) # check the rms accumulators . self.assertallcloseaccordingtotype ( np.array ( [ 0.901 * 0.9 + 0.001 , 0.901 * 0.9 + 0.001 ] ) , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.90001 * 0.9 + 1e-5 , 0.90001 * 0.9 + 1e-5 ] ) , self.evaluate ( rms1 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.5 * ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) + ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1e-5 ) ) , 0.5 * ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) + ( 0.1 * 2.0 / math.sqrt ( 0.901 * 0.9 + 0.001 + 1e-5 ) ) ] ) , self.evaluate ( mom0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.5 * ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) + ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 2e-5 ) ) , 0.5 * ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) + ( 0.01 * 2.0 / math.sqrt ( 0.90001 * 0.9 + 2e-5 ) ) ] ) , self.evaluate ( mom1 ) )
__label__0 def testallapiv1 ( self ) : collect = true v1_symbols = set ( [ ] )
__label__0 this wrapper adds the decorated function as an api that can be overridden using the ` @ dispatch_for_api ` decorator . in the following example , we first define a new api ( ` double ` ) that supports dispatch , then define a custom type ( ` maskedtensor ` ) and finally use ` dispatch_for_api ` to override the default implementation of ` double ` when called with ` maskedtensor ` values :
__label__0 name_found , name = ast_edits.get_arg_value ( node , `` name '' , 0 ) default_found , default_name = ast_edits.get_arg_value ( node , `` default_name '' , 1 )
__label__0 def build_chunks ( self ) - > int : `` '' '' creates a chunk for the entire proto and returns the original size . '' '' '' if self.size_check ( self.proto_size ) : new_proto = type ( self._proto ) ( ) new_proto.mergefrom ( self._proto ) self._proto.clear ( ) self.add_chunk ( new_proto , [ ] ) return self.proto_size return 0
__label__0 1. iterate over it to generate the input for a single device or multiple devices , which is a ` tf.distribute.distributedvalues ` instance . to do this , you can :
__label__0 return module
__label__0 text = `` import foo as f , a as b '' expected_text = `` import bar as f , a as b '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 @ property def unwrapped ( self ) : return self._wrapped ( )
__label__0 def test_contrib_summary_create_file_writer ( self ) : text = ( `` tf.contrib.summary.create_file_writer ( 'my_logdir ' , 0 , 1000 , `` `` '.foo ' , 'shared-name ' ) '' ) expected = ( `` tf.compat.v2.summary.create_file_writer ( logdir='my_logdir ' , `` `` max_queue=0 , flush_millis=1000 , filename_suffix='.foo ' ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'name ' argument '' , errors [ 0 ] ) self.assertin ( `` no longer re-uses existing event files '' , errors [ 1 ] )
__label__0 def __len__ ( self ) : return len ( self._storage )
__label__0 for input_path , output_path in files_to_process : output_directory = os.path.dirname ( output_path ) if not os.path.isdir ( output_directory ) : os.makedirs ( output_directory )
__label__0 the ` save_path ` argument is typically a value previously returned from a ` save ( ) ` call , or a call to ` latest_checkpoint ( ) ` .
__label__0 warning = textwrap.dedent ( `` '' '' \n note : ` tf.raw_ops ` provides direct/low level access to all tensorflow ops . see [ the rfc ] ( https : //github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md ) for details . unless you are library writer , you likely do not need to use these ops directly . '' '' '' )
__label__0 if __name__ == `` __main__ '' : test_lib.main ( )
__label__0 def assert_trace_line_count ( self , fn , count , filtering_enabled=true ) : trace_line_count = -1 if filtering_enabled : traceback_utils.enable_traceback_filtering ( ) else : traceback_utils.disable_traceback_filtering ( ) self.assertequal ( traceback_utils.is_traceback_filtering_enabled ( ) , filtering_enabled ) try : fn ( ) except exception as e : # pylint : disable=broad-except # we must count lines rather than frames because autograph transforms # stack frames into a single large string trace = '\n'.join ( traceback.format_tb ( e.__traceback__ ) ) trace_line_count = len ( trace.split ( '\n ' ) )
__label__0 def build_chunks ( self ) : pass
__label__0 if you want to retry the training loop in case of preemption you can do it as follows :
__label__0 self.assertequal ( [ `` test '' , `` deprecated '' , `` deprecated again '' ] , myclass.init_args )
__label__0 if not var_name_to_prev_var_name : # detect whether the checkpoint is object-based , in which case the # var_name_to_prev_var_name dictionary should map variable names to # checkpoint keys . if the user has specified var_name_to_prev_var_name , we # do not override it . var_name_to_prev_var_name = _get_object_checkpoint_renames ( ckpt_to_initialize_from , grouped_variables.keys ( ) )
__label__0 raw tensorflow stack traces involve many internal frames , which can be challenging to read through , while not being actionable for end users . by default , tensorflow filters internal frames in most exceptions that it raises , to keep stack traces short , readable , and focused on what 's actionable for end users ( their own code ) .
__label__0 args : directory : string of directory origin_tag : str of the old python version tag new_tag : str of the new tag version : the version of the package package : str , name of the package
__label__0 return decorator_utils.add_notice_to_docstring ( doc , instructions , 'deprecated function arguments ' , ' ( deprecated arguments ) ' , [ 'some arguments are deprecated : ` ( % s ) ` . ' 'they will be removed % s . ' % ( deprecation_string , 'in a future version ' if date is none else ( 'after % s ' % date ) ) , 'instructions for updating : ' ] , notice_type='deprecated ' )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' test cases for segment reduction ops . '' '' ''
__label__0 raises : typeerror : if ` shallow_tree ` is a nested structure but ` input_tree ` is not . typeerror : if the structure types of ` shallow_tree ` are different from ` input_tree ` . valueerror : if the structure lengths of ` shallow_tree ` are different from ` input_tree ` . `` '' '' is_nested_fn = _is_nested_or_composite if expand_composites else is_nested assert_shallow_structure ( shallow_tree , input_tree , check_types=check_types , expand_composites=expand_composites ) return list ( nest_util.yield_flat_up_to ( nest_util.modality.core , shallow_tree , input_tree , is_nested_fn ) )
__label__0 def testwarmstartvarwithcolumnvocabprevvarpartitioned ( self ) : prev_vocab_path = self._write_vocab ( [ `` apple '' , `` orange '' ] , `` old_vocab '' ) self._create_prev_run_var ( `` fruit_output_layer '' , shape= [ 4 , 2 ] , initializer= [ [ 0.5 , 0.3 ] , [ 1. , 0.8 ] , [ 1.5 , 1.2 ] , [ 2. , 2.3 ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] )
__label__0 if __name__ == '__main__ ' : ops.enable_eager_execution ( ) test.main ( )
__label__0 class repeatedstringsplitter ( split.composablesplitter ) : `` '' '' splits a repeatedstring proto with n repeated strings into n chunks . '' '' ''
__label__0 def testsavecopyrestorewithsaverelativepaths ( self ) : `` '' '' save , copy checkpoint dir and restore from copied dir .
__label__0 save_path = compat.as_str ( save_path ) if global_step is not none : if not isinstance ( global_step , compat.integral_types ) : global_step = training_util.global_step ( sess , global_step ) checkpoint_file = `` % s- % d '' % ( save_path , global_step ) if self._pad_step_number : # zero-pads the step numbers , so that they are sorted when listed . checkpoint_file = `` % s- % s '' % ( save_path , `` { :08d } '' .format ( global_step ) ) else : checkpoint_file = save_path if os.path.basename ( save_path ) == latest_filename and not self._sharded : # guard against collision between data file and checkpoint state file . raise valueerror ( `` 'latest_filename ' collides with 'save_path ' : ' % s ' and ' % s ' '' % ( latest_filename , save_path ) )
__label__0 def testmathcountnonzerochanges ( self ) : text = ( `` tf.math.count_nonzero ( input_tensor=input , dtype=dtype , name=name , `` `` reduction_indices=axis , keep_dims=keepdims ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) expected_text = ( `` tf.math.count_nonzero ( input=input , dtype=dtype , name=name , `` `` axis=axis , keepdims=keepdims ) \n '' ) self.assertequal ( new_text , expected_text )
__label__0 def _get_argspec_for_partial ( obj ) : `` '' '' implements ` getargspec ` for ` functools.partial ` objects .
__label__0 def testkerassavedmodel ( self ) : text = ( `` tf.contrib.saved_model.save_keras_model ( model , './saved_models ' ) \n '' `` tf.contrib.saved_model.load_keras_model ( saved_model_path ) \n '' ) expected_text = ( `` tf.compat.v1.keras.experimental.export_saved_model ( model , `` `` './saved_models ' ) \ntf.compat.v1.keras.experimental . '' `` load_from_saved_model ( saved_model_path ) \n '' ) _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) expected_info = `` please use model.save '' self.assertin ( expected_info , report )
__label__0 def path_sum ( path , * tensors ) : return ( path , sum ( tensors ) )
__label__0 # work around for the case when object is a stack frame # and only .pyc files are used . in this case , getfile # might return incorrect path . so , we get the path from f_globals # instead . if ( hasattr ( unwrapped_object , 'f_globals ' ) and '__file__ ' in unwrapped_object.f_globals ) : return unwrapped_object.f_globals [ '__file__ ' ] return _inspect.getfile ( unwrapped_object )
__label__0 self.assertprotoequals ( expected_proto , cluster_spec.as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_cluster_def ( ) ) .as_cluster_def ( ) ) self.assertprotoequals ( expected_proto , server_lib.clusterspec ( cluster_spec.as_dict ( ) ) .as_cluster_def ( ) )
__label__0 def __del__ ( self ) : # at shutdown , ` errors ` may have been garbage collected . if errors is not none : exception = errors.unimplementederror else : exception = exception try : c_api.tf_serverstop ( self._server ) # clean shutdown of servers is not yet implemented , so # we leak instead of calling c_api.tf_deleteserver here . # see : # https : //github.com/tensorflow/tensorflow/blob/0495317a6e9dd4cac577b9d5cf9525e62b571018/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h # l73 except attributeerror : # at shutdown , ` c_api ` may have been garbage collected . pass except exception : pass self._server = none
__label__0 def _check_saver_def ( self ) : if not isinstance ( self.saver_def , saver_pb2.saverdef ) : raise valueerror ( `` saver_def must be a saver_pb2.saverdef : % s '' % self.saver_def ) if not context.executing_eagerly ( ) : if not self.saver_def.save_tensor_name : raise valueerror ( `` saver_def must specify the save_tensor_name : % s '' % str ( self.saver_def ) ) if not self.saver_def.restore_op_name : raise valueerror ( `` saver_def must specify the restore_op_name : % s '' % str ( self.saver_def ) )
__label__0 if self.float_size_good : return self._allclose ( self.want_floats , self.got_floats ) else : return false
__label__0 def test_avg_pool_2d ( self ) : text = `` tf.nn.avg_pool ( value=4 ) '' expected_text = `` tf.nn.avg_pool2d ( input=4 ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 blocking_thread = self.checkedthread ( blocking_dequeue ) blocking_thread.start ( ) time.sleep ( 0.5 ) sess.close ( ) blocking_thread.join ( )
__label__0 hooks can use this function to request stop of iterations . ` monitoredsession ` checks whether this is called or not. `` '' '' self._stop_requested = true
__label__0 6 . ` tf.raggedtensor ` : this is a composite tensor thats representation consists of a flattened list of 'values ' and a list of 'row_splits ' which indicate how to chop up the flattened list into different rows . for more details on ` tf.raggedtensor ` , please visit https : //www.tensorflow.org/api_docs/python/tf/raggedtensor .
__label__0 # if obj is any numpy type if type ( obj ) .__module__ == np.__name__ : if isinstance ( obj , np.ndarray ) : return obj.tolist ( ) else : return obj.item ( )
__label__0 from tensorflow.core.example import example_parser_configuration_pb2 from tensorflow.python.framework import tensor_shape from tensorflow.python.framework import tensor_util
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' unit tests for tf_inspect . '' '' ''
__label__0 def test_contrib_layers_layer_norm_deprecation ( self ) : for contrib_alias in [ `` tf.contrib . `` , `` contrib_ '' ] : _ , report , _ , _ = self._upgrade ( contrib_alias + `` layers.layer_norm '' ) self.assertin ( `` ` tf.contrib.layers.layer_norm ` has been deprecated '' , report )
__label__0 from tensorflow.core.protobuf import cluster_pb2 from tensorflow.core.protobuf import config_pb2 from tensorflow.core.protobuf import tensorflow_server_pb2 from tensorflow.python.client import session from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors_impl from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import data_flow_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.training import input as input_ops from tensorflow.python.training import queue_runner_impl from tensorflow.python.training import server_lib from tsl.protobuf import rpc_options_pb2
__label__0 from tensorflow.python.framework import errors from tensorflow.python.framework import ops from tensorflow.python.ops import state_ops from tensorflow.python.ops import variable_scope from tensorflow.python.ops import variables as variables_lib from tensorflow.python.platform import tf_logging as logging from tensorflow.python.training import checkpoint_ops from tensorflow.python.training import checkpoint_utils from tensorflow.python.training import saver as saver_lib from tensorflow.python.training.saving import saveable_object_util from tensorflow.python.util.tf_export import tf_export
__label__0 4. numpy array ( will not flatten ) :
__label__0 def _apply_sparse ( self , grad , var ) : rms = self.get_slot ( var , `` rms '' ) mom = self.get_slot ( var , `` momentum '' ) if self._centered : mg = self.get_slot ( var , `` mg '' ) return gen_training_ops.sparse_apply_centered_rms_prop ( var , mg , rms , mom , math_ops.cast ( self._learning_rate_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , var.dtype.base_dtype ) , grad.values , grad.indices , use_locking=self._use_locking ) else : return gen_training_ops.sparse_apply_rms_prop ( var , rms , mom , math_ops.cast ( self._learning_rate_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._decay_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._momentum_tensor , var.dtype.base_dtype ) , math_ops.cast ( self._epsilon_tensor , var.dtype.base_dtype ) , grad.values , grad.indices , use_locking=self._use_locking )
__label__0 glob_name = `` * . '' + name if name else none transformers = [ ] if full_name in function_transformers : transformers.append ( function_transformers [ full_name ] ) if glob_name in function_transformers : transformers.append ( function_transformers [ glob_name ] ) if `` * '' in function_transformers : transformers.append ( function_transformers [ `` * '' ] ) return transformers
__label__0 ( text_parts , extracted_floats ) = extract_floats ( text ) text_with_wildcards = ' ... '.join ( text_parts )
__label__0 def inheritable_header ( text ) :
__label__0 # pylint : disable=unused-import import sys
__label__0 # pylint : enable=g-doc-return-or-yield , broad-except
__label__0 args : shallow_tree : an arbitrarily nested structure . input_tree : an arbitrarily nested structure . check_types : if ` true ` ( default ) the sequence types of ` shallow_tree ` and ` input_tree ` have to be the same . note that even with check_types==true , this function will consider two different namedtuple classes with the same name and _fields attribute to be the same class . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors . raises : typeerror : if ` shallow_tree ` is a sequence but ` input_tree ` is not . typeerror : if the sequence types of ` shallow_tree ` are different from ` input_tree ` . only raised if ` check_types ` is ` true ` . valueerror : if the sequence lengths of ` shallow_tree ` are different from ` input_tree ` . `` '' '' nest_util.assert_shallow_structure ( nest_util.modality.core , shallow_tree , input_tree , check_types , expand_composites , )
__label__0 trace = func ( 5 ) source_map.update_to ( ( ( ( trace [ 0 ] .filename , trace [ 0 ] .lineno ) , ( `` filename '' , 42 , `` function_name '' ) , ) , ) ) trace = list ( func ( 5 ) ) self.assertequal ( str ( trace [ 0 ] ) , 'file `` filename '' , line 42 , in function_name ' )
__label__0 def deprecated_wrapper ( func_or_class ) : `` '' '' deprecation wrapper . '' '' '' if isinstance ( func_or_class , type ) : # if a class is deprecated , you actually want to wrap the constructor . cls = func_or_class if cls.__new__ is object.__new__ : # if a class defaults to its parent 's constructor , wrap that instead . func = cls.__init__ constructor_name = '__init__ ' decorators , _ = tf_decorator.unwrap ( func ) for decorator in decorators : if decorator.decorator_name == 'deprecated ' : # if the parent is already deprecated , there 's nothing to do . return cls else : func = cls.__new__ constructor_name = '__new__ '
__label__0 self.assertequal ( ( ' a ' , ) , function_utils.fn_args ( double_wrapped_fn ) )
__label__0 def testrestartedmaster ( self ) : master_old = server_lib.server.create_local_server ( ) master_new = server_lib.server.create_local_server ( ) worker = self._cached_server
__label__0 def register_unary_elementwise_api ( func ) : `` '' '' decorator that registers a tensorflow op as a unary elementwise api . '' '' '' _unary_elementwise_apis.append ( func ) for args , handler in _elementwise_api_handlers.items ( ) : if len ( args ) == 1 : _add_dispatch_for_unary_elementwise_api ( func , args [ 0 ] , handler ) return func
__label__0 def _find_cusparse_config ( base_paths , required_version , cuda_version ) :
__label__0 from absl.testing import parameterized import numpy as np
__label__0 return sv , sess , v , vadd , w
__label__0 def _get_global_step_read ( graph=none ) : `` '' '' gets global step read tensor in graph .
__label__0 result = test_op ( x , y , z ) self.assertallequal ( self.evaluate ( result.tensor ) , [ 15 , 21 , 13 ] ) self.assertnear ( result.score , 0.4 , 0.001 )
__label__0 text = `` tf.arg_min ( input , dimension=0 ) '' expected_text = `` tf.argmin ( input , axis=0 ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__1 def ismatch ( s : str , p : str ) - > bool : m , n = len ( s ) , len ( p ) dp = [ [ false ] * ( n + 1 ) for _ in range ( m + 1 ) ] dp [ 0 ] [ 0 ] = true for j in range ( 1 , n + 1 ) : if p [ j - 1 ] == ' * ' : dp [ 0 ] [ j ] = dp [ 0 ] [ j - 1 ] for i in range ( 1 , m + 1 ) : for j in range ( 1 , n + 1 ) : if p [ j - 1 ] == ' ? ' or s [ i - 1 ] == p [ j - 1 ] : dp [ i ] [ j ] = dp [ i - 1 ] [ j - 1 ] elif p [ j - 1 ] == ' * ' : dp [ i ] [ j ] = dp [ i - 1 ] [ j ] or dp [ i ] [ j - 1 ] return dp [ m ] [ n ] # test cases s1 , p1 = `` aa '' , `` a '' s2 , p2 = `` aa '' , `` * '' s3 , p3 = `` cb '' , `` ? a '' print ( ismatch ( s1 , p1 ) ) # output : false print ( ismatch ( s2 , p2 ) ) # output : true print ( ismatch ( s3 , p3 ) ) # output : false
__label__0 # this assertion is expected to pass : the shallow_tree component contains # the shallow structure of the input_tree component . nest.assert_shallow_structure ( shallow_tree=mt , input_tree=nmt , check_types=false )
__label__0 # creates a reader . reader = py_checkpoint_reader.newcheckpointreader ( save_path ) # verifies that the tensors exist . self.asserttrue ( reader.has_tensor ( `` v0 '' ) ) self.asserttrue ( reader.has_tensor ( `` v1 '' ) ) debug_string = reader.debug_string ( ) # verifies that debug string contains the right strings . self.assertin ( compat.as_bytes ( `` v0 ( dt_float ) [ 2,3 ] '' ) , debug_string ) self.assertin ( compat.as_bytes ( `` v1 ( dt_float ) [ 3,2,1 ] '' ) , debug_string ) # verifies get_variable_to_shape_map ( ) returns the correct information . var_map = reader.get_variable_to_shape_map ( ) self.assertequal ( [ 2 , 3 ] , var_map [ `` v0 '' ] ) self.assertequal ( [ 3 , 2 , 1 ] , var_map [ `` v1 '' ] ) # verifies get_tensor ( ) returns the tensor value . v0_tensor = reader.get_tensor ( `` v0 '' ) v1_tensor = reader.get_tensor ( `` v1 '' ) self.assertallequal ( v0 , v0_tensor ) self.assertallequal ( v1 , v1_tensor ) # verifies get_tensor ( ) fails for non-existent tensors . with self.assertraisesregex ( errors.notfounderror , `` v3 not found in checkpoint '' ) : reader.get_tensor ( `` v3 '' )
__label__0 # copyright 2022 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a module to replace deprecated_module .
__label__0 import io import os import re import sys
__label__0 returns : true if the coordinator was told to stop , false otherwise. `` '' '' return self._coord.should_stop ( )
__label__0 args : file_prefix : string prefix of the filepath . the writer will automatically attach a ` .pb ` or ` .cpb ` ( chunked pb ) suffix depending on whether the proto is split .
__label__0 def replace_string_in_line ( search , replace , filename ) : `` '' '' replace with sed when regex is required . '' '' '' with open ( filename , `` r '' ) as source : content = source.read ( ) with open ( filename , `` w '' ) as source : source.write ( re.sub ( search , replace , content ) )
__label__0 expected_text = `` '' '' try : import tensorflow.compat.v1 as tf # import line
__label__0 the supervisor is notified of any exception raised by one of the services . after an exception is raised , ` should_stop ( ) ` returns ` true ` . in that case the training loop should also stop . this is why the training loop has to check for ` sv.should_stop ( ) ` .
__label__0 # copy external code headers into tensorflow/include . # a symlink would do , but the wheel file that gets created ignores # symlink within the directory hierarchy . # note ( keveman ) : figure out how to customize bdist_wheel package so # we can do the symlink . # pylint : disable=line-too-long external_header_locations = { '/tensorflow/include/external/com_google_absl ' : `` , '/tensorflow/include/external/ducc ' : '/ducc ' , '/tensorflow/include/external/eigen_archive ' : `` , '/tensorflow/include/external/ml_dtypes ' : '/ml_dtypes ' , '/tensorflow/include/tensorflow/compiler/xla ' : ( '/tensorflow/include/xla ' ) , '/tensorflow/include/tensorflow/tsl ' : '/tensorflow/include/tsl ' , } # pylint : enable=line-too-long
__label__0 raises : valueerror : if date is not none or in iso 8601 format , or instructions are empty. `` '' '' _validate_deprecation_args ( date , instructions )
__label__0 cuda_header_path , header_version = _find_header ( base_paths , `` cuda.h '' , required_version , get_header_version ) cuda_version = header_version # x.y , see above .
__label__0 # assert calls without the deprecated argument log nothing . self.assertequal ( 3 , _fn ( arg0=1 , arg1=2 ) ) self.assertequal ( 0 , mock_warning.call_count )
__label__0 def _write_checkpoint ( self , sess ) : self.evaluate ( variables.global_variables_initializer ( ) ) saver = saver_lib.saver ( ) ckpt_prefix = os.path.join ( self.get_temp_dir ( ) , `` model '' ) saver.save ( sess , ckpt_prefix , global_step=0 )
__label__0 args : filename_tensor : string tensor . saveables : a list of saveableobject objects .
__label__0 # new vocab with elements in reverse order and one new element . new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] ) ws_util._warm_start_var_with_vocab ( fruit_weights , new_vocab_path , 5 , self.get_temp_dir ( ) , prev_vocab_path ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallclose ( [ [ 2 . ] , [ 1.5 ] , [ 1 . ] , [ 0.5 ] , [ 0 . ] ] , fruit_weights.eval ( sess ) )
__label__0 def from_tensors ( self , tensors : iterator [ core.tensor ] ) - > any : `` '' '' generates a value of this type from tensors .
__label__0 def testiterators ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` sharded_iterators '' )
__label__0 raises : typeerror : if ` bytes_or_text ` is not a binary or unicode string. `` '' '' # validate encoding , a lookuperror will be raised if invalid . encoding = codecs.lookup ( encoding ) .name if isinstance ( bytes_or_text , str ) : return bytes_or_text elif isinstance ( bytes_or_text , bytes ) : return bytes_or_text.decode ( encoding ) else : raise typeerror ( 'expected binary or unicode string , got % r ' % bytes_or_text )
__label__0 returns : consumed integer list based on input bytes and constraints. `` '' '' length = self.get_int ( min_length , max_length ) return self.fdp.consumefloatlistinrange ( length , _min_float , _max_float )
__label__0 text = ( `` slim.variance_scaling_initializer ( `` `` uniform= ( true or false ) , mode= ( \ '' fan\ '' + \ '' _avg\ '' ) ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=2.0 , `` `` distribution= ( \ '' uniform\ '' if true or false else \ '' truncated_normal\ '' ) , '' `` mode= ( \ '' fan\ '' + \ '' _avg\ '' ) .lower ( ) ) \n '' , )
__label__0 the fallback dispatch system was the original dispatch system , but it was somewhat brittle and had limitations , such as an inability to support dispatch for some operations ( like convert_to_tensor ) . we plan to remove the fallback dispatch system in favor of the type-based dispatch system , once all users have been switched over to use it .
__label__0 # handle any detected positional arguments if len ( node.args ) > = 1 : found_scale = true if len ( node.args ) > = 2 : _replace_mode ( node , node.args [ 1 ] ) if len ( node.args ) > = 3 : _replace_distribution ( node , node.args [ 2 ] )
__label__0 with self.cached_session ( ) as sess : v = variable_v1.variablev1 ( 10.0 , name= '' v '' ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 this function therefore will return something with the same base structure as ` shallow_tree ` .
__label__0 # these members allows doctest correctly process this module member without # triggering self._load ( ) . self._load ( ) mutates parant_module_globals and # triggers a dict mutated during iteration error from doctest.py . # - for from_module ( ) super ( ) .__setattr__ ( `` __module__ '' , name.rsplit ( `` . `` , 1 ) [ 0 ] ) # - for is_routine ( ) super ( ) .__setattr__ ( `` __wrapped__ '' , none )
__label__0 # the dispatch-supporting ops the non-singular check calls out to # get traced . trace = linear_operator_diag.linearoperatordiag ( x ) .assert_non_singular ( ) self.assertin ( `` debugging.assert_less '' , str ( trace ) ) self.assertin ( `` message=singular operator : diagonal contained zero values . `` , str ( trace ) )
__label__0 def testmanagedsessionignoreoutofrangeerror ( self ) : logdir = self._test_dir ( `` managed_out_of_range '' ) with ops.graph ( ) .as_default ( ) : my_op = constant_op.constant ( 1.0 ) sv = supervisor.supervisor ( logdir=logdir ) last_step = none with sv.managed_session ( `` '' ) as sess : for step in range ( 10 ) : last_step = step if step == 3 : raise errors_impl.outofrangeerror ( my_op.op.node_def , my_op.op , `` all done '' ) else : self.evaluate ( my_op ) # supervisor has been stopped . outofrangeerror was not thrown . self.asserttrue ( sv.should_stop ( ) ) self.assertequal ( 3 , last_step )
__label__0 subprocess.run ( [ sys.executable , `` tensorflow/tools/pip_package/setup.py '' , `` bdist_wheel '' , f '' -- dist-dir= { dir_path } '' , ] , check=true , cwd=cwd , env=env , )
__label__0 def __init__ ( self , original_args , session ) : `` '' '' initializes sessionruncontext . '' '' '' self._original_args = original_args self._session = session self._stop_requested = false
__label__0 # picked a long key value to minimize the chance of collision with user defined # collection keys . global_step_read_key = 'global_step_read_op_cache '
__label__0 def wrapper ( x ) : return wrapper.__wrapped__ ( x ) * * 2
__label__0 def _check_signature ( api_signature , func ) : `` '' '' checks that a dispatch target 's signature is compatible with an api .
__label__0 returns : sum of args. `` '' '' return arg0 + arg1
__label__0 start_time = time.time ( ) chunks , chunked_message = self.split ( )
__label__0 # run 4 steps of rmsprop for _ in range ( 1 , 5 ) : self.evaluate ( update )
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 # verifies that both sessions connecting to the same target return # the same results . sess_1 = session.session ( server.target ) sess_2 = session.session ( server.target ) sess_1.run ( variables.global_variables_initializer ( ) ) self.assertallequal ( [ [ 4 ] ] , sess_1.run ( v2 ) ) self.assertallequal ( [ [ 4 ] ] , sess_2.run ( v2 ) )
__label__0 def __init ( self ) : pass
__label__0 def hipsparse_version_numbers ( path ) : possible_version_files = [ `` include/hipsparse/hipsparse-version.h '' , # rocm 5.2 `` hipsparse/include/hipsparse-version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` hipsparse version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` hipsparseversionmajor '' ) minor = _get_header_version ( version_file , `` hipsparseversionminor '' ) patch = _get_header_version ( version_file , `` hipsparseversionpatch '' ) return major , minor , patch
__label__0 def testwarmstartvar ( self ) : _ , prev_val = self._create_prev_run_var ( `` fruit_weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 1.5 ] , [ 2 . ] ] )
__label__0 def a ( ) : return 1
__label__0 # note ( mrry ) : ` clusterspec.job_tasks ( ) ` is not recommended for use # with sparse jobs . self.assertequal ( [ `` ps0:2222 '' , `` ps1:2222 '' ] , cluster_spec.job_tasks ( `` ps '' ) ) self.assertequal ( [ `` worker0:2222 '' , `` worker1:2222 '' , `` worker2:2222 '' ] , cluster_spec.job_tasks ( `` worker '' ) ) self.assertequal ( [ `` sparse0:2222 '' , none , none , `` sparse3:2222 '' ] , cluster_spec.job_tasks ( `` sparse '' ) ) with self.assertraises ( valueerror ) : cluster_spec.job_tasks ( `` unknown '' )
__label__0 def testresourcecolocation ( self ) : # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) : partitioner = partitioned_variables.fixed_size_partitioner ( num_shards=2 ) with ops_lib.device ( `` /job : ps/device : gpu:0 '' ) : v = variable_scope.get_variable ( `` v0 '' , shape= [ 10 , 2 ] , partitioner=partitioner , use_resource=true ) saver_module.saver ( { `` v0 '' : v } ) .build ( ) save_op = none for op in ops_lib.get_default_graph ( ) .get_operations ( ) : if op.type == `` savev2 '' : save_op = op break assert save_op is not none for save_inp in save_op.inputs [ 3 : ] : # input to savev2 op is placed on cpu of the same device as # the variable . self.assertequal ( `` /job : ps/device : cpu:0 '' , save_inp.device )
__label__0 if this operation dispatcher can handle the given arguments , then return an appropriate value ( or raise an appropriate exception ) .
__label__0 argspecs = argspec ( args=fullargspecs.args + fullargspecs.kwonlyargs , varargs=fullargspecs.varargs , keywords=fullargspecs.varkw , defaults=defaults , ) return argspecs else : _getargspec = _inspect.getargspec
__label__0 def testimageresize ( self ) : for method in [ `` bilinear '' , `` area '' , `` bicubic '' , `` nearest_neighbor '' ] : text = `` tf.image.resize_ % s ( i , s ) '' % method expected_text = ( `` tf.image.resize ( i , s , `` `` method=tf.image.resizemethod. % s ) '' % method.upper ( ) ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 inp_ab1 = [ ( 1 , 1 ) , ( 2 , 2 ) ] inp_ab2 = [ [ 1 , 1 ] , [ 2 , 2 ] ] with self.assertraiseswithliteralmatch ( typeerror , nest.structures_have_mismatching_types.format ( shallow_type=type ( inp_ab2 [ 0 ] ) , input_type=type ( inp_ab1 [ 0 ] ) ) , ) : nest.assert_shallow_structure ( inp_ab2 , inp_ab1 ) nest.assert_shallow_structure ( inp_ab2 , inp_ab1 , check_types=false )
__label__0 class childfastmodule ( fastmoduletype ) :
__label__0 if isinstance ( obj , tensor_shape.tensorshape ) : return obj.as_list ( )
__label__0 def testpreparesessionsucceedswithlocalinitfeeddict ( self ) : with ops.graph ( ) .as_default ( ) : p = array_ops.placeholder ( dtypes.float32 , shape= ( 3 , ) ) v = variable_v1.variablev1 ( p , name= '' v '' , collections= [ ops.graphkeys.local_variables ] ) sm = session_manager.sessionmanager ( local_init_op=v.initializer , local_init_feed_dict= { p : [ 1.0 , 2.0 , 3.0 ] } , ready_op=variables.report_uninitialized_variables ( ) ) sess = sm.prepare_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) )
__label__0 def test_contrib_summary_audio ( self ) : text = `` tf.contrib.summary.audio ( 'foo ' , myval , 44100 , 3 , 'fam ' , 42 ) '' expected = ( `` tf.compat.v2.summary.audio ( name='foo ' , data=myval , `` `` sample_rate=44100 , max_outputs=3 , step=42 ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'family ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 module = mymodule ( ) self.asserttrue ( tf_inspect.isanytargetmethod ( module ) ) f = module.f self.asserttrue ( tf_inspect.isanytargetmethod ( f ) ) f = functools.partial ( f , 1 ) self.asserttrue ( tf_inspect.isanytargetmethod ( f ) ) f = test_decorator ( 'tf_decorator1 ' ) ( f ) self.asserttrue ( tf_inspect.isanytargetmethod ( f ) ) f = test_decorator ( 'tf_decorator2 ' ) ( f ) self.asserttrue ( tf_inspect.isanytargetmethod ( f ) )
__label__0 @ property def _type_spec ( self ) : pass
__label__0 args : bytes_or_text : a ` bytes ` , ` str ` , or ` unicode ` object . encoding : a string indicating the charset for decoding unicode .
__label__0 def parse_args ( ) - > argparse.namespace : `` '' '' arguments parser . '' '' '' parser = argparse.argumentparser ( description= '' helper for building pip package '' , fromfile_prefix_chars= '' @ '' ) parser.add_argument ( `` -- output-name '' , required=true , help= '' output file for the wheel , mandatory '' ) parser.add_argument ( `` -- project-name '' , required=true , help= '' project name to be passed to setup.py '' ) parser.add_argument ( `` -- headers '' , help= '' header files for the wheel '' , action= '' append '' ) parser.add_argument ( `` -- srcs '' , help= '' source files for the wheel '' , action= '' append '' ) parser.add_argument ( `` -- xla_aot '' , help= '' xla aot compiled sources '' , action= '' append '' ) parser.add_argument ( `` -- version '' , help= '' tf version '' ) parser.add_argument ( `` -- collab '' , help= '' true if collaborator build '' ) return parser.parse_args ( )
__label__0 curand_paths = base_paths if tuple ( int ( v ) for v in cuda_version.split ( `` . '' ) ) < ( 11 , 0 ) : curand_paths = cuda_paths curand_version = os.environ.get ( `` tf_curand_version '' , `` '' ) result.update ( _find_curand_config ( curand_paths , curand_version , cuda_version ) )
__label__0 symbol_renames = renames_v2.renames symbol_renames.update ( manual_symbol_renames )
__label__0 @ tf_export ( v1= [ 'user_ops.my_fact ' ] ) def my_fact ( ) : `` '' '' example of overriding the generated code for an op . '' '' '' return _gen_user_ops.fact ( )
__label__0 def testpreservedatasetandfunctions ( self ) : with ops_lib.graph ( ) .as_default ( ) as g : dataset = dataset_ops.dataset.range ( 10 ) .map ( lambda x : x * x ) iterator = dataset_ops.make_one_shot_iterator ( dataset ) next_element = iterator.get_next ( ) _ = array_ops.identity ( next_element , name= '' output '' )
__label__0 def main ( ) : try : args = [ argv for argv in sys.argv [ 1 : ] ] if len ( args ) % 2 == 1 : raise configerror ( `` expected even number of arguments '' ) checked_paths = [ ] for i in range ( 0 , len ( args ) , 2 ) : path = args [ i ] check_cuda_lib ( path , check_soname=args [ i + 1 ] == `` true '' ) checked_paths.append ( path ) # pylint : disable=superfluous-parens print ( os.linesep.join ( checked_paths ) ) # pylint : enable=superfluous-parens except configerror as e : sys.stderr.write ( str ( e ) ) sys.exit ( 1 )
__label__0 return init_tokens
__label__0 mean_square = decay * mean_square { t-1 } + ( 1-decay ) * gradient * * 2 mom = momentum * mom { t-1 } + learning_rate * g_t / sqrt ( mean_square + epsilon ) delta = - mom
__label__0 def __init__ ( self , override_func , types ) : self._types = types self._override_func = override_func
__label__0 def testdispatchfortypes_signaturemismatchorder ( self ) : with self.assertraisesregex ( assertionerror , `` the decorated function 's non-default arguments must be identical to '' `` that of the overridden op . `` , ) :
__label__0 self.assertequal ( ( ' a ' , ' b ' ) , function_utils.fn_args ( foo ( ) .bar ) )
__label__0 text = `` import tensorflow.google.compat.v1 as tf '' expected_text = `` import tensorflow.google.compat.v1 as tf '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # add labels for duplicate values in enum . if tf_inspect.isclass ( root ) and issubclass ( root , enum.enum ) : for enum_member in root.__members__.items ( ) : if enum_member not in children : children.append ( enum_member ) children = sorted ( children ) except importerror : # children could be missing for one of two reasons : # 1. on some python installations , some modules do not support enumerating # members , leading to import errors . # 2. children are lazy-loaded . try : children = [ ] for child_name in root.__all__ : children.append ( ( child_name , getattr ( root , child_name ) ) ) except attributeerror : children = [ ]
__label__0 edge cases :
__label__0 # tf.raw_ops.abs takes tf.bfloat16 , tf.float32 , tf.float64 , tf.int8 , tf.int16 , # tf.int32 , tf.int64 , tf.half but get_random_numeric_tensor only generates # tf.float16 , tf.float32 , tf.float64 , tf.int32 , tf.int64 input_tensor = fh.get_random_numeric_tensor ( )
__label__0 # save the initialized values in the file at `` save_path '' save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } , restore_sequentially=true ) val = save.save ( sess , save_path ) self.assertisinstance ( val , str ) self.assertequal ( save_path , val )
__label__0 def symbol_collector ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) api_names_v2 = tf_export.get_v2_names ( attr ) for name in api_names_v2 : cls.v2_symbols [ `` tf . '' + name ] = attr
__label__0 from_import = node.module
__label__0 dispatch_target.__name__ = `` elementwise_dispatch_target_for_ '' + api.__name__ dispatch_target.__qualname__ = dispatch_target.__name__ # keep track of what targets we 've registered ( so we can unregister them ) . target_list = _elementwise_api_targets.setdefault ( ( x_type , y_type ) , [ ] ) target_list.append ( ( api , dispatch_target ) )
__label__0 > > > tf.nest.assert_same_structure ( ... collections.namedtuple ( 'bar ' , ' a b ' ) ( 1 , 2 ) , ... collections.namedtuple ( 'foo ' , ' a b ' ) ( 2 , 3 ) ) traceback ( most recent call last ) : ... typeerror : the two structures do n't have the same nested structure
__label__0 def update ( self ) : self.internal_set.update_to ( set ( self.get_filtered_filenames ( ) ) )
__label__0 def teststacktracebuilder ( self ) : stack1 = tf_stack.extract_stack ( ) stack2 = tf_stack.extract_stack ( ) stack3 = tf_stack.extract_stack ( )
__label__0 returns : consumed integer list based on input bytes and constraints. `` '' '' length = self.get_int ( min_length , max_length ) return self.fdp.consumeintlistinrange ( length , min_int , max_int )
__label__0 def values ( self ) : return self._export ( ) [ 1 ]
__label__0 def rocm_version_numbers ( path ) : possible_version_files = [ `` include/rocm-core/rocm_version.h '' , # rocm 5.2 `` include/rocm_version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` rocm version file not found in { } '' .format ( possible_version_files ) )
__label__0 def test_get_field_tag ( self ) : proto = test_message_pb2.manyfields ( )
