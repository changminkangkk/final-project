__label__0 # the global step should have been updated since we only need to collect 2 # gradients . the variables should now have the new values after the average # of the gradients from worker 0/2 are applied . while sessions [ 1 ] .run ( global_step ) ! = 1 : time.sleep ( 0.01 )
__label__0 def teststripdefaultvaluedattrs ( self ) : `` '' '' verifies that default valued attrs are stripped , unless disabled . '' '' ''
__label__0 examples are available in the unit tests ( nest_test.py ) .
__label__0 # print deprecations , only cache functions after deprecation warnings have # stopped . if not ( self._tfmw_print_deprecation_warnings and self._tfmw_add_deprecation_warning ( name , attr ) ) : func__fastdict_insert ( name , attr )
__label__0 returns : if the line jupyter `` magic '' line , not python line
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' functional test for slot_creator . '' '' ''
__label__0 note : ` tf.distribute.distributeddataset ` instances are * not * of type ` tf.data.dataset ` . it only supports two usages we will mention below : iteration and ` element_spec ` . we do n't support any other apis to transform or inspect the dataset .
__label__0 if not ( self.saver_def.filename_tensor_name.startswith ( export_scope ) and self.saver_def.save_tensor_name.startswith ( export_scope ) and self.saver_def.restore_op_name.startswith ( export_scope ) ) : return none
__label__0 def __init__ ( self , proto , * , proto_as_initial_chunk : bool = true , parent_splitter : optional [ `` composablesplitter '' ] = none , fields_in_parent : optional [ util.fieldtypes ] = none , ) : `` '' '' initializes composablesplitter .
__label__0 import_header = `` from tensorflow.foo import bar\n '' text = import_header + old_symbol expected_text = `` from tensorflow.compat.v2.foo import bar\n '' + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor } ) def my_add ( x , y , name=none , extra_arg=none ) : # pylint : disable=unused-variable del x , y , name , extra_arg
__label__0 @ dataclasses.dataclass class nestedmaskedtensor : mask : bool value : maskedtensor
__label__0 try : del compiler except nameerror : pass
__label__0 def testlosses ( self ) : losses = [ `` absolute_difference '' , `` add_loss '' , `` compute_weighted_loss '' , `` cosine_distance '' , `` get_losses '' , `` get_regularization_loss '' , `` get_regularization_losses '' , `` get_total_loss '' , `` hinge_loss '' , `` huber_loss '' , `` log_loss '' , `` mean_pairwise_squared_error '' , `` mean_squared_error '' , `` sigmoid_cross_entropy '' , `` softmax_cross_entropy '' , `` sparse_softmax_cross_entropy '' , ] for l in losses : text = `` tf.losses . '' + l + `` ( a , b ) '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` tf.compat.v1.losses . '' + l + `` ( a , b ) '' , new_text ) self.assertin ( `` tf.losses have been replaced with object oriented versions '' , report )
__label__0 o = settablehash ( ) wrap1 = object_identity._objectidentitywrapper ( o ) wrap2 = object_identity._objectidentitywrapper ( o )
__label__0 abtuple = collections.namedtuple ( `` ab_tuple '' , `` a , b '' ) # pylint : disable=invalid-name
__label__0 # build a graph with 2 parameter nodes on different devices and save . with session.session ( target= '' '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) as sess : with sess.graph.device ( `` /cpu:0 '' ) : ds0 = dataset_ops.dataset.range ( 10 ) it0 = dataset_ops.make_initializable_iterator ( ds0 ) get_next0 = it0.get_next ( ) saveable0 = iterator_ops._iteratorsaveable ( it0._iterator_resource , name= '' saveable_it0 '' )
__label__0 if __name__ == `` __main__ '' : test_lib.main ( )
__label__0 def _tf_data_packed_nest_with_indices ( structure , flat , index ) : `` '' '' helper function for pack_nest_as .
__label__0 @ tf_export ( `` __internal__.nest.sequence_like '' , v1= [ ] ) def _sequence_like ( instance , args ) : `` '' '' converts the sequence ` args ` to the same type as ` instance ` .
__label__0 args : var : current graph 's variable that needs to be warm-started ( initialized ) . can be either of the following : ( i ) ` variable ` ( ii ) ` resourcevariable ` ( iii ) list of ` variable ` : the list must contain slices of the same larger variable . ( iv ) ` partitionedvariable ` current_vocab_path : path to the vocab file used for the given ` var ` . current_vocab_size : an ` int ` specifying the number of entries in the current vocab . prev_ckpt : a string specifying the directory with checkpoint file ( s ) or path to checkpoint . the given checkpoint must have tensor with name ` prev_tensor_name ` ( if not none ) or tensor with name same as given ` var ` . prev_vocab_path : path to the vocab file used for the tensor in ` prev_ckpt ` . previous_vocab_size : if provided , will constrain previous vocab to the first ` previous_vocab_size ` entries . -1 means use the entire previous vocab . current_oov_buckets : an ` int ` specifying the number of out-of-vocabulary buckets used for given ` var ` . prev_tensor_name : name of the tensor to lookup in provided ` prev_ckpt ` . if none , we lookup tensor with same name as given ` var ` . initializer : variable initializer to be used for missing entries . if none , missing entries will be zero-initialized . axis : axis of the variable that the provided vocabulary corresponds to .
__label__0 # adds user_defined proto in three formats : string , bytes and any . # any proto should just pass through . queue_runner = queue_runner_pb2.queuerunnerdef ( queue_name= '' test_queue '' ) ops_lib.add_to_collection ( `` user_defined_string_collection '' , str ( queue_runner ) ) ops_lib.add_to_collection ( `` user_defined_bytes_collection '' , queue_runner.serializetostring ( ) ) any_buf = any ( ) any_buf.pack ( queue_runner ) ops_lib.add_to_collection ( `` user_defined_any_collection '' , any_buf )
__label__0 @ deprecation.deprecated ( date=none , instructions= '' instructions '' ) @ dispatch.add_dispatch_support def some_op ( x ) : return x
__label__0 returns : a feed dictionary or ` none ` . `` '' '' return self._init_feed_dict
__label__0 def testdispatchtargetwithnonameargument ( self ) :
__label__0 from tensorflow.python.platform import googletest from tensorflow.tools.common import public_api
__label__0 def testinitcachesattributes ( self ) : module = mockmodule ( 'test ' ) wrapped_module = module_wrapper.tfmodulewrapper ( module , 'test ' ) self.asserttrue ( wrapped_module._fastdict_key_in ( '_fastdict_key_in ' ) ) self.asserttrue ( wrapped_module._fastdict_key_in ( '_tfmw_module_name ' ) ) self.asserttrue ( wrapped_module._fastdict_key_in ( '__all__ ' ) )
__label__0 def testmanagedmainerrortwoqueues ( self ) : # tests that the supervisor correctly raises a main loop # error even when using multiple queues for input . logdir = self._test_dir ( `` managed_main_error_two_queues '' ) os.makedirs ( logdir ) data_path = self._csv_data ( logdir ) with self.assertraisesregex ( runtimeerror , `` fail at step 3 '' ) : with ops.graph ( ) .as_default ( ) : # create an input pipeline that reads the file 3 times . filename_queue = input_lib.string_input_producer ( [ data_path ] , num_epochs=3 ) reader = io_ops.textlinereader ( ) _ , csv = reader.read ( filename_queue ) rec = parsing_ops.decode_csv ( csv , record_defaults= [ [ 1 ] , [ 1 ] , [ 1 ] ] ) shuff_rec = input_lib.shuffle_batch ( rec , 1 , 6 , 4 ) sv = supervisor.supervisor ( logdir=logdir ) with sv.managed_session ( `` '' ) as sess : for step in range ( 9 ) : if sv.should_stop ( ) : break elif step == 3 : raise runtimeerror ( `` fail at step 3 '' ) else : sess.run ( shuff_rec )
__label__0 def _init_init_op ( self , init_op=use_default , init_feed_dict=none ) : `` '' '' initializes init_op .
__label__0 update_version_h ( old_version , new_version ) update_setup_dot_py ( old_version , new_version ) update_readme ( old_version , new_version ) update_tensorflow_bzl ( old_version , new_version )
__label__0 def test_distribute_strategy ( self ) : text = `` tf.contrib.distribute.crossdeviceops ( ) '' expected = `` tf.distribute.crossdeviceops ( ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 this function is split out to facilitate stringio testing from tf_upgrade_test.py .
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import decorator_utils
__label__0 for name in set ( op_signature.parameters.keys ( ) ) - set ( func_signature.parameters.keys ( ) ) : p = op_signature.parameters [ name ] if p.default is p.empty : raise assertionerror ( `` the decorated function 's signature must implement all of the '' f '' non-default arguments of the overridden op . argument ` { name } ` is '' `` unimplemented . '' ) func_missing_params [ name ] = p
__label__0 args : saveables : a list of basesaverbuilder.saveableobject objects .
__label__0 for example , if we have the following tf.function and classes : `` ` python @ tf.function def get_mixed_flavor ( fruit_a , fruit_b ) : return fruit_a.flavor + fruit_b.flavor
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_int ] , partitioner ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * sc_int . * '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { sc_int : [ prev_int_val ] } , sess )
__label__0 def fn ( test_arg1 , a , test_arg2 ) : if test_arg1 ! = expected_test_arg1 or test_arg2 ! = expected_test_arg2 : return valueerror ( 'partial fn does not work correctly ' ) return a
__label__0 def acquire ( self , group_id ) : `` '' '' acquire the group lock for a specific group ` group_id ` . '' '' '' self._validate_group_id ( group_id )
__label__0 filtered_modules = [ ]
__label__0 isolate_config = config_pb2.configproto ( isolate_session_state=true ) isolate_sess_0 = session.session ( server.target , config=isolate_config ) isolate_sess_1 = session.session ( server.target , config=isolate_config )
__label__0 * tensorflow 1.x and 2.x * python 2 and 3
__label__0 with self.assertraisesregex ( valueerror , ( `` the two structures do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\n '' `` second structure : . * \n\n '' r'more specifically : substructure `` type=list str=\ [ 0 , 1\ ] '' ' 'is a sequence , while substructure `` type=int str=0 '' ' `` is not '' ) ) : nest.assert_same_structure ( 0 , [ 0 , 1 ] )
__label__0 from tensorflow.python.util import module_wrapper
__label__0 partial_func = functools.partial ( func , n=7 ) argspec = tf_inspect.argspec ( args= [ 'm ' , 'n ' ] , varargs=none , keywords=none , defaults= ( 7 , ) )
__label__0 `` ` python v1 = tf.variable ( ... , name='v1 ' ) v2 = tf.variable ( ... , name='v2 ' )
__label__0 return defaults
__label__0 @ deprecation.deprecated_args ( date , instructions , ( `` deprecated_arg1 '' , `` deprecated_arg2 '' ) ) def _fn ( arg0 , arg1 , * , kw1 , deprecated_arg1=none , deprecated_arg2=none ) : res = arg0 + arg1 + kw1 if deprecated_arg1 is not none : res += deprecated_arg1 if deprecated_arg2 is not none : res += deprecated_arg2 return res
__label__0 # please note that the gradients from replicas are averaged instead of summed # ( as in the old sync_replicas_optimizer ) so you need to increase the learning # rate according to the number of replicas . this change is introduced to be # consistent with how gradients are aggregated ( averaged ) within a batch in a # replica . @ tf_export ( v1= [ `` train.syncreplicasoptimizer '' ] ) class syncreplicasoptimizer ( optimizer.optimizer ) : `` '' '' class to synchronize , aggregate gradients and pass them to the optimizer .
__label__0 roctracer_config = { `` roctracer_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 def __call__ ( self , a , b=1 , c='hello ' ) : pass
__label__0 version = ct.c_int ( ) rc = libcudart.cudaruntimegetversion ( ct.byref ( version ) ) if rc ! = 0 : raise valueerror ( `` could not get version '' ) if version.value < 6050 : raise notimplementederror ( `` cuda version must be between > = 6.5 '' )
__label__0 def testextractglimpse ( self ) : text = ( `` tf.image.extract_glimpse ( x , size , off , false , `` `` false , false , name=\ '' foo\ '' ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.image.extract_glimpse ( x , size , off , false , `` `` false , 'uniform ' if ( false ) else 'gaussian ' , name=\ '' foo\ '' ) \n '' , )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' test runner for tensorflow tests . '' '' ''
__label__0 more generally , placeholder values are the arguments of a tf.function , as seen from the function 's body : `` ` python @ tf.function def foo ( x ) : # here ` x ` is be the placeholder value ...
__label__0 # variables that should be changed to functions . self.change_to_function = { }
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 self.assertprotoequals ( `` '' '' cluster { job { name : 'local ' tasks { key : 0 value : 'localhost:2222 ' } tasks { key : 1 value : 'localhost:2223 ' } } } job_name : 'local ' task_index : 1 protocol : 'grpc ' `` '' '' , server_def )
__label__0 def cached_classproperty ( func ) : return _cachedclassproperty ( func )
__label__0 * * if you are debugging a tensorflow-internal issue , you need to call ` tf.debugging.disable_traceback_filtering ` * * . to re-enable traceback filtering afterwards , you can call ` tf.debugging.enable_traceback_filtering ( ) ` . `` '' '' global _enable_traceback_filtering _enable_traceback_filtering.value = false
__label__0 def _get_ld_config_paths ( ) : `` '' '' returns all directories from 'ldconfig -p ' . '' '' '' if not _is_linux ( ) : return [ ] ldconfig_path = which ( `` ldconfig '' ) or `` /sbin/ldconfig '' output = subprocess.check_output ( [ ldconfig_path , `` -p '' ] ) pattern = re.compile ( `` . * = > ( . * ) '' ) result = set ( ) for line in output.splitlines ( ) : try : match = pattern.match ( line.decode ( `` ascii '' ) ) except unicodedecodeerror : match = false if match : result.add ( os.path.dirname ( match.group ( 1 ) ) ) return sorted ( list ( result ) )
__label__0 def testunsortedsegmentsum1dindices3ddata ( self ) : for dtype in self.numeric_types : data = np.array ( [ [ [ 0 , 1 , 2 ] , [ 10 , 11 , 12 ] ] , [ [ 100 , 101 , 102 ] , [ 110 , 111 , 112 ] ] , [ [ 200 , 201 , 202 ] , [ 210 , 211 , 212 ] ] , [ [ 300 , 301 , 302 ] , [ 310 , 311 , 312 ] ] ] , dtype=dtype ) indices = np.array ( [ 3 , 0 , 2 , 5 ] , dtype=np.int32 ) num_segments = 6 y = self._unsortedsegmentsum ( data , indices , num_segments ) self.assertallclose ( np.array ( [ [ [ 100 , 101 , 102 . ] , [ 110 , 111 , 112 ] ] , [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , [ [ 200 , 201 , 202 ] , [ 210 , 211 , 212 ] ] , [ [ 0 , 1 , 2 . ] , [ 10 , 11 , 12 ] ] , [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , [ [ 300 , 301 , 302 ] , [ 310 , 311 , 312 ] ] ] , dtype=dtype ) , y )
__label__0 @ compatibility ( tf2 ) with the deprecation of global graphs , tf no longer tracks variables in collections . in other words , there are no global variables in tf2 . thus , the global step functions have been removed ( ` get_or_create_global_step ` , ` create_global_step ` , ` get_global_step ` ) . you have two options for migrating :
__label__0 flat_structure = ( _tf_core_flatten ( s , expand_composites ) for s in structure ) entries = zip ( * flat_structure )
__label__0 def add ( self , key ) : self._storage.add ( self._wrap_key ( key ) )
__label__0 # pylint : disable=g-import-not-at-top if sys.version_info > = ( 3 , 8 ) : from typing import protocol from typing import runtime_checkable else : from typing_extensions import protocol from typing_extensions import runtime_checkable # pylint : enable=g-import-not-at-top
__label__0 class deprecatemovedmoduletest ( test.testcase ) :
__label__0 def testlotsofnodes ( self ) : # the actual sizes in the generated graph has a slight deviation , but are # between [ 90 , 100 ] ( tested in testmakegraphdef with atol=5 ) . sizes = [ 95 ] * 15 max_size = 500 constants.debug_set_max_size ( 500 )
__label__0 > > > g = tf.graph ( ) > > > with g.as_default ( ) : ... x = tf.compat.v1.placeholder ( tf.float32 , [ ] ) ... loss , var_list = compute_loss ( x ) ... global_step = tf.compat.v1.train.create_global_step ( ) ... global_init = tf.compat.v1.global_variables_initializer ( ) ... optimizer = tf.compat.v1.train.gradientdescentoptimizer ( 0.1 ) ... train_op = optimizer.minimize ( loss , global_step , var_list ) > > > sess = tf.compat.v1.session ( graph=g ) > > > sess.run ( global_init ) > > > print ( `` before training : '' , sess.run ( global_step ) ) before training : 0 > > > sess.run ( train_op , feed_dict= { x : 3 } ) > > > print ( `` after training : '' , sess.run ( global_step ) ) after training : 1
__label__0 text = `` tf.distribute.mirroredstrategy '' expected = `` tf.distribute.mirroredstrategy '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` tf.distribute.mirroredstrategy api has changed '' , report ) self.assertin ( `` make_dataset_iterator- > experimental_distribute_dataset '' , report )
__label__0 import numpy as np
__label__1 from collections import deque def ladderlength ( beginword , endword , wordlist ) : wordset = set ( wordlist ) if endword not in wordset : return 0 queue = deque ( [ ( beginword , 1 ) ] ) visited = set ( ) while queue : word , level = queue.popleft ( ) if word == endword : return level for i in range ( len ( word ) ) : for char in 'abcdefghijklmnopqrstuvwxyz ' : new_word = word [ : i ] + char + word [ i+1 : ] if new_word in wordset and new_word not in visited : visited.add ( new_word ) queue.append ( ( new_word , level + 1 ) ) return 0 # test cases beginword1 , endword1 , wordlist1 = `` hit '' , `` cog '' , [ `` hot '' , '' dot '' , '' dog '' , '' lot '' , '' log '' , '' cog '' ] print ( ladderlength ( beginword1 , endword1 , wordlist1 ) ) # output : 5 beginword2 , endword2 , wordlist2 = `` hit '' , `` cog '' , [ `` hot '' , '' dot '' , '' dog '' , '' lot '' , '' log '' ] print ( ladderlength ( beginword2 , endword2 , wordlist2 ) ) # output : 0
__label__0 the centered version additionally maintains a moving ( discounted ) average of the gradients , and uses that average to estimate the variance :
__label__0 s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s1 ] , save.last_checkpoints ) if save._write_version is saver_pb2.saverdef.v1 : self.assertequal ( 2 , len ( gfile.glob ( s1 ) ) ) else : self.assertequal ( 4 , len ( gfile.glob ( s1 + `` * '' ) ) )
__label__0 def add_type_based_api_dispatcher ( target ) : `` '' '' adds a pythonapidispatcher to the given tensorflow api function . '' '' '' if hasattr ( target , type_based_dispatch_attr ) : raise valueerror ( f '' { target } already has a type-based api dispatcher . '' )
__label__0 from google.protobuf import text_format
__label__0 returns : sum of args. `` '' '' return arg0 + arg1
__label__0 return total_size_diff
__label__0 def summary_computed ( self , sess , summary , global_step=none ) : `` '' '' indicate that a summary was computed .
__label__0 `` ` python with tf.session ( ) as sess : new_saver = tf.train.import_meta_graph ( 'my-save-dir/my-model-10000.meta ' ) new_saver.restore ( sess , 'my-save-dir/my-model-10000 ' ) # tf.get_collection ( ) returns a list . in this example we only want # the first one . train_op = tf.get_collection ( 'train_op ' ) [ 0 ] for step in range ( 1000000 ) : sess.run ( train_op ) `` `
__label__0 upgrader = ast_edits.astcodeupgrader ( renameimports ( ) ) upgrader.process_tree_inplace ( upgrade_dir )
__label__0 # update this object 's dict so that if someone keeps a reference to the # lazyloader , lookups are efficient ( __getattr__ is only called on lookups # that fail ) . self.__dict__.update ( module.__dict__ )
__label__0 `` ` python a `` `
__label__0 init_op = variables.global_variables_initializer ( )
__label__0 shutil.copytree ( os.path.join ( srcs_dir , `` external/local_config_cuda/cuda '' ) , os.path.join ( srcs_dir , `` third_party/gpus '' ) ) shutil.copytree ( os.path.join ( srcs_dir , `` tensorflow/compiler/xla '' ) , os.path.join ( srcs_dir , `` xla '' ) ) shutil.copytree ( os.path.join ( srcs_dir , `` tensorflow/tsl '' ) , os.path.join ( srcs_dir , `` tsl '' ) )
__label__0 # pylint : disable=missing-function-docstring def _tf_data_map_structure ( func , * structure , * * check_types_dict ) : if not callable ( func ) : raise typeerror ( f '' argument ` func ` must be callable , got : { func } '' )
__label__0 return false
__label__0 text = `` optimizer.minimize ( a , colocate_gradients_with_ops=false ) \n '' _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( `` optimizer.minimize ( a ) \n '' , new_text ) self.assertin ( `` optimizer.minimize no longer takes '' , report )
__label__0 def func ( n ) : if n == 0 : return tf_stack._tf_stack.extract_stack ( source_map , tf_stack._tf_stack.pybindfileset ( ) ) else : return func ( n - 1 )
__label__0 the numeric comparison will fail if either :
__label__0 in a typical asynchronous training environment , it 's common to have some stale gradients . for example , with a n-replica asynchronous training , gradients will be applied to the variables n times independently . depending on each replica 's training speed , some gradients might be calculated from copies of the variable from several steps back ( n-1 steps on average ) . this optimizer avoids stale gradients by collecting gradients from all replicas , averaging them , then applying them to the variables in one shot , after which replicas can fetch the new variables and continue .
__label__0 class fruit :
__label__0 class graphdefsplittertest ( test.testcase ) :
__label__0 def format_log ( self , log , in_filename ) : log_string = `` % d : % d : % s : % s '' % ( log [ 1 ] , log [ 2 ] , log [ 0 ] , log [ 3 ] ) if in_filename : return in_filename + `` : '' + log_string else : return log_string
__label__0 * the `` fallback dispatch '' system calls an api 's standard implementation first , and only tries to perform dispatch if that standard implementation raises a typeerror ( or valueerror ) exception .
__label__0 inp_ab1 = { `` a '' : ( 1 , 1 ) , `` b '' : { `` c '' : ( 2 , 2 ) } } inp_ab2 = { `` a '' : ( 1 , 1 ) , `` b '' : { `` d '' : ( 2 , 2 ) } } with self.assertraiseswithliteralmatch ( valueerror , nest.shallow_tree_has_invalid_keys.format ( [ `` d '' ] ) ) : nest.assert_shallow_structure ( inp_ab2 , inp_ab1 )
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testlogdirbutexplicitlynosummarywriter ( self ) : logdir = self._test_dir ( `` explicit_no_summary_writer '' ) with ops.graph ( ) .as_default ( ) : variable_v1.variablev1 ( [ 1.0 ] , name= '' foo '' ) summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sv = supervisor.supervisor ( logdir=logdir , summary_writer=none ) sess = sv.prepare_or_wait_for_session ( `` '' ) # check that a checkpoint is still be generated . self._wait_for_glob ( sv.save_path , 3.0 ) # check that we can not write a summary with self.assertraisesregex ( runtimeerror , `` requires a summary writer '' ) : sv.summary_computed ( sess , sess.run ( summ ) )
__label__0 @ property def pep_440_str ( self ) : if self.version_type == regular_version : return_string = `` % s. % s. % s % s '' % ( self.major , self.minor , self.patch , self.identifier_string ) return return_string.replace ( `` - '' , `` '' ) else : return_string = `` % s. % s. % s '' % ( self.major , self.minor , self.identifier_string ) return return_string.replace ( `` - '' , `` '' )
__label__0 def flatten_with_tuple_paths ( structure , expand_composites=false ) : `` '' '' returns a list of ` ( tuple_path , atom ) ` tuples .
__label__0 import numpy as np
__label__0 # # # # what ` master ` string to use
__label__0 @ test_util.run_v1_only ( `` sparseapplyadagrad op returns a ref , so it is not `` `` supported in eager mode . '' ) def testsparseapplyadagraddim1 ( self ) : for ( dtype , index_type , use_gpu ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ np.int32 , np.int64 ] , [ false , true ] ) : x_val = [ [ 1.0 ] , [ 2.0 ] , [ 3.0 ] ] y_val = [ [ 4.0 ] , [ 5.0 ] , [ 6.0 ] ] x = np.array ( x_val ) .astype ( dtype ) y = np.array ( y_val ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) grad_val = [ [ 1.5 ] , [ 2.5 ] ] grad = np.array ( grad_val ) .astype ( dtype ) indices = np.array ( [ 0 , 2 ] ) .astype ( index_type ) self._testtypesforsparseadagrad ( x , y , lr , grad , indices , use_gpu )
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , nesttest.samenameab ( 0 , 1 ) , nesttest.samenamedtype1 ( 2 , 3 ) )
__label__0 import glob import math import os import random import time
__label__0 def update_reorders ( self ) : # note that these should be in the old order . self.function_reorders [ `` f '' ] = [ `` a '' , `` b '' , `` kw1 '' , `` kw2 '' ]
__label__0 def testinvalidpath ( self ) : v0 = variable_v1.variablev1 ( 0 , name= '' v0 '' ) for ver in ( saver_pb2.saverdef.v1 , saver_pb2.saverdef.v2 ) : with self.cached_session ( ) as sess : save = saver_module.saver ( { `` v0 '' : v0 } , write_version=ver ) with self.assertraisesregex ( valueerror , `` the passed save_path is not a valid checkpoint : '' ) : save.restore ( sess , `` invalid path '' )
__label__0 mode = pasta.parse ( `` \ '' fan_avg\ '' '' ) new_keywords.append ( ast.keyword ( arg= '' mode '' , value=mode ) )
__label__0 # test max_to_keep being none . save = saver_module.saver ( { `` v '' : v } , max_to_keep=none ) self.assertequal ( [ ] , save.last_checkpoints ) s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) s2 = save.save ( sess , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) )
__label__0 class tracebackutilstest ( test.testcase ) :
__label__0 def replace_iterable_params ( args , kwargs , iterable_params ) : `` '' '' returns ( args , kwargs ) with any iterable parameters converted to lists .
__label__0 # shallow tree ends at scalar . input_tree = [ [ [ 2 , 2 ] , [ 3 , 3 ] ] , [ [ 4 , 9 ] , [ 5 , 5 ] ] ] shallow_tree = [ [ true , true ] , [ false , true ] ] ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 0 ) , ( 1 , 1 ) ] ) self.assertequal ( flattened_input_tree , [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 9 ] , [ 5 , 5 ] ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 0 ) , ( 1 , 1 ) ] ) self.assertequal ( flattened_shallow_tree , [ true , true , false , true ] )
__label__0 def _make_graph_def_with_constant_nodes ( self , node_sizes , dtype=none , * * function_node_sizes ) : return test_util.make_graph_def_with_constant_nodes ( node_sizes , dtype , * * function_node_sizes )
__label__0 class testupgrade ( test_util.tensorflowtestcase ) : `` '' '' test various apis that have been changed in 1.0 .
__label__0 cupti_header_path = _find_file ( base_paths , _header_paths ( ) , `` cupti.h '' ) nvml_header_dir = _find_file ( base_paths , _header_paths ( ) , `` nvml.h '' ) cupti_library_path = _find_library ( base_paths , `` cupti '' , required_version )
__label__0 class saverlargevariabletest ( test.testcase ) :
__label__0 this is intended to be overridden by subclasses that want to generate different ops .
__label__0 note : this is implemented by adding a hidden attribute on the object , so it can not be used on objects which do not allow new attributes to be added . so this decorator must go * below * ` @ property ` , ` @ classmethod ` , or ` @ staticmethod ` :
__label__0 def test_double_partial ( self ) : expected_test_arg1 = 123 expected_test_arg2 = 456
__label__0 logging.info ( `` restored model from % s '' , restoring_file ) return sess , is_loaded_from_checkpoint
__label__0 @ test_util.run_v1_only ( `` train.saver and variablev1 are v1 only apis . '' ) def testbasicswithlistofvariables ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` basics_with_list '' )
__label__0 def _find_curand_config ( base_paths , required_version , cuda_version ) :
__label__0 this helps to avoid circular dependencies. `` '' ''
__label__0 `` ` python class dimension ( tracetype ) : def __init__ ( self , value : optional [ int ] ) : self.value = value
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' types internal to tensorflow .
__label__1 def find_common_elements ( lst1 , lst2 ) : return list ( set ( lst1 ) & set ( lst2 ) )
__label__0 new_saver.restore ( sess , filename ) sess.run ( [ `` new_model/optimize '' ] , { `` new_model/image:0 '' : np.random.random ( [ 1 , 784 ] ) , `` new_model/label:0 '' : np.random.randint ( 10 , size= [ 1 , 10 ] ) } )
__label__0 class syncreplicasoptimizerhooktest ( test.testcase ) :
__label__0 if self._var_list is none : # pylint : disable=protected-access self._var_list = variables._all_saveable_objects ( ) if not self._var_list : if self._allow_empty : self._is_empty = true return else : raise valueerror ( `` no variables to save '' ) self._is_empty = false
__label__0 python addresses in the output are replaced with wildcards .
__label__0 the output of a found library is of the form : tf_ < library > _version : x.y.z tf_ < library > _header_dir : ... tf_ < library > _library_dir : ... `` '' ''
__label__0 returns : a testresults proto `` '' ''
__label__0 def testnestedcondsserdes ( self ) : # test conds in a cond . # pylint : disable=g-long-lambda self._testgradientserdes ( lambda x : cond.cond ( x > 0 , lambda : cond.cond ( x > 3 , lambda : array_ops.identity ( x ) , lambda : math_ops.multiply ( x , 2.0 ) ) , lambda : cond.cond ( x < -3 , lambda : constant_op.constant ( 1.0 ) , lambda : math_ops.multiply ( x , -1.0 ) ) ) ) # pylint : enable=g-long-lambda
__label__0 for example , 'v0.10.0-1585-gbb717a6 ' means v0.10.0 was the last tag when compiled . 1585 commits are after that commit tag , and we can get back to this version by running ` git checkout gbb717a6 ` .
__label__0 def _add_summary_recording_cond_transformer ( parent , node , full_name , name , logs , cond ) : `` '' '' adds cond argument to tf.contrib.summary.xxx_record_summaries ( ) .
__label__0 def testsoftmaxcrossentropywithlogitsv2 ( self ) : text = ( `` tf.nn.softmax_cross_entropy_with_logits_v2 ( `` `` labels=labels , logits=logits , dim=2 ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=labels , logits=logits , axis=2 ) '' ) _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 return { `` cublas_version '' : header_version , `` cublas_include_dir '' : os.path.dirname ( header_path ) , `` cublas_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 returns : an op to save the variables. `` '' '' if self._write_version == saver_pb2.saverdef.v2 : return self._addshardedsaveopsforv2 ( filename_tensor , per_device )
__label__0 however , we , as the designers of the ` fruit ` class , know that each subclass has a fixed flavor and we can reuse an existing traced concrete function if it was the same subclass . avoiding such unnecessary tracing of concrete functions can have significant performance benefits .
__label__0 try : with self.assertraisesregex ( valueerror , r '' a binary elementwise dispatch handler \ ( . * \ ) has `` `` already been registered for . * '' ) :
__label__0 args : call_str : call string must be in the form : ` tf.foo ( arg1=val1 , arg2=val2 , ... ) ` .
__label__0 def testassertstatements ( self ) : for name in [ `` assert_greater '' , `` assert_equal '' , `` assert_none_equal '' , `` assert_less '' , `` assert_negative '' , `` assert_positive '' , `` assert_non_negative '' , `` assert_non_positive '' , `` assert_near '' , `` assert_less '' , `` assert_less_equal '' , `` assert_greater '' , `` assert_greater_equal '' , `` assert_scalar '' ] : text = `` tf. % s ( a ) '' % name expected_text = `` tf.compat.v1. % s ( a ) '' % name _ , report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` % s has been '' % name , report )
__label__0 @ test_util.run_all_in_graph_and_eager_modes class dispatchv2test ( test_util.tensorflowtestcase ) :
__label__0 if version.parse ( tf.__version__ ) > = version.parse ( `` 2.14-dev '' ) : raw_ops_doc = self.generate_raw_ops_doc_ge_214 ( ) else : raw_ops_doc = self.generate_raw_ops_doc_lt_214 ( )
__label__0 def testsavewithglobalstepwithpadding ( self ) : self.testsavewithglobalstep ( pad_step_number=true )
__label__0 # the first event should list the file_version . ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version )
__label__0 from typing import optional , typevar
__label__0 class foohaskwargs ( object ) :
__label__0 def testbinaryandtextformat ( self ) : test_dir = self._get_test_dir ( `` binary_and_text '' ) filename = os.path.join ( test_dir , `` metafile '' ) # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.session ( ) : # creates a graph . variable_v1.variablev1 ( 10.0 , name= '' v0 '' ) # exports the graph as binary format . saver_module.export_meta_graph ( filename , as_text=false ) with ops_lib.graph ( ) .as_default ( ) , self.session ( ) : # imports the binary format graph . saver = saver_module.import_meta_graph ( filename ) self.assertisnotnone ( saver ) # exports the graph as text format . saver.export_meta_graph ( filename , as_text=true ) with ops_lib.graph ( ) .as_default ( ) , self.session ( ) : # imports the text format graph . saver_module.import_meta_graph ( filename ) # writes wrong contents to the file . graph_io.write_graph ( saver.as_saver_def ( ) , os.path.dirname ( filename ) , os.path.basename ( filename ) ) with ops_lib.graph ( ) .as_default ( ) , self.session ( ) : # import should fail . with self.assertraiseswithpredicatematch ( ioerror , lambda e : `` can not parse file '' ) : saver_module.import_meta_graph ( filename ) # deletes the file gfile.remove ( filename ) with self.assertraiseswithpredicatematch ( ioerror , lambda e : `` does not exist '' ) : saver_module.import_meta_graph ( filename )
__label__0 wrapped_fn = functools.partial ( fn , 123 )
__label__0 returns : a list of ( attr_name , attr_value ) pairs , sorted by attr_name. `` '' '' attrs = getattr ( obj.__class__ , `` __attrs_attrs__ '' ) attr_names = ( a.name for a in attrs ) return [ ( attr_name , getattr ( obj , attr_name ) ) for attr_name in attr_names ]
__label__0 import functools
__label__0 if _at_least_version ( cuda_version , `` 11.0 '' ) :
__label__0 groups = match.groupdict ( )
__label__0 try :
__label__0 # if the some of the float output is hidden with ` ... ` , ` float_size_good ` # will be false . this is because the floats extracted from the string is # converted into a 1-d numpy array . hence hidding floats is not allowed # anymore . if self.text_good : if not self.float_size_good : got.append ( `` \n\ncaution : tf_doctest does n't work if * some * of the `` `` * float output * is hidden with a \ '' ... \ '' . '' )
__label__0 # verifies round trip from proto- > spec- > proto is correct . cluster_spec = server_lib.clusterspec ( cluster_def ) self.assertprotoequals ( cluster_def , cluster_spec.as_cluster_def ( ) )
__label__0 # pylint : disable=g-bad-import-order # note : cpuinfo and psutil are not installed for you in the tensorflow # oss tree . they are installable via pip . import cpuinfo import psutil # pylint : enable=g-bad-import-order
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 # always try to run local_init_op local_init_success , msg = self._try_run_local_init_op ( sess )
__label__0 def testnnintopk ( self ) : text = `` tf.nn.in_top_k ( predictions , targets , k , name ) '' expected_text = ( `` tf.nn.in_top_k ( predictions=predictions , `` `` targets=targets , k=k , name=name ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 self.asserttrue ( os.path.islink ( file_b ) ) self.assertequal ( file_a , os.readlink ( file_b ) ) with open ( file_a , `` r '' ) as f : self.assertequal ( `` import bar as f '' , f.read ( ) )
__label__0 def generate_raw_ops_doc_ge_214 ( self ) : `` '' '' generates docs for ` tf.raw_ops ` . '' '' '' del self
__label__0 def __init__ ( self , name , args=none , kwargs=none ) : self.name = name self.args = args self.kwargs = kwargs self.shape = array_ops.ones ( shape= ( 4 , 4 ) ) .shape self.dtype = dtypes.float32
__label__0 this version of the decorator is `` inherited '' by subclasses . no docs will be generated for the decorated method in any subclass . even if the sub-class overrides the method .
__label__0 # verifies that there is no saver_def in meta_graph_def . self.assertfalse ( meta_graph_def.hasfield ( `` saver_def '' ) ) # verifies that there is saver_def in meta_graph_def0 and 1. self.asserttrue ( meta_graph_def0.hasfield ( `` saver_def '' ) ) self.asserttrue ( meta_graph_def1.hasfield ( `` saver_def '' ) )
__label__0 results = variable_utils.replace_variables_with_atoms ( data ) expected_results = [ 0 , 0 , 3 , [ 4 ] , 5 ] # only resourcevariables are replaced with int 0s . self.assertisinstance ( results [ 0 ] , int ) self.assertisinstance ( results [ 1 ] , int ) self.assertisinstance ( results [ 2 ] , tensor.tensor ) self.assertisinstance ( results [ 3 ] , list ) self.assertisinstance ( results [ 4 ] , int ) results [ 2 ] = self.evaluate ( results [ 2 ] ) self.assertallequal ( results , expected_results )
__label__0 check_for_old_version ( old_version , new_version )
__label__0 returns : a wrapped version of ` func_or_class ` which prints a deprecation warning on use and has a modified docstring. `` '' '' if tf_inspect.isclass ( func_or_class ) :
__label__0 text = ( contrib_alias + `` layers.xavier_initializer ( `` `` false , 12 , dtypes=tf.float32 ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=1.0 , `` `` mode=\ '' fan_avg\ '' , `` `` distribution= ( \ '' uniform\ '' if false else \ '' truncated_normal\ '' ) , `` `` seed=12 , `` `` dtypes=tf.float32 ) \n '' , )
__label__0 args : date : string or none . the date the function is scheduled to be removed . must be iso 8601 ( yyyy-mm-dd ) , or none . instructions : string . instructions on how to update code using the deprecated function . * deprecated_arg_names_or_tuples : string or 2-tuple ( string , ok_val ) . the string is the deprecated argument name . optionally , an ok-value may be provided . if the user provided argument equals this value , the warning is suppressed . * * kwargs : if ` warn_once=false ` is passed , every call with a deprecated argument will log a warning . the default behavior is to only warn the first time the function is called with any given deprecated argument . all other kwargs raise ` valueerror ` .
__label__0 the decorated function ( known as the `` elementwise api handler '' ) overrides the default implementation for any binary elementwise api whenever the value for the first two arguments ( typically named ` x ` and ` y ` ) match the specified type annotations . the elementwise api handler is called with two arguments :
__label__0 def _convert_maybe_argspec_to_fullargspec ( argspec ) : if isinstance ( argspec , fullargspec ) : return argspec return fullargspec ( args=argspec.args , varargs=argspec.varargs , varkw=argspec.keywords , defaults=argspec.defaults , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 def test_simple_function ( self ) :
__label__0 def testmoduledeprecation ( self ) : text = `` a.b.c ( a.b.x ) '' ( _ , _ , errors ) , new_text = self._upgrade ( moduledeprecationspec ( ) , text ) self.assertequal ( text , new_text ) self.assertin ( `` using member a.b.c '' , errors [ 0 ] ) self.assertin ( `` 1:0 '' , errors [ 0 ] ) self.assertin ( `` using member a.b.c '' , errors [ 0 ] ) self.assertin ( `` 1:6 '' , errors [ 1 ] )
__label__0 def __len__ ( self ) : # iterate , discarding old weak refs return len ( list ( self._storage ) )
__label__0 return { `` cufft_version '' : header_version , `` cufft_include_dir '' : os.path.dirname ( header_path ) , `` cufft_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 def get_next_as_optional ( self ) : # pylint : disable=line-too-long `` '' '' returns a ` tf.experimental.optional ` that contains the next value for all replicas .
__label__0 def testsparsematmul ( self ) : text = ( `` tf.sparse_matmul ( a , b , c , d , e , f , g ) \n '' ) expected_text = ( `` tf.linalg.matmul ( a , b , c , d , a_is_sparse=e , `` `` b_is_sparse=f , name=g ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 the wrapper will emit a warning if it is deleted without any of its properties being accessed or methods being called .
__label__0 inp_ab = collections.ordereddict ( [ ( `` a '' , 1 ) , ( `` b '' , ( 2 , 3 ) ) ] ) inp_ba = collections.ordereddict ( [ ( `` b '' , ( 2 , 3 ) ) , ( `` a '' , 1 ) ] ) nest.assert_shallow_structure ( inp_ab , inp_ba )
__label__0 @ tf_export ( `` __internal__.nest.get_traverse_shallow_structure '' , v1= [ ] ) def get_traverse_shallow_structure ( traverse_fn , structure , expand_composites=false ) : `` '' '' generates a shallow structure from a ` traverse_fn ` and ` structure ` .
__label__0 import functools
__label__0 if ` structure ` is or contains a dict instance , the keys will be sorted to pack the flat sequence in deterministic order . this is true also for ` ordereddict ` instances : their sequence order is ignored , the sorting order of keys is used instead . the same convention is followed in ` flatten ` . this correctly repacks dicts and ` ordereddict ` s after they have been flattened , and also allows flattening an ` ordereddict ` and then repacking it back using a corresponding plain dict , or vice-versa . dictionaries with non-sortable keys can not be flattened .
__label__0 def raw_generate ( output_file , source_dir , git_tag_override=none ) : `` '' '' simple generator used for cmake/make build systems .
__label__0 import_header = `` from tensorflow import foo\n '' text = import_header + old_symbol expected_text = `` from tensorflow.compat.v2 import foo\n '' + new_symbol _ , unused_report , unused_errors , new_text = self._upgrade ( text , import_rename=true ) self.assertequal ( new_text , expected_text )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testflattenandpackmappingviews ( self ) : `` '' '' ` flatten ` orders dicts by key , including ordereddicts . '' '' '' ordered = collections.ordereddict ( [ ( `` d '' , 3 ) , ( `` b '' , 1 ) , ( `` a '' , 0 ) , ( `` c '' , 2 ) ] )
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensor_slices ( [ 5. , 6. , 7. , 8 . ] ) .batch ( 2 ) > > > dataset_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > distributed_values = next ( dataset_iterator ) > > > @ tf.function ... def run ( input ) : ... return input + 1.0 > > > updated_value = strategy.run ( run , args= ( distributed_values , ) ) > > > updated_value perreplica : { 0 : < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 6 . ] , dtype=float32 ) > , 1 : < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 7 . ] , dtype=float32 ) > }
__label__0 decorated = tf_decorator.make_decorator ( func , wrapper , decorator_argspec=tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , kwonlyargs= { } , defaults= ( 3 , 'hello ' ) , kwonlydefaults=none , varkw=none , annotations=none ) )
__label__0 def testcreateslotwithcustomreplicatedxlasharding ( self ) : # slot_creator is used only in optimizer v1 . # we insert our own custom replicated xla sharding that overrides the spmd # sharding copied over by the slot_creator . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) v = xla_sharding.mesh_split ( v , np.array ( [ 0 , 1 ] ) , [ 0 ] , use_sharding_op=false ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 , copy_xla_sharding=true ) slot = xla_sharding.replicate ( slot , use_sharding_op=false )
__label__0 with path.open ( `` w '' ) as f : yaml.dump ( content , f , default_flow_style=false )
__label__0 the fork of nest that tf.data uses treats lists as atoms , while tf.nest treats them as structures to recurse into . keras has chosen to adopt the latter convention , and must therefore deeply replace all lists with tuples before passing structures to dataset.from_generator .
__label__0 non-conformant pair of ` sequenceexample ` s ( mismatched types ) :
__label__0 system = platform.system ( ) if system == `` linux '' : libcudart = ct.cdll.loadlibrary ( `` libcudart.so '' ) elif system == `` darwin '' : libcudart = ct.cdll.loadlibrary ( `` libcudart.dylib '' ) elif system == `` windows '' : libcudart = ct.windll.loadlibrary ( `` libcudart.dll '' ) else : raise notimplementederror ( `` can not identify system . '' )
__label__0 example retrieval gql statements :
__label__0 def _cast_transformer ( parent , node , full_name , name , logs ) : `` '' '' transforms to_int and to_float to cast ( ... , dtype= ... ) . '' '' ''
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) checkpoint_filename = os.path.join ( checkpoint_dir , `` prepare_session_checkpoint '' ) saver.save ( sess , checkpoint_filename ) # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : # renames the checkpoint directory . os.rename ( checkpoint_dir , checkpoint_dir2 ) gfile.makedirs ( checkpoint_dir ) v = variable_v1.variablev1 ( [ 6.0 , 7.0 , 8.0 ] , name= '' v '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) # this should fail as there 's no checkpoint within 2 seconds . with self.assertraisesregex ( runtimeerror , `` no init_op or init_fn or local_init_op was given '' ) : sess = sm.prepare_session ( `` '' , init_op=none , saver=saver , checkpoint_dir=checkpoint_dir , wait_for_checkpoint=true , max_wait_secs=2 ) # rename the checkpoint directory back . gfile.deleterecursively ( checkpoint_dir ) os.rename ( checkpoint_dir2 , checkpoint_dir ) # this should succeed as there 's checkpoint . sess = sm.prepare_session ( `` '' , init_op=none , saver=saver , checkpoint_dir=checkpoint_dir , wait_for_checkpoint=true , max_wait_secs=2 ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) )
__label__0 def __init__ ( self , num_groups=2 ) : `` '' '' initialize a group lock .
__label__0 def testwarmstart_sparsecolumnvocabulary ( self ) : # create vocab for sparse column `` sc_vocab '' . vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` vocab '' ) # create feature column . sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=vocab_path , vocabulary_size=4 )
__label__0 def testbasic ( self ) : self.basicsaverestore ( variables.variable )
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 returns : a ` pysignaturechecker ` . `` '' '' if not ( isinstance ( signature , dict ) and all ( isinstance ( k , ( str , int ) ) for k in signature ) ) : raise typeerror ( `` signatures must be dictionaries mapping parameter names `` `` to type annotations . '' ) checkers = [ ]
__label__0 flat_string_paths = ( stringify_and_join ( path ) for path in flat_paths ) return list ( zip ( flat_string_paths , flatten ( structure , expand_composites=expand_composites ) ) )
__label__0 def _tf_core_pack_sequence_as ( structure , flat_sequence , expand_composites , sequence_fn=none ) : `` '' '' implements sequence packing , with the option to alter the structure . '' '' '' is_nested_fn = ( _is_nested_or_composite if expand_composites else _tf_core_is_nested ) sequence_fn = sequence_fn or sequence_like
__label__0 * the `` type-based dispatch '' system checks the types of the parameters passed to an api , and performs dispatch if those types match any signatures that have been registered for dispatch .
__label__0 def func ( a , b ) : return ( a , b )
__label__0 # resets container . session aborts . session.session.reset ( server.target , [ `` test0 '' ] ) with self.assertraises ( errors_impl.abortederror ) : sess.run ( v1 )
__label__0 args : arg0 : arg 0. arg1 : arg 1. deprecated : deprecated !
__label__0 # list of partitionedvariable is invalid type when warm-starting with vocab . self.assertraises ( typeerror , ws_util._warm_start_var_with_vocab , [ x ] , `` /tmp '' , 5 , `` /tmp '' , `` /tmp '' )
__label__0 for match in self.fence_cell_re.finditer ( string ) : if re.search ( 'doctest . * skip ' , match.group ( 0 ) , re.ignorecase ) : continue
__label__0 > > > class maskedtensor ( tf.experimental.extensiontype ) : ... values : tf.tensor ... mask : tf.tensor > > > @ dispatch_for_binary_elementwise_apis ( maskedtensor , maskedtensor ) ... def binary_elementwise_api_handler ( api_func , x , y ) : ... return maskedtensor ( api_func ( x.values , y.values ) , x.mask & y.mask ) > > > a = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ true , true , true , true , false ] ) > > > b = maskedtensor ( [ 2 , 4 , 6 , 8 , 0 ] , [ true , true , true , false , true ] ) > > > c = tf.add ( a , b ) > > > print ( f '' values= { c.values.numpy ( ) } , mask= { c.mask.numpy ( ) } '' ) values= [ 3 6 9 12 5 ] , mask= [ true true true false false ]
__label__0 @ compatibility ( tf2 ) with the deprecation of global graphs , tf no longer tracks variables in collections . in other words , there are no global variables in tf2 . thus , the global step functions have been removed ( ` get_or_create_global_step ` , ` create_global_step ` , ` get_global_step ` ) . you have two options for migrating :
__label__0 args : fn : function , or function-like object ( e.g. , result of ` functools.partial ` ) .
__label__0 git_tag_override : override the value for the git tag . this is useful for releases where we want to build the release before the git tag is created .
__label__0 `` ` python code `` `
__label__0 def testlocalinitopfornonchief ( self ) : logdir = self._test_dir ( `` default_local_init_op_non_chief '' ) with ops.graph ( ) .as_default ( ) : with ops.device ( `` /job : localhost '' ) : # a local variable . v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , trainable=false , collections= [ ops.graphkeys.local_variables ] ) # this should n't add a variable to the variables collection responsible # for variables that are saved/restored from checkpoints . self.assertequal ( len ( variables.global_variables ( ) ) , 0 )
__label__0 def get_bool ( self ) : `` '' '' consume a bool .
__label__0 def testrecoversession ( self ) : # create a checkpoint . checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` recover_session '' ) try : gfile.deleterecursively ( checkpoint_dir ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 def fn ( test_arg , a ) : if test_arg ! = expected_test_arg : return valueerror ( 'partial fn does not work correctly ' ) return a
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 # verifies that saver0 graph nodes are omitted from the saver1 export self.assertequal ( 33 , len ( meta_graph_def0.graph_def.node ) ) self.assertequal ( 21 , len ( meta_graph_def1.graph_def.node ) )
__label__0 def __init__ ( self , methodname= '' runtest '' ) : # pylint : disable=invalid-name super ( grpcservertest , self ) .__init__ ( methodname ) self._cached_server = server_lib.server.create_local_server ( )
__label__0 text = `` tf.contrib.distribute.mirroredstrategy '' expected = `` tf.contrib.distribute.mirroredstrategy '' _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` migrated to tf.distribute.mirroredstrategy '' , errors [ 0 ] )
__label__0 uses ` str ( value ) ` , except for ` bytes ` typed inputs , which are converted using ` as_str ` .
__label__0 # inspect.signature ( ) is preferred over inspect.getfullargspec ( ) in py3 . # note that while it can handle tfdecorators , it will ignore a tfdecorator 's # provided argspec/fullargspec and instead return the signature of the # inner-most function . def signature ( obj , * , follow_wrapped=true ) : `` '' '' tfdecorator-aware replacement for inspect.signature . '' '' '' return _inspect.signature ( tf_decorator.unwrap ( obj ) [ 1 ] , follow_wrapped=follow_wrapped )
__label__0 # save checkpoint from which to warm-start . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : variable_scope.get_variable ( `` linear_model/sc_hash/weights '' , shape= [ 15 , 1 ] , initializer=norms ( ) ) sc_keys_weights = variable_scope.get_variable ( `` some_other_name '' , shape= [ 4 , 1 ] , initializer=rand ( ) ) variable_scope.get_variable ( `` linear_model/sc_vocab/weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 2 . ] , [ 3 . ] ] ) self._write_checkpoint ( sess ) prev_keys_val = self.evaluate ( sc_keys_weights )
__label__0 text = `` tf.contrib.layers.l1_regularizer ( scale , scope ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l1 ( scale ) \n '' , ) self.assertin ( `` dropping scope '' , unused_report )
__label__0 for mod in all_modules : for submodule in submodules : # the below for loop is a constant time loop . for package in packages : if package + submodule in mod.__name__ : filtered_modules.append ( mod )
__label__0 if __name__ == `` __main__ '' : googletest.main ( )
__label__0 def test_summary_api_warning ( self ) : text = `` tf.summary.scalar ( 'foo ' , 42 ) '' _ , report , _ , _ = self._upgrade ( text ) expected_info = `` tf 1.x summary api can not be automatically migrated '' self.assertin ( expected_info , report )
__label__0 we need to ensure things exported by tf_export , etc . export symbols under disjoint top-level package names .
__label__0 args : structure : the nested structure to flatten . expand_composites : if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 s1 = save.save ( sess , os.path.join ( save_dir , `` s1 '' ) ) self.assertequal ( [ s1 ] , save.last_checkpoints ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) self.assertcheckpointstate ( model_checkpoint_path=s1 , all_model_checkpoint_paths= [ s1 ] , save_dir=save_dir )
__label__0 `` '' '' del restore_sequentially all_tensors = [ ] for saveable in saveables : if saveable.device : device = saveable_object_util.set_cpu0 ( saveable.device ) else : device = none with ops.device ( device ) : all_tensors.extend ( self.restore_op ( filename_tensor , saveable , preferred_shard ) ) return all_tensors
__label__0 returns : a bool indicates if ` object ` or any target along the chain of tf decorators is a method. `` '' '' decorators , target = tf_decorator.unwrap ( object ) for decorator in decorators : if _inspect.ismethod ( decorator.decorated_target ) : return true
__label__0 try : del compiler except nameerror : pass
__label__0 # local_anchor op will be placed on this worker task by default . local_anchor = control_flow_ops.no_op ( ) # colocating local_step variable prevents it being placed on the ps . distribution_strategy = distribute_lib.get_strategy ( ) with distribution_strategy.extended.colocate_vars_with ( local_anchor ) : self._local_step = variable_v1.variablev1 ( initial_value=0 , trainable=false , collections= [ ops.graphkeys.local_variables ] , dtype=global_step.dtype.base_dtype , name= '' sync_rep_local_step '' )
__label__0 def _wrap_decorator ( wrapped_function , decorator_name ) : `` '' '' indicate that one function wraps another .
__label__0 import numpy as np
__label__0 def test_contrib_to_addons_move ( self ) : small_mapping = { `` tf.contrib.layers.poincare_normalize '' : `` tfa.layers.poincarenormalize '' , `` tf.contrib.layers.maxout '' : `` tfa.layers.maxout '' , `` tf.contrib.layers.group_norm '' : `` tfa.layers.groupnormalization '' , `` tf.contrib.layers.instance_norm '' : `` tfa.layers.instancenormalization '' , } for symbol , replacement in small_mapping.items ( ) : text = `` { } ( 'stuff ' , * args , * * kwargs ) '' .format ( symbol ) _ , report , _ , _ = self._upgrade ( text ) self.assertin ( replacement , report )
__label__0 # the next one should be a stop message if we closed cleanly . ev = next ( rr ) self.assertequal ( event_pb2.sessionlog.stop , ev.session_log.status )
__label__0 @ test_util.run_v1_only ( `` this exercises tensor.op which is meaningless in v2 . '' ) def teststrippedoplistdef ( self ) : with self.cached_session ( ) : # creates a graph . v0 = variable_v1.variablev1 ( 0.0 ) var = variable_v1.variablev1 ( 10.0 ) math_ops.add ( v0 , var )
__label__0 def testkeywordreorderandrename ( self ) : `` '' '' test that we get the expected result if kw2 is renamed and moved . '' '' '' text = `` f ( a , b , kw1=c , kw2=d ) \n '' acceptable_outputs = [ `` f ( a , b , kw3=d , kw1=c ) \n '' , `` f ( a=a , b=b , kw1=c , kw3=d ) \n '' , `` f ( a=a , b=b , kw3=d , kw1=c ) \n '' , ] ( _ , report , _ ) , new_text = self._upgrade ( reorderandrenamekeywordspec ( ) , text ) self.assertin ( new_text , acceptable_outputs ) self.assertnotin ( `` manual check required '' , report )
__label__0 `` ` feature = union [ list [ bytes ] , list [ int64 ] , list [ float ] ]
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_instance_fn_with_one_line_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 @ tf_export ( `` __internal__.nest.map_structure_up_to '' , v1= [ ] ) def map_structure_up_to ( shallow_tree , func , * inputs , * * kwargs ) : `` '' '' applies a function or op to a number of partially flattened inputs .
__label__0 def __new__ ( cls , a , b=1 , c='hello ' ) : pass
__label__0 we also test whether a converted file is executable . test_file_v1_10.py aims to exhaustively test that api changes are convertible and actually work when run with current tensorflow. `` '' ''
__label__0 def __iter__ ( self ) : keys = list ( self._storage ) for key in keys : unwrapped = key.unwrapped if unwrapped is none : self.discard ( key ) else : yield unwrapped # lint.thenchange ( //tensorflow/python/keras/utils/object_identity.py )
__label__0 from tensorflow.python.platform import test from tensorflow.tools.proto_splitter import util from tensorflow.tools.proto_splitter.testdata import test_message_pb2
__label__0 aside from the ` compat.v1 ` and ` compat.v2 ` submodules , ` tf.compat ` also contains a set of helper functions for writing code that works in both :
__label__0 expected_values = [ get_const_value ( 0 ) , get_const_value ( 1 ) , get_const_value ( 3 ) , ] for expected , chunk in zip ( expected_values , chunks [ 1 : ] ) : self.assertequal ( expected , chunk )
__label__0 argspec = tf_inspect.argspec ( args= [ 'cls ' , ' a ' , ' b ' , ' c ' ] , varargs=none , keywords=none , defaults= ( 1 , 'hello ' ) )
__label__0 def testrecomputegrad ( self ) : text = `` tf.contrib.layers.recompute_grad ( ) '' expected = `` tf.recompute_grad ( ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__1 class randomizedcollection : def __init__ ( self ) : self.val_to_indices = { } self.nums = [ ] def insert ( self , val : int ) - > bool : self.nums.append ( val ) if val not in self.val_to_indices : self.val_to_indices [ val ] = [ ] self.val_to_indices [ val ] .append ( len ( self.nums ) - 1 ) return len ( self.val_to_indices [ val ] ) == 1 def remove ( self , val : int ) - > bool : if val not in self.val_to_indices : return false idx = self.val_to_indices [ val ] .pop ( ) last_val = self.nums [ -1 ] self.nums [ idx ] = last_val self.val_to_indices [ last_val ] .remove ( len ( self.nums ) - 1 ) if idx < len ( self.nums ) - 1 : self.val_to_indices [ last_val ] .append ( idx ) if not self.val_to_indices [ val ] : del self.val_to_indices [ val ] self.nums.pop ( ) return true def getrandom ( self ) - > int : return random.choice ( self.nums )
__label__0 args : in_filename : filename to parse in_file : opened file ( or stringio ) out_filename : output file to write to out_file : opened file ( or stringio ) returns : a tuple representing number of files processed , log of actions , errors `` '' '' lines = in_file.readlines ( ) processed_file , new_file_content , log , process_errors = ( self.update_string_pasta ( `` '' .join ( lines ) , in_filename ) )
__label__0 def test_saved_model_load ( self ) : text = `` tf.saved_model.load ( sess , [ 'foo_graph ' ] ) '' expected = `` tf.compat.v1.saved_model.load ( sess , [ 'foo_graph ' ] ) '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) expected_info = `` tf.saved_model.load works differently in 2.0 '' self.assertin ( expected_info , report )
__label__0 from tensorflow.core.protobuf import cluster_pb2 from tensorflow.core.protobuf import device_filters_pb2 from tensorflow.core.protobuf import tensorflow_server_pb2 from tensorflow.python.client import pywrap_tf_session as c_api from tensorflow.python.framework import errors from tensorflow.python.util import compat from tensorflow.python.util import deprecation from tensorflow.python.util.tf_export import tf_export
__label__0 # no keyword used , should be no change text = `` g ( a , b , x , c ) \n '' _ , new_text = self._upgrade ( removedeprecatedaliaskeyword ( ) , text ) self.assertequal ( new_text , text )
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' dataset types . '' '' ''
__label__0 def is_forward_ref ( tp ) : `` '' '' returns true if ` tp ` is a typing forward reference . '' '' '' if hasattr ( typing , 'forwardref ' ) : return isinstance ( tp , typing.forwardref ) elif hasattr ( typing , '_forwardref ' ) : return isinstance ( tp , typing._forwardref ) # pylint : disable=protected-access else : return false
__label__0 # check that the parent graphdef still contains small nodes . self.assertprotoequals ( graph_def.node [ 0 ] , chunks [ 0 ] .node [ 0 ] ) self.assertprotoequals ( graph_def.node [ 4 ] , chunks [ 0 ] .node [ 4 ] )
__label__0 raises : valueerror : when the ` graphdef ` is larger than 2gb . runtimeerror : if called with eager execution enabled .
__label__0 this method is a replacement for __getattr__ ( ) . it will be added into the extended python module as a callback to reduce api overhead . instead of relying on implicit attributeerror handling , this added callback function will be called explicitly from the extended c api if the default attribute lookup fails. `` '' '' try : attr = getattr ( self._tfmw_wrapped_module , name ) except attributeerror : # placeholder for google-internal contrib error
__label__0 def testyieldflatstringpaths ( self ) : for inputs_expected in ( { `` inputs '' : [ ] , `` expected '' : [ ] } , { `` inputs '' : 3 , `` expected '' : [ ( ) ] } , { `` inputs '' : [ 3 ] , `` expected '' : [ ( 0 , ) ] } , { `` inputs '' : { `` a '' : 3 } , `` expected '' : [ ( `` a '' , ) ] } , { `` inputs '' : { `` a '' : { `` b '' : 4 } } , `` expected '' : [ ( `` a '' , `` b '' ) ] } , { `` inputs '' : [ { `` a '' : 2 } ] , `` expected '' : [ ( 0 , `` a '' ) ] } , { `` inputs '' : [ { `` a '' : [ 2 ] } ] , `` expected '' : [ ( 0 , `` a '' , 0 ) ] } , { `` inputs '' : [ { `` a '' : [ ( 23 , 42 ) ] } ] , `` expected '' : [ ( 0 , `` a '' , 0 , 0 ) , ( 0 , `` a '' , 0 , 1 ) ] } , { `` inputs '' : [ { `` a '' : ( [ 23 ] , 42 ) } ] , `` expected '' : [ ( 0 , `` a '' , 0 , 0 ) , ( 0 , `` a '' , 1 ) ] } , { `` inputs '' : { `` a '' : { `` a '' : 2 } , `` c '' : [ [ [ 4 ] ] ] } , `` expected '' : [ ( `` a '' , `` a '' ) , ( `` c '' , 0 , 0 , 0 ) ] } , { `` inputs '' : { `` 0 '' : [ { `` 1 '' : 23 } ] } , `` expected '' : [ ( `` 0 '' , 0 , `` 1 '' ) ] } ) : inputs = inputs_expected [ `` inputs '' ] expected = inputs_expected [ `` expected '' ] self.assertequal ( list ( nest.yield_flat_paths ( inputs ) ) , expected )
__label__0 the outermost-decorated object is intended to have the most complete documentation , so the decorated parameter is not unwrapped. `` '' '' return _inspect.getdoc ( object )
__label__0 # identitical signatures , no need to apply compatibility fixes . if op_signature == func_signature : return func
__label__0 # the global step should now be 2 and the gradients should have been # applied again . self.assertallequal ( 2 , sessions [ 1 ] .run ( global_step ) ) self.assertallclose ( -0.6 - ( 0.1 + 0.3 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_0_g_1 ) ) self.assertallclose ( -1.2 - ( 0.9 + 1.1 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_1_g_1 ) )
__label__0 def lookup ( self , keys , default ) : return gen_lookup_ops.lookup_table_find_v2 ( self.table_ref , keys , default )
__label__0 import lit.formats
__label__0 files_processed = processed_file report_text = upgrader._format_log ( log , in_filename , out_filename ) errors = process_errors
__label__0 # the lint error here is incorrect . def __init__ ( self , local_name , parent_module_globals , name ) : self._local_name = local_name self._parent_module_globals = parent_module_globals super ( _lazyloader , self ) .__init__ ( name )
__label__0 from absl import flags from absl.testing import absltest import numpy as np import tensorflow.compat.v2 as tf
__label__0 def testmapandbatch ( self ) : suffix = `` .data.experimental.map_and_batch_with_legacy_function ( args ) '' text = `` tf '' + suffix expected = `` tf.compat.v1 '' + suffix _ , unused_report , unused_errors , actual = self._upgrade ( text ) self.assertequal ( actual , expected )
__label__0 returns : the global step tensor .
__label__0 partial_func = functools.partial ( func , n=7 ) argspec = tf_inspect.argspec ( args= [ 'm ' , 'n ' ] , varargs=none , keywords=none , defaults= ( 1 , 7 ) )
__label__0 path_to_replace = { `` external/com_google_absl/ '' : `` '' , `` external/eigen_archive/ '' : `` '' , `` external/jsoncpp_git/ '' : `` '' , `` external/com_google_protobuf/src/ '' : `` '' , `` external/local_xla/ '' : `` tensorflow/compiler '' , `` external/local_tsl/ '' : `` tensorflow '' , }
__label__0 # # # # # launching fewer services
__label__0 def testsegmentsum ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 1 , 0 , 2 , 12 ] , dtype=dtype ) , self._segmentsumv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 `` ` python input_tree = [ [ ( ' a ' , 1 ) , [ ( ' b ' , 2 ) , [ ( ' c ' , 3 ) , [ ( 'd ' , 4 ) ] ] ] ] ] shallow_tree = [ [ 'level_1 ' , [ 'level_2 ' , [ 'level_3 ' , [ 'level_4 ' ] ] ] ] ]
__label__0 @ test_util.run_in_graph_and_eager_modes def testsavewithglobalstep ( self , pad_step_number=false ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` ckpt_with_global_step '' ) global_step_int = 5 # save and reload one variable named `` var0 '' . self._saveandload ( `` var0 '' , 0.0 , 1.0 , save_path ) for use_tensor in [ true , false ] : with self.session ( graph=ops_lib.graph ( ) ) : var = resource_variable_ops.resourcevariable ( 1.0 , name= '' var0 '' ) save = saver_module.saver ( { var._shared_name : var } , pad_step_number=pad_step_number ) if context.executing_eagerly ( ) : sess = none else : self.evaluate ( var.initializer ) sess = ops_lib.get_default_session ( ) if use_tensor : global_step = constant_op.constant ( global_step_int ) val = save.save ( sess , save_path , global_step=global_step ) else : val = save.save ( sess , save_path , global_step=global_step_int ) if pad_step_number : expected_save_path = `` % s- % s '' % ( save_path , `` { :08d } '' .format ( global_step_int ) ) else : expected_save_path = `` % s- % d '' % ( save_path , global_step_int ) self.assertequal ( expected_save_path , val )
__label__0 raises : runtimeerror : if called with eager execution enabled .
__label__0 def testsetconfiguration ( self ) : config = config_pb2.configproto ( gpu_options=config_pb2.gpuoptions ( per_process_gpu_memory_fraction=0.1 ) )
__label__0 1. we need ` mode= '' exec '' ` ( easy ) 2. we need the last expression to be printed ( harder ) .
__label__0 example : `` ` python class dimension ( tracetype ) : def __init__ ( self , value : optional [ int ] ) : self.value = value
__label__0 def test_contrib_summary_all_summary_ops ( self ) : text = `` tf.contrib.summary.all_summary_ops ( ) '' expected = `` tf.compat.v1.summary.all_v2_summary_ops ( ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 @ test_util.run_v1_only ( `` applyftrlmultiplylinearbylr op returns a ref , so it `` `` is not supported in eager mode . '' ) def testapplyftrlmultiplylinearbylr ( self ) : for dtype in [ np.float16 , np.float32 , np.float64 ] : x = np.arange ( 100 ) .astype ( dtype ) y = np.arange ( 1 , 101 ) .astype ( dtype ) z = np.arange ( 102 , 202 ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) l1 = np.array ( 3.0 ) .astype ( dtype ) l2 = np.array ( 4.0 ) .astype ( dtype ) grad = np.arange ( 100 ) .astype ( dtype ) self._testtypesforftrlmultiplylinearbylr ( x , y , z , lr , grad , use_gpu=false , l1=l1 , l2=l2 )
__label__0 t = 1 beta1 = np.array ( 0.9 , dtype=var.dtype ) beta2 = np.array ( 0.999 , dtype=var.dtype ) beta1_power = beta1 * * t beta2_power = beta2 * * t lr = np.array ( 0.001 , dtype=var.dtype ) epsilon = np.array ( 1e-8 , dtype=var.dtype ) beta1_t = constant_op.constant ( beta1 , self._totype ( var.dtype ) , [ ] ) beta2_t = constant_op.constant ( beta2 , self._totype ( var.dtype ) , [ ] ) beta1_power_t = variable_v1.variablev1 ( beta1_power ) beta2_power_t = variable_v1.variablev1 ( beta2_power ) lr_t = constant_op.constant ( lr , self._totype ( var.dtype ) , [ ] ) epsilon_t = constant_op.constant ( epsilon , self._totype ( var.dtype ) , [ ] ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 major , minor , patch = hipsparse_version_numbers ( rocm_install_path )
__label__0 from tensorflow.tools.compatibility import all_renames_v2 from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import module_deprecations_v2
__label__0 cfg = configparser.configparser ( inline_comment_prefixes= ' ; ' ) cfg.read ( config_path ) ptr_size = int ( cfg [ 'arch ' ] [ 'pointersize ' ] )
__label__0 when = 'in a future version ' if date is none else ( 'after % s ' % date )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' extract parse_example op configuration to a proto . '' '' ''
__label__0 # initializing in an isolated session will only affect the state in that # session . isolate_sess_0.run ( v.initializer , feed_dict= { init_value : 37 } ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_0.run ( v ) ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_1.run ( v ) ) self.assertallequal ( 37 , isolate_sess_0.run ( v ) ) with self.assertraises ( errors_impl.failedpreconditionerror ) : isolate_sess_1.run ( v )
__label__0 x = tensortracer ( `` x '' )
__label__0 returns : a list of test_log_pb2.gpuinfo messages. `` '' '' try : # prefer using /proc if possible , it provides the uuid . dev_info = _gather_gpu_devices_proc ( ) if not dev_info : raise valueerror ( `` no devices found '' ) return dev_info except ( ioerror , valueerror , errors.operror ) : pass
__label__0 def __init__ ( self ) : self.symbols = set ( ) self.last_parent = none self.last_children = none
__label__0 example = doctest.example ( 'none ' , want=want ) result = output_checker.output_difference ( example=example , got=got , optionflags=doctest.ellipsis ) self.assertin ( `` does n't work if * some * of the '' , result )
__label__0 for example :
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] shallow_tree : an arbitrarily nested structure . input_tree : an arbitrarily nested structure . check_types : if ` true ` ( default ) the sequence types of ` shallow_tree ` and ` input_tree ` have to be the same . note that even with check_types==true , this function will consider two different namedtuple classes with the same name and _fields attribute to be the same class . expand_composites : valid for modality.core only . if true , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors .
__label__0 from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import ipynb from tensorflow.tools.compatibility import tf_upgrade_v2 from tensorflow.tools.compatibility import tf_upgrade_v2_safety
__label__0 def test_name_scope ( self ) : text = `` tf.name_scope ( none , default_name , [ some , values ] ) '' expected_text = `` tf.name_scope ( name=default_name ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 # pylint : disable=unused-import import doctest import inspect import pickle import types
__label__0 args : input_str : the string containing $ { variable } or @ variable @ expressions to expand . cmake_vars : a dictionary mapping variable names to their values .
__label__0 returns : node , if it was modified , else none. `` '' ''
__label__0 args : filename_tensor : string tensor . saveable : a basesaverbuilder.saveableobject object . preferred_shard : int . shard to open first when loading a sharded file .
__label__0 if len ( fetched ) ! = len ( fetch_list ) : raise valueerror ( `` len ( fetched ) does not match len ( fetch_list ) `` `` ( % d vs % d ) '' % ( len ( fetched ) , len ( fetch_list ) ) )
__label__0 raises : valueerror : if ` flat_sequence ` and ` structure ` have different atom counts . typeerror : for modality.core only . ` structure ` is or contains a dict with non-sortable keys. `` '' '' if modality == modality.core : return _tf_core_pack_sequence_as ( structure , flat_sequence , expand_composites , sequence_fn ) elif modality == modality.data : return _tf_data_pack_sequence_as ( structure , flat_sequence ) else : raise valueerror ( `` unknown modality used { } for nested structure '' .format ( modality ) )
__label__0 decorator_utils.validate_callable ( func , `` keyword_args_only '' ) @ functools.wraps ( func ) def new_func ( * args , * * kwargs ) : `` '' '' keyword args only wrapper . '' '' '' if args : raise valueerror ( f '' the function { func.__name__ } only accepts keyword arguments. `` `` do not pass positional arguments . received the following positional `` f '' arguments : { args } '' ) return func ( * * kwargs ) return new_func
__label__0 _testparams = [ [ data_type ] + values for data_type , values in itertools.product ( _data_types , _test_param_values ) ]
__label__0 def _add_deprecated_arg_value_notice_to_docstring ( doc , date , instructions , deprecated_name_value_dict ) : `` '' '' adds a deprecation notice to a docstring for deprecated arguments . '' '' ''
__label__0 args : session : a tensorflow session that will be soon closed. `` '' '' pass
__label__0 # we should be done . self.assertraises ( stopiteration , lambda : next ( rr ) )
__label__0 * get the recent start times for a given test : select start from test where test = < test-name > and start > = < recent-datetime > limit < count >
__label__0 args : stacklevel : number of initial frames to skip when producing the stack .
__label__0 def get_int_list ( self , min_length=_min_length , max_length=_max_length , min_int=_min_int , max_int=_max_int ) : `` '' '' consume a signed integer list with given constraints .
__label__0 def _upgrade ( self , old_file_text ) : in_file = io.stringio ( old_file_text ) out_file = io.stringio ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade_v2_safety.tfapichangespec ( ) ) count , report , errors = ( upgrader.process_opened_file ( `` test.py '' , in_file , `` test_out.py '' , out_file ) ) return count , report , errors , out_file.getvalue ( )
__label__0 class ct ( composite_tensor.compositetensor ) : `` '' '' a generic compositetensor , used for constructing tests . '' '' ''
__label__0 self._add_chunk_order = [ id ( chunk ) for chunk in self._chunks ] self._fix_chunk_order = false
__label__0 def _tf_data_assert_same_structure ( nest1 , nest2 , check_types=true ) : _pywrap_utils.assertsamestructurefordata ( nest1 , nest2 , check_types )
__label__0 # sentinel value that can be returned to indicate that an operation # dispatcher does not support a given set of arguments . not_supported = object ( )
__label__0 def deprecated_args ( date , instructions , * deprecated_arg_names_or_tuples , * * kwargs ) : `` '' '' decorator for marking specific function arguments as deprecated .
__label__0 if `` nccl '' in libraries : nccl_paths = _get_legacy_path ( `` nccl_install_path '' , base_paths ) nccl_version = os.environ.get ( `` tf_nccl_version '' , `` '' ) result.update ( _find_nccl_config ( nccl_paths , nccl_version ) )
__label__0 def testcallingadecoratedfunctioncallsthetarget ( self ) : self.assertequal ( ( 2 + 1 ) * 2 , test_decorated_function ( 2 ) )
__label__0 deprecation_string = ' , '.join ( ' % s= % r ' % ( key , value ) for key , value in sorted ( deprecated_name_value_dict.items ( ) ) )
__label__0 class objectidentitywrappertest ( test.testcase ) :
__label__0 # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_hash ] , partitioner ) ws_util.warm_start ( self.get_temp_dir ( ) , vars_to_warm_start= '' . * sc_hash . * '' ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . self._assert_cols_to_vars ( cols_to_vars , { sc_hash : [ prev_hash_val ] } , sess )
__label__0 def test_get_global_step ( self ) : with ops.graph ( ) .as_default ( ) as g : self.assertisnone ( training_util.get_global_step ( ) ) variable_v1.variablev1 ( 0 , trainable=false , dtype=dtypes.int32 , name=ops.graphkeys.global_step , collections= [ ops.graphkeys.global_step ] ) self._assert_global_step ( training_util.get_global_step ( ) , expected_dtype=dtypes.int32 ) self._assert_global_step ( training_util.get_global_step ( g ) , expected_dtype=dtypes.int32 )
__label__0 restoring_file = checkpoint_dir or checkpoint_filename_with_path if not local_init_success : logging.info ( `` restoring model from % s did not make model ready for local init : '' `` % s '' , restoring_file , msg ) return sess , false
__label__0 def filter_on_submodules ( all_modules , submodules ) : `` '' '' filters all the modules based on the modules flag .
__label__0 def document ( obj , doc ) : `` '' '' adds a docstring to typealias by overriding the ` __doc__ ` attribute .
__label__0 # initializes the variables . verifies that the values are correct . sess_0 = session.session ( server0.target ) sess_1 = session.session ( server1.target ) sess_0.run ( v0.initializer ) sess_1.run ( v1.initializer ) self.assertallequal ( 1.0 , sess_0.run ( v0 ) ) self.assertallequal ( 2.0 , sess_1.run ( v1 ) )
__label__0 def intersection ( self , items ) : return self._storage.intersection ( [ self._wrap_key ( item ) for item in items ] )
__label__0 from absl import app from absl import flags
__label__0 returns : a nested structure of ` tf.typespec ` objects matching the structure of an element of this ` tf.distribute.distributeddataset ` . this returned value is typically a ` tf.distribute.distributedvalues ` object and specifies the ` tf.tensorspec ` of individual components. `` '' '' raise notimplementederror ( `` distributeddataset.element_spec must be implemented in descendants . '' )
__label__0 float values in the output compared as using ` np.allclose ` :
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { ... 'my_ints ' : tf.io.raggedfeature ( dtype=tf.int64 ) , ... 'my_floats ' : tf.io.raggedfeature ( dtype=tf.float32 ) , ... 'my_bytes ' : tf.io.raggedfeature ( dtype=tf.string ) } ) { 'my_bytes ' : < tf.tensor : shape= ( 2 , ) , dtype=string , numpy=array ( [ b'abc ' , b'1234 ' ] , dtype=object ) > , 'my_floats ' : < tf.tensor : shape= ( 4 , ) , dtype=float32 , numpy=array ( [ 1. , 2. , 3. , 4 . ] , dtype=float32 ) > , 'my_ints ' : < tf.tensor : shape= ( 4 , ) , dtype=int64 , numpy=array ( [ 1 , 2 , 3 , 4 ] ) > }
__label__0 _write_version = saver_pb2.saverdef.v1
__label__0 # the steps should also be initialized . self.assertallequal ( 0 , sessions [ 1 ] .run ( global_step ) ) self.assertallequal ( 0 , sessions [ 1 ] .run ( local_step_1 ) ) self.assertallclose ( [ [ 3.0 ] , [ 4.0 ] ] , sessions [ 1 ] .run ( var_sparse_g_1 ) )
__label__0 def main ( unused_argv ) : merged_source = pathlib.path ( tempfile.mkdtemp ( ) ) shutil.copytree ( source_path , merged_source / 'java ' )
__label__0 # make sure we can still build gradients and get the same result . var = ops_lib.get_default_graph ( ) .get_tensor_by_name ( var_name ) output = ops_lib.get_default_graph ( ) .get_tensor_by_name ( output_name ) grad = gradients_impl.gradients ( [ output ] , [ var ] )
__label__0 # optimizers . # pylint : disable=g-bad-import-order , unused-import from tensorflow.python.ops.sdca_ops import sdca_optimizer from tensorflow.python.ops.sdca_ops import sdca_fprint from tensorflow.python.ops.sdca_ops import sdca_shrink_l1 from tensorflow.python.training.adadelta import adadeltaoptimizer from tensorflow.python.training.adagrad import adagradoptimizer from tensorflow.python.training.adagrad_da import adagraddaoptimizer from tensorflow.python.training.proximal_adagrad import proximaladagradoptimizer from tensorflow.python.training.adam import adamoptimizer from tensorflow.python.training.ftrl import ftrloptimizer from tensorflow.python.training.experimental.loss_scale_optimizer import mixedprecisionlossscaleoptimizer from tensorflow.python.training.experimental.mixed_precision import enable_mixed_precision_graph_rewrite_v1 from tensorflow.python.training.momentum import momentumoptimizer from tensorflow.python.training.moving_averages import exponentialmovingaverage from tensorflow.python.training.optimizer import optimizer from tensorflow.python.training.rmsprop import rmspropoptimizer from tensorflow.python.training.gradient_descent import gradientdescentoptimizer from tensorflow.python.training.proximal_gradient_descent import proximalgradientdescentoptimizer from tensorflow.python.training.sync_replicas_optimizer import syncreplicasoptimizer
__label__0 `` ` class try :
__label__0 usage : in case a simplified ` bytes ` version of the path is needed from an ` os.pathlike ` object. `` '' '' if hasattr ( path , '__fspath__ ' ) : path = path.__fspath__ ( ) return as_bytes ( path )
__label__0 def testremovedeprecatedkeywordandreorder2 ( self ) : `` '' '' same as testremovedeprecatedkeywordandreorder but on g2 ( more args ) . '' '' '' text = `` g2 ( a , b , kw1=x , c=c , d=d ) \n '' acceptable_outputs = [ `` g2 ( a , b , c=c , d=d , kw1=x ) \n '' , `` g2 ( a=a , b=b , kw1=x , c=c , d=d ) \n '' , ] _ , new_text = self._upgrade ( removedeprecatedaliasandreorderrest ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testnesteddataclassflatten ( self ) : nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 1 ] ) ) leaves = nest.flatten ( nmt ) self.assertlen ( leaves , 1 ) self.assertallequal ( leaves [ 0 ] , [ 1 ] )
__label__0 * ` flush_secs ` : how often , in seconds , to flush the added summaries and events to disk . * ` max_queue ` : maximum number of summaries or events pending to be written to disk before one of the 'add ' calls block .
__label__0 args : key : a string collection key .
__label__0 returns : a tuple ( is_ready , msg ) , where is_ready is true if ready and false otherwise , and msg is ` none ` if the model is ready , a ` string ` with the reason why it is not ready otherwise. `` '' '' return _ready ( self._ready_op , sess , `` model not ready '' )
__label__0 def _is_macos ( ) : return platform.system ( ) == `` darwin ''
__label__0 def testrunscodeafteryield ( self ) : x = [ ] with test_yield_append_before_and_after_yield ( x , `` , 'after ' ) : pass self.assertequal ( 'after ' , x [ -1 ] )
__label__0 def _init_summary_op ( self , summary_op=use_default ) : `` '' '' initializes summary_op .
__label__0 def func ( m , n ) : return 2 * m + n
__label__0 def testspacetodepth ( self ) : text = `` tf.nn.space_to_depth ( input , block_size , name , data_format ) '' expected_text = ( `` tf.nn.space_to_depth ( input , block_size , name=name , `` `` data_format=data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 try : final_index , packed = _tf_core_packed_nest_with_indices ( structure , flat_sequence , 0 , is_nested_fn , sequence_fn ) if final_index < len ( flat_sequence ) : raise indexerror except indexerror : flat_structure = _tf_core_flatten ( structure , expand_composites=expand_composites ) if len ( flat_structure ) ! = len ( flat_sequence ) : # pylint : disable=raise-missing-from raise valueerror ( `` could not pack sequence . structure had % d atoms , but `` `` flat_sequence had % d items . structure : % s , flat_sequence : % s . '' % ( len ( flat_structure ) , len ( flat_sequence ) , structure , flat_sequence ) ) return sequence_fn ( structure , packed )
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 def __init__ ( self , a , b=1 , c='hello ' ) : pass
__label__0 import glob import multiprocessing import platform import re import socket
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' traversing python modules and classes . '' '' ''
__label__0 dict_mt_nmt = { `` mt '' : mt , `` nmt '' : nmt } dict_mt_nmt_flat_paths = nest.flatten_with_tuple_paths ( dict_mt_nmt ) self.assertequal ( dict_mt_nmt_flat_paths [ 0 ] [ 0 ] , ( `` mt '' , 0 ) ) self.assertallequal ( dict_mt_nmt_flat_paths [ 0 ] [ 1 ] , [ 1 ] ) self.assertequal ( dict_mt_nmt_flat_paths [ 1 ] [ 0 ] , ( `` nmt '' , 0 , 0 ) ) self.assertallequal ( dict_mt_nmt_flat_paths [ 1 ] [ 1 ] , [ 2 ] )
__label__0 def func ( m , n ) : return 2 * m + n
__label__0 try : # python3 will handle most callables here ( not partial ) . return _getargspec ( target ) except typeerror : pass
__label__0 def testtwoprocesses ( self ) : cluster_def = server_lib.clusterspec ( { `` local '' : [ `` localhost:2222 '' , `` localhost:2223 '' ] } ) .as_cluster_def ( ) server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_def , job_name= '' local '' , task_index=1 , protocol= '' grpc '' )
__label__0 def test_method ( self ) : decorator_utils.validate_callable ( self.test_method , `` test '' )
__label__0 before migrating :
__label__0 args : * args : inputs to specialize on . * * kwargs : inputs to specialize on .
__label__0 from tensorflow.tools.common import test_module2
__label__0 def get_string ( self , byte_count=_max_int ) : `` '' '' consume a string with given constraints based on a consumed bool .
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' visitor restricting traversal to only the public tensorflow api . '' '' ''
__label__0 # save checkpoint from which to warm-start . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : variable_scope.get_variable ( `` linear_model/sc_vocab_embedding/embedding_weights '' , initializer= [ [ 0.5 , 0.4 ] , [ 1. , 1.1 ] , [ 2. , 2.2 ] , [ 3. , 3.3 ] ] ) variable_scope.get_variable ( `` linear_model/sc_vocab_embedding/weights '' , initializer= [ [ 0.69 ] , [ 0.71 ] ] ) self._write_checkpoint ( sess )
__label__0 def _maybe_modify_args ( self , node , full_name , name ) : `` '' '' rename keyword args if the function called full_name requires it . '' '' '' renamed_keywords = self._get_applicable_dict ( `` function_keyword_renames '' , full_name , name )
__label__0 examples :
__label__0 note : for backwards compatibility , this method returns a list . if the given job was defined with a sparse set of task indices , the length of this list may not reflect the number of tasks defined in this job . use the ` tf.train.clusterspec.num_tasks ` method to find the number of tasks defined in a particular job .
__label__0 @ tf_export ( `` nest.map_structure '' ) def map_structure ( func , * structure , * * kwargs ) : `` '' '' creates a new structure by applying ` func ` to each atom in ` structure ` .
__label__0 # verify that the original names are not in the saved file save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 } ) with self.assertraisesoperror ( `` not found in checkpoint '' ) : save.restore ( sess , save_path )
__label__0 try : del examples except nameerror : pass
__label__0 * these examples will raise exceptions :
__label__0 class traversetest ( googletest.testcase ) :
__label__0 update_xla_tsl_imports ( os.path.join ( srcs_dir , `` tensorflow '' ) ) if not is_windows ( ) : rename_libtensorflow ( os.path.join ( srcs_dir , `` tensorflow '' ) , version ) if not is_macos ( ) and not is_windows ( ) : patch_so ( srcs_dir )
__label__0 # restore the saved values in the parameter nodes . save = saver_module.saver ( { `` save_prefix/v0 '' : v0 , `` save_prefix/v1 '' : v1 } ) save.restore ( sess , save_path )
__label__0 proto = chunk_pb2.chunkmetadata ( ) proto.parsefromstring ( records [ -1 ] ) self.assertlen ( proto.message.chunked_fields , 3 ) self.assertfalse ( proto.message.hasfield ( `` chunk_index '' ) )
__label__0 # this loop processes imports in the format # import foo as f , bar as b for import_alias in node.names : all_import_components = import_alias.name.split ( `` . '' ) # look for rename , starting with longest import levels . found_update = false for i in reversed ( list ( range ( 1 , max_submodule_depth + 1 ) ) ) : import_component = all_import_components [ 0 ] for j in range ( 1 , min ( i , len ( all_import_components ) ) ) : import_component += `` . '' + all_import_components [ j ] import_rename_spec = import_renames.get ( import_component , none )
__label__0 structure1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] with self.assertraisesregex ( typeerror , `` do n't have the same sequence type '' ) : nest.assert_same_structure ( structure1 , structure1_list ) nest.assert_same_structure ( structure1 , structure2 , check_types=false ) nest.assert_same_structure ( structure1 , structure1_list , check_types=false )
__label__0 def dispatch ( op , args , kwargs ) : `` '' '' returns the result from the first successful dispatcher for a given op .
__label__0 return decorator
__label__0 class serializationtests ( test.testcase ) :
__label__0 result [ `` sycl_basekit_path '' ] = basekit_path result [ `` sycl_toolkit_path '' ] = toolkit_path result.update ( _find_sycl_config ( basekit_path ) )
__label__0 > > > dict = { `` key3 '' : { `` c '' : ( 1.0 , 2.0 ) , `` a '' : ( 3.0 ) } , ... `` key1 '' : { `` m '' : `` val1 '' , `` g '' : `` val2 '' } } > > > tf.nest.flatten ( dict ) [ 'val2 ' , 'val1 ' , 3.0 , 1.0 , 2.0 ]
__label__0 return nest_util.map_structure_up_to ( nest_util.modality.core , structure [ 0 ] , wrapper_func , * structure , * * kwargs )
__label__0 def prepare_or_wait_for_session ( self , master= '' '' , config=none , wait_for_checkpoint=false , max_wait_secs=7200 , start_standard_services=true ) : `` '' '' make sure the model is ready to be used .
__label__0 return tf_decorator.make_decorator ( func_or_class , new_func , 'deprecated ' , _add_deprecated_function_notice_to_docstring ( func_or_class.__doc__ , none , 'please use % s instead . ' % name ) )
__label__0 # keep track of which var_names in var_name_to_prev_var_name and # var_name_to_vocab_info have been used . err on the safer side by throwing an # exception if any are unused by the end of the loop . it is easy to misname # a variable during this configuration , in which case without this check , we # would fail to warm-start silently . prev_var_name_used = set ( ) vocab_info_used = set ( )
__label__0 def as_dict ( self ) : `` '' '' returns a dictionary from job names to their tasks .
__label__0 the script searches for rocm library and header files on the system , inspects them to determine their version and prints the configuration to stdout . the path to inspect is specified through an environment variable ( rocm_path ) . if no valid configuration is found , the script prints to stderr and returns an error code .
__label__0 returns : tensor of random shape filled with uniformly random numeric values. `` '' '' # max shape can be 8 in length and randomized from 0-8 without running into # an oom error . if max_size > 8 : raise tf.errors.invalidargumenterror ( none , none , 'given size of { } will result in an oom error'.format ( max_size ) )
__label__0 a slot is a ` variable ` created with the same first m-dimension as a primary variable or ` tensor ` . a slot is always scoped in the namespace of the primary object and typically has the same device and type .
__label__0 returns : a string tensor. `` '' '' return gen_io_ops.sharded_filename ( filename_tensor , shard , num_shards )
__label__0 def testdispatchforonesignature ( self ) :
__label__0 input_tree = [ `` input_tree_0 '' , `` input_tree_1 '' ] shallow_tree = `` shallow_tree '' flattened_input_tree = nest.flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = nest.flatten_up_to ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 def update_docstrings_with_api_lists ( ) : `` '' '' updates the docstrings of dispatch decorators with api lists .
__label__0 class tfstacktest ( test.testcase ) :
__label__0 returns : a list of strings. `` '' '' return self._opt.get_slot_names ( * args , * * kwargs )
__label__0 @ property def do_not_descend_map ( self ) : `` '' '' a map from parents to symbols that should not be descended into .
__label__0 def testtoomanyargs ( self ) : with self.assertraisesregex ( typeerror , 'too many arguments were given . ' ' expected 9 but got 10 . ' ) : self._matmul_func.canonicalize ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 )
__label__0 return argspec ( args , varargs , keywords , tuple ( all_defaults [ first_default : ] ) )
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] func : a callable that accepts as many arguments as there are structures . * structure : - for modality.core : atom or nested structure . - for modality.data : scalar , or tuple or list of constructed scalars and/or other tuples/lists , or scalars . note : numpy arrays are considered scalars . * * kwargs : valid keyword args are : * ` check_types ` : - for modality.core : if set to ` true ` ( default ) the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` . note that namedtuples with identical name and fields are always considered to have the same shallow structure . - for modality.data : only valid keyword argument is ` check_types ` . if set to ` true ` ( default ) the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` . * ` expand_composites ` : valid for modality.core only . if set to ` true ` , then composite tensors such as ` tf.sparse.sparsetensor ` and ` tf.raggedtensor ` are expanded into their component tensors . if ` false ` ( the default ) , then composite tensors are not expanded .
__label__0 mg0_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) mg1_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) rms0_np = np.array ( [ 1.0 , 1.0 ] , dtype=dtype.as_numpy_dtype ) rms1_np = np.array ( [ 1.0 , 1.0 ] , dtype=dtype.as_numpy_dtype ) mom0_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype ) mom1_np = np.array ( [ 0.0 , 0.0 ] , dtype=dtype.as_numpy_dtype )
__label__0 args : node : current node `` '' '' if not node.module : self.generic_visit ( node ) return
__label__0 def _is_ast_str ( node ) : `` '' '' determine whether this node represents a string . '' '' '' allowed_types = [ ast.str ] if hasattr ( ast , `` bytes '' ) : allowed_types += [ ast.bytes ] if hasattr ( ast , `` joinedstr '' ) : allowed_types += [ ast.joinedstr ] if hasattr ( ast , `` formattedvalue '' ) : allowed_types += [ ast.formattedvalue ] return isinstance ( node , allowed_types )
__label__0 class svsummarythread ( coordinator.looperthread ) : `` '' '' a thread to save summaries on a timer . '' '' ''
__label__0 def _build ( self , checkpoint_path , build_save , build_restore ) : `` '' '' builds saver_def . '' '' '' if not context.executing_eagerly ( ) : if self._is_built : return self._is_built = true
__label__0 with ` expand_composites=true ` , we expect that the flattened input contains the tensors making up the ragged tensor i.e . the values and row_splits tensors .
__label__0 if an object is a ` compositetensor ` and overrides its ` _convert_variables_to_tensors ` method , its ` resourcevariable ` components will also be converted to ` tensor ` s . objects other than ` resourcevariable ` s in ` values ` will be returned unchanged .
__label__0 def test_contrib_summary_generic ( self ) : text = `` tf.contrib.summary.generic ( 'foo ' , myval , meta , 'fam ' , 42 ) '' expected = ( `` tf.compat.v2.summary.write ( tag='foo ' , data=myval , `` `` metadata=meta , step=42 ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) # arg errors come in alphabetical order of arguments , not appearance order . self.assertin ( `` 'family ' argument '' , errors [ 0 ] ) self.assertin ( `` 'name ' argument '' , errors [ 1 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 2 ] )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 # private function attributes used to store dispatchers on tensorflow apis . fallback_dispatch_attr = `` _tf_fallback_dispatchers '' type_based_dispatch_attr = `` _tf_type_based_dispatcher ''
__label__0 def main ( ) : parser = argparse.argumentparser ( description='extracts a list of symbols from a shared library ' ) parser.add_argument ( 'library ' , help='path to the .so file . ' ) args = parser.parse_args ( ) syms = implib.collect_syms ( args.library ) funs = [ s [ 'name ' ] for s in syms if _is_exported_function ( s ) ] for f in sorted ( funs ) : print ( f )
__label__0 metrics_comment = ( ast_edits.info , `` tf.metrics have been replaced with object oriented versions in '' `` tf 2.0 and after . the metric function calls have been converted to `` `` compat.v1 for backward compatibility . please update these calls to `` `` the tf 2.0 versions . '' )
__label__0 # not useful for builtin ` help ( ) ` . but these get passed to the # doc generator so that the description is still displayed on the site . _extra_docs : dict [ int , str ] = { }
__label__0 def update_m1_builds ( old_version , new_version ) : `` '' '' update m1 builds . '' '' '' replace_string_in_line ( `` release_branch = ' r % s. % s ' '' % ( old_version.major , old_version.minor ) , `` release_branch = ' r % s. % s ' '' % ( new_version.major , new_version.minor ) , tf_mac_arm64_ci_build , ) replace_string_in_line ( `` release_branch = ' r % s. % s ' '' % ( old_version.major , old_version.minor ) , `` release_branch = ' r % s. % s ' '' % ( new_version.major , new_version.minor ) , tf_mac_arm64_ci_test , )
__label__0 finally : # clean up . dispatch._global_dispatchers = original_global_dispatchers
__label__0 the started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the ` stop ( ) ` method .
__label__0 note : this script is only used in opensource .
__label__0 graph_saver = saver_module.saver ( [ w3 , w4 ] ) graph_saver.save ( none , eager_ckpt_prefix )
__label__0 def start ( self ) : `` '' '' starts this server .
__label__0 examples :
__label__0 this class is deprecated , and should be replaced with tf.summary.filewriter .
__label__0 * specifying ` `` ` requests an in-process session that does not use rpc .
__label__0 example :
__label__0 6 . ` tf.raggedtensor ` : this is a composite tensor thats representation consists of a flattened list of 'values ' and a list of 'row_splits ' which indicate how to chop up the flattened list into different rows . for more details on ` tf.raggedtensor ` , please visit https : //www.tensorflow.org/api_docs/python/tf/raggedtensor .
__label__0 @ classmethod def __tf_unflatten__ ( cls , metadata , components ) : `` '' '' create a user-defined object from ( metadata , components ) .
__label__0 different from ` dispatch_for_binary_elementwise_apis ` , this decorator is used for assert apis , such as assert_equal , assert_none_equal , etc , which return none in eager mode and an op in graph mode .
__label__0 expected_proto = `` '' '' job { name : 'ps ' tasks { key : 0 value : 'ps0:2222 ' } tasks { key : 1 value : 'ps1:2222 ' } } job { name : 'worker ' } `` '' ''
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for warm_starting_util . '' '' ''
__label__0 > > > tf.nest.assert_same_structure ( ... { `` a '' : 1 , `` b '' : 2 , `` c '' : 3 } , ... { `` c '' : 6 , `` b '' : 5 , `` a '' : 4 } )
__label__0 def _remove_annotation ( sig ) : `` '' '' removes annotation from a python signature . '' '' '' parameters = [ p.replace ( annotation=p.empty ) for p in sig.parameters.values ( ) ] return sig.replace ( parameters=parameters , return_annotation=sig.empty )
__label__0 parser = argparse.argumentparser ( description= '' cherry picking automation . '' )
__label__0 if the model is recovered from a checkpoint it is assumed that all global variables have been initialized , in particular neither ` init_op ` nor ` init_fn ` will be executed .
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' compatibility functions .
__label__0 prev_target = test_rewrappable_decorated._tf_decorator._decorated_target # in this case , only the outer decorator ( test_injectable_decorator_square ) # should be preserved . tf_decorator.rewrap ( test_rewrappable_decorated , prev_target , new_target ) self.assertequal ( ( 1 * 3 ) * * 2 , test_rewrappable_decorated ( 1 ) )
__label__0 def testclashingparameternames ( self ) :
__label__0 goodbye `` '' '' ) , ( 'first ' , [ ( 'code ' , none ) ] , `` '' '' `` ` python code `` `
__label__0 class _attributes ( namedtuple ) : names : str constants : str
__label__0 self.assertequal ( `` var/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float64 , slot.dtype.base_dtype ) self.assertallequal ( [ 0.0 , 0.0 ] , self.evaluate ( slot ) )
__label__0 x = customtensor ( [ 1 , 2 , 3 ] , 0.2 ) y = customtensor ( [ 7 , 8 , 2 ] , 0.4 ) z = customtensor ( [ 0 , 1 , 2 ] , 0.6 )
__label__0 # test case where len ( shallow_tree ) < len ( input_tree ) input_tree = { `` a '' : `` a '' , `` b '' : `` b '' , `` c '' : `` c '' } shallow_tree = { `` a '' : 1 , `` c '' : 2 }
__label__0 returns : version : version object of current semver string based on information from core/public/version.h `` '' ''
__label__0 def testfunctionlargenodes ( self ) : sizes = [ ] fn1 = [ 500 , 500 , 50 , 500 ] max_size = 200 constants.debug_set_max_size ( max_size )
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) save.save ( sess , save_path )
__label__0 def testreport ( self ) : text = `` tf.angle ( a ) \n '' _ , report , unused_errors , unused_new_text = self._upgrade ( text ) # this is not a complete test , but it is a sanity test that a report # is generating information . self.asserttrue ( report.find ( `` renamed function ` tf.angle ` to `` `` ` tf.math.angle ` `` ) )
__label__0 there are currently two dispatch systems for tensorflow :
__label__0 def getframeinfo ( * args , * * kwargs ) : return _inspect.getframeinfo ( * args , * * kwargs )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_kwonlyargs_and_args ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__1 class listnode : def __init__ ( self , val=0 , next=none ) : self.val = val self.next = next def mergeklists ( lists ) : if not lists : return none def mergetwolists ( l1 , l2 ) : dummy = listnode ( ) curr = dummy while l1 and l2 : if l1.val < l2.val : curr.next = l1 l1 = l1.next else : curr.next = l2 l2 = l2.next curr = curr.next curr.next = l1 if l1 else l2 return dummy.next while len ( lists ) > 1 : merged_lists = [ ] for i in range ( 0 , len ( lists ) , 2 ) : l1 = lists [ i ] l2 = lists [ i+1 ] if i+1 < len ( lists ) else none merged_lists.append ( mergetwolists ( l1 , l2 ) ) lists = merged_lists return lists [ 0 ] # test cases lists1 = [ [ 1,4,5 ] , [ 1,3,4 ] , [ 2,6 ] ] lists2 = [ [ ] ] lists3 = [ ] print ( mergeklists ( lists1 ) ) # output : [ 1,1,2,3,4,4,5,6 ] print ( mergeklists ( lists2 ) ) # output : [ ] print ( mergeklists ( lists3 ) ) # output : [ ]
__label__0 def _testtypes ( self , x , alpha , delta , use_gpu=none ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) apply_sgd = gen_training_ops.apply_gradient_descent ( var , alpha , delta ) out = self.evaluate ( apply_sgd ) self.assertshapeequal ( out , apply_sgd ) self.assertallcloseaccordingtotype ( x - alpha * delta , out )
__label__0 returns : consumed integer or float list based on input bytes and constraints. `` '' '' if self.get_bool ( ) : return self.get_int_list ( min_length , max_length ) else : return self.get_float_list ( min_length , max_length )
__label__0 @ property def init_feed_dict ( self ) : `` '' '' return the feed dictionary used when evaluating the ` init_op ` .
__label__0 def func ( positional , func=1 , func_and_positional=2 , kwargs=3 ) : return positional , func , func_and_positional , kwargs
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 self.assertequal ( ( ' a ' , ) , function_utils.fn_args ( double_wrapped_fn ) )
__label__0 there are two apis to create a ` tf.distribute.distributeddataset ` object : ` tf.distribute.strategy.experimental_distribute_dataset ( dataset ) ` and ` tf.distribute.strategy.distribute_datasets_from_function ( dataset_fn ) ` . * when to use which ? * when you have a ` tf.data.dataset ` instance , and the regular batch splitting ( i.e . re-batch the input ` tf.data.dataset ` instance with a new batch size that is equal to the global batch size divided by the number of replicas in sync ) and autosharding ( i.e . the ` tf.data.experimental.autoshardpolicy ` options ) work for you , use the former api . otherwise , if you are * not * using a canonical ` tf.data.dataset ` instance , or you would like to customize the batch splitting or sharding , you can wrap these logic in a ` dataset_fn ` and use the latter api . both api handles prefetch to device for the user . for more details and examples , follow the links to the apis .
__label__0 if not context.executing_eagerly ( ) : with self.assertraisesoperror ( `` uninitialized '' ) : self.evaluate ( v0 ) with self.assertraisesoperror ( `` uninitialized '' ) : self.evaluate ( v1 )
__label__0 @ dispatch.dispatch_for_types ( test_op , customtensor ) def override_for_test_op ( x , z , y ) : # pylint : disable=unused-variable return customtensor ( test_op ( x.tensor , y.tensor , z.tensor ) , ( x.score + y.score + z.score ) / 3.0 , )
__label__0 returns : a ` saverdef ` proto. `` '' '' return self.saver_def
__label__0 def __new__ ( cls , value ) : return str.__new__ ( cls , value )
__label__0 # dict+custom mapping . inp_val = dict ( a=2 , b=3 ) inp_ops = _custommapping ( a=dict ( add=1 , mul=2 ) , b=dict ( add=2 , mul=3 ) ) out = nest.map_structure_up_to ( inp_val , lambda val , ops : ( val + ops [ `` add '' ] ) * ops [ `` mul '' ] , inp_val , inp_ops ) self.assertequal ( out [ `` a '' ] , 6 ) self.assertequal ( out [ `` b '' ] , 15 )
__label__0 the input , ` input_tree ` , can be thought of as having the same structure layout as ` shallow_tree ` , but with leaf nodes that are themselves tree structures .
__label__1 class solution : def numbertowords ( self , num : int ) - > str : if num == 0 : return `` zero '' def towords ( n ) : below_20 = [ `` '' , `` one '' , `` two '' , `` three '' , `` four '' , `` five '' , `` six '' , `` seven '' , `` eight '' , `` nine '' , `` ten '' , `` eleven '' , `` twelve '' , `` thirteen '' , `` fourteen '' , `` fifteen '' , `` sixteen '' , `` seventeen '' , `` eighteen '' , `` nineteen '' ] tens = [ `` '' , `` '' , `` twenty '' , `` thirty '' , `` forty '' , `` fifty '' , `` sixty '' , `` seventy '' , `` eighty '' , `` ninety '' ] thousand_units = [ `` '' , `` thousand '' , `` million '' , `` billion '' ] if n == 0 : return `` '' elif n < 20 : return below_20 [ n ] + `` `` elif n < 100 : return tens [ n // 10 ] + `` `` + towords ( n % 10 ) else : return below_20 [ n // 100 ] + `` hundred `` + towords ( n % 100 ) result = `` '' for i in range ( len ( thousand_units ) ) : if num % 1000 ! = 0 : result = towords ( num % 1000 ) + thousand_units [ i ] + `` `` + result num //= 1000 return result.strip ( )
__label__0 def new_target ( x ) : return x * 3
__label__0 example :
__label__0 returns : a python list , the partially flattened version of ` input_tree ` according to the structure of ` shallow_tree ` .
__label__0 @ deprecation.deprecated ( none , `` the ` syncreplicaoptimizer ` class is deprecated . for synchronous `` `` training , please use [ distribution strategies ] ( https : //github.com/ '' `` tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute ) . `` , warn_once=true ) def __init__ ( self , opt , replicas_to_aggregate , total_num_replicas=none , variable_averages=none , variables_to_average=none , use_locking=false , name= '' sync_replicas '' ) : `` '' '' construct a sync_replicas optimizer .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_deprecated_once ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def _softmax_cross_entropy_with_logits_transformer ( parent , node , full_name , name , logs ) : `` '' '' wrap labels argument with stop_gradients . '' '' '' def _wrap_label ( parent , old_value ) : `` '' '' wrap labels with tf.stop_gradient . '' '' '' already_stop_grad = ( isinstance ( old_value , ast.call ) and isinstance ( old_value.func , ast.attribute ) and old_value.func.attr == `` stop_gradient '' and isinstance ( old_value.func.value , ast.name ) and old_value.func.value.id == `` tf '' ) if already_stop_grad : return false try : new_value = ast.call ( ast.name ( id= '' tf.stop_gradient '' , ctx=ast.load ( ) ) , [ old_value ] , [ ] ) except typeerror : new_value = ast.call ( ast.name ( id= '' tf.stop_gradient '' , ctx=ast.load ( ) ) , [ old_value ] , [ ] , none , none )
__label__0 if __name__ == `` __main__ '' : test_lib.main ( ) def testtensorflowdontchangecontrib ( self ) : text = `` import tensorflow.contrib as foo '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( text , new_text )
__label__0 can be extended to create different ops. `` '' ''
__label__0 var0 = variables.variable ( var0_np ) var1 = variables.variable ( var1_np ) grads0_np_indices = np.array ( [ 0 ] , dtype=np.int32 ) grads0 = indexed_slices.indexedslices ( constant_op.constant ( grads0_np ) , constant_op.constant ( grads0_np_indices ) , constant_op.constant ( [ 1 ] ) ) grads1_np_indices = np.array ( [ 1 ] , dtype=np.int32 ) grads1 = indexed_slices.indexedslices ( constant_op.constant ( grads1_np ) , constant_op.constant ( grads1_np_indices ) , constant_op.constant ( [ 1 ] ) ) opt = rmsprop.rmspropoptimizer ( learning_rate=learning_rate , decay=decay , momentum=momentum , epsilon=epsilon , centered=centered ) update = opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 header_path , header_version = _find_header ( base_paths , `` cusolver_common.h '' , required_version , get_header_version ) cusolver_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 def as_str ( bytes_or_text , encoding='utf-8 ' ) : return as_text ( bytes_or_text , encoding )
__label__0 # since tensorflow python code now resides in tensorflow_core but tensorflow # ecosystem code imports tensorflow , we need to do forwarding between the two . # to do so , we use a lazy loader to load and forward the top level modules . we # can not use the lazyloader defined by tensorflow at # tensorflow/python/util/lazy_loader.py as to use that we would already need to # import tensorflow . hence , we define it inline . class _lazyloader ( _types.moduletype ) : `` '' '' lazily import a module so that we can forward it . '' '' ''
__label__0 # both -- module and -- module_prefix_skip are relative to package . packages = [ 'tensorflow.python . ' , 'tensorflow.lite.python . ' , ]
__label__0 # restore the saved values in the parameter nodes . save.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( np.int64 ( 15 ) , self.evaluate ( v ) )
__label__0 raises : valueerror : if save_path is none or not a valid checkpoint. `` '' '' start_time = time.time ( ) if self._is_empty : return if save_path is none : raise valueerror ( `` ca n't load save_path when it is none . '' )
__label__0 lines [ 1:1 ] = notice else : lines += notice
__label__0 def func ( m , * arg ) : return m + len ( arg )
__label__0 def testsetstfdecoratorargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs='args ' , kwonlyargs= { } , defaults= ( 1 , 'hello ' ) , kwonlydefaults=none , varkw='kwargs ' , annotations=none ) decorated = tf_decorator.make_decorator ( test_function , test_wrapper , `` , `` , argspec ) decorator = getattr ( decorated , '_tf_decorator ' ) self.assertequal ( argspec , decorator.decorator_argspec ) self.assertequal ( inspect.signature ( decorated ) , inspect.signature ( [ inspect.parameter ( ' a ' , inspect.parameter.positional_or_keyword ) , inspect.parameter ( ' b ' , inspect.parameter.positional_or_keyword , default=1 ) , inspect.parameter ( ' c ' , inspect.parameter.positional_or_keyword , default='hello ' , ) , inspect.parameter ( 'args ' , inspect.parameter.var_positional ) , inspect.parameter ( 'kwargs ' , inspect.parameter.var_keyword ) , ] ) , )
__label__0 when called , the default graph is the one that will be launched in the session . the hook can modify the graph by adding new operations to it . after the ` begin ( ) ` call the graph will be finalized and the other callbacks can not modify the graph anymore . second call of ` begin ( ) ` on the same graph , should not change the graph. `` '' '' pass
__label__0 args : op : python function : the tensorflow operation that should be handled . must have a dispatch list ( which is added automatically for generated ops , and can be added to python ops using the ` add_dispatch_support ` decorator ) . `` '' '' if not hasattr ( op , fallback_dispatch_attr ) : raise assertionerror ( `` dispatching not enabled for % s '' % op ) getattr ( op , fallback_dispatch_attr ) .append ( self )
__label__0 def testsegmentmaxnumsegmentsless ( self ) : for dtype in self.int_types | self.float_types : minval = dtypes.as_dtype ( dtype ) .min if dtype == np.float64 and self._finddevice ( `` tpu '' ) : minval = -np.inf self.assertallclose ( np.array ( [ 1 , minval , 2 ] , dtype=dtype ) , self._segmentmaxv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 3 ) )
__label__0 def decorator ( dispatch_target ) :
__label__0 new_file_b = os.path.join ( output_dir , `` b.py '' ) self.asserttrue ( os.path.islink ( new_file_b ) ) self.assertequal ( file_a , os.readlink ( new_file_b ) ) with open ( file_a , `` r '' ) as f : self.assertequal ( `` import foo as f '' , f.read ( ) )
__label__0 returns : a decorator. `` '' ''
__label__0 if fullargspec.varargs is not none : parameters.append ( inspect.parameter ( fullargspec.varargs , inspect.parameter.var_positional ) )
__label__0 def testcompatv1apiinstrumenting ( self ) : self.assertfalse ( module_wrapper.tfmodulewrapper.compat_v1_usage_recorded ) apis = { 'cosh ' : ( `` , 'cmd ' ) }
__label__0 if not self.patched : # the default doctest compiles in `` single '' mode . the fenced block may # contain multiple statements . the ` _patch_compile ` function fixes the # compile mode . doctest.compile = _patch_compile print ( textwrap.dedent ( `` '' '' * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * caution : ` fenced_doctest ` patches ` doctest.compile ` do n't use this * in the same binary as any other doctests . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * `` '' '' ) ) type ( self ) .patched = true
__label__0 lowercase_dir_contents = lowercase_directories + lowercase_files if len ( lowercase_dir_contents ) ! = len ( set ( lowercase_dir_contents ) ) : raise assertionerror ( error_message.format ( dirpath ) )
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) apply_ftrl = gen_training_ops.apply_ftrl ( var , accum , linear , grad , lr , l1 , l2 , lr_power ) out = self.evaluate ( apply_ftrl ) self.assertshapeequal ( out , apply_ftrl ) accum_update = y + grad * grad linear_update = z + grad - ( accum_update * * ( -lr_power ) - y * * ( -lr_power ) ) / lr * x quadratic = 1.0 / ( accum_update * * ( lr_power ) * lr ) + 2 * l2 expected_out = np.array ( [ ( np.sign ( linear_update [ i ] ) * l1 - linear_update [ i ] ) / ( quadratic [ i ] ) if np.abs ( linear_update [ i ] ) > l1 else 0.0 for i in range ( linear_update.size ) ] ) self.assertallcloseaccordingtotype ( accum_update , self.evaluate ( accum ) ) if x.dtype == np.float16 : # the calculations here really are not very precise in float16 . self.assertallclose ( linear_update , self.evaluate ( linear ) , rtol=2e-2 , atol=2e-2 ) self.assertallclose ( expected_out , out , rtol=2e-2 , atol=2e-2 ) elif x.dtype == np.float32 : # the calculations here not sufficiently precise in float32 . self.assertallclose ( linear_update , self.evaluate ( linear ) , rtol=1e-5 , atol=1e-5 ) self.assertallclose ( expected_out , out , rtol=1e-5 , atol=1e-5 ) else : self.assertallclose ( linear_update , self.evaluate ( linear ) ) self.assertallclose ( expected_out , out )
__label__0 def __model_class1_method__ ( self ) : pass
__label__0 raises : runtimeerror : if any dependencies for py_tests exist in subset
__label__0 def testgetfullargspecignoresdecoratorsthatdontprovidefullargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations= { } )
__label__0 # check the chunkedmessage proto . self.assertlen ( chunked_message.chunked_fields , 4 ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 1 ] ) , chunked_message.chunked_fields [ 0 ] .field_tag , ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 2 ] ) , chunked_message.chunked_fields [ 1 ] .field_tag , ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 3 ] ) , chunked_message.chunked_fields [ 2 ] .field_tag , ) self._assert_field_tags ( util.get_field_tag ( graph_def , [ `` node '' , 5 ] ) , chunked_message.chunked_fields [ 3 ] .field_tag , )
__label__0 def testinitcapturestarget ( self ) : self.assertis ( test_function , tf_decorator.tfdecorator ( `` , test_function ) .decorated_target )
__label__0 # create a noop that has control dependencies from all the updates . return control_flow_ops.group ( * assign_ops , name=name )
__label__0 returns : decorated function or method. `` '' '' if sys.version_info.major ! = 3 or sys.version_info.minor < 7 : return fn
__label__0 class sentinelfilter ( stacktracefilter ) :
__label__0 @ property def _type_spec ( self ) : pass
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( 1 , 2 , true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 class _dotstring ( object ) : __slots__ = [ ]
__label__0 def isbuiltin ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isbuiltin . '' '' '' return _inspect.isbuiltin ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 # save checkpoint from which to warm-start . also create a bias variable , # so we can check that it 's also warm-started . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : sc_int_weights = variable_scope.get_variable ( `` linear_model/sc_int/weights '' , shape= [ 10 , 1 ] , initializer=ones ( ) ) sc_hash_weights = variable_scope.get_variable ( `` linear_model/sc_hash/weights '' , shape= [ 15 , 1 ] , initializer=norms ( ) ) sc_keys_weights = variable_scope.get_variable ( `` linear_model/sc_keys/weights '' , shape= [ 4 , 1 ] , initializer=rand ( ) ) sc_vocab_weights = variable_scope.get_variable ( `` linear_model/sc_vocab/weights '' , shape= [ 4 , 1 ] , initializer=ones ( ) ) real_bucket_weights = variable_scope.get_variable ( `` linear_model/real_bucketized/weights '' , shape= [ 5 , 1 ] , initializer=norms ( ) ) cross_weights = variable_scope.get_variable ( `` linear_model/sc_keys_x_sc_vocab/weights '' , shape= [ 20 , 1 ] , initializer=rand ( ) ) bias = variable_scope.get_variable ( `` linear_model/bias_weights '' , shape= [ 1 ] , initializer=rand ( ) ) self._write_checkpoint ( sess ) ( prev_int_val , prev_hash_val , prev_keys_val , prev_vocab_val , prev_bucket_val , prev_cross_val , prev_bias_val ) = sess.run ( [ sc_int_weights , sc_hash_weights , sc_keys_weights , sc_vocab_weights , real_bucket_weights , cross_weights , bias ] )
__label__0 args : primary : the primary ` variable ` or ` tensor ` . initializer : an ` initializer ` . the initial value of the slot . shape : shape of the initial value of the slot . dtype : type of the value of the slot . name : name to use for the slot variable . colocate_with_primary : boolean . if true the slot is located on the same device as ` primary ` . copy_xla_sharding : boolean . if true also copies xla sharding from primary .
__label__0 def testrewrapmutatesaffectedfunction ( self ) :
__label__0 def pack_sequence_as ( modality , structure , flat_sequence , expand_composites , sequence_fn=none ) : `` '' '' returns a given flattened sequence packed into a given structure .
__label__0 returns : the wrapped function. `` '' '' def decorated ( fn ) : `` '' '' decorates the input function . '' '' '' def wrapped ( * args , * * kwargs ) : return _add_should_use_warning ( fn ( * args , * * kwargs ) , warn_in_eager=warn_in_eager , error_in_function=error_in_function ) fn_doc = fn.__doc__ or `` split_doc = fn_doc.split ( '\n ' , 1 ) if len ( split_doc ) == 1 : updated_doc = fn_doc else : brief , rest = split_doc updated_doc = '\n'.join ( [ brief , textwrap.dedent ( rest ) ] )
__label__0 returns : a structured output value based on the inputs. `` '' ''
__label__0 import enum import re import sys
__label__0 from collections.abc import sequence from typing import any , optional , union
__label__0 from tensorflow.python.util import decorator_utils
__label__0 def find_sycl_config ( ) : `` '' '' returns a dictionary of sycl components config info . '' '' '' basekit_path = _get_basekit_path ( ) toolkit_path = _get_toolkit_path ( ) if not os.path.exists ( basekit_path ) : raise configerror ( 'specified sycl_toolkit_path `` { } '' does not exist'.format ( basekit_path ) )
__label__0 # verifies behavior of multiple variables with multiple sessions connecting to # the same server . # todo ( b/34465411 ) : starting multiple servers with different configurations # in the same test is flaky . move this test case back into # `` server_lib_test.py '' when this is no longer the case . @ test_util.run_v1_only ( `` this exercises tensor lookup via names which is not supported in v2 . '' ) def testsamevariablesnoclear ( self ) : server = server_lib.server.create_local_server ( )
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 import argparse import datetime import fcntl import json import os import shutil
__label__0 class maskedtensor2 ( maskedtensor ) : pass
__label__0 it also edits the docstring of the function : ' ( deprecated ) ' is appended to the first line of the docstring and a deprecation notice is prepended to the rest of the docstring .
__label__0 save_path_parent = os.path.dirname ( save_path ) if not self._is_empty : try : if context.executing_eagerly ( ) : self._build_eager ( checkpoint_file , build_save=true , build_restore=false ) model_checkpoint_path = self.saver_def.save_tensor_name else : model_checkpoint_path = sess.run ( self.saver_def.save_tensor_name , { self.saver_def.filename_tensor_name : checkpoint_file } )
__label__1 def __init__ ( self ) : self.intervals = [ ] def addnum ( self , val : int ) - > none : # convert value to an interval interval = [ val , val ] # merge the new interval with existing ones merged = [ ] for start , end in self.intervals : if interval [ 1 ] + 1 < start : # if interval ends before current interval starts merged.append ( interval ) interval = [ start , end ] elif interval [ 0 ] - 1 > end : # if interval starts after current interval ends merged.append ( [ start , end ] ) else : # if there 's overlap , merge intervals interval [ 0 ] = min ( interval [ 0 ] , start ) interval [ 1 ] = max ( interval [ 1 ] , end ) merged.append ( interval ) self.intervals = merged def getintervals ( self ) - > list [ list [ int ] ] : return self.intervals
__label__0 def _finddevice ( self , device_name ) : devices = device_lib.list_local_devices ( ) for d in devices : if d.device_type == device_name : return true return false
__label__0 # verifies no containers are reset with non-existent container . server = self._cached_server sess = session.session ( server.target ) sess.run ( variables.global_variables_initializer ( ) ) self.assertallequal ( 1.0 , sess.run ( v0 ) ) self.assertallequal ( 2.0 , sess.run ( v1 ) ) # no container is reset , but the server is reset . session.session.reset ( server.target , [ `` test1 '' ] ) # verifies that both variables are still valid . sess = session.session ( server.target ) self.assertallequal ( 1.0 , sess.run ( v0 ) ) self.assertallequal ( 2.0 , sess.run ( v1 ) )
__label__0 class _object ( object ) :
__label__0 # move manifest and third_party_notices to the root shutil.move ( os.path.join ( srcs_dir , `` tensorflow/tools/pip_package/manifest.in '' ) , os.path.join ( srcs_dir , `` manifest.in '' ) , ) shutil.move ( os.path.join ( srcs_dir , `` tensorflow/tools/pip_package/third_party_notices.txt '' ) , os.path.join ( srcs_dir , `` tensorflow/third_party_notices.txt '' ) , )
__label__0 def testdeferredbuild ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` deferred_build '' ) with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : one = variable_v1.variablev1 ( 1.0 ) save = saver_module.saver ( defer_build=true ) # if build is not deferred , saver can not save the ` twos ` . twos = variable_v1.variablev1 ( [ 2.0 , 2.0 , 2.0 ] ) init = variables.global_variables_initializer ( ) save.build ( ) init.run ( ) save.save ( sess , save_path )
__label__0 > > > @ add_dispatch_support ( iterable_parameters= [ 'ys ' ] ) ... def add_tensor_to_list_of_tensors ( x , ys ) : ... return [ x + y for y in ys ] > > > @ dispatch_for_api ( add_tensor_to_list_of_tensors , ... { 'ys ' : typing.list [ maskedtensor ] } ) ... def masked_add_tensor_to_list_of_tensors ( x , ys ) : ... return [ maskedtensor ( x+y.values , y.mask ) for y in ys ]
__label__0 the transformation calls ` reduce_func ` successively on each element . the ` initial_state ` argument is used for the initial state and the final state is returned as the result .
__label__0 # the next one should be a stop message if we closed cleanly . ev = next ( rr ) self.assertequal ( event_pb2.sessionlog.stop , ev.session_log.status )
__label__0 args : shallow_tree : nested structure . traverse no further than its leaf nodes . input_tree : nested structure . return the paths and values from this tree . must have the same upper structure as shallow_tree . is_nested_fn : function used to test if a value should be treated as a nested structure . path : tuple . optional argument , only used when recursing . the path from the root of the original shallow_tree , down to the root of the shallow_tree arg of this recursive call .
__label__0 import os
__label__0 the new api is
__label__0 def func ( m , * arg ) : return m + len ( arg )
__label__0 def testparseerror ( self ) : _ , report , unused_errors , unused_new_text = self._upgrade ( `` import tensorflow as tf\na + \n '' ) self.assertnotequal ( report.find ( `` failed to parse '' ) , -1 )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 class deprecationtest ( test.testcase ) :
__label__0 import importlib import os import types from tensorflow.python.platform import tf_logging as logging
__label__0 > > > structure = { `` foo '' : tf.ragged.constant ( [ [ 1. , 2 . ] , [ 3 . ] ] ) , ... `` bar '' : tf.constant ( [ [ 5 . ] ] ) } > > > tensors = tf.nest.flatten ( structure , expand_composites=true ) > > > print ( tensors ) [ < tf.tensor : shape= ( 1 , 1 ) , dtype=float32 , numpy=array ( [ [ 5 . ] ] , dtype=float32 ) > , < tf.tensor : shape= ( 3 , ) , dtype=float32 , numpy=array ( [ 1. , 2. , 3 . ] , dtype=float32 ) > , < tf.tensor : shape= ( 3 , ) , dtype=int64 , numpy=array ( [ 0 , 2 , 3 ] ) > ] > > > verified_tensors = [ tf.debugging.check_numerics ( t , 'invalid tensor : ' ) ... if t.dtype==tf.float32 else t ... for t in tensors ] > > > tf.nest.pack_sequence_as ( structure , verified_tensors , ... expand_composites=true ) { 'foo ' : < tf.raggedtensor [ [ 1.0 , 2.0 ] , [ 3.0 ] ] > , 'bar ' : < tf.tensor : shape= ( 1 , 1 ) , dtype=float32 , numpy=array ( [ [ 5 . ] ] , dtype=float32 ) > }
__label__0 def testisbuiltin ( self ) : self.assertequal ( tf_inspect.isbuiltin ( testdecoratedclass ) , inspect.isbuiltin ( testdecoratedclass ) ) self.assertequal ( tf_inspect.isbuiltin ( test_decorated_function ) , inspect.isbuiltin ( test_decorated_function ) ) self.assertequal ( tf_inspect.isbuiltin ( test_undecorated_function ) , inspect.isbuiltin ( test_undecorated_function ) ) self.assertequal ( tf_inspect.isbuiltin ( range ) , inspect.isbuiltin ( range ) ) self.assertequal ( tf_inspect.isbuiltin ( max ) , inspect.isbuiltin ( max ) )
__label__0 def set_root_name ( self , root_name ) : `` '' '' override the default root name of 'tf ' . '' '' '' self._root_name = root_name
__label__1 def find_median ( lst ) : sorted_lst = sorted ( lst ) n = len ( sorted_lst ) if n % 2 == 0 : return ( sorted_lst [ n // 2 - 1 ] + sorted_lst [ n // 2 ] ) / 2 else : return sorted_lst [ n // 2 ]
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect
__label__0 if filtering_enabled : self.assertless ( trace_line_count , count ) else : self.assertgreater ( trace_line_count , count )
__label__0 examples : `` ` python $ tf.compat.path_to_str ( ' c : \xyz\tensorflow\./ .. /./tensorflow ' ) ' c : \xyz\tensorflow\./ .. /./tensorflow ' # windows os $ tf.compat.path_to_str ( path ( ' c : \xyz\tensorflow\./ .. /./tensorflow ' ) ) ' c : \xyz\tensorflow\ .. \tensorflow ' # windows os $ tf.compat.path_to_str ( path ( './corpus ' ) ) 'corpus ' # linux os $ tf.compat.path_to_str ( './ .. /./corpus ' ) './ .. /./corpus ' # linux os $ tf.compat.path_to_str ( path ( './ .. /./corpus ' ) ) ' .. /corpus ' # linux os $ tf.compat.path_to_str ( path ( './ .. //// .. / ' ) ) ' .. / .. ' # linux os
__label__0 @ parameterized.parameters ( # rename parameter : delimiter - > sep and add .to_sparse ( ) [ `` tf.string_split ( 'test ' , delimiter= ' ' ) '' , `` tf.strings.split ( input='test ' , sep= ' ' ) .to_sparse ( ) '' ] , # rename parameter : source - > input [ `` tf.strings.split ( source='test1 ' ) '' , `` tf.strings.split ( input='test1 ' ) .to_sparse ( ) '' ] , # use compat.v1 for skip_empty parameter . [ `` tf.string_split ( 'test ' , ' ' , true ) '' , `` tf.compat.v1.string_split ( source='test ' , sep= ' ' , skip_empty=true ) '' ] , [ `` tf.string_split ( 'test ' , ' ' , skip_empty=false ) '' , `` tf.strings.split ( input='test ' , sep= ' ' ) .to_sparse ( ) '' ] , # split behavior for sep=none changed . ( in particular , it now splits on # all whitespace , not just the space character ) [ `` tf.string_split ( x ) '' , `` tf.compat.v1.string_split ( source=x ) '' ] , # split behavior for sep= '' changed : [ `` tf.string_split ( x , `` ) '' , `` tf.strings.bytes_split ( input=x ) .to_sparse ( ) '' ] , [ `` tf.string_split ( x , sep= '' ) '' , `` tf.strings.bytes_split ( input=x ) .to_sparse ( ) '' ] , [ `` tf.string_split ( x , delimiter= '' ) '' , `` tf.strings.bytes_split ( input=x ) .to_sparse ( ) '' ] , [ `` tf.string_split ( x , `` , result_type='raggedtensor ' ) '' , `` tf.strings.bytes_split ( input=x ) '' ] , # if sep is a variable , we ca n't tell if it 's empty : [ `` tf.string_split ( x , sep ) '' , `` tf.compat.v1.string_split ( source=x , sep=sep ) '' ] , # if sep is a non-empty string literal , then we do n't need compat.v1 . [ `` tf.string_split ( x , 'non-empty-sep ' ) '' , `` tf.strings.split ( input=x , sep='non-empty-sep ' ) .to_sparse ( ) '' ] , # add to_sparse unless result_type is raggedtensor : [ `` tf.string_split ( x , ' ' ) '' , `` tf.strings.split ( input=x , sep= ' ' ) .to_sparse ( ) '' ] , [ `` tf.string_split ( x , ' ' , result_type='sparsetensor ' ) '' , `` tf.strings.split ( input=x , sep= ' ' ) .to_sparse ( ) '' ] , [ `` tf.string_split ( x , ' ' , result_type='raggedtensor ' ) '' , `` tf.strings.split ( input=x , sep= ' ' ) '' ] , [ `` tf.string_split ( x , ' ' , result_type=x ) '' , `` tf.compat.v1.string_split ( source=x , sep= ' ' , result_type=x ) '' ] , ) # pyformat : disable # todo ( b/129398290 ) def disabled_test_string_split ( self , text , expected_text ) : `` '' '' tests for transforming from tf.string_split . '' '' '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 def _save ( partitioner=none ) : # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.session ( ) as sess : # calls .eval ( ) to return the ndarray that makes up the full variable . rnd = random_ops.random_uniform ( var_full_shape ) .eval ( )
__label__0 copied from pep-257 : https : //www.python.org/dev/peps/pep-0257/ # handling-docstring-indentation
__label__0 def _get_basekit_path ( ) : return _get_toolkit_path ( ) .split ( `` /compiler/ '' ) [ 0 ]
__label__0 _keras_call_context_function = none _keras_clear_session_function = none _keras_get_session_function = none _keras_load_model_function = none
__label__0 dict_mt_nmt = { `` mt '' : mt , `` nmt '' : nmt , `` mt_nmt_list '' : [ mt , nmt ] } dict_mt_nmt_flat_paths = list ( nest.yield_flat_paths ( dict_mt_nmt ) ) self.assertequal ( dict_mt_nmt_flat_paths , [ ( `` mt '' , 0 ) , ( `` mt_nmt_list '' , 0 , 0 ) , ( `` mt_nmt_list '' , 1 , 0 , 0 ) , ( `` nmt '' , 0 , 0 ) , ] , )
__label__0 class myclass ( object ) :
__label__0 def secs_remaining ( self ) : diff = self._duration_secs - ( time.time ( ) - self._start_time_secs ) return max ( 0 , diff )
__label__0 # assert that the variables are not initialized . if not context.executing_eagerly ( ) : self.assertequal ( len ( variables.report_uninitialized_variables ( ) .eval ( ) ) , 2 ) self.assertequal ( 0 , len ( self.evaluate ( v2.keys ( ) ) ) ) self.assertequal ( 0 , len ( self.evaluate ( v2.values ( ) ) ) ) # restore the saved values in the parameter nodes . save = saver_module.saver ( { `` v0 '' : v0 , `` v1 '' : v1 , `` v2 '' : v2.saveable } ) save.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 30.0 , self.evaluate ( v2.values ( ) ) )
__label__0 @ test_util.run_in_graph_and_eager_modes def testmorecomplexsaveablereturned ( self ) : v = _ownsmirroredvariables ( ) test_dir = self.get_temp_dir ( ) prefix = os.path.join ( test_dir , `` ckpt '' ) self.evaluate ( v.non_dep_variable.assign ( 42 . ) ) for saver in ( saver_module.saver ( var_list= [ v ] ) , saver_module.saver ( var_list= { `` v '' : v } ) ) : with self.cached_session ( ) as sess : save_path = saver.save ( sess , prefix ) self.evaluate ( v.non_dep_variable.assign ( 43 . ) ) self.evaluate ( v.mirrored.assign ( 44 . ) ) saver.restore ( sess , save_path ) self.assertequal ( 42. , self.evaluate ( v.non_dep_variable ) ) self.assertequal ( 42. , self.evaluate ( v.mirrored ) )
__label__0 note that you still have to call the ` save ( ) ` method to save the model . passing these arguments to the constructor will not save variables automatically for you .
__label__0 raises : valueerror : if something is not good. `` '' '' # not running as chief means that replicas are used . # in that case all variables must have their device set . if not self._is_chief : for op in self._graph.get_operations ( ) : if op.type in [ `` variable '' , `` variablev2 '' ] and not op.device : raise valueerror ( `` when using replicas , all variables must have `` `` their device set : % s '' % op )
__label__0 def getmembers ( object , predicate=none ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.getmembers . '' '' '' return _inspect.getmembers ( object , predicate )
__label__0 returns : true if the input is a nested structure or a composite. `` '' '' return _is_nested_or_composite ( seq )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassflattenuptoincompatible ( self ) : simple_list = [ 2 ] mt = maskedtensor ( mask=true , value=constant_op.constant ( [ 1 ] ) )
__label__0 partitioner = lambda shape , dtype : [ 1 ] * len ( shape ) # new graph , new session without warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = self._create_linear_model ( [ sc_vocab ] , partitioner ) self.evaluate ( variables.global_variables_initializer ( ) ) # without warm-starting , the weights should be initialized using default # initializer ( which is init_ops.zeros_initializer ) . self._assert_cols_to_vars ( cols_to_vars , { sc_vocab : [ np.zeros ( [ 4 , 1 ] ) ] } , sess )
__label__0 flatten_with_tuple_paths_up_to ( [ 0 , 1 , 2 ] , 0 ) # output : typeerror
__label__0 otherwise , ` compat.v1 ` is inserted between tf and the function name .
__label__0 tf.funcion traces with the placeholder value rather than the actual value . for example , a placeholder value can represent multiple different actual values . this means that the trace generated with that placeholder value is more general and reusable which saves expensive retracing .
__label__0 class foo ( object ) :
__label__0 _ = tf.raw_ops.abs ( x=input_tensor )
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = constant_op.constant ( [ 10 , 20 , 30 , 40 , 50 ] ) z = math_ops.add ( x , y ) self.assertallequal ( z.values , x.values + y ) self.assertallequal ( z.mask , x.mask )
__label__0 with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 1 , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertfalse ( initialized ) sess.run ( v.initializer ) self.assertequal ( 1 , sess.run ( v ) ) saver.save ( sess , os.path.join ( checkpoint_dir , `` recover_session_checkpoint '' ) ) # create a new graph and sessionmanager and recover . with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( 2 , name= '' v '' ) w = variable_v1.variablev1 ( v , trainable=false , collections= [ ops.graphkeys.local_variables ] , name= '' w '' ) with self.cached_session ( ) : self.assertequal ( false , variable_v1.is_variable_initialized ( v ) .eval ( ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( w ) .eval ( ) ) sm2 = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) , ready_for_local_init_op=variables.report_uninitialized_variables ( ) , local_init_op=w.initializer ) saver = saver_lib.saver ( { `` v '' : v } ) sess , initialized = sm2.recover_session ( `` '' , saver=saver , checkpoint_dir=checkpoint_dir ) self.assertfalse ( initialized ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( false , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) )
__label__0 in the case of dict instances , the sequence consists of the values , sorted by key to ensure deterministic behavior . this is true also for ordereddict instances : their sequence order is ignored , the sorting order of keys is used instead . the same convention is followed in ` nest.pack_sequence_as ` . this correctly repacks dicts and ordereddicts after they have been flattened , and also allows flattening an ordereddict and then repacking it back using a corresponding plain dict , or vice-versa . dictionaries with non-sortable keys can not be flattened .
__label__0 @ property def saveable ( self ) : if context.executing_eagerly ( ) : return checkpointedop.customsaveable ( self , self.name ) else : return self._saveable
__label__0 def _rename_if_arg_found_and_add_loss_reduction_transformer ( parent , node , full_name , name , logs , arg_names=none , arg_ok_predicate=none , remove_if_ok=false , message=none ) : `` '' '' combination of _rename_if_arg_found and _add_loss_reduction transformers .
__label__0 @ dispatch.dispatch_for_api ( foo , { `` y '' : maskedtensor } ) def masked_foo ( x , * , y ) : # pylint : disable=unused-variable del x , y
__label__0 with session.session ( server.target , config=self._userpcconfig ( ) ) as sess : with self.assertraises ( errors_impl.deadlineexceedederror ) : sess.run ( blocking_t , options=config_pb2.runoptions ( timeout_in_ms=1000 ) )
__label__0 # put the whole batch of entities in the datastore . client.put_multi ( batch )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' generates a python module containing information about the build . '' '' '' import argparse
__label__0 > > > tensor = tf.constant ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] ) > > > tf.nest.flatten ( tensor ) [ < tf.tensor : shape= ( 3 , 3 ) , dtype=float32 , numpy= array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] , dtype=float32 ) > ]
__label__0 # check positional args if arg_pos is not none : idx = 0 for arg in node.args : if sys.version_info [ :2 ] > = ( 3 , 5 ) and isinstance ( arg , ast.starred ) : continue # ca n't parse starred if idx == arg_pos : return ( true , arg ) idx += 1
__label__0 self.assertlen ( chunks , 2 ) self.assertisinstance ( chunks [ 0 ] , graph_pb2.graphdef ) self.assertisinstance ( chunks [ 1 ] , function_pb2.functiondef ) self._assert_chunk_sizes ( chunks , max_size )
__label__0 def create_slot_with_initializer ( primary , initializer , shape , dtype , name , colocate_with_primary=true , * , copy_xla_sharding=false ) : `` '' '' creates a slot initialized using an ` initializer ` .
__label__0 def __del__ ( self ) : self._check_sated ( raise_error=false )
__label__0 # create token queue . with ops.device ( global_step.device ) , ops.name_scope ( `` '' ) : sync_token_queue = ( data_flow_ops.fifoqueue ( -1 , global_step.dtype.base_dtype , shapes= ( ) , name= '' sync_token_q '' , shared_name= '' sync_token_q '' ) ) self._sync_token_queue = sync_token_queue
__label__0 trace = func ( 5 ) self.assertin ( 'func ' , repr ( trace [ -1 ] ) )
__label__0 # partial function may give default value to any argument , therefore length # of default value list must be len ( args ) to allow each argument to # potentially be given a default value . no_default = object ( ) all_defaults = [ no_default ] * len ( args )
__label__0 def __delete__ ( self , obj ) : raise attributeerror ( 'property % s is read-only ' % self._func.__name__ )
__label__0 major , minor , patch = rocblas_version_numbers ( rocm_install_path )
__label__0 def call_with_captures ( self , args , kwargs , captures ) : `` '' '' calls this atomicfunction with captures as defined by its functiontype .
__label__0 # since tensorflow python code now resides in tensorflow_core but tensorflow # ecosystem code imports tensorflow , we need to do forwarding between the two . # to do so , we use a lazy loader to load and forward the top level modules . we # can not use the lazyloader defined by tensorflow at # tensorflow/python/util/lazy_loader.py as to use that we would already need to # import tensorflow . hence , we define it inline . class _lazyloader ( _types.moduletype ) : `` '' '' lazily import a module so that we can forward it . '' '' ''
__label__0 def add_fallback_dispatch_list ( target ) : `` '' '' decorator that adds a dispatch_list attribute to an op . '' '' '' if hasattr ( target , fallback_dispatch_attr ) : raise assertionerror ( `` % s already has a dispatch list '' % target ) setattr ( target , fallback_dispatch_attr , [ ] ) return target
__label__0 raises : valueerror : if the two structures do not have the same number of atoms or if the two structures are not nested in the same way . typeerror : if the two structures differ in the type of sequence in any of their substructures . only possible if ` check_types ` is ` true ` . `` '' '' nest_util.assert_same_structure ( nest_util.modality.core , nest1 , nest2 , check_types , expand_composites )
__label__0 decorated_function = get_wrapper ( self._test_function )
__label__0 @ atheris.instrument_func def testoneinput ( input_bytes ) : `` '' '' test randomized integer fuzzing input for tf.raw_ops.immutableconst . '' '' '' fh = fuzzinghelper ( input_bytes )
__label__0 # build and run the gradients of the while loop . we use this below to # verify that the gradients are correct with an imported metagraphdef . grad = gradients_impl.gradients ( [ output ] , [ var ] ) # turn off constant folding to avoid breaking testnestedcontrolflowserdes . # it appears that a missing control dependency in the gradient graph # causes the fetch node to not be triggered . no_constfold_config = config_pb2.configproto ( ) no_constfold_config.graph_options.rewrite_options.constant_folding = ( rewriter_config_pb2.rewriterconfig.off ) with session.session ( config=no_constfold_config ) as sess : self.evaluate ( init_op ) expected_grad_value = self.evaluate ( grad )
__label__0 @ deprecation.deprecated ( date , instructions , warn_once=true ) def _fn ( ) : pass
__label__0 def testsoftmaxcrossentropywithlogits ( self ) : text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=labels , logits=logits , dim=2 ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( labels ) , logits=logits , axis=2 ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 > > > structure = { `` key3 '' : { `` c '' : ( 'alpha ' , 'beta ' ) , `` a '' : ( 'gamma ' ) } , ... `` key1 '' : { `` e '' : `` val1 '' , `` d '' : `` val2 '' } } > > > flat_sequence = [ 'val2 ' , 'val1 ' , 3.0 , 1.0 , 2.0 ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) { 'key3 ' : { ' c ' : ( 1.0 , 2.0 ) , ' a ' : 3.0 } , 'key1 ' : { ' e ' : 'val1 ' , 'd ' : 'val2 ' } }
__label__0 @ property def saver ( self ) : `` '' '' return the saver used by the supervisor .
__label__0 # hidden attributes are attributes that have been hidden by # ` remove_undocumented ` . they can be re-instated by ` reveal_undocumented ` . # this maps symbol names to a tuple , containing : # ( module object , attribute value ) _hidden_attributes = { }
__label__0 def _find_cudnn_config ( base_paths , required_version ) :
__label__0 _tensorflow_lazy_loader_prefix = `` _tfll ''
__label__0 def __init__ ( self , sv , sess ) : `` '' '' create a svsummarythread .
__label__0 def test_saved_model_load_v2 ( self ) : text = `` tf.saved_model.load_v2 ( '/tmp/blah ' ) '' expected = `` tf.compat.v2.saved_model.load ( '/tmp/blah ' ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 class mockmodule ( object ) :
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( partial_func ) )
__label__0 returns : the ` variable ` for the slot if it was created , ` none ` otherwise. `` '' '' return self._opt.get_slot ( * args , * * kwargs )
__label__0 def testsplitnochunks ( self ) : sizes = [ 50 , 100 , 50 , 50 , 100 ] max_size = 500 constants.debug_set_max_size ( max_size )
__label__0 examples :
__label__0 examples :
__label__0 def _get_applicable_entries ( self , transformer_field , full_name , name ) : `` '' '' get all list entries indexed by name that apply to full_name or name . '' '' '' # transformers are indexed to full name , name , or no name # as a performance optimization . function_transformers = getattr ( self._api_change_spec , transformer_field , { } )
__label__0 `` python apis '' refers to python functions that have been exported with ` tf_export ` , such as ` tf.add ` and ` tf.linalg.matmul ` ; they are sometimes also referred to as `` ops '' .
__label__0 class grouplock ( object ) : `` '' '' a lock to allow many members of a group to access a resource exclusively .
__label__0 return inspect.signature ( parameters )
__label__0 def testdispatcherrorsignaturemismatchparamname ( self ) : with self.assertraisesregex ( valueerror , r '' dispatch function 's signature \ ( x , why , name=none\ ) does `` r '' not match api 's signature \ ( x , y , name=none\ ) . `` ) :
__label__0 def testlocalserver ( self ) : cluster_def = server_lib.clusterspec ( { `` local '' : [ `` localhost:2222 '' ] } ) .as_cluster_def ( ) server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_def , job_name= '' local '' , task_index=0 , protocol= '' grpc '' )
__label__0 self.mkpath ( self.install_dir ) for header in hdrs : ( out , _ ) = self.mkdir_and_copy_file ( header ) self.outfiles.append ( out )
__label__1 class solution : def minpatches ( self , nums : list [ int ] , n : int ) - > int : miss = 1 added = 0 i = 0 while miss < = n : if i < len ( nums ) and nums [ i ] < = miss : miss += nums [ i ] i += 1 else : miss * = 2 added += 1 return added
__label__0 def get_examples ( self , string : str , name : str = ' < string > ' ) - > iterable [ doctest.example ] : # check for a file-level skip comment . if re.search ( ' < ! -- . * ? doctest. * ? skip. * ? all . * ? -- > ' , string , re.ignorecase ) : return
__label__0 args : checkpoint_prefix : scalar string tensor . interpreted * not as a filename * , but as a prefix of a v2 checkpoint ; per_device : a list of ( device , basesaverbuilder.vartosave ) pairs , as returned by _groupbydevices ( ) .
__label__0 def loop ( self , timer_interval_secs , target , args=none , kwargs=none ) : `` '' '' start a looperthread that calls a function periodically .
__label__0 def register_dispatchable_type ( cls ) : `` '' '' class decorator that registers a type for use with type-based dispatch .
__label__0 class sentinelmapper ( stacktracemapper ) :
__label__1 def find_largest_number ( lst ) : return max ( lst )
__label__0 this proto implements the ` dict ` .
__label__0 @ classmethod def __tf_unflatten__ ( cls , metadata , components ) : mask = metadata [ 0 ] value = components [ 0 ] return nestedmaskedtensor ( mask=mask , value=value )
__label__0 new_notebook = copy.deepcopy ( original_notebook )
__label__0 by default , dispatch support is added to the generated op wrappers for any visible ops by default . apis/ops that are implemented in python can opt in to dispatch support using the ` add_dispatch_support ` decorator. `` '' ''
__label__0 def testusesoutermostdecoratorsargspec ( self ) :
__label__0 in all cases , the keys will be iterated in sorted order .
__label__0 @ tf_export ( `` types.experimental.distributed.perreplica '' , v1= [ ] ) class perreplica ( distributedvalues ) : `` '' '' holds a distributed value : a map from replica id to unsynchronized values .
__label__0 @ deprecation.deprecated ( date , instructions ) def _fn ( self , arg0 , arg1 ) : `` '' '' fn doc .
__label__0 the inserted argument value is tf.compat.v1.train.get_or_create_global_step ( ) . `` '' '' for keyword_arg in node.keywords : if keyword_arg.arg == `` step '' : return node default_value = `` tf.compat.v1.train.get_or_create_global_step ( ) '' ast_value = ast.parse ( default_value ) .body [ 0 ] .value del ast_value.lineno # hack to prevent spurious reordering of call args node.keywords.append ( ast.keyword ( arg= '' step '' , value=ast_value ) ) logs.append ( ( ast_edits.warning , node.lineno , node.col_offset , `` summary api writing function % s now requires a 'step ' argument ; `` `` inserting default of % s . '' % ( full_name or name , default_value ) ) ) return node
__label__0 def _wrap_key ( self , key ) : return _weakobjectidentitywrapper ( key )
__label__0 this decorator wraps a function using ` tf_decorator.make_decorator ` so that doc generation scripts can pick up original function signature . it would be better to use @ functools.wrap decorator , but it would not update function signature to match wrapped function in python 2 .
__label__0 # or pass them as a list . saver = tf.compat.v1.train.saver ( [ v1 , v2 ] ) # passing a list is equivalent to passing a dict with the variable op names # as keys : saver = tf.compat.v1.train.saver ( { v.op.name : v for v in [ v1 , v2 ] } ) `` `
__label__0 def testsessionconfig ( self ) : logdir = self._test_dir ( `` session_config '' ) with ops.graph ( ) .as_default ( ) : with ops.device ( `` /cpu:1 '' ) : my_op = constant_op.constant ( [ 1.0 ] ) sv = supervisor.supervisor ( logdir=logdir ) sess = sv.prepare_or_wait_for_session ( `` '' , config=config_pb2.configproto ( device_count= { `` cpu '' : 2 } ) ) for _ in range ( 10 ) : self.evaluate ( my_op ) sess.close ( ) sv.stop ( )
__label__0 flatten_with_tuple_paths_up_to ( [ 0 , 1 , 2 ] , [ 0 , 1 , 2 ] ) # output : [ ( ( 0 , ) 0 ) , ( ( 1 , ) , 1 ) , ( ( 2 , ) , 2 ) ] `` `
__label__0 def __getitem__ ( self , key ) : return self._wrapped [ key ]
__label__0 @ test_util.run_v1_only ( `` train.syncreplicasoptimizer and train.adamoptimizer are v1 only apis . '' ) def testfetchvariablelist ( self ) : opt = training.syncreplicasoptimizer ( opt=adam.adamoptimizer ( 0.01 ) , replicas_to_aggregate=1 , total_num_replicas=1 ) v = variable_v1.variablev1 ( [ 0 . ] , name= '' fetch_variable_test '' ) global_step = variable_v1.variablev1 ( 0 , name= '' global_step '' , trainable=false ) opt.minimize ( v , global_step=global_step ) opt_variables = opt.variables ( ) beta1_power , beta2_power = opt._opt._get_beta_accumulators ( ) self.assertin ( beta1_power , opt_variables ) self.assertin ( beta2_power , opt_variables )
__label__0 else : header_version = cuda_version header_path = _find_file ( base_paths , _header_paths ( ) , `` curand.h '' ) curand_version = required_version
__label__0 if default_found : # new name scope does n't have name , but it has a default name . we use # name=default_name , and values can be dropped ( it 's only for # error reporting and useless outside of graph mode ) . logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` using default_name as name in call to name_scope.\n '' ) ) # remove all args other than name node.args = [ ] node.keywords = [ ast.keyword ( arg= '' name '' , value=default_name ) ] return node
__label__0 def _find_miopen_config ( rocm_install_path ) :
__label__0 def _summary_computed ( ) : with ops.graph ( ) .as_default ( ) : sv = supervisor.supervisor ( is_chief=false ) sess = sv.prepare_or_wait_for_session ( `` '' ) summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summ = summary.merge_all ( ) sv.summary_computed ( sess , sess.run ( summ ) )
__label__0 return base_dirs , code_url_prefixes
__label__0 def __call__ ( self , path , parent , children ) : self.call_log += [ ( path , parent , children ) ]
__label__0 def test_contrib_to_addons_move ( self ) : small_mapping = { `` tf.contrib.layers.poincare_normalize '' : `` tfa.layers.poincarenormalize '' , `` tf.contrib.layers.maxout '' : `` tfa.layers.maxout '' , `` tf.contrib.layers.group_norm '' : `` tfa.layers.groupnormalization '' , `` tf.contrib.layers.instance_norm '' : `` tfa.layers.instancenormalization '' , } for symbol , replacement in small_mapping.items ( ) : text = `` { } ( 'stuff ' , * args , * * kwargs ) '' .format ( symbol ) _ , report , _ , _ = self._upgrade ( text ) self.assertin ( replacement , report )
__label__0 @ test_decorator ( 'decorator ' ) def test_decorated_function_with_varargs_and_kwonlyargs ( * args , b=2 , c='hello ' ) : `` '' '' test decorated function with both varargs and keyword args . '' '' '' return [ args , b , c ]
__label__0 new_stack = stack + [ root ] visit ( path , root , children ) for name , child in children : # do not descend into built-in modules if tf_inspect.ismodule ( child ) and child.__name__ in sys.builtin_module_names : continue
__label__0 uid = uuid.uuid4 ( ) .hex
__label__0 @ test_util.run_v1_only ( `` sparseapplyadagrad op returns a ref , so it is not `` `` supported in eager mode . '' ) def testsparseapplyadagrad ( self ) : for ( dtype , index_type , use_gpu ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ np.int32 , np.int64 ] , [ false , true ] ) : x_val = [ np.arange ( 10 ) , np.arange ( 10 , 20 ) , np.arange ( 20 , 30 ) ] y_val = [ np.arange ( 1 , 11 ) , np.arange ( 11 , 21 ) , np.arange ( 21 , 31 ) ] x = np.array ( x_val ) .astype ( dtype ) y = np.array ( y_val ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) grad_val = [ np.arange ( 10 ) , np.arange ( 10 ) ] grad = np.array ( grad_val ) .astype ( dtype ) indices = np.array ( [ 0 , 2 ] ) .astype ( index_type ) self._testtypesforsparseadagrad ( x , y , lr , grad , indices , use_gpu ) # empty sparse gradients . empty_grad = np.zeros ( [ 0 , 10 ] , dtype=dtype ) empty_indices = np.zeros ( [ 0 ] , dtype=index_type ) self._testtypesforsparseadagrad ( x , y , lr , empty_grad , empty_indices , use_gpu )
__label__0 args = parser.parse_args ( )
__label__0 # add additional renames not in renames_v2.py here . # important : for the renames in here , if you also need to add to # function_reorders or function_keyword_renames in tf_upgrade_v2.py , # use the old function name . # these renames happen after the arguments have been processed . # after modifying this dict , run the following to update reorders_v2.py : # bazel run tensorflow/tools/compatibility/update : generate_v2_reorders_map manual_symbol_renames = { `` tf.batch_to_space_nd '' : `` tf.batch_to_space '' , `` tf.batch_gather '' : `` tf.compat.v1.batch_gather '' , `` tf.space_to_batch_nd '' : `` tf.space_to_batch '' , `` tf.nn.space_to_batch '' : `` tf.space_to_batch '' , `` tf.extract_image_patches '' : `` tf.image.extract_patches '' , `` tf.image.extract_image_patches '' : `` tf.image.extract_patches '' , `` tf.gfile.copy '' : `` tf.io.gfile.copy '' , `` tf.gfile.deleterecursively '' : `` tf.io.gfile.rmtree '' , `` tf.gfile.exists '' : `` tf.io.gfile.exists '' , `` tf.gfile.glob '' : `` tf.io.gfile.glob '' , `` tf.gfile.gfile '' : `` tf.io.gfile.gfile '' , `` tf.gfile.isdirectory '' : `` tf.io.gfile.isdir '' , `` tf.gfile.listdirectory '' : `` tf.io.gfile.listdir '' , `` tf.gfile.makedirs '' : `` tf.io.gfile.makedirs '' , `` tf.gfile.mkdir '' : `` tf.io.gfile.mkdir '' , `` tf.gfile.open '' : `` tf.io.gfile.gfile '' , `` tf.gfile.remove '' : `` tf.io.gfile.remove '' , `` tf.gfile.rename '' : `` tf.io.gfile.rename '' , `` tf.gfile.stat '' : `` tf.io.gfile.stat '' , `` tf.gfile.walk '' : `` tf.io.gfile.walk '' , `` tf.contrib.cluster_resolver.clusterresolver '' : ( `` tf.distribute.cluster_resolver.clusterresolver '' ) , `` tf.contrib.cluster_resolver.gceclusterresolver '' : ( `` tf.distribute.cluster_resolver.gceclusterresolver '' ) , `` tf.contrib.cluster_resolver.kubernetesclusterresolver '' : ( `` tf.distribute.cluster_resolver.kubernetesclusterresolver '' ) , `` tf.contrib.cluster_resolver.simpleclusterresolver '' : ( `` tf.distribute.cluster_resolver.simpleclusterresolver '' ) , `` tf.contrib.cluster_resolver.slurmclusterresolver '' : ( `` tf.distribute.cluster_resolver.slurmclusterresolver '' ) , `` tf.contrib.cluster_resolver.tfconfigclusterresolver '' : ( `` tf.distribute.cluster_resolver.tfconfigclusterresolver '' ) , `` tf.contrib.cluster_resolver.tpuclusterresolver '' : ( `` tf.distribute.cluster_resolver.tpuclusterresolver '' ) , `` tf.contrib.cluster_resolver.unionclusterresolver '' : ( `` tf.distribute.cluster_resolver.unionclusterresolver '' ) , `` tf.contrib.data.autotune '' : `` tf.data.experimental.autotune '' , `` tf.contrib.data.counter '' : `` tf.data.experimental.counter '' , `` tf.contrib.data.checkpointinputpipelinehook '' : ( `` tf.data.experimental.checkpointinputpipelinehook '' ) , `` tf.contrib.data.csvdataset '' : `` tf.data.experimental.csvdataset '' , `` tf.contrib.data.optional '' : `` tf.data.experimental.optional '' , `` tf.contrib.data.randomdataset '' : `` tf.data.experimental.randomdataset '' , `` tf.contrib.data.reducer '' : `` tf.data.experimental.reducer '' , `` tf.contrib.data.sqldataset '' : `` tf.data.experimental.sqldataset '' , `` tf.contrib.data.statsaggregator '' : `` tf.data.experimental.statsaggregator '' , `` tf.contrib.data.tfrecordwriter '' : `` tf.data.experimental.tfrecordwriter '' , `` tf.contrib.data.assert_element_shape '' : ( `` tf.data.experimental.assert_element_shape '' ) , `` tf.contrib.data.bucket_by_sequence_length '' : ( `` tf.data.experimental.bucket_by_sequence_length '' ) , `` tf.contrib.data.choose_from_datasets '' : ( `` tf.data.experimental.choose_from_datasets '' ) , `` tf.contrib.data.copy_to_device '' : `` tf.data.experimental.copy_to_device '' , `` tf.contrib.data.dense_to_sparse_batch '' : ( `` tf.data.experimental.dense_to_sparse_batch '' ) , `` tf.contrib.data.enumerate_dataset '' : ( `` tf.data.experimental.enumerate_dataset '' ) , `` tf.contrib.data.get_next_as_optional '' : ( `` tf.data.experimental.get_next_as_optional '' ) , `` tf.contrib.data.get_single_element '' : ( `` tf.data.experimental.get_single_element '' ) , `` tf.contrib.data.group_by_reducer '' : `` tf.data.experimental.group_by_reducer '' , `` tf.contrib.data.group_by_window '' : `` tf.data.experimental.group_by_window '' , `` tf.contrib.data.ignore_errors '' : `` tf.data.experimental.ignore_errors '' , `` tf.contrib.data.latency_stats '' : `` tf.data.experimental.latency_stats '' , `` tf.contrib.data.make_batched_features_dataset '' : ( `` tf.data.experimental.make_batched_features_dataset '' ) , `` tf.contrib.data.make_csv_dataset '' : `` tf.data.experimental.make_csv_dataset '' , `` tf.contrib.data.make_saveable_from_iterator '' : ( `` tf.data.experimental.make_saveable_from_iterator '' ) , `` tf.contrib.data.map_and_batch '' : `` tf.data.experimental.map_and_batch '' , `` tf.contrib.data.parallel_interleave '' : ( `` tf.data.experimental.parallel_interleave '' ) , `` tf.contrib.data.parse_example_dataset '' : ( `` tf.data.experimental.parse_example_dataset '' ) , `` tf.contrib.data.prefetch_to_device '' : ( `` tf.data.experimental.prefetch_to_device '' ) , `` tf.contrib.data.rejection_resample '' : ( `` tf.data.experimental.rejection_resample '' ) , `` tf.contrib.data.sample_from_datasets '' : ( `` tf.data.experimental.sample_from_datasets '' ) , `` tf.contrib.data.scan '' : `` tf.data.experimental.scan '' , `` tf.contrib.data.set_stats_aggregator '' : ( `` tf.data.experimental.set_stats_aggregator '' ) , `` tf.contrib.data.shuffle_and_repeat '' : ( `` tf.data.experimental.shuffle_and_repeat '' ) , `` tf.contrib.data.unbatch '' : `` tf.data.experimental.unbatch '' , `` tf.contrib.data.unique '' : `` tf.data.experimental.unique '' , `` tf.contrib.distribute.crossdeviceops '' : `` tf.distribute.crossdeviceops '' , `` tf.contrib.distribute.reductiontoonedevicecrossdeviceops '' : ( `` tf.distribute.reductiontoonedevice '' ) , `` tf.contrib.framework.criticalsection '' : `` tf.criticalsection '' , `` tf.contrib.framework.is_tensor '' : `` tf.is_tensor '' , `` tf.contrib.framework.load_variable '' : `` tf.train.load_variable '' , `` tf.contrib.framework.nest.assert_same_structure '' : ( `` tf.nest.assert_same_structure '' ) , `` tf.contrib.framework.nest.flatten '' : `` tf.nest.flatten '' , `` tf.contrib.framework.nest.is_nested '' : `` tf.nest.is_nested '' , `` tf.contrib.framework.nest.map_structure '' : `` tf.nest.map_structure '' , `` tf.contrib.framework.nest.pack_sequence_as '' : `` tf.nest.pack_sequence_as '' , `` tf.contrib.batching.batch_function '' : `` tf.nondifferentiable_batch_function '' , `` tf.contrib.util.constant_value '' : `` tf.get_static_value '' , `` tf.contrib.saved_model.load_keras_model '' : ( `` tf.compat.v1.keras.experimental.load_from_saved_model '' ) , `` tf.contrib.saved_model.save_keras_model '' : ( `` tf.compat.v1.keras.experimental.export_saved_model '' ) , `` tf.contrib.rnn.rnncell '' : `` tf.compat.v1.nn.rnn_cell.rnncell '' , `` tf.contrib.rnn.lstmstatetuple '' : `` tf.nn.rnn_cell.lstmstatetuple '' , `` tf.contrib.rnn.basiclstmcell '' : `` tf.compat.v1.nn.rnn_cell.basiclstmcell '' , `` tf.contrib.rnn.basicrnncell '' : `` tf.compat.v1.nn.rnn_cell.basicrnncell '' , `` tf.contrib.rnn.grucell '' : `` tf.compat.v1.nn.rnn_cell.grucell '' , `` tf.contrib.rnn.lstmcell '' : `` tf.compat.v1.nn.rnn_cell.lstmcell '' , `` tf.contrib.rnn.multirnncell '' : `` tf.compat.v1.nn.rnn_cell.multirnncell '' , `` tf.contrib.rnn.static_rnn '' : `` tf.compat.v1.nn.static_rnn '' , `` tf.contrib.rnn.static_state_saving_rnn '' : ( `` tf.compat.v1.nn.static_state_saving_rnn '' ) , `` tf.contrib.rnn.static_bidirectional_rnn '' : ( `` tf.compat.v1.nn.static_bidirectional_rnn '' ) , `` tf.contrib.framework.sort '' : `` tf.sort '' , `` tf.contrib.framework.argsort '' : `` tf.argsort '' , `` tf.contrib.summary.all_summary_ops '' : ( `` tf.compat.v1.summary.all_v2_summary_ops '' ) , `` tf.contrib.summary.always_record_summaries '' : ( `` tf.compat.v2.summary.record_if '' ) , `` tf.contrib.summary.audio '' : `` tf.compat.v2.summary.audio '' , `` tf.contrib.summary.create_file_writer '' : ( `` tf.compat.v2.summary.create_file_writer '' ) , `` tf.contrib.summary.flush '' : `` tf.compat.v2.summary.flush '' , `` tf.contrib.summary.generic '' : `` tf.compat.v2.summary.write '' , `` tf.contrib.summary.histogram '' : `` tf.compat.v2.summary.histogram '' , `` tf.contrib.summary.image '' : `` tf.compat.v2.summary.image '' , `` tf.contrib.summary.initialize '' : `` tf.compat.v1.summary.initialize '' , `` tf.contrib.summary.never_record_summaries '' : ( `` tf.compat.v2.summary.record_if '' ) , `` tf.contrib.summary.scalar '' : `` tf.compat.v2.summary.scalar '' , `` tf.contrib.tpu.crossshardoptimizer '' : ( `` tf.compat.v1.tpu.crossshardoptimizer '' ) , `` tf.contrib.tpu.batch_parallel '' : `` tf.compat.v1.tpu.batch_parallel '' , `` tf.contrib.tpu.bfloat16_scope '' : `` tf.compat.v1.tpu.bfloat16_scope '' , `` tf.contrib.tpu.core '' : `` tf.compat.v1.tpu.core '' , `` tf.contrib.tpu.cross_replica_sum '' : `` tf.compat.v1.tpu.cross_replica_sum '' , `` tf.contrib.tpu.initialize_system '' : `` tf.compat.v1.tpu.initialize_system '' , `` tf.contrib.tpu.outside_compilation '' : ( `` tf.compat.v1.tpu.outside_compilation '' ) , `` tf.contrib.tpu.replicate '' : `` tf.compat.v1.tpu.replicate '' , `` tf.contrib.tpu.rewrite '' : `` tf.compat.v1.tpu.rewrite '' , `` tf.contrib.tpu.shard '' : `` tf.compat.v1.tpu.shard '' , `` tf.contrib.tpu.shutdown_system '' : `` tf.compat.v1.tpu.shutdown_system '' , `` tf.contrib.training.checkpoints_iterator '' : `` tf.train.checkpoints_iterator '' , `` tf.contrib.layers.recompute_grad '' : `` tf.recompute_grad '' , `` tf.count_nonzero '' : `` tf.math.count_nonzero '' , `` tf.decode_raw '' : `` tf.io.decode_raw '' , `` tf.manip.batch_to_space_nd '' : `` tf.batch_to_space '' , `` tf.quantize_v2 '' : `` tf.quantization.quantize '' , `` tf.sparse_matmul '' : `` tf.linalg.matmul '' , `` tf.random.stateless_multinomial '' : `` tf.random.stateless_categorical '' , `` tf.substr '' : `` tf.strings.substr '' , # todo ( b/129398290 ) `` tf.string_split '' : `` tf.compat.v1.string_split '' , `` tf.string_to_hash_bucket '' : `` tf.strings.to_hash_bucket '' , `` tf.string_to_number '' : `` tf.strings.to_number '' , `` tf.multinomial '' : `` tf.random.categorical '' , `` tf.random.multinomial '' : `` tf.random.categorical '' , `` tf.reduce_join '' : `` tf.strings.reduce_join '' , `` tf.load_file_system_library '' : `` tf.load_library '' , `` tf.bincount '' : `` tf.math.bincount '' , `` tf.confusion_matrix '' : `` tf.math.confusion_matrix '' , `` tf.train.confusion_matrix '' : `` tf.math.confusion_matrix '' , `` tf.train.sdca_fprint '' : `` tf.raw_ops.sdcafprint '' , `` tf.train.sdca_optimizer '' : `` tf.raw_ops.sdcaoptimizer '' , `` tf.train.sdca_shrink_l1 '' : `` tf.raw_ops.sdcashrinkl1 '' , `` tf.decode_csv '' : `` tf.io.decode_csv '' , `` tf.data.iterator '' : `` tf.compat.v1.data.iterator '' , `` tf.data.experimental.datasetstructure '' : `` tf.data.datasetspec '' , `` tf.data.experimental.optionalstructure '' : `` tf.optionalspec '' , `` tf.data.experimental.raggedtensorstructure '' : `` tf.raggedtensorspec '' , `` tf.data.experimental.sparsetensorstructure '' : `` tf.sparsetensorspec '' , `` tf.data.experimental.structure '' : `` tf.typespec '' , `` tf.data.experimental.tensorarraystructure '' : `` tf.tensorarrayspec '' , `` tf.data.experimental.tensorstructure '' : `` tf.tensorspec '' , `` tf.parse_example '' : `` tf.io.parse_example '' , `` tf.parse_single_example '' : `` tf.io.parse_single_example '' , `` tf.nn.fused_batch_norm '' : `` tf.compat.v1.nn.fused_batch_norm '' , `` tf.nn.softmax_cross_entropy_with_logits_v2 '' : ( `` tf.nn.softmax_cross_entropy_with_logits '' ) , `` tf.losses.reduction.mean '' : `` tf.compat.v1.losses.reduction.mean '' , `` tf.losses.reduction.sum_by_nonzero_weights '' : ( `` tf.compat.v1.losses.reduction.sum_by_nonzero_weights '' ) , `` tf.losses.reduction.sum_over_nonzero_weights '' : ( `` tf.compat.v1.losses.reduction.sum_over_nonzero_weights '' ) , `` tf.lite.constants.float '' : `` tf.float32 '' , `` tf.lite.constants.float16 '' : `` tf.float16 '' , `` tf.lite.constants.int16 '' : `` tf.int16 '' , `` tf.lite.constants.int32 '' : `` tf.int32 '' , `` tf.lite.constants.int64 '' : `` tf.int64 '' , `` tf.lite.constants.int8 '' : `` tf.int8 '' , `` tf.lite.constants.string '' : `` tf.string '' , `` tf.lite.constants.quantized_uint8 '' : `` tf.uint8 '' , `` tf.arg_max '' : `` tf.argmax '' , `` tf.arg_min '' : `` tf.argmin '' , # tf.nn.ctc_loss is still available in 2.0 but behavior # changed significantly . `` tf.nn.ctc_loss '' : `` tf.compat.v1.nn.ctc_loss '' , # tf.saved_model.load in 1.x has no equivalent in 2.x , but there is a # symbol with the same name . `` tf.saved_model.load '' : `` tf.compat.v1.saved_model.load '' , `` tf.saved_model.loader.load '' : `` tf.compat.v1.saved_model.load '' , `` tf.saved_model.load_v2 '' : `` tf.compat.v2.saved_model.load '' , `` tf.image.resize_images '' : `` tf.image.resize '' , `` tf.assert_equal '' : `` tf.compat.v1.assert_equal '' , `` tf.assert_greater '' : `` tf.compat.v1.assert_greater '' , `` tf.assert_greater_equal '' : `` tf.compat.v1.assert_greater_equal '' , `` tf.assert_integer '' : `` tf.compat.v1.assert_integer '' , `` tf.assert_less '' : `` tf.compat.v1.assert_less '' , `` tf.assert_less_equal '' : `` tf.compat.v1.assert_less_equal '' , `` tf.assert_near '' : `` tf.compat.v1.assert_near '' , `` tf.assert_negative '' : `` tf.compat.v1.assert_negative '' , `` tf.assert_non_negative '' : `` tf.compat.v1.assert_non_negative '' , `` tf.assert_non_positive '' : `` tf.compat.v1.assert_non_positive '' , `` tf.assert_none_equal '' : `` tf.compat.v1.assert_none_equal '' , `` tf.assert_positive '' : `` tf.compat.v1.assert_positive '' , `` tf.assert_rank '' : `` tf.compat.v1.assert_rank '' , `` tf.assert_rank_at_least '' : `` tf.compat.v1.assert_rank_at_least '' , `` tf.assert_rank_in '' : `` tf.compat.v1.assert_rank_in '' , `` tf.assert_scalar '' : `` tf.compat.v1.assert_scalar '' , `` tf.assert_type '' : `` tf.compat.v1.assert_type '' , `` tf.assert_variables_initialized '' : ( `` tf.compat.v1.assert_variables_initialized '' ) , `` tf.debugging.assert_equal '' : `` tf.compat.v1.debugging.assert_equal '' , `` tf.debugging.assert_greater '' : `` tf.compat.v1.debugging.assert_greater '' , `` tf.debugging.assert_greater_equal '' : ( `` tf.compat.v1.debugging.assert_greater_equal '' ) , `` tf.debugging.assert_integer '' : `` tf.compat.v1.debugging.assert_integer '' , `` tf.debugging.assert_less '' : `` tf.compat.v1.debugging.assert_less '' , `` tf.debugging.assert_less_equal '' : ( `` tf.compat.v1.debugging.assert_less_equal '' ) , `` tf.debugging.assert_near '' : `` tf.compat.v1.debugging.assert_near '' , `` tf.debugging.assert_negative '' : `` tf.compat.v1.debugging.assert_negative '' , `` tf.debugging.assert_non_negative '' : ( `` tf.compat.v1.debugging.assert_non_negative '' ) , `` tf.debugging.assert_non_positive '' : ( `` tf.compat.v1.debugging.assert_non_positive '' ) , `` tf.debugging.assert_none_equal '' : ( `` tf.compat.v1.debugging.assert_none_equal '' ) , `` tf.debugging.assert_positive '' : `` tf.compat.v1.debugging.assert_positive '' , `` tf.debugging.assert_rank '' : `` tf.compat.v1.debugging.assert_rank '' , `` tf.debugging.assert_rank_at_least '' : ( `` tf.compat.v1.debugging.assert_rank_at_least '' ) , `` tf.debugging.assert_rank_in '' : `` tf.compat.v1.debugging.assert_rank_in '' , `` tf.debugging.assert_scalar '' : `` tf.compat.v1.debugging.assert_scalar '' , `` tf.debugging.assert_type '' : `` tf.compat.v1.debugging.assert_type '' , `` tf.errors.exception_type_from_error_code '' : ( `` tf.compat.v1.errors.exception_type_from_error_code '' ) , `` tf.errors.error_code_from_exception_type '' : ( `` tf.compat.v1.errors.error_code_from_exception_type '' ) , `` tf.errors.raise_exception_on_not_ok_status '' : ( `` tf.compat.v1.errors.raise_exception_on_not_ok_status '' ) , `` tf.nn.max_pool '' : `` tf.nn.max_pool2d '' , `` tf.nn.avg_pool '' : `` tf.nn.avg_pool2d '' , `` tf.keras.initializers.zeros '' : `` tf.compat.v1.keras.initializers.zeros '' , `` tf.keras.initializers.zeros '' : `` tf.compat.v1.keras.initializers.zeros '' , `` tf.keras.initializers.ones '' : `` tf.compat.v1.keras.initializers.ones '' , `` tf.keras.initializers.ones '' : `` tf.compat.v1.keras.initializers.ones '' , `` tf.keras.initializers.constant '' : ( `` tf.compat.v1.keras.initializers.constant '' ) , `` tf.keras.initializers.constant '' : ( `` tf.compat.v1.keras.initializers.constant '' ) , `` tf.keras.initializers.variancescaling '' : ( `` tf.compat.v1.keras.initializers.variancescaling '' ) , `` tf.keras.initializers.orthogonal '' : ( `` tf.compat.v1.keras.initializers.orthogonal '' ) , `` tf.keras.initializers.orthogonal '' : ( `` tf.compat.v1.keras.initializers.orthogonal '' ) , `` tf.keras.initializers.identity '' : ( `` tf.compat.v1.keras.initializers.identity '' ) , `` tf.keras.initializers.identity '' : ( `` tf.compat.v1.keras.initializers.identity '' ) , `` tf.keras.initializers.glorot_uniform '' : ( `` tf.compat.v1.keras.initializers.glorot_uniform '' ) , `` tf.keras.initializers.glorot_normal '' : ( `` tf.compat.v1.keras.initializers.glorot_normal '' ) , `` tf.keras.initializers.lecun_normal '' : ( `` tf.compat.v1.keras.initializers.lecun_normal '' ) , `` tf.keras.initializers.lecun_uniform '' : ( `` tf.compat.v1.keras.initializers.lecun_uniform '' ) , `` tf.keras.initializers.he_normal '' : ( `` tf.compat.v1.keras.initializers.he_normal '' ) , `` tf.keras.initializers.he_uniform '' : ( `` tf.compat.v1.keras.initializers.he_uniform '' ) , `` tf.keras.initializers.truncatednormal '' : ( `` tf.compat.v1.keras.initializers.truncatednormal '' ) , `` tf.keras.initializers.truncated_normal '' : ( `` tf.compat.v1.keras.initializers.truncated_normal '' ) , `` tf.keras.initializers.randomuniform '' : ( `` tf.compat.v1.keras.initializers.randomuniform '' ) , `` tf.keras.initializers.uniform '' : `` tf.compat.v1.keras.initializers.uniform '' , `` tf.keras.initializers.random_uniform '' : ( `` tf.compat.v1.keras.initializers.random_uniform '' ) , `` tf.keras.initializers.randomnormal '' : ( `` tf.compat.v1.keras.initializers.randomnormal '' ) , `` tf.keras.initializers.normal '' : `` tf.compat.v1.keras.initializers.normal '' , `` tf.keras.initializers.random_normal '' : ( `` tf.compat.v1.keras.initializers.random_normal '' ) , `` tf.zeros_initializer '' : `` tf.compat.v1.zeros_initializer '' , `` tf.initializers.zeros '' : `` tf.compat.v1.initializers.zeros '' , `` tf.ones_initializer '' : `` tf.compat.v1.ones_initializer '' , `` tf.initializers.ones '' : `` tf.compat.v1.initializers.ones '' , `` tf.constant_initializer '' : `` tf.compat.v1.constant_initializer '' , `` tf.initializers.constant '' : `` tf.compat.v1.initializers.constant '' , `` tf.random_uniform_initializer '' : `` tf.compat.v1.random_uniform_initializer '' , `` tf.initializers.random_uniform '' : ( `` tf.compat.v1.initializers.random_uniform '' ) , `` tf.random_normal_initializer '' : `` tf.compat.v1.random_normal_initializer '' , `` tf.initializers.random_normal '' : `` tf.compat.v1.initializers.random_normal '' , `` tf.truncated_normal_initializer '' : ( `` tf.compat.v1.truncated_normal_initializer '' ) , `` tf.initializers.truncated_normal '' : ( `` tf.compat.v1.initializers.truncated_normal '' ) , `` tf.variance_scaling_initializer '' : ( `` tf.compat.v1.variance_scaling_initializer '' ) , `` tf.initializers.variance_scaling '' : ( `` tf.compat.v1.initializers.variance_scaling '' ) , `` tf.orthogonal_initializer '' : `` tf.compat.v1.orthogonal_initializer '' , `` tf.initializers.orthogonal '' : `` tf.compat.v1.initializers.orthogonal '' , `` tf.glorot_uniform_initializer '' : `` tf.compat.v1.glorot_uniform_initializer '' , `` tf.initializers.glorot_uniform '' : ( `` tf.compat.v1.initializers.glorot_uniform '' ) , `` tf.glorot_normal_initializer '' : `` tf.compat.v1.glorot_normal_initializer '' , `` tf.initializers.glorot_normal '' : `` tf.compat.v1.initializers.glorot_normal '' , `` tf.initializers.identity '' : `` tf.compat.v1.initializers.identity '' , `` tf.initializers.lecun_normal '' : `` tf.compat.v1.initializers.lecun_normal '' , `` tf.initializers.lecun_uniform '' : `` tf.compat.v1.initializers.lecun_uniform '' , `` tf.initializers.he_normal '' : `` tf.compat.v1.initializers.he_normal '' , `` tf.initializers.he_uniform '' : `` tf.compat.v1.initializers.he_uniform '' , `` tf.data.experimental.map_and_batch_with_legacy_function '' : ( `` tf.compat.v1.data.experimental.map_and_batch_with_legacy_function '' ) , `` tf.nn.conv2d_backprop_input '' : `` tf.nn.conv2d_transpose '' , `` tf.test.compute_gradient '' : `` tf.compat.v1.test.compute_gradient '' , `` tf.floor_div '' : `` tf.math.floordiv '' , `` tf.where '' : `` tf.compat.v1.where '' , `` tf.where_v2 '' : `` tf.compat.v2.where '' , `` tf.app.flags '' : `` tf.compat.v1.app.flags '' , } # pylint : enable=line-too-long
__label__0 raises : typeerror : if ` cluster_spec ` is not a dictionary mapping strings to lists of strings. `` '' '' self._cluster_def = cluster_pb2.clusterdef ( )
__label__0 def testframesummaryequalityandhash ( self ) : # both defined on the same line to produce identical stacks . frame1 , frame2 = tf_stack.extract_stack ( ) , tf_stack.extract_stack ( ) self.assertequal ( len ( frame1 ) , len ( frame2 ) ) for f1 , f2 in zip ( frame1 , frame2 ) : self.assertequal ( f1 , f2 ) self.assertequal ( hash ( f1 ) , hash ( f1 ) ) self.assertequal ( hash ( f1 ) , hash ( f2 ) ) self.assertequal ( frame1 , frame2 ) self.assertequal ( hash ( tuple ( frame1 ) ) , hash ( tuple ( frame2 ) ) )
__label__0 refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 def __getattr__ ( self , name ) : if name.startswith ( ' _ ' ) : raise attributeerror ( ) mod = automodule ( name ) setattr ( self , name , mod ) return mod
__label__0 @ test_decorator ( 'decorator 1 ' ) @ test_decorator ( 'decorator 2 ' ) @ test_decorator ( 'decorator 3 ' ) def test_decorated_function ( x ) : `` '' '' test decorated function docstring . '' '' '' return x * 2
__label__0 @ property def version_def ( self ) - > versions_pb2.versiondef : `` '' '' version info about the splitter and join implementation required . '' '' '' return versions_pb2.versiondef ( splitter_version=1 , join_version=0 , bad_consumers=version_lib.get_bad_versions ( ) , )
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 , 6.0 ] } } } } } `` `
__label__0 from setuptools import command from setuptools import find_namespace_packages from setuptools import setup from setuptools.command.install import install as installcommandbase from setuptools.dist import distribution
__label__0 # assert calling new fn with default deprecated value issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 ) ) self.assertequal ( 2 , mock_warning.call_count )
__label__0 class grpcservertest ( test.testcase ) :
__label__0 returns : dictionary mapping tensor names to checkpoint keys. `` '' '' reader = py_checkpoint_reader.newcheckpointreader ( checkpoint_path ) object_graph_string = reader.get_tensor ( trackable.object_graph_proto_key ) object_graph_proto = ( trackable_object_graph_pb2.trackableobjectgraph ( ) ) object_graph_proto.parsefromstring ( object_graph_string ) names_to_keys = { } for node in object_graph_proto.nodes : for attribute in node.attributes : names_to_keys [ attribute.full_name ] = attribute.checkpoint_key return names_to_keys
__label__0 for unary elementwise operations that take extra arguments beyond ` x ` , those arguments are * not * passed to the elementwise api handler , but are automatically added when ` api_func ` is called . e.g. , in the following example , the ` dtype ` parameter is not passed to ` unary_elementwise_api_handler ` , but is added by ` api_func ` .
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor , `` y '' : maskedtensor } ) def masked_add ( * args , * * kwargs ) : self.assertallequal ( args [ 0 ] .values , x.values ) self.assertallequal ( args [ 1 ] .values , y.values ) self.assertempty ( kwargs ) return `` stub ''
__label__0 if target is none : return decorator else : return decorator ( target )
__label__0 # verifies that we can copy the subgraph under `` hidden1 '' and copy it # to different name scope in the same graph or different graph . def testcopyscopedgraph ( self ) : test_dir = self._get_test_dir ( `` scoped_copy '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) graph1 = ops_lib.graph ( ) with graph1.as_default ( ) : with ops_lib.name_scope ( `` hidden1 '' ) : images = constant_op.constant ( 1.0 , dtypes.float32 , shape= [ 3 , 2 ] , name= '' images '' ) weights1 = variable_v1.variablev1 ( [ [ 1.0 , 2.0 , 3.0 ] , [ 4.0 , 5.0 , 6.0 ] ] , name= '' weights '' ) biases1 = variable_v1.variablev1 ( [ 0.1 ] * 3 , name= '' biases '' ) nn_ops.relu ( math_ops.matmul ( images , weights1 ) + biases1 , name= '' relu '' )
__label__0 def testnologdirbutexplicitsummarywriter ( self ) : logdir = self._test_dir ( `` explicit_summary_writer '' ) with ops.graph ( ) .as_default ( ) : summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sw = writer.filewriter ( logdir ) sv = supervisor.supervisor ( logdir= '' '' , summary_op=none , summary_writer=sw ) meta_graph_def = meta_graph.create_meta_graph_def ( ) sess = sv.prepare_or_wait_for_session ( `` '' ) sv.summary_computed ( sess , sess.run ( summ ) ) sess.close ( ) # wait to make sure everything is written to file before stopping . time.sleep ( 1 ) sv.stop ( )
__label__0 it also edits the docstring of the function : ' ( deprecated arguments ) ' is appended to the first line of the docstring and a deprecation notice is prepended to the rest of the docstring .
__label__0 @ test_util.run_v1_only ( `` applyadam op returns a ref , so it is not `` `` supported in eager mode . '' ) def testapplyadam ( self ) : for dtype , use_gpu in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ false , true ] ) : var = np.arange ( 100 ) .astype ( dtype ) m = np.arange ( 1 , 101 ) .astype ( dtype ) v = np.arange ( 101 , 201 ) .astype ( dtype ) grad = np.arange ( 100 ) .astype ( dtype ) self._testtypesforadam ( var , m , v , grad , use_gpu )
__label__0 # verifies that we can run successfully after restoring . graph2 = ops_lib.graph ( ) with graph2.as_default ( ) : new_var_list_1 = meta_graph.copy_scoped_meta_graph ( from_scope= '' hidden1 '' , to_scope= '' new_hidden1 '' , from_graph=graph1 , to_graph=graph2 )
__label__0 def visit_call ( self , node ) : # pylint : disable=invalid-name `` '' '' handle visiting a call node in the ast .
__label__0 def testprotodictdefequivalenceswithzeroworker ( self ) : cluster_spec = server_lib.clusterspec ( { `` ps '' : [ `` ps0:2222 '' , `` ps1:2222 '' ] , `` worker '' : [ ] } )
__label__0 def get_canonical_name_for_symbol ( symbol : any , api_name : str = tensorflow_api_name , add_prefix_to_v1_names : bool = false , ) - > optional [ str ] : `` '' '' get canonical name for the api symbol .
__label__0 wrapped_fn = functools.partial ( fn , test_arg2=456 ) double_wrapped_fn = functools.partial ( wrapped_fn , test_arg1=123 )
__label__0 def test_replace_variables_with_atoms ( self ) : data = [ resource_variable_ops.resourcevariable ( 1 ) , resource_variable_ops.resourcevariable ( 2 ) , constant_op.constant ( 3 ) , [ 4 ] , 5 ] if not context.executing_eagerly ( ) : self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 # cache for instancetypechecker objects ( we only want to create one # instancetypechecker for each type , since each one uses an internal cache # to avoid repeated calls back into python 's isinstance ) . _is_instance_checker_cache = { }
__label__0 > > > tf.io.parse_example ( ... example.serializetostring ( ) , ... features = { 'my_feature ' : tf.io.raggedfeature ( dtype=tf.int64 ) } ) { 'my_feature ' : < tf.tensor : shape= ( 4 , ) , dtype=float32 , numpy=array ( [ 1 , 2 , 3 , 4 ] , dtype=int64 ) > }
__label__0 w3 = resource_variable_ops.resourcevariable ( 3.0 , name= '' w3 '' ) w4 = resource_variable_ops.resourcevariable ( 4.0 , name= '' w4 '' )
__label__0 an ` example ` proto is a representation of the following python type :
__label__0 # check that the parameters . self.assertallcloseaccordingtotype ( np.array ( [ 1.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) , 2.0 - ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) ] ) , self.evaluate ( var0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 3.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) , 4.0 - ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) ] ) , self.evaluate ( var1 ) )
__label__0 def testrecoversessionwithreadyforlocalinitop ( self ) : # create a checkpoint . checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` recover_session_ready_for_local_init '' ) try : gfile.deleterecursively ( checkpoint_dir ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 def testrecoversessionfailsstillrunslocalinitop ( self ) : # create a checkpoint . checkpoint_dir = os.path.join ( self.get_temp_dir ( ) , `` recover_session_ready_for_local_init_fails_stil_run '' ) try : gfile.deleterecursively ( checkpoint_dir ) except errors.operror : pass # ignore gfile.makedirs ( checkpoint_dir )
__label__0 goodbye `` '' '' ) , ( 'doctest_skip ' , [ ] , `` '' '' hello
__label__0 this includes the operations to synchronize replicas : aggregate gradients , apply to variables , increment global step , insert tokens to token queue .
__label__0 args , varargs , keywords , defaults = getargspec ( obj.func )
__label__0 return dev_info
__label__0 def test_bound_method ( self ) :
__label__0 def run_and_gather_logs ( name , test_name , test_args , benchmark_type , skip_processing_logs=false ) : `` '' '' run the bazel test given by test_name . gather and return the logs .
__label__0 use case :
__label__0 argspec = tf_inspect.argspec ( args= [ 'self ' , ' a ' , ' b ' , ' c ' ] , varargs=none , keywords=none , defaults= ( 1 , 'hello ' ) )
__label__0 # todo ( b/194845243 ) : implement the long term solution with inspect.signature . # a functools.partial object is not a function or method . but if the wrapped # func is a method , the argspec will contain self/cls . while isinstance ( target , functools.partial ) : target = target.func
__label__0 def testreverse ( self ) : text = `` tf.reverse ( a , b ) \n '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , new_text ) self.assertin ( `` tf.reverse requires manual check '' , errors [ 0 ] )
__label__0 def _replace_mode ( parent , old_value ) : `` '' '' replaces old_value with ( old_value ) .lower ( ) . '' '' '' new_value = pasta.parse ( `` mode.lower ( ) '' ) mode = new_value.body [ 0 ] .value.func pasta.ast_utils.replace_child ( mode , mode.value , old_value )
__label__0 # assert function docs are properly updated . self.assertequal ( `` _fn '' , _fn.__name__ ) self.assertequal ( `` deprecated function arguments '' `` \n '' `` \ndeprecated : some arguments are deprecated : ` ( deprecated ) ` . `` `` they will be removed after % s . '' `` \ninstructions for updating : '' `` \n % s '' % ( date , instructions ) , _fn.__doc__ )
__label__0 saver_def = saver_pb2.saverdef ( ) saver_def.copyfrom ( self.saver_def ) saver_def.filename_tensor_name = ops.strip_name_scope ( saver_def.filename_tensor_name , export_scope ) saver_def.save_tensor_name = ops.strip_name_scope ( saver_def.save_tensor_name , export_scope ) saver_def.restore_op_name = ops.strip_name_scope ( saver_def.restore_op_name , export_scope ) return saver_def
__label__0 return rocsolver_config
__label__0 # pylint : disable=missing-function-docstring def _tf_core_assert_shallow_structure ( shallow_tree , input_tree , check_types=true , expand_composites=false ) : is_nested_fn = ( _is_nested_or_composite if expand_composites else _tf_core_is_nested ) if is_nested_fn ( shallow_tree ) : if not is_nested_fn ( input_tree ) : raise typeerror ( `` if shallow structure is a sequence , input must also be a sequence. `` `` input has type : % s . '' % type ( input_tree ) )
__label__1 class solution : def countrangesum ( self , nums : list [ int ] , lower : int , upper : int ) - > int : def mergesort ( prefixsums , start , end ) : if end - start < = 1 : return 0 mid = ( start + end ) // 2 count = mergesort ( prefixsums , start , mid ) + mergesort ( prefixsums , mid , end ) j = k = t = mid temp = [ 0 ] * ( end - start ) for i in range ( start , mid ) : while k < end and prefixsums [ k ] - prefixsums [ i ] < lower : k += 1 while j < end and prefixsums [ j ] - prefixsums [ i ] < = upper : j += 1 while t < end and prefixsums [ t ] < prefixsums [ i ] : temp [ t - start ] = prefixsums [ t ] t += 1 temp [ i - start ] = prefixsums [ i ] count += j - k prefixsums [ start : t ] = temp [ : t - start ] return count prefixsums = [ 0 ] * ( len ( nums ) + 1 ) for i in range ( len ( nums ) ) : prefixsums [ i + 1 ] = prefixsums [ i ] + nums [ i ] return mergesort ( prefixsums , 0 , len ( prefixsums ) )
__label__0 self.asserttrue ( gfile.exists ( checkpoint_management.meta_graph_filename ( s1 ) ) )
__label__0 def f ( a , b , kw3 , kw1 ) : ...
__label__0 mg0 = opt.get_slot ( var0 , `` mg '' ) self.assertequal ( mg0 is not none , centered ) mg1 = opt.get_slot ( var1 , `` mg '' ) self.assertequal ( mg1 is not none , centered ) rms0 = opt.get_slot ( var0 , `` rms '' ) self.asserttrue ( rms0 is not none ) rms1 = opt.get_slot ( var1 , `` rms '' ) self.asserttrue ( rms1 is not none ) mom0 = opt.get_slot ( var0 , `` momentum '' ) self.asserttrue ( mom0 is not none ) mom1 = opt.get_slot ( var1 , `` momentum '' ) self.asserttrue ( mom1 is not none )
__label__0 and handles the case where no factor is provided and scale needs to be set to 2.0 to match contrib 's default instead of tf.keras.initializer 's default of 1.0 `` '' '' def _replace_distribution ( parent , old_value ) : `` '' '' replaces old_value : ( `` uniform '' if ( old_value ) else `` truncated_normal '' ) '' '' '' new_value = pasta.parse ( `` \ '' uniform\ '' if old_value else \ '' truncated_normal\ '' '' ) ifexpr = new_value.body [ 0 ] .value pasta.ast_utils.replace_child ( ifexpr , ifexpr.test , old_value )
__label__0 # creates and returns all the workers . sessions , graphs , train_ops = get_workers ( num_workers , replicas_to_aggregate , workers )
__label__0 @ contextlib.contextmanager def edit_yaml_file ( path ) : content = yaml.safe_load ( path.read_text ( ) ) yield content
__label__0 function_name = call_str [ : call_str.find ( `` ( `` ) ] args = call_str [ open_paren_index + 1 : close_paren_index ] .split ( `` , '' ) args = [ arg.split ( `` = '' ) [ 0 ] .strip ( ) for arg in args ] args = [ arg for arg in args if arg ] # filter out empty strings return function_name , args
__label__0 def _wrap_key ( self , key ) : return _objectidentitywrapper ( key )
__label__0 if version.parse ( tf.__version__ ) > = version.parse ( `` 2.16 '' ) : # first match takes precedence . # objects are dropped if they have no match . base_dirs = [ # the real keras source files are now in ` site-packages/keras/src/ ... ` pathlib.path ( keras.__file__ ) .parent / `` src '' , # the generated module files in tensorflow are in keras # under ` site-packages/keras/api/_v2/keras/ ... ` . pathlib.path ( tf.keras.__file__ ) .parent , # the generated api-module files are now in ` site-packages/keras/ ... ` pathlib.path ( keras.__file__ ) .parent , pathlib.path ( tensorboard.__file__ ) .parent , # the tensorflow base dir goes last because ` tf.keras `` base_dir , ]
__label__0 def count_calls ( target ) : return callcounter ( target ) `` '' '' import inspect from typing import dict , any
__label__0 # copyright 2019 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' module deprecation warnings for tensorflow 2.0 . '' '' ''
__label__0 def testunsortedsegmentsum1dindices1ddatanegativeindices ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 6 , 3 , 0 , 6 ] , dtype=dtype ) , self._unsortedsegmentsum ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] , dtype=dtype ) , np.array ( [ 3 , -1 , 0 , 1 , 0 , -1 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 # output is : # [ ( ' a ' , 1 ) , ( ' b ' , 2 ) , ( ' c ' , 3 ) , ( 'd ' , 4 ) ] # [ ' a ' , 1 , ' b ' , 2 , ' c ' , 3 , 'd ' , 4 ] `` `
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for supervisor.py . '' '' ''
__label__0 - ` tf.train.byteslist ` - ` tf.train.floatlist ` - ` tf.train.int64list `
__label__0 # an equivalent subclass from the typing module class foo ( namedtuple ) : a : int b : int self.asserttrue ( nest.same_namedtuples ( foo1 ( 1 , 2 ) , foo ( 3 , 4 ) ) )
__label__0 def __init ( self ) : pass
__label__0 @ test_util.run_v1_only ( `` train.syncreplicasoptimizer and train.gradientdescentoptimizer `` `` are v1 only apis . '' ) def testcancreatedbeforeminimizecalled ( self ) : opt = training.syncreplicasoptimizer ( opt=gradient_descent.gradientdescentoptimizer ( 1.0 ) , replicas_to_aggregate=1 , total_num_replicas=1 ) hook = opt.make_session_run_hook ( true ) v = variable_v1.variablev1 ( [ 0 . ] ) global_step = variable_v1.variablev1 ( 0 , name= '' global_step '' , trainable=false ) opt.minimize ( v , global_step=global_step ) hook.begin ( )
__label__0 @ dispatch.dispatch_for_api ( math_ops.add , { `` x '' : maskedtensor } ) def my_add ( x , why , name=none ) : # pylint : disable=unused-variable del x , why , name
__label__0 from tensorflow.python.user_ops.ops import gen_user_ops as _gen_user_ops
__label__0 self.assertraises ( typeerror , nest.assert_same_structure , nesttest.samenameab ( 0 , 1 ) , nesttest.notsamename ( 2 , 3 ) )
__label__0 @ test_util.assert_no_new_pyobjects_executing_eagerly ( ) def testdataclassmapstructurewithtuplepaths ( self ) : mt = maskedtensor ( mask=false , value=constant_op.constant ( [ 1 ] ) ) mt2 = maskedtensor ( mask=true , value=constant_op.constant ( [ 2 ] ) ) mt3 = maskedtensor ( mask=true , value=constant_op.constant ( [ 3 ] ) )
__label__0 if invalid_default_values : raise valueerror ( f ' { obj } has some keyword-only arguments , which are not ' f ' supported : { invalid_default_values } . ' )
__label__0 @ test_util.run_v1_only ( `` queuerunner removed from v2 '' ) class queuerunnertest ( test.testcase ) :
__label__0 def test_contrib_summary_never_record_summaries ( self ) : text = `` tf.contrib.summary.never_record_summaries ( ) '' expected = `` tf.compat.v2.summary.record_if ( false ) '' _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 wrapped_fn = functools.partial ( fn , test_arg2=456 ) double_wrapped_fn = functools.partial ( wrapped_fn , 123 )
__label__0 class testvisitor ( object ) :
__label__0 def testpicklelazyloader ( self ) : name = pickletest.__module__ # try to pickle current module . lazy_loader_module = lazy_loader.lazyloader ( `` lazy_loader_module '' , globals ( ) , name ) restored = pickle.loads ( pickle.dumps ( lazy_loader_module ) ) self.assertequal ( restored.__name__ , name ) self.assertisnotnone ( restored.pickletest )
__label__0 this implementation of rmsprop uses plain momentum , not nesterov momentum .
__label__0 if out_file and processed_file : out_file.write ( new_file_content )
__label__0 `` ` python input_tree = [ [ [ 2 , 2 ] , [ 3 , 3 ] ] , [ [ 4 , 9 ] , [ 5 , 5 ] ] ] shallow_tree = [ [ true , true ] , [ false , true ] ]
__label__0 self.assertequal ( original_dict , cluster_spec.as_dict ( ) )
__label__0 args : fetches : exactly like the 'fetches ' argument to session.run ( ) . can be a single tensor or op , a list of 'fetches ' or a dictionary of fetches . for example : fetches = global_step_tensor fetches = [ train_op , summary_op , global_step_tensor ] fetches = { 'step ' : global_step_tensor , 'summ ' : summary_op } note that this can recurse as expected : fetches = { 'step ' : global_step_tensor , 'ops ' : [ train_op , check_nan_op ] } feed_dict : exactly like the ` feed_dict ` argument to ` session.run ( ) ` options : exactly like the ` options ` argument to ` session.run ( ) ` , i.e. , a config_pb2.runoptions proto. `` '' ''
__label__0 raises : ` tf.errors.outofrangeerror ` : if the end of the iterator has been reached. `` '' '' raise notimplementederror ( `` distributediterator.get_next ( ) must be implemented in descendants . '' )
__label__0 class _typebaseddispatcher ( opdispatcher ) : `` '' '' dispatcher that handles op if any arguments have a specified type .
__label__0 module2.py : `` ` python import module1
__label__0 @ mock.patch.object ( generate2 , 'tf ' , fake_tf ) def test_end_to_end ( self ) : generate2.min_num_files_expected = 1 output_dir = pathlib.path ( googletest.gettempdir ( ) ) /'output ' if os.path.exists ( output_dir ) : shutil.rmtree ( output_dir ) os.makedirs ( output_dir ) generate2.build_docs ( output_dir=output_dir , code_url_prefix= '' , search_hints=true , )
__label__0 import json
__label__0 return ( process_test_logs ( test_adjusted_name , test_name=test_name , test_args=test_args , benchmark_type=benchmark_type , start_time=int ( start_time ) , run_time=run_time , log_files=log_files ) , test_adjusted_name )
__label__0 missing_dependencies = [ ] # file extensions and endings to ignore ignore_extensions = [ `` _test '' , `` _test.py '' , `` _test_cpu '' , `` _test_cpu.py '' , `` _test_gpu '' , `` _test_gpu.py '' , `` _test_lib '' ]
__label__0 # make sure output directory does n't exist if output_root_directory and os.path.exists ( output_root_directory ) : print ( `` output directory % r must not already exist . '' % ( output_root_directory ) ) sys.exit ( 1 )
__label__0 unlike normal calls , ` get_concrete_function ` allow type specifiers instead of tensorflow objects , so for example ` tf.tensor ` s may be replaced with ` tf.tensorspec ` s .
__label__0 def testduplicatedispatchforbinaryelementwiseapiserror ( self ) :
__label__0 # verifies that collection with unsupported key will not be added . ops_lib.add_to_collection ( save , 3 ) save._add_collection_def ( meta_graph_def , save ) self.assertequal ( len ( meta_graph_def.collection_def ) , 0 )
__label__0 @ dispatch.dispatch_for_api ( math_ops.abs ) def silly_abs ( x : sillytensor ) : del x
__label__0 args : doc : the original docstring . instructions : a string , describing how to fix the problem . no_doc_str : the default value to use for ` doc ` if ` doc ` is empty . suffix_str : is added to the end of the first line . notice : a list of strings . the main notice warning body . notice_type : the type of notice to use . should be one of ` [ caution , deprecated , important , note , warning ] `
__label__0 sess_1.close ( ) sess_2.close ( ) # todo ( mrry ) : add ` server.stop ( ) ` and ` server.join ( ) ` when these work .
__label__0 _data_types = [ dtypes.half , dtypes.float32 ]
__label__0 def testunsortedsegmentsum1dindices2ddatanondisjoint ( self ) : for dtype in self.numeric_types : data = np.array ( [ [ 0 , 1 , 2 , 3 ] , [ 20 , 21 , 22 , 23 ] , [ 30 , 31 , 32 , 33 ] , [ 40 , 41 , 42 , 43 ] , [ 50 , 51 , 52 , 53 ] ] , dtype=dtype ) indices = np.array ( [ 0 , 1 , 2 , 0 , 1 ] , dtype=np.int32 ) num_segments = 4 y = self._unsortedsegmentsum ( data , indices , num_segments ) self.assertallclose ( np.array ( [ [ 40 , 42 , 44 , 46 ] , [ 70 , 72 , 74 , 76 ] , [ 30 , 31 , 32 , 33 ] , [ 0 , 0 , 0 , 0 ] ] , dtype=dtype ) , y )
__label__0 raises : assertionerror : if last_checkpoints_with_time is not a list. `` '' '' assert isinstance ( last_checkpoints_with_time , list ) self._last_checkpoints = last_checkpoints_with_time
__label__0 def set_deprecated ( obj : t ) - > t : `` '' '' explicitly tag an object as deprecated for the doc generator . '' '' '' setattr ( obj , _deprecated , none ) return obj
__label__0 the coordinator can be useful if you want to run multiple threads during your training .
__label__0 # pylint : disable=g-import-not-at-top try : from shutil import which except importerror : from distutils.spawn import find_executable as which # pylint : enable=g-import-not-at-top
__label__0 returns : a list of file names relative to the given directory path. `` '' '' files = [ f for f in os.listdir ( dirpath ) if is_real_file ( dirpath , f ) ] return sorted ( files , key=lambda f : get_mtime ( dirpath , f ) )
__label__0 if vocabless_vars : checkpoint_utils.init_from_checkpoint ( ckpt_to_initialize_from , vocabless_vars ) prev_var_name_not_used = set ( var_name_to_prev_var_name.keys ( ) ) - prev_var_name_used vocab_info_not_used = set ( var_name_to_vocab_info.keys ( ) ) - vocab_info_used
__label__0 if not self._logdir : logging.warning ( `` standard services need a 'logdir ' `` `` passed to the sessionmanager '' ) return
__label__0 # the use of while_loop.while_loop here is purely for adding test # coverage the save and restore of control flow context ( which does n't # make any sense here from a machine learning perspective ) . the typical # biases is a simple variable without the conditions . def loop_cond ( it , _ ) : return it < 2
__label__0 try : x = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) y = maskedtensor ( [ 1 , 1 , 1 ] , [ 1 , 1 , 0 ] ) z = array_ops.concat ( [ x , y ] , axis=0 ) self.assertallequal ( z.values , array_ops.concat ( [ x.values , y.values ] , 0 ) ) self.assertallequal ( z.mask , array_ops.concat ( [ x.mask , y.mask ] , 0 ) )
__label__0 class _weakobjectidentitywrapper ( _objectidentitywrapper ) :
__label__0 example use :
__label__0 # additional bits we receive from bazel test_results.build_configuration.copyfrom ( gather_build_configuration ( ) ) # add os.environ data to test_results . test_results.run_configuration.env_vars.update ( os.environ )
__label__0 def testlargeconstant ( self ) : sizes = [ 50 , 50 , 1000 , 50 , 1000 ] max_size = 500 constants.debug_set_max_size ( max_size )
__label__0 with self.assertraisesregexp ( valueerror , r '' dispatch function 's signature \ ( self , other , name=none\ ) `` r '' does not match api 's signature \ ( self , other\ ) \ . `` ) :
__label__0 class compattest ( test.testcase ) :
__label__0 in the future we may use this object to add more information about result of run without changing the hook api .
__label__0 2. derive from tfdecorator . if your decorator needs to be stateful , you can implement it in terms of a tfdecorator . store whatever state you need in your derived class , and implement the ` __call__ ` method to do your work before calling into your target . you can retrieve the target via ` super ( mydecoratorclass , self ) .decorated_target ` , and call it with whatever parameters it needs .
__label__0 def __contains__ ( self , key ) : return self._wrap_key ( key ) in self._storage
__label__0 args : date : string or none . the date the function is scheduled to be removed . must be iso 8601 ( yyyy-mm-dd ) , or none instructions : string . instructions on how to update code using the deprecated function . warn_once : if ` true ` , warn only the first time this function is called with deprecated argument values . otherwise , every call ( with a deprecated argument value ) will log a warning . * * deprecated_kwargs : the deprecated argument values .
__label__0 def _maybe_add_module_deprecation_warning ( self , node , full_name , whole_name ) : `` '' '' adds a warning if full_name is a deprecated module . '' '' '' warnings = self._api_change_spec.module_deprecations if full_name in warnings : level , message = warnings [ full_name ] message = message.replace ( `` < function name > '' , whole_name ) self.add_log ( level , node.lineno , node.col_offset , `` using member % s in deprecated module % s . % s '' % ( whole_name , full_name , message ) ) return true else : return false
__label__0 tf.losses = tf.keras.losses tf.metrics = tf.keras.metrics tf.optimizers = tf.keras.optimizers tf.initializers = tf.keras.initializers
__label__0 def testdispatchforoptional ( self ) : # note : typing.optional [ x ] == typing.union [ x , nonetype ] .
__label__0 args : instance : an instance of ` tuple ` , ` list ` , ` namedtuple ` , ` dict ` , ` collections.ordereddict ` , or ` composite_tensor.composite_tensor ` or ` type_spec.typespec ` . args : items to be converted to the ` instance ` type .
__label__0 ` tracingcontext ` is a container class for flags and variables that have any kind of influence on the tracing behaviour of the class implementing the __tf_tracing_type__ . this context will be shared across all __tf_tracing_type__ calls while constructing the tracetype for a particular set of objects. `` '' ''
__label__0 def testimport ( self ) : # foo should be renamed to bar . text = `` import foo as f '' expected_text = `` import bar as f '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( partial_function ) )
__label__0 it specifies how to map imports and symbols to ` analysisresult ` s .
__label__0 # by default the name used for `` v2 '' will be `` v1 '' and raise an error . with self.assertraisesregex ( valueerror , `` same name : v1 '' ) : saver_module.saver ( [ v0 , v1 , v2 ] )
__label__0 from tensorflow.python.framework import dtypes from tensorflow.python.framework import tensor_shape from tensorflow.python.util.compat import collections_abc
__label__0 ` _patch_compile ` uses this on the final expression in each cell .
__label__0 class functiondefsplitter ( splitbasedonsize ) : `` '' '' splits the functiondef message type . '' '' ''
__label__0 args : git_base_path : where the .git directory is located git_tag_override : override the value for the git tag . this is useful for releases where we want to build the release before the git tag is created . returns : a bytestring representing the git version `` '' '' unknown_label = b '' unknown '' try : # force to bytes so this works on python 2 and python 3 val = bytes ( subprocess.check_output ( [ `` git '' , str ( `` -- git-dir= % s/.git '' % git_base_path ) , str ( `` -- work-tree= % s '' % git_base_path ) , `` describe '' , `` -- long '' , `` -- tags '' ] ) .strip ( ) ) version_separator = b '' - '' if git_tag_override and val : split_val = val.split ( version_separator ) if len ( split_val ) < 3 : raise exception ( ( `` expected git version in format 'tag-commits after tag-hash ' `` `` but got ' % s ' '' ) % val ) # there might be `` - '' in the tag name . but we can be sure that the final # two `` - '' are those inserted by the git describe command . abbrev_commit = split_val [ -1 ] val = version_separator.join ( [ bytes ( git_tag_override , `` utf-8 '' ) , b '' 0 '' , abbrev_commit ] ) return val if val else unknown_label except ( subprocess.calledprocesserror , oserror ) : return unknown_label
__label__0 config = config_pb2.configproto ( cluster_def=get_cluster_def ( master_new , worker ) ) sess_new = session.session ( master_new.target , config=config ) check_session_devices ( sess_new )
__label__0 sess = sm.wait_for_session ( server.target , max_wait_secs=3 ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` v:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( true , variable_v1.is_variable_initialized ( sess.graph.get_tensor_by_name ( `` w:0 '' ) ) .eval ( session=sess ) ) self.assertequal ( 1 , sess.run ( v ) ) self.assertequal ( 1 , sess.run ( w ) )
__label__0 structure = nesttest.unsortedsampleattr ( * values ) new_structure = nest.map_structure ( lambda x : x , structure ) self.assertequal ( structure , new_structure )
__label__0 in the training program , every worker will run the train_op as if not synchronized .
__label__0 def __dir__ ( self ) : if self._tfmw_public_apis : return list ( set ( self._tfmw_public_apis.keys ( ) ) .union ( set ( [ attr for attr in dir ( self._tfmw_wrapped_module ) if not attr.startswith ( ' _ ' ) ] ) ) ) else : return dir ( self._tfmw_wrapped_module )
__label__0 try : a = maskedtensor ( [ 1 , 2 , 3 , 4 , 5 ] , [ 1 , 0 , 1 , 1 , 1 ] ) b = constant_op.constant ( [ 10 , 20 , 30 , 40 , 50 ] ) c = [ 10 , 20 , 30 , 40 , 50 ] d = 50 e = none # as long as ` x ` is a maskedtensor , the dispatcher will be called # ( regardless of the type for ` y ` ) : self.assertallequal ( math_ops.add ( a , b ) .values , [ 11 , 22 , 33 , 44 , 55 ] ) self.assertallequal ( math_ops.add ( a , c ) .values , [ 11 , 22 , 33 , 44 , 55 ] ) self.assertallequal ( math_ops.add ( a , d ) .values , [ 51 , 52 , 53 , 54 , 55 ] ) self.assertallequal ( math_ops.add ( a , e ) .values , [ 1 , 2 , 3 , 4 , 5 ] )
__label__0 args : obj : the class-attribute to hide from the generated docs .
__label__0 > > > import dataclasses > > > @ dataclasses.dataclass ... class maskedtensor : ... mask : bool ... value : tf.tensor ... ... def __tf_flatten__ ( self ) : ... metadata = ( self.mask , ) # static config . ... components = ( self.value , ) # dynamic values . ... return metadata , components ... ... @ classmethod ... def __tf_unflatten__ ( cls , metadata , components ) : ... mask = metadata [ 0 ] ... value = components [ 0 ] ... return maskedtensor ( mask=mask , value=value ) ... > > > mt = maskedtensor ( mask=true , value=tf.constant ( [ 1 ] ) ) > > > mt maskedtensor ( mask=true , value= < tf.tensor : ... numpy=array ( [ 1 ] , dtype=int32 ) > ) > > > tf.nest.is_nested ( mt ) true > > > mt2 = maskedtensor ( mask=false , value=tf.constant ( [ 2 ] ) ) > > > tf.nest.assert_same_structure ( mt , mt2 )
__label__0 return super ( vocabinfo , cls ) .__new__ ( cls , new_vocab , new_vocab_size , num_oov_buckets , old_vocab , old_vocab_size , backup_initializer , axis , )
__label__0 return rocfft_config
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # pylint : disable=line-too-long `` '' '' list of renames to apply when converting from tf 1.0 to tf 2.0 .
__label__0 fixed_config.dtype = dense_types [ i ] .as_datatype_enum # get the output tensor name . fixed_config.values_output_tensor_name = parse_example_op.outputs [ dense_values_start + i ] .name
__label__0 def _add_summary_step_transformer ( parent , node , full_name , name , logs ) : `` '' '' adds a step argument to the summary api call if not specified .
__label__0 for dirpath , dirnames , filenames in os.walk ( base_dir , followlinks=true ) : lowercase_directories = [ x.lower ( ) for x in dirnames ] lowercase_files = [ x.lower ( ) for x in filenames ]
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) apply_ftrl = ( gen_training_ops.apply_ftrl ( var , accum , linear , grad , lr , l1 , l2 , lr_power , multiply_linear_by_lr=true ) ) out = self.evaluate ( apply_ftrl ) self.assertshapeequal ( out , apply_ftrl ) accum_update = y + grad * grad linear_update = z + grad * lr - ( accum_update * * ( -lr_power ) - y * * ( -lr_power ) ) * x quadratic = accum_update * * ( -lr_power ) + 2 * l2 * lr expected_out = np.array ( [ ( np.sign ( linear_update [ i ] ) * l1 * lr - linear_update [ i ] ) / ( quadratic [ i ] ) if np.abs ( linear_update [ i ] ) > l1 * lr else 0.0 for i in range ( linear_update.size ) ] ) self.assertallcloseaccordingtotype ( accum_update , self.evaluate ( accum ) ) if x.dtype == np.float16 : # the calculations here really are not very precise in float16 . self.assertallclose ( linear_update , self.evaluate ( linear ) , rtol=2e-2 , atol=2e-2 ) self.assertallclose ( expected_out , out , rtol=2e-2 , atol=2e-2 ) elif x.dtype == np.float32 : # the calculations here not sufficiently precise in float32 . self.assertallclose ( linear_update , self.evaluate ( linear ) , rtol=1e-5 , atol=1e-5 ) self.assertallclose ( expected_out , out , rtol=1e-5 , atol=1e-5 ) else : self.assertallclose ( linear_update , self.evaluate ( linear ) ) self.assertallclose ( expected_out , out )
__label__0 def testlargeconstant ( self ) : server = self._cached_server with session.session ( server.target , config=self._userpcconfig ( ) ) as sess : const_val = np.empty ( [ 10000 , 3000 ] , dtype=np.float32 ) const_val.fill ( 0.5 ) c = constant_op.constant ( const_val ) shape_t = array_ops.shape ( c ) self.assertallequal ( [ 10000 , 3000 ] , sess.run ( shape_t ) )
__label__0 logs.append ( ( ast_edits.error , node.lineno , node.col_offset , `` name_scope call with neither name nor default_name can not be `` `` converted properly . '' ) )
__label__0 checker = make_type_checker ( param_type ) index = param_names.index ( param_name ) checkers.append ( ( index , checker ) )
__label__0 from absl.testing import absltest from absl.testing import parameterized
__label__0 def _get_rocm_install_path ( ) : `` '' '' determines and returns the rocm installation path . '' '' '' rocm_install_path = _get_default_rocm_path ( ) if `` rocm_path '' in os.environ : rocm_install_path = os.environ [ `` rocm_path '' ] # rocm_install_path = os.path.realpath ( rocm_install_path ) return rocm_install_path
__label__0 import os
__label__0 args : global_step_tensor : ` tensor ` to test. `` '' '' if not ( isinstance ( global_step_tensor , variables.variable ) or isinstance ( global_step_tensor , tensor.tensor ) or resource_variable_ops.is_resource_variable ( global_step_tensor ) ) : raise typeerror ( 'existing `` global_step '' must be a variable or tensor : % s . ' % global_step_tensor )
__label__0 def difference ( self , items ) : return objectidentityset._from_storage ( self._storage.difference ( [ self._wrap_key ( item ) for item in items ] ) )
__label__0 def _segmentprodv2 ( self , data , indices , num_segments ) : return self._segmentreduction ( math_ops.segment_prod_v2 , data , indices , num_segments )
__label__0 # if the union contains two or more simple types , then use a single # instancechecker to check them . simple_types = [ t for t in type_args if isinstance ( t , type ) ] simple_types = tuple ( sorted ( simple_types , key=id ) ) if len ( simple_types ) > 1 : if simple_types not in _is_instance_checker_cache : checker = _api_dispatcher.makeinstancechecker ( * simple_types ) _is_instance_checker_cache [ simple_types ] = checker options = ( [ _is_instance_checker_cache [ simple_types ] ] + [ make_type_checker ( t ) for t in type_args if not isinstance ( t , type ) ] ) return _api_dispatcher.makeunionchecker ( options )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 def rewrite_argument_docstring ( old_doc , old_argument , new_argument ) : return old_doc.replace ( ' ` % s ` ' % old_argument , ' ` % s ` ' % new_argument ) .replace ( ' % s : ' % old_argument , ' % s : ' % new_argument )
__label__0 class tftestcase ( tf.test.testcase ) :
__label__0 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # dispatch support # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # @ tf_export ( `` __internal__.dispatch.add_dispatch_support '' , v1= [ ] ) def add_dispatch_support ( target=none , iterable_parameters=none ) : `` '' '' decorator that adds a dispatch handling wrapper to a tensorflow python api .
__label__0 self._validate_symbol_names ( )
__label__0 def tuple_path_sum ( tuple_path , * tensors ) : return ( tuple_path , sum ( tensors ) )
__label__0 # check that the parent graphdef contains empty nodes where the large nodes # were originally . self.assertequal ( 0 , chunks [ 0 ] .node [ 1 ] .bytesize ( ) ) self.assertequal ( 0 , chunks [ 0 ] .node [ 2 ] .bytesize ( ) ) self.assertequal ( 0 , chunks [ 0 ] .node [ 3 ] .bytesize ( ) ) self.assertequal ( 0 , chunks [ 0 ] .node [ 5 ] .bytesize ( ) )
__label__0 return deprecated_wrapper
__label__0 usage : in case a simplified ` str ` version of the path is needed from an ` os.pathlike ` object .
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( arg0=1 , arg1=2 , deprecated=2 ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 def main ( ) : atheris.setup ( sys.argv , testoneinput , enable_python_coverage=true ) atheris.fuzz ( )
__label__0 # run 4 steps of rmsprop for _ in range ( 1 , 5 ) : self.evaluate ( update )
__label__0 self.assertlen ( splitter.split ( ) [ 0 ] , 5 ) # adds 3 chunks .
__label__0 class raggedtensor ( object ) : `` '' '' interface for internal isinstance checks to ops/ragged/ragged_tensor.py .
__label__0 def f ( a , b , kw2 , kw1 ) : ...
__label__0 the returned ` concretefunction ` can be called normally :
__label__0 import sys as _sys import importlib as _importlib import types as _types
__label__0 def testcreatezerosslotfromdynamicshapedtensor ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = random_ops.random_uniform ( [ 2 ] , dtype=dtypes.float64 ) v = array_ops.placeholder_with_default ( v , shape= [ none ] , name= '' const '' ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 )
__label__0 def _image_resize_transformer ( parent , node , full_name , name , logs ) : `` '' '' transforms image.resize_ * to image.resize ( ... , method= * , ... ) . '' '' '' resize_method = name [ 7 : ] .upper ( ) new_arg = ast.keyword ( arg= '' method '' , value=ast.attribute ( value=ast.attribute ( value=ast.attribute ( value=ast.name ( id= '' tf '' , ctx=ast.load ( ) ) , attr= '' image '' , ctx=ast.load ( ) ) , attr= '' resizemethod '' , ctx=ast.load ( ) ) , attr=resize_method , ctx=ast.load ( ) ) )
__label__1 def calculate_perimeter_of_rectangle ( length , width ) : return 2 * ( length + width )
__label__0 @ test_decorator_increment_first_int_arg def return_params ( self , a , b , c ) : `` '' '' return parameters . '' '' '' return [ a , b , c ]
__label__0 @ parameterized.parameters ( [ ' < ... > ' , ( ' < ... > ' , false ) ] , [ 'tensorflow ' , ( 'tensorflow ' , false ) ] , [ 'tf.variable ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ' , ( 'tf.variable ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ' , false ) ] , [ ' < tf.tensor : shape= ( ) , dtype=float32 , numpy=inf > ' , ( 'inf ' , true ) ] , [ ' < tf.raggedtensor : ... shape= ( 2 , 2 ) , numpy=1 > ' , ( ' < tf.raggedtensor : ... shape= ( 2 , 2 ) , numpy=1 > ' , false ) ] , [ `` '' '' < tf.tensor : shape= ( 2 , 2 ) , dtype=int32 , numpy= array ( [ [ 2 , 2 ] , [ 3 , 5 ] ] , dtype=int32 ) > '' '' '' , ( '\n array ( [ [ 2 , 2 ] , \n [ 3 , 5 ] ] , ' 'dtype=int32 ) ' , true ) ] , [ ' [ < tf.tensor : shape= ( 2 , ) , dtype=int32 , numpy=array ( [ 1 , 2 ] , ' 'dtype=int32 ) > , ' ' < tf.tensor : shape= ( 2 , ) , dtype=int32 , numpy=array ( [ 3 , 4 ] , ' 'dtype=int32 ) > ] ' , ( ' [ array ( [ 1 , 2 ] , dtype=int32 ) , array ( [ 3 , 4 ] , dtype=int32 ) ] ' , true ) ] , ) def test_tf_tensor_numpy_output ( self , string , expected_output ) : output_checker = tf_doctest_lib.tfdoctestoutputchecker ( ) output = output_checker._tf_tensor_numpy_output ( string ) self.assertequal ( expected_output , output )
__label__0 no docs will generated for this class attribute in sub-classes .
__label__0 ` prepare_session ( ) ` initializes or restores a model . it requires ` init_op ` and ` saver ` as an argument .
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 _test_param_values = [ # learning_rate , decay , momentum , epsilon , centered , use_resource [ 0.5 , 0.9 , 0.0 , 1e-3 , true , false ] , [ 0.5 , 0.9 , 0.0 , 1e-3 , false , false ] , [ 0.5 , 0.9 , 0.0 , 1e-3 , true , true ] , [ 0.5 , 0.9 , 0.0 , 1e-3 , false , true ] , [ 0.1 , 0.9 , 0.0 , 1e-3 , true , false ] , [ 0.5 , 0.95 , 0.0 , 1e-3 , false , false ] , [ 0.5 , 0.95 , 0.0 , 1e-5 , true , false ] , [ 0.5 , 0.95 , 0.9 , 1e-5 , true , false ] , ]
__label__0 > > > with g.as_default ( ) : ... print ( sess.run ( tf.compat.v1.train.get_global_step ( ) ) ) 1
__label__0 if hasattr ( _inspect , 'argspec ' ) : argspec = _inspect.argspec else : argspec = collections.namedtuple ( 'argspec ' , [ 'args ' , 'varargs ' , 'keywords ' , 'defaults ' , ] , )
__label__0 return sorted ( new_children , key=lambda x : x [ 0 ] )
__label__0 from collections.abc import sequence import itertools from typing import optional , type
__label__0 def testinteractivesession ( self ) : server = self._cached_server # session creation will warn ( in c++ ) that the place_pruned_graph option # is not supported , but it should successfully ignore it . sess = session.interactivesession ( server.target ) c = constant_op.constant ( 42.0 ) self.assertequal ( 42.0 , self.evaluate ( c ) ) sess.close ( )
__label__0 args : module : tensorflow module .
__label__0 class _pastaeditvisitor ( ast.nodevisitor ) : `` '' '' ast visitor that processes function calls .
__label__0 def testnonemptyclusterspecistrue ( self ) : self.asserttrue ( server_lib.clusterspec ( { `` job '' : [ `` host : port '' ] } ) )
__label__0 for sequences , the key will be an int , the array index of a value . for mappings , the key will be the dictionary key . for objects ( e.g . namedtuples ) , the key will be the attribute name .
__label__0 def testunboundfuncwithtwoparamsdefaulttwopositional ( self ) :
__label__0 def testunboundfuncwithtwoparamsdefaultonekeywordsecond ( self ) :
__label__0 def testmanagedsessionkeepsummarywriter ( self ) : logdir = self._test_dir ( `` managed_keep_summary_writer '' ) with ops.graph ( ) .as_default ( ) : summary.scalar ( `` c1 '' , constant_op.constant ( 1 ) ) summary.scalar ( `` c2 '' , constant_op.constant ( 2 ) ) summary.scalar ( `` c3 '' , constant_op.constant ( 3 ) ) summ = summary.merge_all ( ) sv = supervisor.supervisor ( logdir=logdir ) with sv.managed_session ( `` '' , close_summary_writer=false , start_standard_services=false ) as sess : sv.summary_computed ( sess , sess.run ( summ ) ) with sv.managed_session ( `` '' , close_summary_writer=false , start_standard_services=false ) as sess : sv.summary_computed ( sess , sess.run ( summ ) ) # now close the summary writer to flush the events . sv.summary_writer.close ( ) # the summary iterator should report the summary twice as we reused # the same summary writer across the 2 sessions . rr = _summary_iterator ( logdir ) # the first event should list the file_version . ev = next ( rr ) self.assertequal ( `` brain.event:2 '' , ev.file_version )
__label__0 self.add_log ( info , node.lineno , node.col_offset , `` changed import from % r to % r % s . '' % ( pasta.dump ( node ) , pasta.dump ( updated_node ) , additional_import_log ) )
__label__0 # linearly iterate through all nodes . it may be possible to optimize this # further by making best guesses as to where to split the nodes , since # most nodes ( aside from constants ) are relatively small . for n , ele in enumerate ( field ) : size = ele.bytesize ( )
__label__0 # verifies round trip from proto- > spec- > proto is correct . cluster_spec = server_lib.clusterspec ( cluster_def ) self.assertprotoequals ( cluster_def , cluster_spec.as_cluster_def ( ) )
__label__0 def restore ( self , restored_tensors , restored_shapes ) : `` '' '' restore the same value into both variables . '' '' '' tensor , = restored_tensors return control_flow_ops.group ( self._primary_variable.assign ( tensor ) , self._mirrored_variable.assign ( tensor ) )
__label__0 def _extract_name_arg ( args , kwargs , name_index ) : `` '' '' extracts the parameter ` name ` and returns ` ( args , kwargs , name_value ) ` . '' '' '' if name_index < 0 : name_value = none elif name_index < len ( args ) : name_value = args [ name_index ] args = args [ : name_index ] + args [ name_index + 1 : ] else : name_value = kwargs.pop ( `` name '' , none ) return args , kwargs , name_value
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' opensource base_dir configuration for tensorflow doc-generator . '' '' '' import pathlib
__label__0 using ` get_global_step ` :
__label__0 returns : decorated function or method .
__label__0 args : func : the function that implements the binary elementwise assert api .
__label__0 this is in anticipation of them being renamed to tf.summary.record_if ( ) , which requires the cond argument. `` '' '' node.args.append ( pasta.parse ( cond ) ) logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` adding ` % s ` argument to % s in anticipation of it being renamed to `` `` tf.compat.v2.summary.record_if ( ) '' % ( cond , full_name or name ) ) ) return node
__label__0 def get_float ( self , min_float=_min_float , max_float=_max_float ) : `` '' '' consume a float with given constraints .
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf upgrader . '' '' ''
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__1 def calculate_average ( lst ) : return sum ( lst ) / len ( lst )
__label__0 def testiterators ( self ) : for ( text , expected ) in [ ( `` ( expr + yielding ( data ) ) .make_one_shot_iterator ( ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( ( expr + yielding ( data ) ) ) '' ) , ( `` dataset.make_one_shot_iterator ( ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset ) '' ) , ( `` dataset.make_one_shot_iterator ( shared_name=foo ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , shared_name=foo ) '' ) , ( `` dataset.make_one_shot_iterator ( x , y , z ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , x , y , z ) '' ) , ( `` dataset.make_initializable_iterator ( ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset ) '' ) , ( `` ds.make_initializable_iterator ( shared_name=foo ) '' , `` tf.compat.v1.data.make_initializable_iterator ( ds , shared_name=foo ) '' ) , ( `` dataset.make_initializable_iterator ( x , y , z ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset , x , y , z ) '' ) , ( `` tf.data.make_one_shot_iterator ( dataset ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset ) '' ) , ( `` tf.data.make_one_shot_iterator ( dataset , shared_name=foo ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , shared_name=foo ) '' ) , ( `` tf.data.make_one_shot_iterator ( dataset , x , y , z ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , x , y , z ) '' ) , ( `` tf.data.make_initializable_iterator ( dataset ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset ) '' ) , ( `` tf.data.make_initializable_iterator ( ds , shared_name=foo ) '' , `` tf.compat.v1.data.make_initializable_iterator ( ds , shared_name=foo ) '' ) , ( `` tf.data.make_initializable_iterator ( dataset , x , y , z ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset , x , y , z ) '' ) , ( `` tf.compat.v1.data.make_one_shot_iterator ( dataset ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset ) '' ) , ( `` tf.compat.v1.data.make_one_shot_iterator ( dataset , shared_name=foo ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , shared_name=foo ) '' ) , ( `` tf.compat.v1.data.make_one_shot_iterator ( dataset , x , y , z ) '' , `` tf.compat.v1.data.make_one_shot_iterator ( dataset , x , y , z ) '' ) , ( `` tf.compat.v1.data.make_initializable_iterator ( dataset ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset ) '' ) , ( `` tf.compat.v1.data.make_initializable_iterator ( ds , shared_name=foo ) '' , `` tf.compat.v1.data.make_initializable_iterator ( ds , shared_name=foo ) '' ) , ( `` tf.compat.v1.data.make_initializable_iterator ( dataset , x , y , z ) '' , `` tf.compat.v1.data.make_initializable_iterator ( dataset , x , y , z ) '' ) ] : _ , unused_report , unused_errors , actual = self._upgrade ( text ) self.assertequal ( actual , expected )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tfdecorator-aware replacements for the contextlib module . '' '' '' from collections.abc import callable , iterator import contextlib as _contextlib
__label__0 contrib_summary_record_every_n_comment = ( ast_edits.error , `` ( manual edit required ) `` `` tf.contrib.summary.record_summaries_every_n_global_steps ( n , step ) `` `` should be replaced by a call to tf.compat.v2.summary.record_if ( ) with `` `` the argument ` lambda : tf.math.equal ( 0 , global_step % n ) ` ( or in graph `` `` mode , the lambda body can be used directly ) . if no global step was `` `` passed , instead use tf.compat.v1.train.get_or_create_global_step ( ) . '' )
__label__0 def get_v2_names ( symbol : any ) - > sequence [ str ] : `` '' '' get a list of tf 2.0 names for this symbol .
__label__0 else : raise valueerror ( f '' type annotation { annotation } is not currently supported '' `` by dispatch . supported annotations : type objects , `` `` list [ ... ] , and union [ ... ] '' )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_static_fn_no_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 ` contrib ` , and ` ffmpeg ` are examples of modules that are large and not always needed , and this allows them to only be loaded when they are used. `` '' ''
__label__0 sharing_config = config_pb2.configproto ( isolate_session_state=false ) sharing_sess_0 = session.session ( server.target , config=sharing_config ) sharing_sess_1 = session.session ( server.target , config=sharing_config )
__label__0 class _customlist ( list ) : pass
__label__0 class removemultiplekeywordarguments ( ast_edits.noupdatespec ) : `` '' '' a specification where both keyword aliases are removed from h .
__label__0 > > > structure = { `` key3 '' : `` '' , `` key1 '' : `` '' , `` key2 '' : `` '' } > > > flat_sequence = [ `` value1 '' , `` value2 '' , `` value3 '' ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) { 'key3 ' : 'value3 ' , 'key1 ' : 'value1 ' , 'key2 ' : 'value2 ' }
__label__0 def test_contrib_summary_audio_nostep ( self ) : text = `` tf.contrib.summary.audio ( 'foo ' , myval , 44100 ) '' expected = ( `` tf.compat.v2.summary.audio ( name='foo ' , data=myval , `` `` sample_rate=44100 , `` `` step=tf.compat.v1.train.get_or_create_global_step ( ) ) '' ) _ , _ , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) self.assertin ( `` 'step ' argument '' , errors [ 0 ] ) self.assertin ( `` tf.compat.v2.summary . * '' , errors [ 1 ] )
__label__0 # assert that the variables are not initialized . self.assertequal ( len ( variables.report_uninitialized_variables ( ) .eval ( ) ) , 2 ) self.assertequal ( 0 , len ( self.evaluate ( v2.keys ( ) ) ) ) self.assertequal ( 0 , len ( self.evaluate ( v2.values ( ) ) ) )
__label__0 return node
__label__0 this function validates that all variables in the variables list are remapped in the object-based checkpoint ( or ` names_to_keys ` dict if provided ) . a saver will be created with the list of remapped variables .
__label__0 import abc import inspect import sys import textwrap from typing import union
__label__0 parsed_proto = test_message_pb2.repeatedstring ( ) with open ( expected_file_path , `` rb '' ) as f : parsed_proto.parsefromstring ( f.read ( ) ) self.assertprotoequals ( proto , parsed_proto )
__label__0 def after_run ( self , run_context , # pylint : disable=unused-argument run_values ) : # pylint : disable=unused-argument `` '' '' called after each call to run ( ) .
__label__0 > > > int_feature = tf.train.feature ( ... int64_list=tf.train.int64list ( value= [ 1 , 2 , 3 , 4 ] ) ) > > > float_feature = tf.train.feature ( ... float_list=tf.train.floatlist ( value= [ 1. , 2. , 3. , 4 . ] ) ) > > > bytes_feature = tf.train.feature ( ... bytes_list=tf.train.byteslist ( value= [ b '' abc '' , b '' 1234 '' ] ) ) > > > > > > example = tf.train.example ( ... features=tf.train.features ( feature= { ... 'my_ints ' : int_feature , ... 'my_floats ' : float_feature , ... 'my_bytes ' : bytes_feature , ... } ) )
__label__0 `` ` python x = tf.variable ( [ 1,2,3 ] , dtype=tf.float32 ) grad = tf.constant ( [ 0.1 , 0.2 , 0.3 ] ) optimizer = tf.keras.optimizers.rmsprop ( learning_rate=0.001 ) optimizer.apply_gradients ( zip ( [ grad ] , [ x ] ) ) `` `
__label__0 lineno = node.func.value.lineno col_offset = node.func.value.col_offset node.func.value = ast_edits.full_name_node ( `` tf.compat.v1.keras.initializers '' ) node.func.value.lineno = lineno node.func.value.col_offset = col_offset node.func.attr = `` variancescaling ''
__label__0 this wraps ` variables ( ) ` from the actual optimizer . it does not include the ` syncreplicasoptimizer ` 's local step .
__label__0 # this assertion is expected to pass : two list-types with same number # of fields are considered identical . inp_shallow = _customlist ( [ 1 , 2 ] ) inp_deep = [ 1 , 2 ] nest.assert_shallow_structure ( inp_shallow , inp_deep , check_types=false ) nest.assert_shallow_structure ( inp_shallow , inp_deep , check_types=true )
__label__0 def testsegmentprod ( self ) : for dtype in self.numeric_types : self.assertallclose ( np.array ( [ 0 , 1 , 2 , 60 ] , dtype=dtype ) , self._segmentprodv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 whether you are running on your machine or in the cluster you can use the following values for the -- master flag :
__label__0 @ dispatch_for_api ( api , { x_name : x_type , y_name : y_type } ) def dispatch_target ( * args , * * kwargs ) : args , kwargs , name = _extract_name_arg ( args , kwargs , name_index ) if len ( args ) > 1 : x , y , args = args [ 0 ] , args [ 1 ] , args [ 2 : ] elif args : x , args = args [ 0 ] , args [ 1 : ] y = kwargs.pop ( y_name , none ) else : x = kwargs.pop ( x_name , none ) y = kwargs.pop ( y_name , none )
__label__0 def fn_has_kwargs ( test_arg , * * x ) : if test_arg ! = expected_test_arg : return valueerror ( 'partial fn does not work correctly ' ) return x
__label__0 args = parser.parse_args ( )
__label__0 _typebaseddispatcher ( get_compatible_func ( op , func ) , types ) .register ( op ) return func
__label__0 from tensorflow.python.util.tf_export import tf_export from tensorflow.tools.docs import doc_controls
__label__0 the main interface for the type-based dispatch system is the ` dispatch_for_api ` decorator , which overrides the default implementation for a tensorflow api . the decorated function ( known as the `` dispatch target '' ) will override the default implementation for the api when the api is called with parameters that match a specified type signature .
__label__0 def deprecated_wrapper ( func ) : # pylint : disable=protected-access if '_tf_deprecated_api_names ' in func.__dict__ : raise deprecatednamesalreadyseterror ( f' can not set deprecated names for { func.__name__ } to { args } . ' 'deprecated names are already set to ' f ' { func._tf_deprecated_api_names } . ' ) func._tf_deprecated_api_names = args # pylint : disable=protected-access return func
__label__0 def _find_library ( base_paths , library_name , required_version ) : `` '' '' returns first valid path to the requested library . '' '' '' if _is_windows ( ) : filepattern = library_name + `` .lib '' elif _is_macos ( ) : filepattern = `` % s * .dylib '' % ( `` . `` .join ( [ `` lib '' + library_name ] + required_version.split ( `` . `` ) [ :1 ] ) ) else : filepattern = `` . `` .join ( [ `` lib '' + library_name , `` so '' ] + required_version.split ( `` . `` ) [ :1 ] ) + `` * '' return _find_file ( base_paths , _library_paths ( ) , filepattern )
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 source = textwrap.dedent ( groups [ 'doctest ' ] ) want = groups [ 'output ' ] if want is not none : want = textwrap.dedent ( want )
__label__0 # the wildcard text should match the input text , according to the # outputchecker base class . try : self.asserttrue ( doctest.outputchecker ( ) .check_output ( want=text_with_wildcards , got=text , optionflags=doctest.ellipsis ) ) except assertionerror as e : msg = '\n\n expected : { } \n found : { } '.format ( text_with_wildcards , text ) e.args = ( e.args [ 0 ] + msg , ) raise e
__label__0 if version.parse ( tf.__version__ ) > = version.parse ( `` 2.14-dev '' ) : from tensorflow.python.util.pywrap_xla_ops import get_gpu_kernel_names # pylint : disable=g-import-not-at-top
__label__0 _ , unwrapped = tf_decorator.unwrap ( target ) target_argspec = tf_inspect.getargspec ( unwrapped ) if target_argspec.varargs or target_argspec.keywords : # @ todo ( b/194903203 ) add v2 dispatch support for apis that take varargs # and keywords . examples of apis that take varargs and kwargs : meshgrid , # einsum , map_values , map_flat_values . return target
__label__0 @ property def results ( self ) : return self._results
__label__0 # shared sessions will see each other 's updates , but isolated sessions # will not . sharing_sess_0.run ( v.initializer , feed_dict= { init_value : 86 } ) self.assertallequal ( 86 , sharing_sess_0.run ( v ) ) self.assertallequal ( 86 , sharing_sess_1.run ( v ) ) with self.assertraises ( errors_impl.failedpreconditionerror ) : isolate_sess_0.run ( v ) with self.assertraises ( errors_impl.failedpreconditionerror ) : isolate_sess_1.run ( v )
__label__0 code_url_prefixes = ( keras_url_prefix , # none - > do n't link to the generated keras api-module files . none , none , f '' https : //github.com/tensorflow/tensorboard/tree/ { tensorboard.__version__ } /tensorboard '' , code_url_prefix , )
__label__0 def __init__ ( self , * args , * * kwargs ) : super ( tfdoctestoutputchecker , self ) .__init__ ( * args , * * kwargs ) self.extract_floats = _floatextractor ( ) self.text_good = none self.float_size_good = none
__label__0 def testvariables ( self ) : save_path = os.path.join ( self.get_temp_dir ( ) , `` variables '' ) with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : one = variable_v1.variablev1 ( 1.0 ) twos = variable_v1.variablev1 ( [ 2.0 , 2.0 , 2.0 ] ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) init = variables.global_variables_initializer ( ) save = saver_module.saver ( ) init.run ( ) v2.insert ( `` k1 '' , 3.0 ) .run ( ) save.save ( sess , save_path )
__label__0 args = parser.parse_args ( )
__label__0 # if current import is # import foo # then new import should preserve imported name : # import new_foo as foo # this happens when module has just one component . new_asname = import_alias.asname if not new_asname and `` . '' not in import_alias.name : new_asname = import_alias.name
__label__0 def get_header_version ( path ) : version = ( _get_header_version ( path , name ) for name in ( `` cufft_ver_major '' , `` cufft_ver_minor '' , `` cufft_ver_patch '' ) ) return `` . `` .join ( version )
__label__0 def get_cluster_def ( master , worker ) : cluster_def = cluster_pb2.clusterdef ( ) job = cluster_def.job.add ( ) job.name = `` master '' job.tasks [ 0 ] = master.target [ len ( `` grpc : // '' ) : ] job = cluster_def.job.add ( ) job.name = `` worker '' job.tasks [ 0 ] = worker.target [ len ( `` grpc : // '' ) : ] return cluster_def
__label__0 def testpositionsmatcharggiven ( self ) : full_dict = tf_upgrade_v2.tfapichangespec ( ) .function_arg_warnings method_names = list ( full_dict.keys ( ) ) for method_name in method_names : args = list ( full_dict [ method_name ] .keys ( ) ) if `` contrib '' in method_name : # skip descending and fetching contrib methods during test . these are # not available in the repo anymore . continue elif method_name.startswith ( `` * . `` ) : # special case for optimizer methods method = method_name.replace ( `` * '' , `` tf.train.optimizer '' ) else : method = method_name
__label__0 call_saver_with_dict = false # updated by test loop below
__label__0 this should be used by tensorflow internal tests only . it explicitly defeats the encapsulation afforded by ` remove_undocumented ` .
__label__0 config = config_pb2.configproto ( cluster_def=get_cluster_def ( master_old , worker ) ) sess_old = session.session ( master_old.target , config=config ) check_session_devices ( sess_old )
__label__0 finally : # clean up dispatch table . dispatch.unregister_dispatch_for ( masked_add )
__label__0 in the * chief * task , the ` supervisor ` works exactly as in the first example above . in the other tasks ` sv.managed_session ( ) ` waits for the model to have been initialized before returning a session to the training code . the non-chief tasks depend on the chief task for initializing the model .
__label__0 def testcreatezerosslotfromvariable ( self ) : # slot_creator is used only in optimizer v1 . with ops.graph ( ) .as_default ( ) , self.cached_session ( ) : v = variables.variable ( [ 1.0 , 2.5 ] , name= '' var '' ) with ops.control_dependencies ( none ) : slot = slot_creator.create_zeros_slot ( v , name= '' slot '' , dtype=dtypes.float64 )
__label__0 > > > # unregister the dispatcher , so ` tf.abs ` no longer calls ` my_abs ` . > > > unregister_dispatch_for ( my_abs ) > > > tf.abs ( mytensor ( 5 ) ) traceback ( most recent call last ) : ... valueerror : attempt to convert a value ... to a tensor .
__label__0 if name is none : return elementwise_api_handler ( tensor_api , x ) else : with ops.name_scope ( name , none , [ x ] ) : return elementwise_api_handler ( tensor_api , x )
__label__0 deprecatednameforclass = deprecated_alias ( deprecated_name='module2.deprecatednameforclass ' , name='module1.newnameforclass ' , func_or_class=module1.newnameforclass ) `` `
__label__0 def testnoadditionalopsaddedbysaverforresourcevariablesoutsidesavescope ( self ) : with ops_lib.graph ( ) .as_default ( ) as g : v = resource_variable_ops.resourcevariable ( 1.0 , name= '' v '' ) with ops_lib.name_scope ( `` saver1 '' ) : saver_module.saver ( ) with ops_lib.name_scope ( `` saver2 '' ) : saver_module.saver ( { `` name '' : v } ) ops_in_saver1_scope_but_not_save_scope = [ op for op in g.get_operations ( ) if ( op.name.startswith ( `` saver1/ '' ) and not op.name.startswith ( `` saver1/save/ '' ) ) ] self.assertequal ( ops_in_saver1_scope_but_not_save_scope , [ ] ) ops_in_saver2_scope_but_not_save_scope = [ op for op in g.get_operations ( ) if ( op.name.startswith ( `` saver2/ '' ) and not op.name.startswith ( `` saver2/save/ '' ) ) ] self.assertequal ( ops_in_saver2_scope_but_not_save_scope , [ ] )
__label__0 # skip over the serialized input , and the names input . fetched = sess.run ( parse_example_op.inputs [ 2 : ] ) sparse_keys = fetched [ 0 ] .tolist ( ) dense_keys = fetched [ 1 ] .tolist ( ) ragged_keys = fetched [ 2 ] .tolist ( ) dense_defaults = fetched [ 3 : ] assert len ( sparse_keys ) == num_sparse assert len ( dense_keys ) == num_dense assert len ( ragged_keys ) == num_ragged
__label__0 def testgetfullargspeconpartialnoargumentsleft ( self ) : `` '' '' tests getfullargspec on partial function that prunes all arguments . '' '' ''
__label__0 # pylint : disable=undefined-variable
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' utilities to warm-start tf.learn estimators . '' '' ''
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for training_util . '' '' ''
__label__0 # check that the lengths match before doing anything else . try : self.assertlen ( extracted_floats , len ( expected_floats ) ) except assertionerror as e : msg = '\n\n expected : { } \n found : { } '.format ( expected_floats , extracted_floats ) e.args = ( e.args [ 0 ] + msg , ) raise e
__label__0 `` ` # tf.train.feature feature = union [ list [ bytes ] , list [ int64 ] , list [ float ] ]
__label__0 def testeagerbasic ( self ) : with context.eager_mode ( ) : ckpt_prefix = os.path.join ( self.get_temp_dir ( ) , `` ckpt '' )
__label__0 class checkpointedop : `` '' '' op with a custom checkpointing implementation .
__label__0 def testvariancescalinginitializer ( self ) : text = ( `` tf.contrib.layers.variance_scaling_initializer ( `` `` mode= ( \ '' fan\ '' + \ '' _avg\ '' ) ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.compat.v1.keras.initializers.variancescaling ( scale=2.0 , `` `` mode= ( \ '' fan\ '' + \ '' _avg\ '' ) .lower ( ) ) \n '' , )
__label__0 > > > global_batch_size = 4 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensors ( ( [ 1. ] , [ 1 . ] ) ) .repeat ( 4 ) .batch ( global_batch_size ) > > > dist_dataset = strategy.experimental_distribute_dataset ( dataset ) > > > @ tf.function ... def train_step ( input ) : ... features , labels = input ... return labels - 0.3 * features > > > for x in dist_dataset : ... # train_step trains the model using the dataset elements ... loss = strategy.run ( train_step , args= ( x , ) ) ... print ( `` loss is '' , loss ) loss is perreplica : { 0 : tf.tensor ( [ [ 0.7 ] [ 0.7 ] ] , shape= ( 2 , 1 ) , dtype=float32 ) , 1 : tf.tensor ( [ [ 0.7 ] [ 0.7 ] ] , shape= ( 2 , 1 ) , dtype=float32 ) }
__label__0 def testwithclassmethod ( self ) : self.assertequal ( 'getfuncnametest.testwithclassmethod ' , function_utils.get_func_name ( self.testwithclassmethod ) )
__label__0 all of the tests assume that we want to change from an api containing
__label__0 return ( processed_file , self._format_log ( log , in_filename , out_filename ) , process_errors )
__label__0 return node
__label__0 config.substitutions.extend ( [ ( `` % python '' , os.getenv ( `` python '' , sys.executable ) ) , ] )
__label__0 2. for a nested python tuple :
__label__0 import re
__label__0 decorator_func.__signature__ = signature
__label__0 @ tf_export ( v1= [ `` train.warm_start '' ] ) def warm_start ( ckpt_to_initialize_from , vars_to_warm_start= '' . * '' , var_name_to_vocab_info=none , var_name_to_prev_var_name=none ) : `` '' '' warm-starts a model using the given settings .
__label__0 self.assertisinstance ( mt_doubled , nestedmaskedtensor ) self.assertequal ( mt_doubled.mask , expected.mask ) self.assertequal ( mt_doubled.value.mask , expected.value.mask ) self.assertallequal ( mt_doubled.value.value , expected.value.value )
__label__0 fixed_config.dtype = dense_types [ i ] .as_datatype_enum # get the output tensor name . fixed_config.values_output_tensor_name = parse_example_op.outputs [ dense_values_start + i ] .name
__label__0 deprecated_cls = deprecation.deprecated_alias ( `` deprecated.cls '' , `` real.cls '' , myclass )
__label__0 # put parentheses around keep_prob.value ( and remove the old prefix/ # suffix , they should only be around new_value ) . pasta.base.formatting.set ( old_value , `` prefix '' , `` ( `` ) pasta.base.formatting.set ( old_value , `` suffix '' , `` ) '' )
__label__0 returns : structure mapped to replace all lists with tuples. `` '' '' def sequence_fn ( instance , args ) : if isinstance ( instance , list ) : return tuple ( args ) return nest_util.sequence_like ( instance , args )
__label__0 # create a first session and then stop . sess = sv.prepare_or_wait_for_session ( `` '' ) sv.stop ( ) sess.close ( ) self.asserttrue ( sv.should_stop ( ) )
__label__0 this function works with the ast call node format of python3.5+ as well as the different ast format of earlier versions of python .
__label__0 def build_chunks ( self ) : for n , s in enumerate ( self._proto.strings ) : b = bytes ( s , encoding= '' utf-8 '' ) self.add_chunk ( b , [ `` strings '' , n ] ) self._proto.clearfield ( `` strings '' )
__label__0 > > > tuple = ( ( 1.0 , 2.0 ) , ( 3.0 , 4.0 , 5.0 ) , 6.0 ) > > > tf.nest.flatten ( tuple ) [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ]
__label__0 this function takes a ` metagraphdef ` protocol buffer as input . if the argument is a file containing a ` metagraphdef ` protocol buffer , it constructs a protocol buffer from the file content . the function then adds all the nodes from the ` graph_def ` field to the current graph , recreates all the collections , and returns a saver constructed from the ` saver_def ` field .
__label__0 def testpacksequenceas_notiterableerror ( self ) : with self.assertraisesregex ( typeerror , self.bad_pack_pattern ) : nest.pack_sequence_as ( `` hi '' , `` bye '' )
__label__0 def _prepare ( self ) : lr = self._call_if_callable ( self._learning_rate ) decay = self._call_if_callable ( self._decay ) momentum = self._call_if_callable ( self._momentum ) epsilon = self._call_if_callable ( self._epsilon )
__label__0 self.assertequal ( { ' a ' : 3 , ' b ' : 4 } , tf_inspect.getcallargs ( func , a=3 , b=4 ) )
__label__0 if __name__ == `` __main__ '' : app.run ( main )
__label__0 > > > def compute_loss ( x ) : ... v = tf.variable ( 3.0 ) ... y = x * v ... loss = x * 5 - x * v ... return loss , [ v ]
__label__0 def setup ( self ) : super ( functionparametercanonicalizertest , self ) .setup ( ) self._matmul_func = ( _function_parameter_canonicalizer_binding_for_test .functionparametercanonicalizer ( [ ' a ' , ' b ' , 'transpose_a ' , 'transpose_b ' , 'adjoint_a ' , 'adjoint_b ' , 'a_is_sparse ' , 'b_is_sparse ' , 'name ' ] , ( false , false , false , false , false , false , none ) ) )
__label__0 t = test ( ) self.assertequal ( { 'self ' : t , ' a ' : 3 , ' b ' : 2 , ' c ' : 'goodbye ' } , tf_inspect.getcallargs ( t.bound , 3 , c='goodbye ' ) )
__label__0 - for modality.core : refer to [ tf.nest ] ( https : //www.tensorflow.org/api_docs/python/tf/nest ) for the definition of a structure .
__label__0 def main ( ) : filename = sys.argv [ 1 ] with open ( filename , `` r '' ) as f : lines = f.readlines ( ) parse_results ( lines ) print ( `` showing runtimes in microseconds . ` ? ` means not available . '' ) print ( `` % 20s , % 6s , % 14s , % 14s , % 10s '' % ( `` model '' , `` batch '' , `` vanilla '' , `` onednn '' , `` speedup '' ) ) for model in sorted ( models ) : for batch in sorted ( batch_sizes ) : key = ( model , batch , 0 ) eigen = db [ key ] if key in db else `` ? '' key = ( model , batch , 1 ) onednn = db [ key ] if key in db else `` ? '' speedup = `` % 10.2f '' % ( eigen / onednn ) if `` ? '' not in ( eigen , onednn ) else `` ? '' print ( `` % 20s , % 6d , % 14s , % 14s , % 10s '' % ( model , batch , str ( eigen ) , str ( onednn ) , speedup ) )
__label__0 class astcodeupgrader : `` '' '' handles upgrading a set of python files using a given api change spec . '' '' ''
__label__0 # pylint : disable=line-too-long
__label__0 `` `` '' given a list of symbols , generates a stub . '' '' ''
__label__0 def _createmockmodule ( self , name ) : mock_module = self.mockmodule ( name ) sys.modules [ name ] = mock_module self._modules.append ( name ) return mock_module
__label__0 self.assertequal ( _get_write_histogram_proto ( ) .num , num_writes_start + 2 ) self.assertequal ( _get_read_histogram_proto ( ) .num , num_reads_start + 1 ) # check that training time saved has increased . self.assertgreater ( metrics.gettrainingtimesaved ( api_label=api_label ) , time_after_one_save ) self.assertequal ( metrics.getcheckpointsize ( api_label=api_label , filesize=filesize ) , count_after_one_save + 1 )
__label__0 self.assertallequal ( 1 , sessions [ 1 ] .run ( global_step ) ) self.assertallclose ( 0 - ( 0.1 + 0.5 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_0_g_1 ) ) self.assertallclose ( 1 - ( 0.9 + 1.3 ) / 2 * 2.0 , sessions [ 1 ] .run ( var_1_g_1 ) )
__label__0 def testconv2d ( self ) : text = ( `` tf.nn.conv2d ( input , filter , strides , padding , use_cudnn_on_gpu , `` `` data_format ) '' ) expected_text = ( `` tf.nn.conv2d ( input , filters=filter , strides=strides , padding=padding , `` `` data_format=data_format ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 args : sess : a ` session ` .
__label__0 def _testmultisavercollectionsave ( self , test_dir ) : filename = os.path.join ( test_dir , `` metafile '' ) saver0_ckpt = os.path.join ( test_dir , `` saver0.ckpt '' ) saver1_ckpt = os.path.join ( test_dir , `` saver1.ckpt '' ) with self.session ( graph=ops_lib.graph ( ) ) as sess : # creates a graph . v0 = variable_v1.variablev1 ( [ [ 1.0 , 2.0 ] , [ 3.0 , 4.0 ] , [ 5.0 , 6.0 ] ] , name= '' v0 '' ) v1 = variable_v1.variablev1 ( 11.0 , name= '' v1 '' ) # creates 2 savers . saver0 = saver_module.saver ( { `` v0 '' : v0 } , name= '' saver0 '' ) saver1 = saver_module.saver ( { `` v1 '' : v1 } , name= '' saver1 '' ) ops_lib.add_to_collection ( `` savers '' , saver0 ) ops_lib.add_to_collection ( `` savers '' , saver1 ) self.evaluate ( variables.global_variables_initializer ( ) ) # saves to different checkpoints . saver0.save ( sess , saver0_ckpt ) saver1.save ( sess , saver1_ckpt ) # generates metagraphdef . meta_graph_def = saver_module.export_meta_graph ( filename ) meta_graph_def0 = saver0.export_meta_graph ( ) meta_graph_def1 = saver1.export_meta_graph ( )
__label__0 node.keywords.append ( new_arg ) if isinstance ( node.func , ast.attribute ) : node.func.attr = `` resize '' else : assert isinstance ( node.func , ast.name ) node.func.id = `` resize ''
__label__0 def get_int ( self , min_int=_min_int , max_int=_max_int ) : `` '' '' consume a signed integer with given constraints .
__label__0 the returned ` tracetype ` is a supertype of ` self ` and ` others ` , that is , they are all subtypes ( see ` is_subtype_of ` ) of it . it is also most specific , that is , there it has no subtype that is also a common supertype of ` self ` and ` others ` .
__label__0 # todo ( aselle ) : no files generated or symlinked here are deleted by # the build system . i do n't know of a way to do it in bazel . it # should only be a problem if somebody moves a sandbox directory # without running ./configure again .
__label__0 with session.session ( `` '' , graph=ops_lib.graph ( ) ) as sess : one = variable_v1.variablev1 ( 0.0 ) twos = variable_v1.variablev1 ( [ 0.0 , 0.0 , 0.0 ] ) v2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) # saver with no arg , defaults to 'all variables ' . save = saver_module.saver ( ) save.restore ( sess , save_path ) self.assertallclose ( 1.0 , self.evaluate ( one ) ) self.assertallclose ( [ 2.0 , 2.0 , 2.0 ] , self.evaluate ( twos ) ) self.assertequal ( b '' k1 '' , self.evaluate ( v2.keys ( ) ) ) self.assertequal ( 3.0 , self.evaluate ( v2.values ( ) ) )
__label__0 `` ` python b `` `
__label__0 # save checkpoint from which to warm-start . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : variable_scope.get_variable ( `` linear_model/sc_hash/weights '' , shape= [ 15 , 1 ] , initializer=norms ( ) ) sc_keys_weights = variable_scope.get_variable ( `` some_other_name '' , shape= [ 4 , 1 ] , initializer=rand ( ) ) variable_scope.get_variable ( `` linear_model/sc_vocab/weights '' , initializer= [ [ 0.5 ] , [ 1 . ] , [ 2 . ] , [ 3 . ] ] ) self._write_checkpoint ( sess ) prev_keys_val = self.evaluate ( sc_keys_weights )
__label__0 this creates symlinks from the internal git repository directory so that the build system can see changes in the version state . we also remember what branch git was on so when the branch changes we can detect that the ref file is no longer correct ( so we can suggest users run ./configure again ) .
__label__0 self.assertequal ( `` var/part_ % d/slot '' % i , slot.op.name ) self.assertequal ( [ 2 ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float32 , slot.dtype.base_dtype ) self.assertallequal ( [ 1.0 , 2.5 ] , slot ) self.assertallequal ( [ 2 ] , si.full_shape ) self.assertallequal ( [ i ] , si.var_offset ) self.assertallequal ( [ 1 ] , si.var_shape )
__label__0 def binary_elementwise_apis ( ) : `` '' '' returns a list of apis that have been registered as binary elementwise . '' '' '' return tuple ( _binary_elementwise_apis )
__label__0 @ staticmethod def _from_storage ( storage ) : result = objectidentityset ( ) result._storage = storage # pylint : disable=protected-access return result
__label__0 def test_no_empty_line ( self ) : expected = `` brief ( suffix ) \n\nwarning : go away\ninstructions\n\ndocstring '' # no second line indent self._check ( `` brief\ndocstring '' , expected ) # 2 space second line indent self._check ( `` brief\n docstring '' , expected ) # no second line indent , first line blank self._check ( `` \nbrief\ndocstring '' , expected ) # 2 space second line indent , first line blank self._check ( `` \n brief\n docstring '' , expected )
__label__0 logging.error ( `` fake error logged '' ) self.assertequal ( 0 , mock_warning.call_count ) deprecated_func ( `` fake error ! '' ) self.assertequal ( 1 , mock_warning.call_count ) # make sure the error points to the right file . self.assertregex ( mock_warning.call_args [ 0 ] [ 1 ] , r '' deprecation_test\.py : '' ) deprecated_func ( `` another fake error ! '' ) self.assertequal ( 1 , mock_warning.call_count )
__label__0 finally : dispatch.unregister_dispatch_for ( handler )
__label__0 @ parameterized.parameters ( { `` mapping_type '' : _custommapping } , { `` mapping_type '' : dict } ) def testflattendictitems ( self , mapping_type ) : dictionary = mapping_type ( { ( 4 , 5 , ( 6 , 8 ) ) : ( `` a '' , `` b '' , ( `` c '' , `` d '' ) ) } ) flat = { 4 : `` a '' , 5 : `` b '' , 6 : `` c '' , 8 : `` d '' } self.assertequal ( nest.flatten_dict_items ( dictionary ) , flat )
__label__0 result = test_op_with_optional ( x , y , z ) self.assertallequal ( self.evaluate ( result.tensor ) , [ 15 , 21 , 13 ] ) self.assertnear ( result.score , 0.4 , 0.001 )
__label__0 @ parameterized.named_parameters ( ( `` tuples '' , ( 1 , 2 , 3 ) , ( 4 , 5 ) , valueerror ) , ( `` dicts '' , { `` a '' : 1 } , { `` b '' : 2 } , valueerror ) , ( `` mixed '' , ( 1 , 2 ) , [ 3 , 4 ] , typeerror ) , ( `` nested '' , { `` a '' : [ 2 , 3 , 4 ] , `` b '' : [ 1 , 3 ] } , { `` b '' : [ 5 , 6 ] , `` a '' : [ 8 , 9 ] } , valueerror ) ) def testmapwithpathsincompatiblestructures ( self , s1 , s2 , error_type ) : with self.assertraises ( error_type ) : nest.map_structure_with_paths ( lambda path , * s : 0 , s1 , s2 )
__label__0 the following code will raise an exception : `` ` python shallow_tree = [ `` a '' , `` b '' ] input_tree = [ `` c '' , [ `` d '' , `` e '' ] , `` f '' ] assert_shallow_structure ( shallow_tree , input_tree ) `` `
__label__0 @ end_compatibility `` '' '' graph = graph or ops.get_default_graph ( ) global_step_tensor = none global_step_tensors = graph.get_collection ( ops.graphkeys.global_step ) if len ( global_step_tensors ) == 1 : global_step_tensor = global_step_tensors [ 0 ] elif not global_step_tensors : try : global_step_tensor = graph.get_tensor_by_name ( 'global_step:0 ' ) except keyerror : return none else : logging.error ( 'multiple tensors in global_step collection . ' ) return none
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] shallow_tree : a shallow structure , common to all the inputs . func : callable which will be applied to each input individually . * inputs : structures that are compatible with shallow_tree . the function ` func ` is applied to corresponding structures due to partial flattening of each input , so the function must support arity of ` len ( inputs ) ` . * * kwargs : arg valid for modality.core only . kwargs to feed to func ( ) . special kwarg ` check_types ` is not passed to func , but instead determines whether the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` .
__label__0 def testwarmstartmoresettingsnopartitioning ( self ) : # create old and new vocabs for sparse column `` sc_vocab '' . prev_vocab_path = self._write_vocab ( [ `` apple '' , `` banana '' , `` guava '' , `` orange '' ] , `` old_vocab '' ) new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' ) # create feature columns . sc_hash = fc.categorical_column_with_hash_bucket ( `` sc_hash '' , hash_bucket_size=15 ) sc_keys = fc.categorical_column_with_vocabulary_list ( `` sc_keys '' , vocabulary_list= [ `` a '' , `` b '' , `` c '' , `` e '' ] ) sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=new_vocab_path , vocabulary_size=6 ) all_linear_cols = [ sc_hash , sc_keys , sc_vocab ]
__label__0 args : visitor : a visitor to call for the public api. `` '' '' self._visitor = visitor self._root_name = 'tf '
__label__0 class obsoletesessionmanagertest ( test.testcase ) :
__label__0 raises : valueerror : if the object-based checkpoint is missing variables. `` '' '' fname = checkpoint_utils._get_checkpoint_filename ( path ) # pylint : disable=protected-access try : names_to_keys = saver_lib.object_graph_key_mapping ( fname ) except errors.notfounderror : # if an error is raised from ` object_graph_key_mapping ` , then the # checkpoint is name-based . there are no renames , so return an empty dict . return { }
__label__0 @ test_util.run_in_graph_and_eager_modes def testresourcebasic ( self ) : self.basicsaverestore ( resource_variable_ops.resourcevariable )
__label__0 returns : returns one of the following in decreasing preference : - first non-deprecated endpoint - first endpoint - none `` '' '' non_deprecated_name = next ( ( name for name in api_names if name not in deprecated_api_names ) , none ) if non_deprecated_name : return non_deprecated_name if api_names : return api_names [ 0 ] return none
__label__0 - ` t ! = 0 ` . in this case , comparison is done on types / ids . - ` isinstance ( t , tf.tensor ) ` . similar to above .
__label__0 returns : the global step value. `` '' '' if context.executing_eagerly ( ) : return int ( global_step_tensor.numpy ( ) ) return int ( sess.run ( global_step_tensor ) )
__label__0 def join ( self ) : `` '' '' blocks until the server has shut down .
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 the ` compat.v1 ` and ` compat.v2 ` submodules provide a complete copy of both the ` v1 ` and ` v2 ` apis for backwards and forwards compatibility across tensorflow versions 1.x and 2.x . see the [ migration guide ] ( https : //www.tensorflow.org/guide/migrate ) for details .
__label__0 def check_all_files ( ) : `` '' '' check all relevant files necessary for upgrade . '' '' '' for file_name in relevant_files : check_existence ( file_name )
__label__0 class _object ( object ) :
__label__0 distribute_strategy_api_changes = ( `` if you 're using the strategy with a `` `` custom training loop , note the following changes in methods : `` `` make_dataset_iterator- > experimental_distribute_dataset , `` `` experimental_make_numpy_iterator- > experimental_make_numpy_dataset , `` `` extended.call_for_each_replica- > run , `` `` reduce requires an axis argument , `` `` unwrap- > experimental_local_results `` `` experimental_initialize and experimental_finalize no longer needed `` )
__label__0 returns : whether an error was recorded. `` '' '' # only look for * .-warnings here , the other will be handled by the attribute # visitor . also , do not warn for bare functions , only if the call func is # an attribute . warned = false if isinstance ( node.func , ast.attribute ) : warned = self._maybe_add_warning ( node , `` * . '' + name )
__label__0 # build another graph with 2 nodes , initialized # differently , and a restore node for them . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0_2 = variable_op ( 1000.0 , name= '' v0 '' ) v1_2 = variable_op ( 2000.0 , name= '' v1 '' ) v2_2 = saver_test_utils.checkpointedop ( name= '' v2 '' ) v2_init = v2_2.insert ( `` k1000 '' , 3000.0 )
__label__0 args : api_signature : the signature of the tensorflow api . func : the dispatch target .
__label__0 for arg in key_value_list : key , value = arg.split ( `` = '' ) if value.lower ( ) == `` true '' : build_info [ key ] = true elif value.lower ( ) == `` false '' : build_info [ key ] = false else : build_info [ key ] = value.format ( * * build_info )
__label__0 class tfapichangespec ( ast_edits.apichangespec ) : `` '' '' list of maps that describe what changed in the api . '' '' ''
__label__0 this class is deprecated . for synchronous training , please use [ distribution strategies ] ( https : //github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute ) .
__label__0 # pylint : disable=superfluous-parens
__label__0 num_shards = len ( per_device ) sharded_saves = [ ] sharded_prefixes = [ ] num_shards_tensor = constant_op.constant ( num_shards , name= '' num_shards '' ) last_device = none for shard , ( device , saveables ) in enumerate ( per_device ) : last_device = device with ops.device ( saveable_object_util.set_cpu0 ( device ) ) : sharded_filename = self.sharded_filename ( tmp_checkpoint_prefix , shard , num_shards_tensor ) sharded_prefixes.append ( sharded_filename ) sharded_saves.append ( self._addsaveops ( sharded_filename , saveables ) )
__label__0 sv0 , sess0 , v0 , _ , w0 = get_session ( true ) sv1 , sess1 , _ , vadd1 , w1 = get_session ( false )
__label__0 ` atomicfunction ` can be called directly only if no captures are needed according to the ` functiontype ` . if captures are present , please use ` call_with_captures ` instead .
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 def __init__ ( self ) : reorderkeywordspec.__init__ ( self ) renamekeywordspec.__init__ ( self ) self.update_renames ( ) self.update_reorders ( )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' a python interface for creating tensorflow servers . '' '' ''
__label__0 def testimportintoimplicitnamescope ( self ) : # test that we can import a meta graph into an implicit namescope . test_dir = self._get_test_dir ( `` import_into_namescope '' ) filename = os.path.join ( test_dir , `` ckpt '' ) # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) : image = array_ops.placeholder ( dtypes.float32 , [ none , 784 ] , name= '' image '' ) label = array_ops.placeholder ( dtypes.float32 , [ none , 10 ] , name= '' label '' ) with session.session ( ) as sess : weights = variable_v1.variablev1 ( random_ops.random_uniform ( [ 784 , 10 ] ) , name= '' weights '' ) bias = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' bias '' ) logit = nn_ops.relu ( math_ops.matmul ( image , weights ) + bias , name= '' logits '' ) nn_ops.softmax ( logit , name= '' prediction '' ) cost = nn_ops.softmax_cross_entropy_with_logits ( labels=label , logits=logit , name= '' cost '' ) adam.adamoptimizer ( ) .minimize ( cost , name= '' optimize '' ) saver = saver_module.saver ( ) self.evaluate ( variables.global_variables_initializer ( ) ) saver.save ( sess , filename )
__label__0 logging.info ( `` chunked file exported to % s '' , path ) logging.info ( `` total time spent splitting and writing the message : % s '' , end - start_time , ) logging.info ( `` number of chunks created ( including initial message ) : % s '' , len ( chunks ) , ) return path
__label__0 import codecs import collections.abc as collections_abc # pylint : disable=unused-import import numbers as _numbers
__label__0 flags = flags.flags
__label__0 text = `` tf.gradients ( y , x , grad_ys , name , colocate , gate ) \n '' expected = ( `` tf.gradients ( y , x , grad_ys , name , gate_gradients=gate ) \n '' ) _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text )
__label__0 class newclass ( object ) :
__label__0 # version type parameters . nightly_version = 1 regular_version = 0
__label__0 when generating docs for a class 's arributes , the ` __mro__ ` is searched and the attribute will be skipped if this decorator is detected on the attribute on any * * parent * * class in the ` __mro__ ` .
__label__0 optionally writes it to filename .
__label__0 def testerrorifusedbeforeminimizecalled ( self ) : opt = training.syncreplicasoptimizer ( opt=gradient_descent.gradientdescentoptimizer ( 1.0 ) , replicas_to_aggregate=1 , total_num_replicas=1 ) hook = opt.make_session_run_hook ( true ) with self.assertraisesregex ( valueerror , `` apply_gradient should be called '' ) : hook.begin ( )
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 # misc functions ( e.g . loss function ) if callable ( obj ) : return obj.__name__
__label__0 args : filename_tensor : string tensor . saveables : list of basesaverbuilder.saveableobject objects . preferred_shard : int . shard to open first when loading a sharded file . restore_sequentially : unused . bool . if true , each restore is sequential .
__label__0 @ property def summary_writer ( self ) : `` '' '' return the summarywriter used by the chief supervisor .
__label__0 defaults to ` ' . * ' ` , which warm-starts all variables in the trainable_variables collection . note that this excludes variables such as accumulators and moving statistics from batch norm . var_name_to_vocab_info : [ optional ] dict of variable names ( strings ) to ` tf.estimator.vocabinfo ` . the variable names should be `` full '' variables , not the names of the partitions . if not explicitly provided , the variable is assumed to have no ( changes to ) vocabulary . var_name_to_prev_var_name : [ optional ] dict of variable names ( strings ) to name of the previously-trained variable in ` ckpt_to_initialize_from ` . if not explicitly provided , the name of the variable is assumed to be same between previous checkpoint and current model . note that this has no effect on the set of variables that is warm-started , and only controls name mapping ( use ` vars_to_warm_start ` for controlling what variables to warm-start ) .
__label__0 try : return _getargspec ( target.__new__ ) except typeerror : pass
__label__0 > > > leaves = tf.nest.flatten ( mt ) > > > leaves [ < tf.tensor : shape= ( 1 , ) , dtype=int32 , numpy=array ( [ 1 ] , dtype=int32 ) > ]
__label__0 class tensorspec ( object ) : `` '' '' interface for internal isinstance checks to framework/tensor_spec.py .
__label__0 def _get_header_version ( path , name ) : `` '' '' returns preprocessor defines in c header file . '' '' '' for line in io.open ( path , `` r '' , encoding= '' utf-8 '' ) : match = re.match ( r '' # define % s + ( \d+ ) '' % name , line ) if match : value = match.group ( 1 ) return int ( value )
__label__0 def process_file ( in_filename , out_filename , upgrader ) : `` '' '' process a file of type ` .py ` or ` .ipynb ` . '' '' ''
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tfdecorator-aware replacements for the inspect module . '' '' '' import collections import functools import inspect as _inspect
__label__0 # * * kwargs passed in that we can not inspect , should warn text = `` f ( a , b , kw1=c , * * kwargs ) \n '' ( _ , report , _ ) , _ = self._upgrade ( renamekeywordspec ( ) , text ) self.assertin ( `` manual check required '' , report )
__label__0 args : string : the string to extract floats from .
__label__0 # get all build files for all valid targets . build_files = set ( ) for target in valid_targets : build_files.add ( os.path.join ( target [ 2 : ] .split ( ' : ' ) [ 0 ] , 'build ' ) )
__label__0 from typing import contextmanager , typevar
__label__0 def update ( self , items ) : self._storage.update ( [ self._wrap_key ( item ) for item in items ] )
__label__1 import bisect
__label__0 def _testtypesforadagrad ( self , x , y , lr , grad , use_gpu=none ) : self.setup ( ) with self.session ( use_gpu=use_gpu ) : var = variable_v1.variablev1 ( x ) accum = variable_v1.variablev1 ( y ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 returns : a tuple ( is_ready , msg ) , where is_ready is true if ready and false otherwise , and msg is ` none ` if the model is ready , a ` string ` with the reason why it is not ready otherwise. `` '' '' if op is none : return true , none else : try : ready_value = sess.run ( op ) # the model is considered ready if ready_op returns an empty 1-d tensor . # also compare to ` none ` and dtype being int32 for backward # compatibility . if ( ready_value is none or ready_value.dtype == np.int32 or ready_value.size == 0 ) : return true , none else : # todo ( sherrym ) : if a custom ready_op returns other types of tensor , # or strings other than variable names , this message could be # confusing . non_initialized_varnames = `` , `` .join ( [ i.decode ( `` utf-8 '' ) for i in ready_value ] ) return false , `` variables not initialized : `` + non_initialized_varnames except errors.failedpreconditionerror as e : if `` uninitialized '' not in str ( e ) : logging.warning ( `` % s : error [ % s ] '' , msg , str ( e ) ) raise e return false , str ( e )
__label__0 args : api : the tensorflow api to override . * signatures : dictionaries mapping parameter names or indices to type annotations , specifying when the dispatch target should be called . in particular , the dispatch target will be called if any signature matches ; and a signature matches if all of the specified parameters have types that match with the indicated type annotations . if no signatures are specified , then a signature will be read from the dispatch target function 's type annotations .
__label__0 def testunsortedsegmentsumshapeerror ( self ) : for dtype in self.numeric_types : data = np.ones ( ( 4 , 8 , 7 ) , dtype=dtype ) indices = np.ones ( ( 3 , 2 ) , dtype=np.int32 ) num_segments = 4 self.assertraises ( valueerror , functools.partial ( self._segmentreduction , math_ops.unsorted_segment_sum , data , indices , num_segments ) )
__label__0 def ismodule ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.ismodule . '' '' '' return _inspect.ismodule ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tf.grpcserver . '' '' ''
__label__0 if __name__ == '__main__ ' : test.main ( )
__label__0 def get_bad_versions ( ) - > tuple [ int , ... ] : return _bad_versions
__label__0 raises : tf.errors.operror : or one of its subclasses if an error occurs while creating the tensorflow server. `` '' '' self._server_def = _make_server_def ( server_or_cluster_def , job_name , task_index , protocol , config ) self._server = c_api.tf_newserver ( self._server_def.serializetostring ( ) ) if start : self.start ( )
__label__0 `` ` dict [ str , union [ list [ bytes ] , list [ int64 ] , list [ float ] ] ] `` `
__label__0 def _create_slots ( self , var_list ) : for v in var_list : if v.get_shape ( ) .is_fully_defined ( ) : init_rms = init_ops.ones_initializer ( dtype=v.dtype.base_dtype ) else : init_rms = array_ops.ones_like ( v ) self._get_or_make_slot_with_initializer ( v , init_rms , v.get_shape ( ) , v.dtype.base_dtype , `` rms '' , self._name ) if self._centered : self._zeros_slot ( v , `` mg '' , self._name ) self._zeros_slot ( v , `` momentum '' , self._name )
__label__0 _major_api_version = 2
__label__0 def testconcatreorderwithkeywordargs ( self ) : text = `` tf.concat ( concat_dim=a , values=b ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.concat ( axis=a , values=b ) \n '' ) text = `` tf.concat ( values=b , concat_dim=a ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.concat ( values=b , axis=a ) \n '' ) text = `` tf.concat ( a , values=b ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.concat ( axis=a , values=b ) \n '' )
__label__0 def _get_test_dir ( self , dirname ) : test_dir = os.path.join ( self.get_temp_dir ( ) , dirname ) gfile.makedirs ( test_dir ) return test_dir
__label__0 def main ( unused_args ) : config = system_info_lib.gather_machine_configuration ( ) print ( config )
__label__0 > > > tf.nest.assert_same_structure ( ... { `` a '' : 1 , `` b '' : 2 , `` c '' : 3 } , ... { `` c '' : 6 , `` b '' : 5 , `` a '' : 4 } )
__label__0 raises : valueerror : if the test_name is not a valid target . subprocess.calledprocesserror : if the target itself fails . ioerror : if there are problems gathering test log output from the test . missinglogserror : if we could n't find benchmark logs. `` '' '' if not ( test_name and test_name.startswith ( `` // '' ) and `` .. '' not in test_name and not test_name.endswith ( `` : '' ) and not test_name.endswith ( `` : all '' ) and not test_name.endswith ( `` ... '' ) and len ( test_name.split ( `` : '' ) ) == 2 ) : raise valueerror ( `` expected test_name parameter with a unique test , e.g . : `` `` -- test_name=//path/to : test '' ) test_executable = test_name.rstrip ( ) .strip ( `` / '' ) .replace ( `` : '' , `` / '' )
__label__0 class saverlargepartitionedvariabletest ( test.testcase ) :
__label__0 # ready_for_local_init_op defaults to none for backward compatibility if ready_for_local_init_op is supervisor.use_default : ready_for_local_init_op = self._get_first_op_from_collection ( ops.graphkeys.ready_for_local_init_op ) self._ready_for_local_init_op = ready_for_local_init_op
__label__0 this is used before any rewriting is done to detect if any symbols are used that require changing imports or disabling rewriting altogether. `` '' ''
__label__0 if partitioner : return new_vs [ 0 ] .as_tensor ( ) .eval ( ) else : return new_vs [ 0 ] .eval ( )
__label__0 note = ( '\n\nnote : the output of this function should be used . if it is ' 'not , a warning will be logged or an error may be raised . ' 'to mark the output as used , call its .mark_used ( ) method . ' ) return tf_decorator.make_decorator ( target=fn , decorator_func=wrapped , decorator_name='should_use_result ' , decorator_doc=updated_doc + note )
__label__0 > > > a = [ 24 , 76 , `` ab '' ] > > > tf.nest.map_structure ( lambda p : p * 2 , a ) [ 48 , 152 , 'abab ' ]
__label__0 __all__ = [ 'traverse ' ]
__label__0 returns : the global step ` tensor ` or ` none ` . `` '' '' try : gs = ops.get_default_graph ( ) .get_tensor_by_name ( `` global_step:0 '' ) if gs.dtype.base_dtype in [ dtypes.int32 , dtypes.int64 ] : return gs else : logging.warning ( `` found 'global_step ' is not an int type : % s '' , gs.dtype ) return none except keyerror : return none
__label__0 objects other than ` resourcevariable ` s in ` values ` will be returned unchanged .
__label__0 conformant pair of ` sequenceexample ` s :
__label__0 def testkeywordreorderwithparens ( self ) : `` '' '' test that we get the expected result if there are parens around args . '' '' '' text = `` f ( ( a ) , ( ( b ) ) ) \n '' acceptable_outputs = [ # no change is a valid output text , # also cases where all arguments are fully specified are allowed `` f ( a= ( a ) , b= ( ( b ) ) ) \n '' , # making the parens canonical is ok `` f ( a= ( a ) , b= ( ( b ) ) ) \n '' , ] _ , new_text = self._upgrade ( reorderkeywordspec ( ) , text ) self.assertin ( new_text , acceptable_outputs )
__label__0 # train.saver is v1 only api . with ops_lib.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : v = variable_v1.variablev1 ( 10.0 , name= '' v '' ) save = saver_module.saver ( { `` v '' : v } , max_to_keep=2 ) self.evaluate ( variables.global_variables_initializer ( ) ) self.assertequal ( [ ] , save.last_checkpoints )
__label__0 if __name__ == `` __main__ '' : main ( )
__label__0 def new_target ( x ) : return x * 3
__label__0 trace_map = tf_stack.loadtracesfromdebuginfo ( debug_info ) self.assertsameelements ( trace_map.keys ( ) , [ 'node1 @ func1 ' , 'node2 @ func2 ' , 'node3 @ func3 ' ] )
__label__0 if multiple type signatures are specified , then the dispatch target will be called if any of the signatures match . for example , the following code registers ` masked_add ` to be called if ` x ` is a ` maskedtensor ` * or * ` y ` is a ` maskedtensor ` .
__label__0 graph = ops_lib.graph ( ) with session.session ( graph=graph ) as sess : new_saver = saver_module.import_meta_graph ( filename + `` .meta '' , graph=graph , import_scope= '' new_model '' ) new_saver.restore ( sess , filename ) sess.run ( [ `` new_model/optimize '' ] , { `` new_model/image:0 '' : np.random.random ( [ 1 , 784 ] ) , `` new_model/label:0 '' : np.random.randint ( 10 , size= [ 1 , 10 ] ) } )
__label__0 @ classmethod def test ( cls , a , b=3 , c='hello ' ) : return ( a , b , c )
__label__0 @ test_util.run_v1_only ( `` applyadagrad op returns a ref , so it is not `` `` supported in eager mode . '' ) def testapplyadagrad ( self ) : for ( dtype , use_gpu ) in itertools.product ( [ np.float16 , np.float32 , np.float64 ] , [ false , true ] ) : x = np.arange ( 100 ) .astype ( dtype ) y = np.arange ( 1 , 101 ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) grad = np.arange ( 100 ) .astype ( dtype ) self._testtypesforadagrad ( x , y , lr , grad , use_gpu )
__label__0 examples :
__label__0 text = `` tf.contrib.layers.l2_regularizer ( scale , scope ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l2 ( 0.5 * ( scale ) ) \n '' , ) self.assertin ( `` dropping scope '' , unused_report )
__label__0 try : with self.assertraisesregex ( valueerror , r '' a unary elementwise dispatch handler \ ( . * \ ) has `` `` already been registered for . * '' ) :
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) @ test_util.run_deprecated_v1 def test_kwargs ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 def major_minor_change ( old_version , new_version ) : `` '' '' check if a major or minor change occurred . '' '' '' major_mismatch = old_version.major ! = new_version.major minor_mismatch = old_version.minor ! = new_version.minor if major_mismatch or minor_mismatch : return true return false
__label__0 # todo ( lukaszkaiser ) : consider allowing partitioners to be set in the current # scope . current_partitioner = variable_scope.get_variable_scope ( ) .partitioner variable_scope.get_variable_scope ( ) .set_partitioner ( none ) # when init from val instead of callable initializer , the shape is expected to # be none , not < unknown > or any fully defined shape . shape = shape if callable ( val ) else none if resource_variable_ops.is_resource_variable ( primary ) : use_resource = true elif isinstance ( primary , ref_variable.refvariable ) : use_resource = false else : use_resource = none slot = variable_scope.get_variable ( scope , initializer=val , trainable=false , use_resource=use_resource , shape=shape , dtype=dtype , validate_shape=validate_shape ) variable_scope.get_variable_scope ( ) .set_partitioner ( current_partitioner )
__label__0 def testgetargspeconpartialnoargumentsleft ( self ) : `` '' '' tests getargspec on partial function that prunes all arguments . '' '' ''
__label__0 def testcleardevicesonimport ( self ) : # test that we import a graph without its devices and run successfully . with ops_lib.graph ( ) .as_default ( ) : with ops_lib.device ( `` /job : ps/replica:0/task:0/device : gpu:0 '' ) : image = array_ops.placeholder ( dtypes.float32 , [ none , 784 ] , name= '' image '' ) label = array_ops.placeholder ( dtypes.float32 , [ none , 10 ] , name= '' label '' ) weights = variable_v1.variablev1 ( random_ops.random_uniform ( [ 784 , 10 ] ) , name= '' weights '' ) bias = variable_v1.variablev1 ( array_ops.zeros ( [ 10 ] ) , name= '' bias '' ) logit = nn_ops.relu ( math_ops.matmul ( image , weights ) + bias ) nn_ops.softmax ( logit , name= '' prediction '' ) cost = nn_ops.softmax_cross_entropy_with_logits ( labels=label , logits=logit ) adam.adamoptimizer ( ) .minimize ( cost , name= '' optimize '' ) meta_graph_def = saver_module.export_meta_graph ( )
__label__0 `` `` '' synchronize replicas for training . '' '' '' from tensorflow.python.distribute import distribute_lib from tensorflow.python.framework import indexed_slices from tensorflow.python.framework import ops from tensorflow.python.framework import tensor from tensorflow.python.ops import array_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import data_flow_ops from tensorflow.python.ops import state_ops from tensorflow.python.ops import variable_v1 from tensorflow.python.ops import variables from tensorflow.python.platform import tf_logging as logging from tensorflow.python.training import optimizer from tensorflow.python.training import queue_runner from tensorflow.python.training import session_manager from tensorflow.python.training import session_run_hook from tensorflow.python.util import deprecation from tensorflow.python.util.tf_export import tf_export
__label__0 def write ( self , file_prefix : str ) - > str : `` '' '' serializes a proto to disk .
__label__0 see the readme on how to use or subclass this class. `` '' ''
__label__0 def func ( m , n ) : return 2 * m + n
__label__0 def testgetargspecreturnswrappedargspec ( self ) : argspec = tf_inspect.getargspec ( test_params_and_defaults ) self.assertequal ( [ ' a ' , ' b ' , ' c ' , 'd ' ] , argspec.args ) self.assertequal ( ( 2 , true , 'hello ' ) , argspec.defaults )
__label__0 with ops.graph ( ) .as_default ( ) : # construct a simple graph that runs ops on remote worker with ops.device ( `` /job : worker/replica:0/task:0/device : cpu:0 '' ) : a = constant_op.constant ( [ 1.0 ] ) b = a + a
__label__0 hipfft_config = { `` hipfft_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 # with strip_default_attrs disabled , attributes `` t '' ( float32 ) and `` tout '' # ( complex64 ) in the `` complex '' op must * not * be removed , even if they map # to their defaults . with ops_lib.graph ( ) .as_default ( ) , self.session ( ) : real_num = variable_v1.variablev1 ( 1.0 , dtype=dtypes.float32 , name= '' real '' ) imag_num = variable_v1.variablev1 ( 2.0 , dtype=dtypes.float32 , name= '' imag '' ) math_ops.complex ( real_num , imag_num , name= '' complex '' )
__label__0 args : export_scope : optional ` string ` . name scope to remove .
__label__0 try : children = tf_inspect.getmembers ( root )
__label__0 def check_output_despite_error ( args ) : `` '' '' get output of args from command line , even if there are errors .
__label__0 for arg in fullargspec.args : parameters.append ( inspect.parameter ( arg , inspect.parameter.positional_or_keyword , default=defaults.get ( arg , inspect.parameter.empty ) , ) )
__label__0 class _custommapping ( collections.abc.mapping ) :
__label__0 text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=foo ( ) .zz ( ) ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( foo ( ) .zz ( ) ) ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 if gfile.exists ( os.path.join ( `` bazel-bin '' , test_executable ) ) : # running in standalone mode from core of the repository test_executable = os.path.join ( `` bazel-bin '' , test_executable ) else : # hopefully running in sandboxed mode test_executable = os.path.join ( `` . `` , test_executable )
__label__0 text = `` tf.arg_max ( input , dimension=0 ) '' expected_text = `` tf.argmax ( input , axis=0 ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 save_graph = ops_lib.graph ( ) with save_graph.as_default ( ) , self.session ( graph=save_graph ) as sess : orig_vars = _model ( ) self.evaluate ( variables.global_variables_initializer ( ) ) save = saver_module.saver ( max_to_keep=1 ) self.evaluate ( variables.global_variables_initializer ( ) ) save.save ( sess , save_dir ) orig_vals = self.evaluate ( orig_vars )
__label__0 with ` expand_composites=true ` , we return the component tensors that make up the raggedtensor representation ( the values and row_splits tensors )
__label__0 this file is autogenerated : to update , please run : bazel run tensorflow/tools/compatibility/update : generate_v2_renames_map this file should be updated whenever endpoints are deprecated. `` '' '' renames = { 'tf.auto_reuse ' : 'tf.compat.v1.auto_reuse ' , 'tf.attrvalue ' : 'tf.compat.v1.attrvalue ' , 'tf.compiler_version ' : 'tf.version.compiler_version ' , 'tf.cxx11_abi_flag ' : 'tf.sysconfig.cxx11_abi_flag ' , 'tf.cxx_version ' : 'tf.sysconfig.cxx_version ' , 'tf.conditionalaccumulator ' : 'tf.compat.v1.conditionalaccumulator ' , 'tf.conditionalaccumulatorbase ' : 'tf.compat.v1.conditionalaccumulatorbase ' , 'tf.configproto ' : 'tf.compat.v1.configproto ' , 'tf.dimension ' : 'tf.compat.v1.dimension ' , 'tf.event ' : 'tf.compat.v1.event ' , 'tf.fifoqueue ' : 'tf.queue.fifoqueue ' , 'tf.fixedlenfeature ' : 'tf.io.fixedlenfeature ' , 'tf.fixedlensequencefeature ' : 'tf.io.fixedlensequencefeature ' , 'tf.fixedlengthrecordreader ' : 'tf.compat.v1.fixedlengthrecordreader ' , 'tf.git_version ' : 'tf.version.git_version ' , 'tf.gpuoptions ' : 'tf.compat.v1.gpuoptions ' , 'tf.graph_def_version ' : 'tf.version.graph_def_version ' , 'tf.graph_def_version_min_consumer ' : 'tf.version.graph_def_version_min_consumer ' , 'tf.graph_def_version_min_producer ' : 'tf.version.graph_def_version_min_producer ' , 'tf.graphdef ' : 'tf.compat.v1.graphdef ' , 'tf.graphkeys ' : 'tf.compat.v1.graphkeys ' , 'tf.graphoptions ' : 'tf.compat.v1.graphoptions ' , 'tf.histogramproto ' : 'tf.compat.v1.histogramproto ' , 'tf.identityreader ' : 'tf.compat.v1.identityreader ' , 'tf.interactivesession ' : 'tf.compat.v1.interactivesession ' , 'tf.lmdbreader ' : 'tf.compat.v1.lmdbreader ' , 'tf.logmessage ' : 'tf.compat.v1.logmessage ' , 'tf.monolithic_build ' : 'tf.sysconfig.monolithic_build ' , 'tf.metagraphdef ' : 'tf.compat.v1.metagraphdef ' , 'tf.nameattrlist ' : 'tf.compat.v1.nameattrlist ' , 'tf.nogradient ' : 'tf.no_gradient ' , 'tf.nodedef ' : 'tf.compat.v1.nodedef ' , 'tf.notdifferentiable ' : 'tf.no_gradient ' , 'tf.operror ' : 'tf.errors.operror ' , 'tf.optimizeroptions ' : 'tf.compat.v1.optimizeroptions ' , 'tf.paddingfifoqueue ' : 'tf.queue.paddingfifoqueue ' , 'tf.print ' : 'tf.compat.v1.print ' , 'tf.priorityqueue ' : 'tf.queue.priorityqueue ' , 'tf.quantized_dtypes ' : 'tf.dtypes.quantized_dtypes ' , 'tf.queuebase ' : 'tf.queue.queuebase ' , 'tf.randomshufflequeue ' : 'tf.queue.randomshufflequeue ' , 'tf.readerbase ' : 'tf.compat.v1.readerbase ' , 'tf.runmetadata ' : 'tf.compat.v1.runmetadata ' , 'tf.runoptions ' : 'tf.compat.v1.runoptions ' , 'tf.session ' : 'tf.compat.v1.session ' , 'tf.sessionlog ' : 'tf.compat.v1.sessionlog ' , 'tf.sparseconditionalaccumulator ' : 'tf.compat.v1.sparseconditionalaccumulator ' , 'tf.sparsefeature ' : 'tf.io.sparsefeature ' , 'tf.sparsetensorvalue ' : 'tf.compat.v1.sparsetensorvalue ' , 'tf.summary ' : 'tf.compat.v1.summary ' , 'tf.summarymetadata ' : 'tf.compat.v1.summarymetadata ' , 'tf.tfrecordreader ' : 'tf.compat.v1.tfrecordreader ' , 'tf.tensorinfo ' : 'tf.compat.v1.tensorinfo ' , 'tf.textlinereader ' : 'tf.compat.v1.textlinereader ' , 'tf.version ' : 'tf.version.version ' , 'tf.varlenfeature ' : 'tf.io.varlenfeature ' , 'tf.variablescope ' : 'tf.compat.v1.variablescope ' , 'tf.wholefilereader ' : 'tf.compat.v1.wholefilereader ' , 'tf.accumulate_n ' : 'tf.math.accumulate_n ' , 'tf.add_check_numerics_ops ' : 'tf.compat.v1.add_check_numerics_ops ' , 'tf.add_to_collection ' : 'tf.compat.v1.add_to_collection ' , 'tf.add_to_collections ' : 'tf.compat.v1.add_to_collections ' , 'tf.all_variables ' : 'tf.compat.v1.all_variables ' , 'tf.angle ' : 'tf.math.angle ' , 'tf.app.run ' : 'tf.compat.v1.app.run ' , 'tf.assert_proper_iterable ' : 'tf.debugging.assert_proper_iterable ' , 'tf.assert_same_float_dtype ' : 'tf.debugging.assert_same_float_dtype ' , 'tf.assign ' : 'tf.compat.v1.assign ' , 'tf.assign_add ' : 'tf.compat.v1.assign_add ' , 'tf.assign_sub ' : 'tf.compat.v1.assign_sub ' , 'tf.batch_scatter_update ' : 'tf.compat.v1.batch_scatter_update ' , 'tf.betainc ' : 'tf.math.betainc ' , 'tf.ceil ' : 'tf.math.ceil ' , 'tf.check_numerics ' : 'tf.debugging.check_numerics ' , 'tf.cholesky ' : 'tf.linalg.cholesky ' , 'tf.cholesky_solve ' : 'tf.linalg.cholesky_solve ' , 'tf.clip_by_average_norm ' : 'tf.compat.v1.clip_by_average_norm ' , 'tf.colocate_with ' : 'tf.compat.v1.colocate_with ' , 'tf.conj ' : 'tf.math.conj ' , 'tf.container ' : 'tf.compat.v1.container ' , 'tf.control_flow_v2_enabled ' : 'tf.compat.v1.control_flow_v2_enabled ' , 'tf.convert_to_tensor_or_indexed_slices ' : 'tf.compat.v1.convert_to_tensor_or_indexed_slices ' , 'tf.convert_to_tensor_or_sparse_tensor ' : 'tf.compat.v1.convert_to_tensor_or_sparse_tensor ' , 'tf.count_up_to ' : 'tf.compat.v1.count_up_to ' , 'tf.create_partitioned_variables ' : 'tf.compat.v1.create_partitioned_variables ' , 'tf.cross ' : 'tf.linalg.cross ' , 'tf.cumprod ' : 'tf.math.cumprod ' , 'tf.data.get_output_classes ' : 'tf.compat.v1.data.get_output_classes ' , 'tf.data.get_output_shapes ' : 'tf.compat.v1.data.get_output_shapes ' , 'tf.data.get_output_types ' : 'tf.compat.v1.data.get_output_types ' , 'tf.data.make_initializable_iterator ' : 'tf.compat.v1.data.make_initializable_iterator ' , 'tf.data.make_one_shot_iterator ' : 'tf.compat.v1.data.make_one_shot_iterator ' , 'tf.debugging.is_finite ' : 'tf.math.is_finite ' , 'tf.debugging.is_inf ' : 'tf.math.is_inf ' , 'tf.debugging.is_nan ' : 'tf.math.is_nan ' , 'tf.debugging.is_non_decreasing ' : 'tf.math.is_non_decreasing ' , 'tf.debugging.is_strictly_increasing ' : 'tf.math.is_strictly_increasing ' , 'tf.decode_base64 ' : 'tf.io.decode_base64 ' , 'tf.decode_compressed ' : 'tf.io.decode_compressed ' , 'tf.decode_json_example ' : 'tf.io.decode_json_example ' , 'tf.delete_session_tensor ' : 'tf.compat.v1.delete_session_tensor ' , 'tf.depth_to_space ' : 'tf.nn.depth_to_space ' , 'tf.dequantize ' : 'tf.quantization.dequantize ' , 'tf.deserialize_many_sparse ' : 'tf.io.deserialize_many_sparse ' , 'tf.diag ' : 'tf.linalg.tensor_diag ' , 'tf.diag_part ' : 'tf.linalg.tensor_diag_part ' , 'tf.digamma ' : 'tf.math.digamma ' , 'tf.dimension_at_index ' : 'tf.compat.dimension_at_index ' , 'tf.dimension_value ' : 'tf.compat.dimension_value ' , 'tf.disable_control_flow_v2 ' : 'tf.compat.v1.disable_control_flow_v2 ' , 'tf.disable_eager_execution ' : 'tf.compat.v1.disable_eager_execution ' , 'tf.disable_resource_variables ' : 'tf.compat.v1.disable_resource_variables ' , 'tf.disable_tensor_equality ' : 'tf.compat.v1.disable_tensor_equality ' , 'tf.disable_v2_behavior ' : 'tf.compat.v1.disable_v2_behavior ' , 'tf.disable_v2_tensorshape ' : 'tf.compat.v1.disable_v2_tensorshape ' , 'tf.distribute.get_loss_reduction ' : 'tf.compat.v1.distribute.get_loss_reduction ' , 'tf.distributions.bernoulli ' : 'tf.compat.v1.distributions.bernoulli ' , 'tf.distributions.beta ' : 'tf.compat.v1.distributions.beta ' , 'tf.distributions.categorical ' : 'tf.compat.v1.distributions.categorical ' , 'tf.distributions.dirichlet ' : 'tf.compat.v1.distributions.dirichlet ' , 'tf.distributions.dirichletmultinomial ' : 'tf.compat.v1.distributions.dirichletmultinomial ' , 'tf.distributions.distribution ' : 'tf.compat.v1.distributions.distribution ' , 'tf.distributions.exponential ' : 'tf.compat.v1.distributions.exponential ' , 'tf.distributions.fully_reparameterized ' : 'tf.compat.v1.distributions.fully_reparameterized ' , 'tf.distributions.gamma ' : 'tf.compat.v1.distributions.gamma ' , 'tf.distributions.laplace ' : 'tf.compat.v1.distributions.laplace ' , 'tf.distributions.multinomial ' : 'tf.compat.v1.distributions.multinomial ' , 'tf.distributions.not_reparameterized ' : 'tf.compat.v1.distributions.not_reparameterized ' , 'tf.distributions.normal ' : 'tf.compat.v1.distributions.normal ' , 'tf.distributions.registerkl ' : 'tf.compat.v1.distributions.registerkl ' , 'tf.distributions.reparameterizationtype ' : 'tf.compat.v1.distributions.reparameterizationtype ' , 'tf.distributions.studentt ' : 'tf.compat.v1.distributions.studentt ' , 'tf.distributions.uniform ' : 'tf.compat.v1.distributions.uniform ' , 'tf.distributions.kl_divergence ' : 'tf.compat.v1.distributions.kl_divergence ' , 'tf.div ' : 'tf.compat.v1.div ' , 'tf.div_no_nan ' : 'tf.math.divide_no_nan ' , 'tf.dtypes.as_string ' : 'tf.strings.as_string ' , 'tf.enable_control_flow_v2 ' : 'tf.compat.v1.enable_control_flow_v2 ' , 'tf.enable_eager_execution ' : 'tf.compat.v1.enable_eager_execution ' , 'tf.enable_resource_variables ' : 'tf.compat.v1.enable_resource_variables ' , 'tf.enable_tensor_equality ' : 'tf.compat.v1.enable_tensor_equality ' , 'tf.enable_v2_behavior ' : 'tf.compat.v1.enable_v2_behavior ' , 'tf.enable_v2_tensorshape ' : 'tf.compat.v1.enable_v2_tensorshape ' , 'tf.encode_base64 ' : 'tf.io.encode_base64 ' , 'tf.erf ' : 'tf.math.erf ' , 'tf.erfc ' : 'tf.math.erfc ' , 'tf.executing_eagerly_outside_functions ' : 'tf.compat.v1.executing_eagerly_outside_functions ' , 'tf.experimental.output_all_intermediates ' : 'tf.compat.v1.experimental.output_all_intermediates ' , 'tf.expm1 ' : 'tf.math.expm1 ' , 'tf.fake_quant_with_min_max_args ' : 'tf.quantization.fake_quant_with_min_max_args ' , 'tf.fake_quant_with_min_max_args_gradient ' : 'tf.quantization.fake_quant_with_min_max_args_gradient ' , 'tf.fake_quant_with_min_max_vars ' : 'tf.quantization.fake_quant_with_min_max_vars ' , 'tf.fake_quant_with_min_max_vars_gradient ' : 'tf.quantization.fake_quant_with_min_max_vars_gradient ' , 'tf.fake_quant_with_min_max_vars_per_channel ' : 'tf.quantization.fake_quant_with_min_max_vars_per_channel ' , 'tf.fake_quant_with_min_max_vars_per_channel_gradient ' : 'tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient ' , 'tf.feature_column.input_layer ' : 'tf.compat.v1.feature_column.input_layer ' , 'tf.feature_column.linear_model ' : 'tf.compat.v1.feature_column.linear_model ' , 'tf.feature_column.shared_embedding_columns ' : 'tf.compat.v1.feature_column.shared_embedding_columns ' , 'tf.fft ' : 'tf.signal.fft ' , 'tf.fft2d ' : 'tf.signal.fft2d ' , 'tf.fft3d ' : 'tf.signal.fft3d ' , 'tf.fixed_size_partitioner ' : 'tf.compat.v1.fixed_size_partitioner ' , 'tf.floordiv ' : 'tf.math.floordiv ' , 'tf.floormod ' : 'tf.math.floormod ' , 'tf.get_collection ' : 'tf.compat.v1.get_collection ' , 'tf.get_collection_ref ' : 'tf.compat.v1.get_collection_ref ' , 'tf.get_default_graph ' : 'tf.compat.v1.get_default_graph ' , 'tf.get_default_session ' : 'tf.compat.v1.get_default_session ' , 'tf.get_local_variable ' : 'tf.compat.v1.get_local_variable ' , 'tf.get_seed ' : 'tf.compat.v1.get_seed ' , 'tf.get_session_handle ' : 'tf.compat.v1.get_session_handle ' , 'tf.get_session_tensor ' : 'tf.compat.v1.get_session_tensor ' , 'tf.get_variable ' : 'tf.compat.v1.get_variable ' , 'tf.get_variable_scope ' : 'tf.compat.v1.get_variable_scope ' , 'tf.gfile.fastgfile ' : 'tf.compat.v1.gfile.fastgfile ' , 'tf.global_norm ' : 'tf.linalg.global_norm ' , 'tf.global_variables ' : 'tf.compat.v1.global_variables ' , 'tf.global_variables_initializer ' : 'tf.compat.v1.global_variables_initializer ' , 'tf.graph_util.convert_variables_to_constants ' : 'tf.compat.v1.graph_util.convert_variables_to_constants ' , 'tf.graph_util.extract_sub_graph ' : 'tf.compat.v1.graph_util.extract_sub_graph ' , 'tf.graph_util.must_run_on_cpu ' : 'tf.compat.v1.graph_util.must_run_on_cpu ' , 'tf.graph_util.remove_training_nodes ' : 'tf.compat.v1.graph_util.remove_training_nodes ' , 'tf.graph_util.tensor_shape_from_node_def_name ' : 'tf.compat.v1.graph_util.tensor_shape_from_node_def_name ' , 'tf.ifft ' : 'tf.signal.ifft ' , 'tf.ifft2d ' : 'tf.signal.ifft2d ' , 'tf.ifft3d ' : 'tf.signal.ifft3d ' , 'tf.igamma ' : 'tf.math.igamma ' , 'tf.igammac ' : 'tf.math.igammac ' , 'tf.imag ' : 'tf.math.imag ' , 'tf.image.resize_area ' : 'tf.compat.v1.image.resize_area ' , 'tf.image.resize_bicubic ' : 'tf.compat.v1.image.resize_bicubic ' , 'tf.image.resize_bilinear ' : 'tf.compat.v1.image.resize_bilinear ' , 'tf.image.resize_image_with_crop_or_pad ' : 'tf.image.resize_with_crop_or_pad ' , 'tf.image.resize_image_with_pad ' : 'tf.compat.v1.image.resize_image_with_pad ' , 'tf.image.resize_nearest_neighbor ' : 'tf.compat.v1.image.resize_nearest_neighbor ' , 'tf.image.transpose_image ' : 'tf.image.transpose ' , 'tf.initialize_all_tables ' : 'tf.compat.v1.initialize_all_tables ' , 'tf.initialize_all_variables ' : 'tf.compat.v1.initialize_all_variables ' , 'tf.initialize_local_variables ' : 'tf.compat.v1.initialize_local_variables ' , 'tf.initialize_variables ' : 'tf.compat.v1.initialize_variables ' , 'tf.initializers.global_variables ' : 'tf.compat.v1.initializers.global_variables ' , 'tf.initializers.local_variables ' : 'tf.compat.v1.initializers.local_variables ' , 'tf.initializers.tables_initializer ' : 'tf.compat.v1.initializers.tables_initializer ' , 'tf.initializers.uniform_unit_scaling ' : 'tf.compat.v1.initializers.uniform_unit_scaling ' , 'tf.initializers.variables ' : 'tf.compat.v1.initializers.variables ' , 'tf.invert_permutation ' : 'tf.math.invert_permutation ' , 'tf.io.paddingfifoqueue ' : 'tf.queue.paddingfifoqueue ' , 'tf.io.priorityqueue ' : 'tf.queue.priorityqueue ' , 'tf.io.queuebase ' : 'tf.queue.queuebase ' , 'tf.io.randomshufflequeue ' : 'tf.queue.randomshufflequeue ' , 'tf.io.tfrecordcompressiontype ' : 'tf.compat.v1.io.tfrecordcompressiontype ' , 'tf.io.tf_record_iterator ' : 'tf.compat.v1.io.tf_record_iterator ' , 'tf.is_finite ' : 'tf.math.is_finite ' , 'tf.is_inf ' : 'tf.math.is_inf ' , 'tf.is_nan ' : 'tf.math.is_nan ' , 'tf.is_non_decreasing ' : 'tf.math.is_non_decreasing ' , 'tf.is_numeric_tensor ' : 'tf.debugging.is_numeric_tensor ' , 'tf.is_strictly_increasing ' : 'tf.math.is_strictly_increasing ' , 'tf.is_variable_initialized ' : 'tf.compat.v1.is_variable_initialized ' , 'tf.keras.backend.get_session ' : 'tf.compat.v1.keras.backend.get_session ' , 'tf.keras.backend.set_session ' : 'tf.compat.v1.keras.backend.set_session ' , 'tf.keras.layers.cudnngru ' : 'tf.compat.v1.keras.layers.cudnngru ' , 'tf.keras.layers.cudnnlstm ' : 'tf.compat.v1.keras.layers.cudnnlstm ' , 'tf.keras.layers.disable_v2_dtype_behavior ' : 'tf.compat.v1.keras.layers.disable_v2_dtype_behavior ' , 'tf.keras.layers.enable_v2_dtype_behavior ' : 'tf.compat.v1.keras.layers.enable_v2_dtype_behavior ' , 'tf.keras.losses.cosine ' : 'tf.keras.losses.cosine_similarity ' , 'tf.keras.losses.cosine_proximity ' : 'tf.keras.losses.cosine_similarity ' , 'tf.keras.metrics.cosine ' : 'tf.keras.losses.cosine_similarity ' , 'tf.keras.metrics.cosine_proximity ' : 'tf.keras.losses.cosine_similarity ' , 'tf.keras.models.linearmodel ' : 'tf.keras.experimental.linearmodel ' , 'tf.keras.models.widedeepmodel ' : 'tf.keras.experimental.widedeepmodel ' , 'tf.keras.optimizers.adadelta ' : 'tf.keras.optimizers.legacy.adadelta ' , 'tf.keras.optimizers.adagrad ' : 'tf.keras.optimizers.legacy.adagrad ' , 'tf.keras.optimizers.adam ' : 'tf.keras.optimizers.legacy.adam ' , 'tf.keras.optimizers.adamax ' : 'tf.keras.optimizers.legacy.adamax ' , 'tf.keras.optimizers.ftrl ' : 'tf.keras.optimizers.legacy.ftrl ' , 'tf.keras.optimizers.nadam ' : 'tf.keras.optimizers.legacy.nadam ' , 'tf.keras.optimizers.optimizer ' : 'tf.keras.optimizers.legacy.optimizer ' , 'tf.keras.optimizers.rmsprop ' : 'tf.keras.optimizers.legacy.rmsprop ' , 'tf.keras.optimizers.sgd ' : 'tf.keras.optimizers.legacy.sgd ' , 'tf.keras.utils.deterministicrandomtesttool ' : 'tf.compat.v1.keras.utils.deterministicrandomtesttool ' , 'tf.keras.utils.get_or_create_layer ' : 'tf.compat.v1.keras.utils.get_or_create_layer ' , 'tf.keras.utils.track_tf1_style_variables ' : 'tf.compat.v1.keras.utils.track_tf1_style_variables ' , 'tf.layers.batchnormalization ' : 'tf.compat.v1.layers.batchnormalization ' , 'tf.layers.inputspec ' : 'tf.keras.layers.inputspec ' , 'tf.layers.batch_normalization ' : 'tf.compat.v1.layers.batch_normalization ' , 'tf.lbeta ' : 'tf.math.lbeta ' , 'tf.lgamma ' : 'tf.math.lgamma ' , 'tf.lin_space ' : 'tf.linspace ' , 'tf.linalg.transpose ' : 'tf.linalg.matrix_transpose ' , 'tf.lite.ophint ' : 'tf.compat.v1.lite.ophint ' , 'tf.lite.tococonverter ' : 'tf.compat.v1.lite.tococonverter ' , 'tf.lite.constants.graphviz_dot ' : 'tf.compat.v1.lite.constants.graphviz_dot ' , 'tf.lite.constants.tflite ' : 'tf.compat.v1.lite.constants.tflite ' , 'tf.lite.experimental.convert_op_hints_to_stubs ' : 'tf.compat.v1.lite.experimental.convert_op_hints_to_stubs ' , 'tf.lite.toco_convert ' : 'tf.compat.v1.lite.toco_convert ' , 'tf.local_variables ' : 'tf.compat.v1.local_variables ' , 'tf.local_variables_initializer ' : 'tf.compat.v1.local_variables_initializer ' , 'tf.log ' : 'tf.math.log ' , 'tf.log1p ' : 'tf.math.log1p ' , 'tf.log_sigmoid ' : 'tf.math.log_sigmoid ' , 'tf.logging.debug ' : 'tf.compat.v1.logging.debug ' , 'tf.logging.error ' : 'tf.compat.v1.logging.error ' , 'tf.logging.fatal ' : 'tf.compat.v1.logging.fatal ' , 'tf.logging.info ' : 'tf.compat.v1.logging.info ' , 'tf.logging.tasklevelstatusmessage ' : 'tf.compat.v1.logging.tasklevelstatusmessage ' , 'tf.logging.warn ' : 'tf.compat.v1.logging.warn ' , 'tf.logging.debug ' : 'tf.compat.v1.logging.debug ' , 'tf.logging.error ' : 'tf.compat.v1.logging.error ' , 'tf.logging.fatal ' : 'tf.compat.v1.logging.fatal ' , 'tf.logging.flush ' : 'tf.compat.v1.logging.flush ' , 'tf.logging.get_verbosity ' : 'tf.compat.v1.logging.get_verbosity ' , 'tf.logging.info ' : 'tf.compat.v1.logging.info ' , 'tf.logging.log ' : 'tf.compat.v1.logging.log ' , 'tf.logging.log_every_n ' : 'tf.compat.v1.logging.log_every_n ' , 'tf.logging.log_first_n ' : 'tf.compat.v1.logging.log_first_n ' , 'tf.logging.log_if ' : 'tf.compat.v1.logging.log_if ' , 'tf.logging.set_verbosity ' : 'tf.compat.v1.logging.set_verbosity ' , 'tf.logging.vlog ' : 'tf.compat.v1.logging.vlog ' , 'tf.logging.warn ' : 'tf.compat.v1.logging.warn ' , 'tf.logging.warning ' : 'tf.compat.v1.logging.warning ' , 'tf.logical_xor ' : 'tf.math.logical_xor ' , 'tf.losses.reduction ' : 'tf.compat.v1.losses.reduction ' , 'tf.losses.absolute_difference ' : 'tf.compat.v1.losses.absolute_difference ' , 'tf.losses.add_loss ' : 'tf.compat.v1.losses.add_loss ' , 'tf.losses.compute_weighted_loss ' : 'tf.compat.v1.losses.compute_weighted_loss ' , 'tf.losses.cosine_distance ' : 'tf.compat.v1.losses.cosine_distance ' , 'tf.losses.get_losses ' : 'tf.compat.v1.losses.get_losses ' , 'tf.losses.get_regularization_loss ' : 'tf.compat.v1.losses.get_regularization_loss ' , 'tf.losses.get_regularization_losses ' : 'tf.compat.v1.losses.get_regularization_losses ' , 'tf.losses.get_total_loss ' : 'tf.compat.v1.losses.get_total_loss ' , 'tf.losses.hinge_loss ' : 'tf.compat.v1.losses.hinge_loss ' , 'tf.losses.huber_loss ' : 'tf.compat.v1.losses.huber_loss ' , 'tf.losses.log_loss ' : 'tf.compat.v1.losses.log_loss ' , 'tf.losses.mean_pairwise_squared_error ' : 'tf.compat.v1.losses.mean_pairwise_squared_error ' , 'tf.losses.mean_squared_error ' : 'tf.compat.v1.losses.mean_squared_error ' , 'tf.losses.sigmoid_cross_entropy ' : 'tf.compat.v1.losses.sigmoid_cross_entropy ' , 'tf.losses.softmax_cross_entropy ' : 'tf.compat.v1.losses.softmax_cross_entropy ' , 'tf.losses.sparse_softmax_cross_entropy ' : 'tf.compat.v1.losses.sparse_softmax_cross_entropy ' , 'tf.make_template ' : 'tf.compat.v1.make_template ' , 'tf.manip.gather_nd ' : 'tf.gather_nd ' , 'tf.manip.reshape ' : 'tf.reshape ' , 'tf.manip.reverse ' : 'tf.reverse ' , 'tf.manip.roll ' : 'tf.roll ' , 'tf.manip.scatter_nd ' : 'tf.scatter_nd ' , 'tf.manip.space_to_batch_nd ' : 'tf.space_to_batch_nd ' , 'tf.manip.tile ' : 'tf.tile ' , 'tf.matching_files ' : 'tf.io.matching_files ' , 'tf.matrix_band_part ' : 'tf.linalg.band_part ' , 'tf.matrix_determinant ' : 'tf.linalg.det ' , 'tf.matrix_diag ' : 'tf.linalg.diag ' , 'tf.matrix_diag_part ' : 'tf.linalg.diag_part ' , 'tf.matrix_inverse ' : 'tf.linalg.inv ' , 'tf.matrix_set_diag ' : 'tf.linalg.set_diag ' , 'tf.matrix_solve ' : 'tf.linalg.solve ' , 'tf.matrix_solve_ls ' : 'tf.linalg.lstsq ' , 'tf.matrix_transpose ' : 'tf.linalg.matrix_transpose ' , 'tf.matrix_triangular_solve ' : 'tf.linalg.triangular_solve ' , 'tf.metrics.accuracy ' : 'tf.compat.v1.metrics.accuracy ' , 'tf.metrics.auc ' : 'tf.compat.v1.metrics.auc ' , 'tf.metrics.average_precision_at_k ' : 'tf.compat.v1.metrics.average_precision_at_k ' , 'tf.metrics.false_negatives ' : 'tf.compat.v1.metrics.false_negatives ' , 'tf.metrics.false_negatives_at_thresholds ' : 'tf.compat.v1.metrics.false_negatives_at_thresholds ' , 'tf.metrics.false_positives ' : 'tf.compat.v1.metrics.false_positives ' , 'tf.metrics.false_positives_at_thresholds ' : 'tf.compat.v1.metrics.false_positives_at_thresholds ' , 'tf.metrics.mean ' : 'tf.compat.v1.metrics.mean ' , 'tf.metrics.mean_absolute_error ' : 'tf.compat.v1.metrics.mean_absolute_error ' , 'tf.metrics.mean_cosine_distance ' : 'tf.compat.v1.metrics.mean_cosine_distance ' , 'tf.metrics.mean_iou ' : 'tf.compat.v1.metrics.mean_iou ' , 'tf.metrics.mean_per_class_accuracy ' : 'tf.compat.v1.metrics.mean_per_class_accuracy ' , 'tf.metrics.mean_relative_error ' : 'tf.compat.v1.metrics.mean_relative_error ' , 'tf.metrics.mean_squared_error ' : 'tf.compat.v1.metrics.mean_squared_error ' , 'tf.metrics.mean_tensor ' : 'tf.compat.v1.metrics.mean_tensor ' , 'tf.metrics.percentage_below ' : 'tf.compat.v1.metrics.percentage_below ' , 'tf.metrics.precision ' : 'tf.compat.v1.metrics.precision ' , 'tf.metrics.precision_at_k ' : 'tf.compat.v1.metrics.precision_at_k ' , 'tf.metrics.precision_at_thresholds ' : 'tf.compat.v1.metrics.precision_at_thresholds ' , 'tf.metrics.precision_at_top_k ' : 'tf.compat.v1.metrics.precision_at_top_k ' , 'tf.metrics.recall ' : 'tf.compat.v1.metrics.recall ' , 'tf.metrics.recall_at_k ' : 'tf.compat.v1.metrics.recall_at_k ' , 'tf.metrics.recall_at_thresholds ' : 'tf.compat.v1.metrics.recall_at_thresholds ' , 'tf.metrics.recall_at_top_k ' : 'tf.compat.v1.metrics.recall_at_top_k ' , 'tf.metrics.root_mean_squared_error ' : 'tf.compat.v1.metrics.root_mean_squared_error ' , 'tf.metrics.sensitivity_at_specificity ' : 'tf.compat.v1.metrics.sensitivity_at_specificity ' , 'tf.metrics.sparse_average_precision_at_k ' : 'tf.compat.v1.metrics.sparse_average_precision_at_k ' , 'tf.metrics.sparse_precision_at_k ' : 'tf.compat.v1.metrics.sparse_precision_at_k ' , 'tf.metrics.specificity_at_sensitivity ' : 'tf.compat.v1.metrics.specificity_at_sensitivity ' , 'tf.metrics.true_negatives ' : 'tf.compat.v1.metrics.true_negatives ' , 'tf.metrics.true_negatives_at_thresholds ' : 'tf.compat.v1.metrics.true_negatives_at_thresholds ' , 'tf.metrics.true_positives ' : 'tf.compat.v1.metrics.true_positives ' , 'tf.metrics.true_positives_at_thresholds ' : 'tf.compat.v1.metrics.true_positives_at_thresholds ' , 'tf.min_max_variable_partitioner ' : 'tf.compat.v1.min_max_variable_partitioner ' , 'tf.mixed_precision.dynamiclossscale ' : 'tf.compat.v1.mixed_precision.dynamiclossscale ' , 'tf.mixed_precision.fixedlossscale ' : 'tf.compat.v1.mixed_precision.fixedlossscale ' , 'tf.mixed_precision.lossscale ' : 'tf.compat.v1.mixed_precision.lossscale ' , 'tf.mixed_precision.mixedprecisionlossscaleoptimizer ' : 'tf.compat.v1.mixed_precision.mixedprecisionlossscaleoptimizer ' , 'tf.mixed_precision.disable_mixed_precision_graph_rewrite ' : 'tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite ' , 'tf.mixed_precision.enable_mixed_precision_graph_rewrite ' : 'tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite ' , 'tf.mixed_precision.experimental.dynamiclossscale ' : 'tf.compat.v1.mixed_precision.experimental.dynamiclossscale ' , 'tf.mixed_precision.experimental.fixedlossscale ' : 'tf.compat.v1.mixed_precision.experimental.fixedlossscale ' , 'tf.mixed_precision.experimental.lossscale ' : 'tf.compat.v1.mixed_precision.experimental.lossscale ' , 'tf.mod ' : 'tf.math.floormod ' , 'tf.model_variables ' : 'tf.compat.v1.model_variables ' , 'tf.moving_average_variables ' : 'tf.compat.v1.moving_average_variables ' , 'tf.nn.avg_pool_v2 ' : 'tf.nn.avg_pool ' , 'tf.nn.bidirectional_dynamic_rnn ' : 'tf.compat.v1.nn.bidirectional_dynamic_rnn ' , 'tf.nn.conv2d_backprop_filter ' : 'tf.compat.v1.nn.conv2d_backprop_filter ' , 'tf.nn.conv3d_backprop_filter ' : 'tf.compat.v1.nn.conv3d_backprop_filter ' , 'tf.nn.conv3d_backprop_filter_v2 ' : 'tf.compat.v1.nn.conv3d_backprop_filter_v2 ' , 'tf.nn.ctc_beam_search_decoder_v2 ' : 'tf.nn.ctc_beam_search_decoder ' , 'tf.nn.ctc_loss_v2 ' : 'tf.compat.v1.nn.ctc_loss_v2 ' , 'tf.nn.depthwise_conv2d_native ' : 'tf.compat.v1.nn.depthwise_conv2d_native ' , 'tf.nn.depthwise_conv2d_native_backprop_filter ' : 'tf.nn.depthwise_conv2d_backprop_filter ' , 'tf.nn.depthwise_conv2d_native_backprop_input ' : 'tf.nn.depthwise_conv2d_backprop_input ' , 'tf.nn.dynamic_rnn ' : 'tf.compat.v1.nn.dynamic_rnn ' , 'tf.nn.log_uniform_candidate_sampler ' : 'tf.random.log_uniform_candidate_sampler ' , 'tf.nn.max_pool_v2 ' : 'tf.nn.max_pool ' , 'tf.nn.quantized_avg_pool ' : 'tf.compat.v1.nn.quantized_avg_pool ' , 'tf.nn.quantized_conv2d ' : 'tf.compat.v1.nn.quantized_conv2d ' , 'tf.nn.quantized_max_pool ' : 'tf.compat.v1.nn.quantized_max_pool ' , 'tf.nn.quantized_relu_x ' : 'tf.compat.v1.nn.quantized_relu_x ' , 'tf.nn.raw_rnn ' : 'tf.compat.v1.nn.raw_rnn ' , 'tf.nn.relu_layer ' : 'tf.compat.v1.nn.relu_layer ' , 'tf.nn.rnn_cell.basiclstmcell ' : 'tf.compat.v1.nn.rnn_cell.basiclstmcell ' , 'tf.nn.rnn_cell.basicrnncell ' : 'tf.compat.v1.nn.rnn_cell.basicrnncell ' , 'tf.nn.rnn_cell.devicewrapper ' : 'tf.compat.v1.nn.rnn_cell.devicewrapper ' , 'tf.nn.rnn_cell.dropoutwrapper ' : 'tf.compat.v1.nn.rnn_cell.dropoutwrapper ' , 'tf.nn.rnn_cell.grucell ' : 'tf.compat.v1.nn.rnn_cell.grucell ' , 'tf.nn.rnn_cell.lstmcell ' : 'tf.compat.v1.nn.rnn_cell.lstmcell ' , 'tf.nn.rnn_cell.lstmstatetuple ' : 'tf.compat.v1.nn.rnn_cell.lstmstatetuple ' , 'tf.nn.rnn_cell.multirnncell ' : 'tf.compat.v1.nn.rnn_cell.multirnncell ' , 'tf.nn.rnn_cell.rnncell ' : 'tf.compat.v1.nn.rnn_cell.rnncell ' , 'tf.nn.rnn_cell.residualwrapper ' : 'tf.compat.v1.nn.rnn_cell.residualwrapper ' , 'tf.nn.static_bidirectional_rnn ' : 'tf.compat.v1.nn.static_bidirectional_rnn ' , 'tf.nn.static_rnn ' : 'tf.compat.v1.nn.static_rnn ' , 'tf.nn.static_state_saving_rnn ' : 'tf.compat.v1.nn.static_state_saving_rnn ' , 'tf.nn.uniform_candidate_sampler ' : 'tf.random.uniform_candidate_sampler ' , 'tf.nn.xw_plus_b ' : 'tf.compat.v1.nn.xw_plus_b ' , 'tf.no_regularizer ' : 'tf.compat.v1.no_regularizer ' , 'tf.op_scope ' : 'tf.compat.v1.op_scope ' , 'tf.parse_single_sequence_example ' : 'tf.io.parse_single_sequence_example ' , 'tf.parse_tensor ' : 'tf.io.parse_tensor ' , 'tf.placeholder ' : 'tf.compat.v1.placeholder ' , 'tf.placeholder_with_default ' : 'tf.compat.v1.placeholder_with_default ' , 'tf.polygamma ' : 'tf.math.polygamma ' , 'tf.profiler.adviceproto ' : 'tf.compat.v1.profiler.adviceproto ' , 'tf.profiler.graphnodeproto ' : 'tf.compat.v1.profiler.graphnodeproto ' , 'tf.profiler.multigraphnodeproto ' : 'tf.compat.v1.profiler.multigraphnodeproto ' , 'tf.profiler.oplogproto ' : 'tf.compat.v1.profiler.oplogproto ' , 'tf.profiler.profileoptionbuilder ' : 'tf.compat.v1.profiler.profileoptionbuilder ' , 'tf.profiler.profiler ' : 'tf.compat.v1.profiler.profiler ' , 'tf.profiler.advise ' : 'tf.compat.v1.profiler.advise ' , 'tf.profiler.profile ' : 'tf.compat.v1.profiler.profile ' , 'tf.profiler.write_op_log ' : 'tf.compat.v1.profiler.write_op_log ' , 'tf.py_func ' : 'tf.compat.v1.py_func ' , 'tf.python_io.tfrecordcompressiontype ' : 'tf.compat.v1.python_io.tfrecordcompressiontype ' , 'tf.python_io.tfrecordoptions ' : 'tf.io.tfrecordoptions ' , 'tf.python_io.tfrecordwriter ' : 'tf.io.tfrecordwriter ' , 'tf.python_io.tf_record_iterator ' : 'tf.compat.v1.python_io.tf_record_iterator ' , 'tf.qr ' : 'tf.linalg.qr ' , 'tf.quantize ' : 'tf.quantization.quantize ' , 'tf.quantized_concat ' : 'tf.quantization.quantized_concat ' , 'tf.ragged.raggedtensorvalue ' : 'tf.compat.v1.ragged.raggedtensorvalue ' , 'tf.ragged.constant_value ' : 'tf.compat.v1.ragged.constant_value ' , 'tf.ragged.placeholder ' : 'tf.compat.v1.ragged.placeholder ' , 'tf.random.get_seed ' : 'tf.compat.v1.random.get_seed ' , 'tf.random.set_random_seed ' : 'tf.compat.v1.random.set_random_seed ' , 'tf.random_crop ' : 'tf.image.random_crop ' , 'tf.random_gamma ' : 'tf.random.gamma ' , 'tf.random_normal ' : 'tf.random.normal ' , 'tf.random_poisson ' : 'tf.random.poisson ' , 'tf.random_shuffle ' : 'tf.random.shuffle ' , 'tf.random_uniform ' : 'tf.random.uniform ' , 'tf.read_file ' : 'tf.io.read_file ' , 'tf.real ' : 'tf.math.real ' , 'tf.reciprocal ' : 'tf.math.reciprocal ' , 'tf.regex_replace ' : 'tf.strings.regex_replace ' , 'tf.report_uninitialized_variables ' : 'tf.compat.v1.report_uninitialized_variables ' , 'tf.reset_default_graph ' : 'tf.compat.v1.reset_default_graph ' , 'tf.resource_loader.get_data_files_path ' : 'tf.compat.v1.resource_loader.get_data_files_path ' , 'tf.resource_loader.get_path_to_datafile ' : 'tf.compat.v1.resource_loader.get_path_to_datafile ' , 'tf.resource_loader.get_root_dir_with_all_resources ' : 'tf.compat.v1.resource_loader.get_root_dir_with_all_resources ' , 'tf.resource_loader.load_resource ' : 'tf.compat.v1.resource_loader.load_resource ' , 'tf.resource_loader.readahead_file_path ' : 'tf.compat.v1.resource_loader.readahead_file_path ' , 'tf.resource_variables_enabled ' : 'tf.compat.v1.resource_variables_enabled ' , 'tf.reverse_v2 ' : 'tf.reverse ' , 'tf.rint ' : 'tf.math.rint ' , 'tf.rsqrt ' : 'tf.math.rsqrt ' , 'tf.saved_model.builder ' : 'tf.compat.v1.saved_model.builder ' , 'tf.saved_model.legacy_init_op_key ' : 'tf.compat.v1.saved_model.legacy_init_op_key ' , 'tf.saved_model.main_op_key ' : 'tf.compat.v1.saved_model.main_op_key ' , 'tf.saved_model.build_signature_def ' : 'tf.compat.v1.saved_model.build_signature_def ' , 'tf.saved_model.build_tensor_info ' : 'tf.compat.v1.saved_model.build_tensor_info ' , 'tf.saved_model.builder.savedmodelbuilder ' : 'tf.compat.v1.saved_model.builder.savedmodelbuilder ' , 'tf.saved_model.classification_signature_def ' : 'tf.compat.v1.saved_model.classification_signature_def ' , 'tf.saved_model.constants.assets_directory ' : 'tf.saved_model.assets_directory ' , 'tf.saved_model.constants.assets_key ' : 'tf.saved_model.assets_key ' , 'tf.saved_model.constants.debug_directory ' : 'tf.saved_model.debug_directory ' , 'tf.saved_model.constants.debug_info_filename_pb ' : 'tf.saved_model.debug_info_filename_pb ' , 'tf.saved_model.constants.legacy_init_op_key ' : 'tf.compat.v1.saved_model.constants.legacy_init_op_key ' , 'tf.saved_model.constants.main_op_key ' : 'tf.compat.v1.saved_model.constants.main_op_key ' , 'tf.saved_model.constants.saved_model_filename_pb ' : 'tf.saved_model.saved_model_filename_pb ' , 'tf.saved_model.constants.saved_model_filename_pbtxt ' : 'tf.saved_model.saved_model_filename_pbtxt ' , 'tf.saved_model.constants.saved_model_schema_version ' : 'tf.saved_model.saved_model_schema_version ' , 'tf.saved_model.constants.variables_directory ' : 'tf.saved_model.variables_directory ' , 'tf.saved_model.constants.variables_filename ' : 'tf.saved_model.variables_filename ' , 'tf.saved_model.experimental.save ' : 'tf.saved_model.save ' , 'tf.saved_model.get_tensor_from_tensor_info ' : 'tf.compat.v1.saved_model.get_tensor_from_tensor_info ' , 'tf.saved_model.is_valid_signature ' : 'tf.compat.v1.saved_model.is_valid_signature ' , 'tf.saved_model.loader.maybe_saved_model_directory ' : 'tf.saved_model.contains_saved_model ' , 'tf.saved_model.main_op.main_op ' : 'tf.compat.v1.saved_model.main_op.main_op ' , 'tf.saved_model.main_op.main_op_with_restore ' : 'tf.compat.v1.saved_model.main_op.main_op_with_restore ' , 'tf.saved_model.main_op_with_restore ' : 'tf.compat.v1.saved_model.main_op_with_restore ' , 'tf.saved_model.maybe_saved_model_directory ' : 'tf.saved_model.contains_saved_model ' , 'tf.saved_model.predict_signature_def ' : 'tf.compat.v1.saved_model.predict_signature_def ' , 'tf.saved_model.regression_signature_def ' : 'tf.compat.v1.saved_model.regression_signature_def ' , 'tf.saved_model.signature_constants.classify_inputs ' : 'tf.saved_model.classify_inputs ' , 'tf.saved_model.signature_constants.classify_method_name ' : 'tf.saved_model.classify_method_name ' , 'tf.saved_model.signature_constants.classify_output_classes ' : 'tf.saved_model.classify_output_classes ' , 'tf.saved_model.signature_constants.classify_output_scores ' : 'tf.saved_model.classify_output_scores ' , 'tf.saved_model.signature_constants.default_serving_signature_def_key ' : 'tf.saved_model.default_serving_signature_def_key ' , 'tf.saved_model.signature_constants.predict_inputs ' : 'tf.saved_model.predict_inputs ' , 'tf.saved_model.signature_constants.predict_method_name ' : 'tf.saved_model.predict_method_name ' , 'tf.saved_model.signature_constants.predict_outputs ' : 'tf.saved_model.predict_outputs ' , 'tf.saved_model.signature_constants.regress_inputs ' : 'tf.saved_model.regress_inputs ' , 'tf.saved_model.signature_constants.regress_method_name ' : 'tf.saved_model.regress_method_name ' , 'tf.saved_model.signature_constants.regress_outputs ' : 'tf.saved_model.regress_outputs ' , 'tf.saved_model.signature_def_utils.methodnameupdater ' : 'tf.compat.v1.saved_model.signature_def_utils.methodnameupdater ' , 'tf.saved_model.signature_def_utils.build_signature_def ' : 'tf.compat.v1.saved_model.signature_def_utils.build_signature_def ' , 'tf.saved_model.signature_def_utils.classification_signature_def ' : 'tf.compat.v1.saved_model.signature_def_utils.classification_signature_def ' , 'tf.saved_model.signature_def_utils.is_valid_signature ' : 'tf.compat.v1.saved_model.signature_def_utils.is_valid_signature ' , 'tf.saved_model.signature_def_utils.predict_signature_def ' : 'tf.compat.v1.saved_model.signature_def_utils.predict_signature_def ' , 'tf.saved_model.signature_def_utils.regression_signature_def ' : 'tf.compat.v1.saved_model.signature_def_utils.regression_signature_def ' , 'tf.saved_model.simple_save ' : 'tf.compat.v1.saved_model.simple_save ' , 'tf.saved_model.tag_constants.gpu ' : 'tf.saved_model.gpu ' , 'tf.saved_model.tag_constants.serving ' : 'tf.saved_model.serving ' , 'tf.saved_model.tag_constants.tpu ' : 'tf.saved_model.tpu ' , 'tf.saved_model.tag_constants.training ' : 'tf.saved_model.training ' , 'tf.saved_model.utils.build_tensor_info ' : 'tf.compat.v1.saved_model.utils.build_tensor_info ' , 'tf.saved_model.utils.get_tensor_from_tensor_info ' : 'tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info ' , 'tf.scatter_add ' : 'tf.compat.v1.scatter_add ' , 'tf.scatter_div ' : 'tf.compat.v1.scatter_div ' , 'tf.scatter_max ' : 'tf.compat.v1.scatter_max ' , 'tf.scatter_min ' : 'tf.compat.v1.scatter_min ' , 'tf.scatter_mul ' : 'tf.compat.v1.scatter_mul ' , 'tf.scatter_nd_add ' : 'tf.compat.v1.scatter_nd_add ' , 'tf.scatter_nd_sub ' : 'tf.compat.v1.scatter_nd_sub ' , 'tf.scatter_nd_update ' : 'tf.compat.v1.scatter_nd_update ' , 'tf.scatter_sub ' : 'tf.compat.v1.scatter_sub ' , 'tf.scatter_update ' : 'tf.compat.v1.scatter_update ' , 'tf.segment_max ' : 'tf.math.segment_max ' , 'tf.segment_mean ' : 'tf.math.segment_mean ' , 'tf.segment_min ' : 'tf.math.segment_min ' , 'tf.segment_prod ' : 'tf.math.segment_prod ' , 'tf.segment_sum ' : 'tf.math.segment_sum ' , 'tf.self_adjoint_eig ' : 'tf.linalg.eigh ' , 'tf.self_adjoint_eigvals ' : 'tf.linalg.eigvalsh ' , 'tf.serialize_many_sparse ' : 'tf.io.serialize_many_sparse ' , 'tf.serialize_sparse ' : 'tf.io.serialize_sparse ' , 'tf.serialize_tensor ' : 'tf.io.serialize_tensor ' , 'tf.set_random_seed ' : 'tf.compat.v1.set_random_seed ' , 'tf.setdiff1d ' : 'tf.compat.v1.setdiff1d ' , 'tf.sets.set_difference ' : 'tf.sets.difference ' , 'tf.sets.set_intersection ' : 'tf.sets.intersection ' , 'tf.sets.set_size ' : 'tf.sets.size ' , 'tf.sets.set_union ' : 'tf.sets.union ' , 'tf.space_to_depth ' : 'tf.nn.space_to_depth ' , 'tf.sparse.sparseconditionalaccumulator ' : 'tf.compat.v1.sparse.sparseconditionalaccumulator ' , 'tf.sparse.matmul ' : 'tf.sparse.sparse_dense_matmul ' , 'tf.sparse.merge ' : 'tf.compat.v1.sparse.merge ' , 'tf.sparse.placeholder ' : 'tf.compat.v1.sparse.placeholder ' , 'tf.sparse.reduce_max_sparse ' : 'tf.compat.v1.sparse.reduce_max_sparse ' , 'tf.sparse.reduce_sum_sparse ' : 'tf.compat.v1.sparse.reduce_sum_sparse ' , 'tf.sparse_add ' : 'tf.sparse.add ' , 'tf.sparse_concat ' : 'tf.sparse.concat ' , 'tf.sparse_fill_empty_rows ' : 'tf.sparse.fill_empty_rows ' , 'tf.sparse_mask ' : 'tf.sparse.mask ' , 'tf.sparse_maximum ' : 'tf.sparse.maximum ' , 'tf.sparse_merge ' : 'tf.compat.v1.sparse_merge ' , 'tf.sparse_minimum ' : 'tf.sparse.minimum ' , 'tf.sparse_placeholder ' : 'tf.compat.v1.sparse_placeholder ' , 'tf.sparse_reduce_max ' : 'tf.sparse.reduce_max ' , 'tf.sparse_reduce_max_sparse ' : 'tf.compat.v1.sparse_reduce_max_sparse ' , 'tf.sparse_reduce_sum ' : 'tf.sparse.reduce_sum ' , 'tf.sparse_reduce_sum_sparse ' : 'tf.compat.v1.sparse_reduce_sum_sparse ' , 'tf.sparse_reorder ' : 'tf.sparse.reorder ' , 'tf.sparse_reset_shape ' : 'tf.sparse.reset_shape ' , 'tf.sparse_reshape ' : 'tf.sparse.reshape ' , 'tf.sparse_retain ' : 'tf.sparse.retain ' , 'tf.sparse_segment_mean ' : 'tf.sparse.segment_mean ' , 'tf.sparse_segment_sqrt_n ' : 'tf.sparse.segment_sqrt_n ' , 'tf.sparse_segment_sum ' : 'tf.sparse.segment_sum ' , 'tf.sparse_slice ' : 'tf.sparse.slice ' , 'tf.sparse_softmax ' : 'tf.sparse.softmax ' , 'tf.sparse_split ' : 'tf.sparse.split ' , 'tf.sparse_tensor_dense_matmul ' : 'tf.sparse.sparse_dense_matmul ' , 'tf.sparse_tensor_to_dense ' : 'tf.sparse.to_dense ' , 'tf.sparse_to_dense ' : 'tf.compat.v1.sparse_to_dense ' , 'tf.sparse_to_indicator ' : 'tf.sparse.to_indicator ' , 'tf.sparse_transpose ' : 'tf.sparse.transpose ' , 'tf.spectral.dct ' : 'tf.signal.dct ' , 'tf.spectral.fft ' : 'tf.signal.fft ' , 'tf.spectral.fft2d ' : 'tf.signal.fft2d ' , 'tf.spectral.fft3d ' : 'tf.signal.fft3d ' , 'tf.spectral.idct ' : 'tf.signal.idct ' , 'tf.spectral.ifft ' : 'tf.signal.ifft ' , 'tf.spectral.ifft2d ' : 'tf.signal.ifft2d ' , 'tf.spectral.ifft3d ' : 'tf.signal.ifft3d ' , 'tf.spectral.irfft ' : 'tf.signal.irfft ' , 'tf.spectral.irfft2d ' : 'tf.signal.irfft2d ' , 'tf.spectral.irfft3d ' : 'tf.signal.irfft3d ' , 'tf.spectral.rfft ' : 'tf.signal.rfft ' , 'tf.spectral.rfft2d ' : 'tf.signal.rfft2d ' , 'tf.spectral.rfft3d ' : 'tf.signal.rfft3d ' , 'tf.squared_difference ' : 'tf.math.squared_difference ' , 'tf.string_join ' : 'tf.strings.join ' , 'tf.string_strip ' : 'tf.strings.strip ' , 'tf.string_to_hash_bucket_fast ' : 'tf.strings.to_hash_bucket_fast ' , 'tf.string_to_hash_bucket_strong ' : 'tf.strings.to_hash_bucket_strong ' , 'tf.summary.event ' : 'tf.compat.v1.summary.event ' , 'tf.summary.filewriter ' : 'tf.compat.v1.summary.filewriter ' , 'tf.summary.filewritercache ' : 'tf.compat.v1.summary.filewritercache ' , 'tf.summary.sessionlog ' : 'tf.compat.v1.summary.sessionlog ' , 'tf.summary.summary ' : 'tf.compat.v1.summary.summary ' , 'tf.summary.summarydescription ' : 'tf.compat.v1.summary.summarydescription ' , 'tf.summary.taggedrunmetadata ' : 'tf.compat.v1.summary.taggedrunmetadata ' , 'tf.summary.all_v2_summary_ops ' : 'tf.compat.v1.summary.all_v2_summary_ops ' , 'tf.summary.get_summary_description ' : 'tf.compat.v1.summary.get_summary_description ' , 'tf.summary.initialize ' : 'tf.compat.v1.summary.initialize ' , 'tf.summary.merge ' : 'tf.compat.v1.summary.merge ' , 'tf.summary.merge_all ' : 'tf.compat.v1.summary.merge_all ' , 'tf.summary.tensor_summary ' : 'tf.compat.v1.summary.tensor_summary ' , 'tf.svd ' : 'tf.linalg.svd ' , 'tf.tables_initializer ' : 'tf.compat.v1.tables_initializer ' , 'tf.tensor_scatter_add ' : 'tf.tensor_scatter_nd_add ' , 'tf.tensor_scatter_sub ' : 'tf.tensor_scatter_nd_sub ' , 'tf.tensor_scatter_update ' : 'tf.tensor_scatter_nd_update ' , 'tf.test.stuboutfortesting ' : 'tf.compat.v1.test.stuboutfortesting ' , 'tf.test.compute_gradient_error ' : 'tf.compat.v1.test.compute_gradient_error ' , 'tf.test.get_temp_dir ' : 'tf.compat.v1.test.get_temp_dir ' , 'tf.test.mock ' : 'tf.compat.v1.test.mock ' , 'tf.test.test_src_dir_path ' : 'tf.compat.v1.test.test_src_dir_path ' , 'tf.to_bfloat16 ' : 'tf.compat.v1.to_bfloat16 ' , 'tf.to_complex128 ' : 'tf.compat.v1.to_complex128 ' , 'tf.to_complex64 ' : 'tf.compat.v1.to_complex64 ' , 'tf.to_double ' : 'tf.compat.v1.to_double ' , 'tf.to_float ' : 'tf.compat.v1.to_float ' , 'tf.to_int32 ' : 'tf.compat.v1.to_int32 ' , 'tf.to_int64 ' : 'tf.compat.v1.to_int64 ' , 'tf.tpu.crossshardoptimizer ' : 'tf.compat.v1.tpu.crossshardoptimizer ' , 'tf.tpu.paddingspec ' : 'tf.compat.v1.tpu.paddingspec ' , 'tf.tpu.batch_parallel ' : 'tf.compat.v1.tpu.batch_parallel ' , 'tf.tpu.bfloat16_scope ' : 'tf.compat.v1.tpu.bfloat16_scope ' , 'tf.tpu.core ' : 'tf.compat.v1.tpu.core ' , 'tf.tpu.cross_replica_sum ' : 'tf.compat.v1.tpu.cross_replica_sum ' , 'tf.tpu.experimental.adagradparameters ' : 'tf.compat.v1.tpu.experimental.adagradparameters ' , 'tf.tpu.experimental.adamparameters ' : 'tf.compat.v1.tpu.experimental.adamparameters ' , 'tf.tpu.experimental.ftrlparameters ' : 'tf.compat.v1.tpu.experimental.ftrlparameters ' , 'tf.tpu.experimental.stochasticgradientdescentparameters ' : 'tf.compat.v1.tpu.experimental.stochasticgradientdescentparameters ' , 'tf.tpu.experimental.embedding_column ' : 'tf.compat.v1.tpu.experimental.embedding_column ' , 'tf.tpu.experimental.shared_embedding_columns ' : 'tf.compat.v1.tpu.experimental.shared_embedding_columns ' , 'tf.tpu.initialize_system ' : 'tf.compat.v1.tpu.initialize_system ' , 'tf.tpu.outside_compilation ' : 'tf.compat.v1.tpu.outside_compilation ' , 'tf.tpu.replicate ' : 'tf.compat.v1.tpu.replicate ' , 'tf.tpu.rewrite ' : 'tf.compat.v1.tpu.rewrite ' , 'tf.tpu.shard ' : 'tf.compat.v1.tpu.shard ' , 'tf.tpu.shutdown_system ' : 'tf.compat.v1.tpu.shutdown_system ' , 'tf.trace ' : 'tf.linalg.trace ' , 'tf.train.adadeltaoptimizer ' : 'tf.compat.v1.train.adadeltaoptimizer ' , 'tf.train.adagraddaoptimizer ' : 'tf.compat.v1.train.adagraddaoptimizer ' , 'tf.train.adagradoptimizer ' : 'tf.compat.v1.train.adagradoptimizer ' , 'tf.train.adamoptimizer ' : 'tf.compat.v1.train.adamoptimizer ' , 'tf.train.checkpointsaverhook ' : 'tf.compat.v1.train.checkpointsaverhook ' , 'tf.train.checkpointsaverlistener ' : 'tf.compat.v1.train.checkpointsaverlistener ' , 'tf.train.chiefsessioncreator ' : 'tf.compat.v1.train.chiefsessioncreator ' , 'tf.train.feedfnhook ' : 'tf.compat.v1.train.feedfnhook ' , 'tf.train.finalopshook ' : 'tf.compat.v1.train.finalopshook ' , 'tf.train.ftrloptimizer ' : 'tf.compat.v1.train.ftrloptimizer ' , 'tf.train.globalstepwaiterhook ' : 'tf.compat.v1.train.globalstepwaiterhook ' , 'tf.train.gradientdescentoptimizer ' : 'tf.compat.v1.train.gradientdescentoptimizer ' , 'tf.train.loggingtensorhook ' : 'tf.compat.v1.train.loggingtensorhook ' , 'tf.train.looperthread ' : 'tf.compat.v1.train.looperthread ' , 'tf.train.momentumoptimizer ' : 'tf.compat.v1.train.momentumoptimizer ' , 'tf.train.monitoredsession ' : 'tf.compat.v1.train.monitoredsession ' , 'tf.train.monitoredtrainingsession ' : 'tf.compat.v1.train.monitoredtrainingsession ' , 'tf.train.nanlossduringtrainingerror ' : 'tf.compat.v1.train.nanlossduringtrainingerror ' , 'tf.train.nantensorhook ' : 'tf.compat.v1.train.nantensorhook ' , 'tf.train.newcheckpointreader ' : 'tf.compat.v1.train.newcheckpointreader ' , 'tf.train.optimizer ' : 'tf.compat.v1.train.optimizer ' , 'tf.train.profilerhook ' : 'tf.compat.v1.train.profilerhook ' , 'tf.train.proximaladagradoptimizer ' : 'tf.compat.v1.train.proximaladagradoptimizer ' , 'tf.train.proximalgradientdescentoptimizer ' : 'tf.compat.v1.train.proximalgradientdescentoptimizer ' , 'tf.train.queuerunner ' : 'tf.compat.v1.train.queuerunner ' , 'tf.train.rmspropoptimizer ' : 'tf.compat.v1.train.rmspropoptimizer ' , 'tf.train.saver ' : 'tf.compat.v1.train.saver ' , 'tf.train.saverdef ' : 'tf.compat.v1.train.saverdef ' , 'tf.train.scaffold ' : 'tf.compat.v1.train.scaffold ' , 'tf.train.secondorsteptimer ' : 'tf.compat.v1.train.secondorsteptimer ' , 'tf.train.server ' : 'tf.distribute.server ' , 'tf.train.sessioncreator ' : 'tf.compat.v1.train.sessioncreator ' , 'tf.train.sessionmanager ' : 'tf.compat.v1.train.sessionmanager ' , 'tf.train.sessionrunargs ' : 'tf.compat.v1.train.sessionrunargs ' , 'tf.train.sessionruncontext ' : 'tf.compat.v1.train.sessionruncontext ' , 'tf.train.sessionrunhook ' : 'tf.compat.v1.train.sessionrunhook ' , 'tf.train.sessionrunvalues ' : 'tf.compat.v1.train.sessionrunvalues ' , 'tf.train.singularmonitoredsession ' : 'tf.compat.v1.train.singularmonitoredsession ' , 'tf.train.stepcounterhook ' : 'tf.compat.v1.train.stepcounterhook ' , 'tf.train.stopatstephook ' : 'tf.compat.v1.train.stopatstephook ' , 'tf.train.summarysaverhook ' : 'tf.compat.v1.train.summarysaverhook ' , 'tf.train.supervisor ' : 'tf.compat.v1.train.supervisor ' , 'tf.train.syncreplicasoptimizer ' : 'tf.compat.v1.train.syncreplicasoptimizer ' , 'tf.train.vocabinfo ' : 'tf.compat.v1.train.vocabinfo ' , 'tf.train.workersessioncreator ' : 'tf.compat.v1.train.workersessioncreator ' , 'tf.train.add_queue_runner ' : 'tf.compat.v1.train.add_queue_runner ' , 'tf.train.assert_global_step ' : 'tf.compat.v1.train.assert_global_step ' , 'tf.train.basic_train_loop ' : 'tf.compat.v1.train.basic_train_loop ' , 'tf.train.batch ' : 'tf.compat.v1.train.batch ' , 'tf.train.batch_join ' : 'tf.compat.v1.train.batch_join ' , 'tf.train.checkpoint_exists ' : 'tf.compat.v1.train.checkpoint_exists ' , 'tf.train.cosine_decay ' : 'tf.compat.v1.train.cosine_decay ' , 'tf.train.cosine_decay_restarts ' : 'tf.compat.v1.train.cosine_decay_restarts ' , 'tf.train.create_global_step ' : 'tf.compat.v1.train.create_global_step ' , 'tf.train.do_quantize_training_on_graphdef ' : 'tf.compat.v1.train.do_quantize_training_on_graphdef ' , 'tf.train.experimental.dynamiclossscale ' : 'tf.compat.v1.train.experimental.dynamiclossscale ' , 'tf.train.experimental.fixedlossscale ' : 'tf.compat.v1.train.experimental.fixedlossscale ' , 'tf.train.experimental.lossscale ' : 'tf.compat.v1.train.experimental.lossscale ' , 'tf.train.experimental.mixedprecisionlossscaleoptimizer ' : 'tf.compat.v1.train.experimental.mixedprecisionlossscaleoptimizer ' , 'tf.train.experimental.disable_mixed_precision_graph_rewrite ' : 'tf.compat.v1.train.experimental.disable_mixed_precision_graph_rewrite ' , 'tf.train.experimental.enable_mixed_precision_graph_rewrite ' : 'tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite ' , 'tf.train.exponential_decay ' : 'tf.compat.v1.train.exponential_decay ' , 'tf.train.export_meta_graph ' : 'tf.compat.v1.train.export_meta_graph ' , 'tf.train.generate_checkpoint_state_proto ' : 'tf.compat.v1.train.generate_checkpoint_state_proto ' , 'tf.train.get_checkpoint_mtimes ' : 'tf.compat.v1.train.get_checkpoint_mtimes ' , 'tf.train.get_global_step ' : 'tf.compat.v1.train.get_global_step ' , 'tf.train.get_or_create_global_step ' : 'tf.compat.v1.train.get_or_create_global_step ' , 'tf.train.global_step ' : 'tf.compat.v1.train.global_step ' , 'tf.train.import_meta_graph ' : 'tf.compat.v1.train.import_meta_graph ' , 'tf.train.init_from_checkpoint ' : 'tf.compat.v1.train.init_from_checkpoint ' , 'tf.train.input_producer ' : 'tf.compat.v1.train.input_producer ' , 'tf.train.inverse_time_decay ' : 'tf.compat.v1.train.inverse_time_decay ' , 'tf.train.limit_epochs ' : 'tf.compat.v1.train.limit_epochs ' , 'tf.train.linear_cosine_decay ' : 'tf.compat.v1.train.linear_cosine_decay ' , 'tf.train.match_filenames_once ' : 'tf.io.match_filenames_once ' , 'tf.train.maybe_batch ' : 'tf.compat.v1.train.maybe_batch ' , 'tf.train.maybe_batch_join ' : 'tf.compat.v1.train.maybe_batch_join ' , 'tf.train.maybe_shuffle_batch ' : 'tf.compat.v1.train.maybe_shuffle_batch ' , 'tf.train.maybe_shuffle_batch_join ' : 'tf.compat.v1.train.maybe_shuffle_batch_join ' , 'tf.train.natural_exp_decay ' : 'tf.compat.v1.train.natural_exp_decay ' , 'tf.train.noisy_linear_cosine_decay ' : 'tf.compat.v1.train.noisy_linear_cosine_decay ' , 'tf.train.piecewise_constant ' : 'tf.compat.v1.train.piecewise_constant ' , 'tf.train.piecewise_constant_decay ' : 'tf.compat.v1.train.piecewise_constant_decay ' , 'tf.train.polynomial_decay ' : 'tf.compat.v1.train.polynomial_decay ' , 'tf.train.queue_runner.queuerunner ' : 'tf.compat.v1.train.queue_runner.queuerunner ' , 'tf.train.queue_runner.add_queue_runner ' : 'tf.compat.v1.train.queue_runner.add_queue_runner ' , 'tf.train.queue_runner.start_queue_runners ' : 'tf.compat.v1.train.queue_runner.start_queue_runners ' , 'tf.train.range_input_producer ' : 'tf.compat.v1.train.range_input_producer ' , 'tf.train.remove_checkpoint ' : 'tf.compat.v1.train.remove_checkpoint ' , 'tf.train.replica_device_setter ' : 'tf.compat.v1.train.replica_device_setter ' , 'tf.train.shuffle_batch ' : 'tf.compat.v1.train.shuffle_batch ' , 'tf.train.shuffle_batch_join ' : 'tf.compat.v1.train.shuffle_batch_join ' , 'tf.train.slice_input_producer ' : 'tf.compat.v1.train.slice_input_producer ' , 'tf.train.start_queue_runners ' : 'tf.compat.v1.train.start_queue_runners ' , 'tf.train.string_input_producer ' : 'tf.compat.v1.train.string_input_producer ' , 'tf.train.summary_iterator ' : 'tf.compat.v1.train.summary_iterator ' , 'tf.train.update_checkpoint_state ' : 'tf.compat.v1.train.update_checkpoint_state ' , 'tf.train.warm_start ' : 'tf.compat.v1.train.warm_start ' , 'tf.train.write_graph ' : 'tf.io.write_graph ' , 'tf.trainable_variables ' : 'tf.compat.v1.trainable_variables ' , 'tf.truncated_normal ' : 'tf.random.truncated_normal ' , 'tf.uniform_unit_scaling_initializer ' : 'tf.compat.v1.uniform_unit_scaling_initializer ' , 'tf.unsorted_segment_max ' : 'tf.math.unsorted_segment_max ' , 'tf.unsorted_segment_mean ' : 'tf.math.unsorted_segment_mean ' , 'tf.unsorted_segment_min ' : 'tf.math.unsorted_segment_min ' , 'tf.unsorted_segment_prod ' : 'tf.math.unsorted_segment_prod ' , 'tf.unsorted_segment_sqrt_n ' : 'tf.math.unsorted_segment_sqrt_n ' , 'tf.unsorted_segment_sum ' : 'tf.math.unsorted_segment_sum ' , 'tf.variable_axis_size_partitioner ' : 'tf.compat.v1.variable_axis_size_partitioner ' , 'tf.variable_op_scope ' : 'tf.compat.v1.variable_op_scope ' , 'tf.variable_scope ' : 'tf.compat.v1.variable_scope ' , 'tf.variables_initializer ' : 'tf.compat.v1.variables_initializer ' , 'tf.verify_tensor_all_finite ' : 'tf.debugging.assert_all_finite ' , 'tf.wrap_function ' : 'tf.compat.v1.wrap_function ' , 'tf.write_file ' : 'tf.io.write_file ' , 'tf.zeta ' : 'tf.math.zeta ' }
__label__0 self.assertraises ( valueerror , ws_util.warm_start , self.get_temp_dir ( ) , var_name_to_vocab_info= { `` y '' : ws_util.vocabinfo ( `` '' , 1 , 0 , `` '' ) } ) self.assertraises ( valueerror , ws_util.warm_start , self.get_temp_dir ( ) , var_name_to_prev_var_name= { `` y '' : `` y2 '' } )
__label__0 @ parameterized.named_parameters ( ( `` tuples '' , ( 1 , 2 ) , ( 3 , 4 ) , true , ( ( `` 0 '' , 4 ) , ( `` 1 '' , 6 ) ) ) , ( `` dicts '' , { `` a '' : 1 , `` b '' : 2 } , { `` b '' : 4 , `` a '' : 3 } , true , { `` a '' : ( `` a '' , 4 ) , `` b '' : ( `` b '' , 6 ) } ) , ( `` mixed '' , ( 1 , 2 ) , [ 3 , 4 ] , false , ( ( `` 0 '' , 4 ) , ( `` 1 '' , 6 ) ) ) , ( `` nested '' , { `` a '' : [ 2 , 3 ] , `` b '' : [ 1 , 2 , 3 ] } , { `` b '' : [ 5 , 6 , 7 ] , `` a '' : [ 8 , 9 ] } , true , { `` a '' : [ ( `` a/0 '' , 10 ) , ( `` a/1 '' , 12 ) ] , `` b '' : [ ( `` b/0 '' , 6 ) , ( `` b/1 '' , 8 ) , ( `` b/2 '' , 10 ) ] } ) ) def testmapwithpathscompatiblestructures ( self , s1 , s2 , check_types , expected ) : def format_sum ( path , * values ) : return ( path , sum ( values ) ) result = nest.map_structure_with_paths ( format_sum , s1 , s2 , check_types=check_types ) self.assertequal ( expected , result )
__label__0 # assert calling new fn with deprecated value issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 # isolated sessions can have different shapes for the same variable . isolate_sess_1.run ( v.initializer , feed_dict= { init_value : [ 19 , 86 ] } ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_0.run ( v ) ) self.assertallequal ( [ 86 , 99 ] , sharing_sess_1.run ( v ) ) self.assertallequal ( 37 , isolate_sess_0.run ( v ) ) self.assertallequal ( [ 19 , 86 ] , isolate_sess_1.run ( v ) )
__label__0 self.assertisinstance ( foo.foo , descr )
__label__0 rocm_config = { `` rocm_version_number '' : _get_composite_version_number ( major , minor , patch ) }
__label__0 @ dispatch.dispatch_for_binary_elementwise_apis ( maskedtensor , maskedtensor ) def handler ( api_func , x , y ) : return maskedtensor ( api_func ( x.values , y.values ) , x.mask & y.mask )
__label__0 def fn ( ) : wrapped_fn ( [ 0 , 1 ] )
__label__0 class addnoticetodocstringtest ( test.testcase ) :
__label__0 def test_convert_variables_to_tensors ( self ) : ct = ct ( ) data = [ resource_variable_ops.resourcevariable ( 1 ) , resource_variable_ops.resourcevariable ( 2 ) , constant_op.constant ( 3 ) , [ 4 ] , 5 , ct ] if not context.executing_eagerly ( ) : self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 e.g . if we have a tuple ` value = foo ( a=3 , b=bar ( c=23 , d=42 ) ) `
__label__0 text = ( `` slim.l2_regularizer ( # stuff before\n '' `` scale=.4 , '' `` scope=\ '' foo\ '' ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l2 ( # stuff before\n '' `` l=0.5 * ( .4 ) ) \n '' , ) self.assertin ( `` dropping scope '' , unused_report )
__label__0 def testreorder ( self ) : text = `` tf.boolean_mask ( a , b , c , d ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.boolean_mask ( a , b , name=c , axis=d ) \n '' )
__label__0 try : x = maskedtensor ( [ 1 , -2 , -3 ] , [ true , true , false ] ) y = maskedtensor ( [ 10 , 20 , 30 ] , [ true , false , true ] ) # test calls with positional & keyword arguments ( & combinations ) x_times_y = math_ops.multiply ( x , y ) x_plus_y = math_ops.add ( x , y=y ) x_minus_y = math_ops.subtract ( x=x , y=y ) min_x_y = math_ops.minimum ( x , y , `` min_x_y '' ) y_times_x = math_ops.multiply ( y , x , name= '' y_times_x '' ) y_plus_x = math_ops.add ( y , y=x , name= '' y_plus_x '' ) y_minus_x = math_ops.subtract ( x=y , y=x , name= '' y_minus_x '' ) self.assertallequal ( x_times_y.values , [ 10 , -40 , -90 ] ) self.assertallequal ( x_plus_y.values , [ 11 , 18 , 27 ] ) self.assertallequal ( x_minus_y.values , [ -9 , -22 , -33 ] ) self.assertallequal ( min_x_y.values , [ 1 , -2 , -3 ] ) self.assertallequal ( y_times_x.values , [ 10 , -40 , -90 ] ) self.assertallequal ( y_plus_x.values , [ 11 , 18 , 27 ] ) self.assertallequal ( y_minus_x.values , [ 9 , 22 , 33 ] ) for result in [ x_times_y , x_plus_y , x_minus_y , min_x_y , y_times_x , y_plus_x , y_minus_x ] : self.assertallequal ( result.mask , [ true , false , false ] ) if not context.executing_eagerly ( ) : # names not defined in eager mode . self.assertregex ( min_x_y.values.name , r '' ^min_x_y/minimum : . * '' ) self.assertregex ( min_x_y.mask.name , r '' ^min_x_y/and : . * '' ) self.assertregex ( y_times_x.values.name , r '' ^y_times_x/ . * '' ) self.assertregex ( y_plus_x.values.name , r '' ^y_plus_x/ . * '' ) self.assertregex ( y_minus_x.values.name , r '' ^y_minus_x/ . * '' )
__label__0 def testcontribwarning ( self ) : text = `` tf.contrib.foo ( ) '' _ , report , _ , _ = self._upgrade ( text ) expected_info = `` tf.contrib will not be distributed '' self.assertin ( expected_info , report )
__label__0 def main ( argv ) : del argv build_headers ( pathlib.path ( flags.output_dir ) )
__label__0 # assert calls with the deprecated argument log a warning . self.assertequal ( 3 , _fn ( 1 , 2 , true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 a = resource_variable_ops.resourcevariable ( 1. , name= '' a '' ) b = resource_variable_ops.resourcevariable ( 1. , name= '' b '' ) a_saver = saver_module.saver ( [ a ] ) b_saver = saver_module.saver ( [ b ] ) with self.cached_session ( ) as sess : self.evaluate ( a.initializer ) save_path = a_saver.save ( sess=sess , save_path=checkpoint_prefix ) with self.assertraisesregex ( errors.notfounderror , `` key b not found in checkpoint '' ) : b_saver.restore ( sess=sess , save_path=save_path )
__label__0 self.generic_visit ( node )
__label__0 def __init__ ( self , sv , sess ) : `` '' '' create a ` svtimercheckpointthread ` .
__label__0 initializers = [ `` zeros_initializer '' , `` ones_initializer '' , `` constant_initializer '' , `` random_uniform_initializer '' , `` random_normal_initializer '' , `` truncated_normal_initializer '' , `` variance_scaling_initializer '' , `` orthogonal_initializer '' , `` glorot_uniform_initializer '' , `` glorot_normal_initializer '' , ] self.verify_compat_v1_rename_correctness ( initializers )
__label__0 import foo as f
__label__0 the returned iterator implements the python iterator protocol .
__label__0 return decorator_func
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow/python/util/type_annotations.py . '' '' ''
__label__0 with self.cached_session ( ) : return_const ( 0.0 ) .mark_used ( )
__label__0 def decorator ( handler ) : if ( x_type , y_type ) in _elementwise_api_handlers : raise valueerror ( `` a binary elementwise dispatch handler `` f '' ( { _elementwise_api_handlers [ x_type , y_type ] } ) `` f '' has already been registered for ( { x_type } , { y_type } ) . '' ) _elementwise_api_handlers [ x_type , y_type ] = handler for api in _binary_elementwise_apis : _add_dispatch_for_binary_elementwise_api ( api , x_type , y_type , handler )
__label__0 if os.path.split ( latest_filename ) [ 0 ] : raise valueerror ( `` 'latest_filename ' must not contain path components '' )
__label__0 `` ` python code `` `
__label__0 def testlastuserframe ( self ) : trace = tf_stack.extract_stack ( ) frame = trace.last_user_frame ( ) self.assertregex ( repr ( frame ) , 'testlastuserframe ' )
__label__0 @ test_util.run_v1_only ( `` applyftrl op returns a ref , so it is not `` `` supported in eager mode . '' ) def testapplyftrl ( self ) : for dtype in [ np.float16 , np.float32 , np.float64 ] : x = np.arange ( 100 ) .astype ( dtype ) y = np.arange ( 1 , 101 ) .astype ( dtype ) z = np.arange ( 102 , 202 ) .astype ( dtype ) lr = np.array ( 2.0 ) .astype ( dtype ) l1 = np.array ( 3.0 ) .astype ( dtype ) l2 = np.array ( 4.0 ) .astype ( dtype ) grad = np.arange ( 100 ) .astype ( dtype ) self._testtypesforftrl ( x , y , z , lr , grad , use_gpu=false , l1=l1 , l2=l2 )
__label__0 @ classmethod def setupclass ( cls ) : super ( testupgrade , cls ) .setupclass ( ) cls.v2_symbols = { } cls.v1_symbols = { } if hasattr ( tf.compat , `` v2 '' ) :
__label__0 __slots__ = [ `` _storage '' ]
__label__0 # lazy load all of the _top_level_modules , we do n't need their names anymore for _m in _top_level_modules : _forward_module ( _m )
__label__0 logs.append ( ( ast_edits.info , node.lineno , node.col_offset , `` changed % s call to tf.image.resize ( ... , `` `` method=tf.image.resizemethod. % s ) . '' % ( full_name , resize_method ) ) ) return node
__label__0 # create feature columns . sc_vocab = fc.categorical_column_with_vocabulary_file ( `` sc_vocab '' , vocabulary_file=new_vocab_path , vocabulary_size=6 ) emb_vocab_column = fc.embedding_column ( categorical_column=sc_vocab , dimension=2 ) all_deep_cols = [ emb_vocab_column ] # new graph , new session with warm-starting . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : cols_to_vars = { } with variable_scope.variable_scope ( `` '' , partitioner=_partitioner ) : # create the variables . fc.input_layer ( features=self._create_dummy_inputs ( ) , feature_columns=all_deep_cols , cols_to_vars=cols_to_vars ) vocab_info = ws_util.vocabinfo ( new_vocab=sc_vocab.vocabulary_file , new_vocab_size=sc_vocab.vocabulary_size , num_oov_buckets=sc_vocab.num_oov_buckets , old_vocab=prev_vocab_path , # ca n't use constant_initializer with load_and_remap . in practice , # use a truncated normal initializer . backup_initializer=init_ops.random_uniform_initializer ( minval=0.42 , maxval=0.42 ) ) ws_util.warm_start ( self.get_temp_dir ( ) , var_name_to_vocab_info= { ws_util._infer_var_name ( cols_to_vars [ emb_vocab_column ] ) : vocab_info } ) self.evaluate ( variables.global_variables_initializer ( ) ) # verify weights were correctly warm-started . var corresponding to # emb_vocab_column should be correctly warm-started after vocab # remapping . missing values are filled in with the embeddingcolumn 's # initializer . self._assert_cols_to_vars ( cols_to_vars , { emb_vocab_column : [ np.array ( [ [ 3. , 3.3 ] , [ 2. , 2.2 ] , [ 1. , 1.1 ] ] ) , np.array ( [ [ 0.5 , 0.4 ] , [ 0.42 , 0.42 ] , [ 0.42 , 0.42 ] ] ) ] } , sess )
__label__0 # copyright 2018 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' upgrader for python scripts from 1 . * tensorflow to 2.0 tensorflow . '' '' ''
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' upgrader for python scripts from pre-1.0 tensorflow to 1.0 tensorflow . '' '' ''
__label__0 @ dispatch.dispatch_for_unary_elementwise_apis ( maskedtensor ) def unary_elementwise_api_handler ( api_func , x ) : return maskedtensor ( api_func ( x.values ) , x.mask )
__label__0 def currentframe ( ) : `` '' '' tfdecorator-aware replacement for inspect.currentframe . '' '' '' return _inspect.stack ( ) [ 1 ] [ 0 ]
__label__0 def test_contrib_rnn_cell ( self ) : api_symbols = [ `` rnncell '' , `` basiclstmcell '' , `` basicrnncell '' , `` grucell '' , `` lstmcell '' , `` multirnncell '' ] for symbol in api_symbols : text = `` tf.contrib.rnn . '' + symbol expected_text = `` tf.compat.v1.nn.rnn_cell . '' + symbol _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 class renameimports ( ast_edits.noupdatespec ) : `` '' '' specification for renaming imports . '' '' ''
__label__0 # lint.thenchange ( //tensorflow/virtual_root_template_v2.__init__.py.oss )
__label__0 def __init__ ( self , duration_secs ) : self._start_time_secs = time.time ( ) self._duration_secs = duration_secs
__label__0 def testclear ( self ) : a = object ( ) b = object ( ) set1 = object_identity.objectidentityset ( [ a , b ] ) set1.clear ( ) self.assertlen ( set1 , 0 )
__label__0 some_op ( 5 )
__label__0 def testgetfullargspeconinitclass ( self ) :
__label__0 def _get_toolkit_path ( ) : `` '' '' determines and returns the sycl installation path . '' '' '' sycl_toolkit_path = none sycl_toolkit_path = _get_default_sycl_toolkit_path ( ) if `` sycl_toolkit_path '' in os.environ : sycl_toolkit_path = os.environ [ `` sycl_toolkit_path '' ] return os.path.realpath ( sycl_toolkit_path )
__label__0 text = `` import foo.test as t '' expected_text = `` import bar.test as t '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 if __name__ == `` __main__ '' : googletest.main ( )
__label__0 def load_tests ( unused_loader , tests , unused_ignore ) : `` '' '' loads all the tests in the docstrings and runs them . '' '' ''
__label__0 sycl_config = { `` sycl_version_number '' : _get_composite_version_number ( major , minor , patch ) , `` sycl_basekit_version_number '' : _get_basekit_version ( ) , }
__label__0 # in the case where the parent directory does n't exist , whether or not the # save succeeds or fails is implementation dependent . therefore we allow # both cases . try : with self.cached_session ( ) as sess : # initialize all variables self.evaluate ( init_all_op )
__label__0 def testgetfullargspeconnewclass ( self ) :
__label__0 def type_based_dispatch_signatures_for ( cls ) : `` '' '' returns dispatch signatures that have been registered for a given class .
__label__0 # check if dispatch_target registered by ` @ dispatch_for_ * _elementwise_apis ` elementwise_keys_to_delete = [ key for ( key , handler ) in _elementwise_api_handlers.items ( ) if handler is dispatch_target ] for key in set ( elementwise_keys_to_delete ) : for _ , target in _elementwise_api_targets [ key ] : unregister_dispatch_for ( target ) del _elementwise_api_handlers [ key ] del _elementwise_api_targets [ key ] found = true
__label__0 def _sparse_rmsprop_update_numpy ( self , var , gindexs , gvalues , mg , rms , mom , lr , decay , momentum , epsilon , centered ) : mg_t = copy.deepcopy ( mg ) rms_t = copy.deepcopy ( rms ) mom_t = copy.deepcopy ( mom ) var_t = copy.deepcopy ( var ) for i in range ( len ( gindexs ) ) : gindex = gindexs [ i ] gvalue = gvalues [ i ] rms_t [ gindex ] = rms [ gindex ] * decay + ( 1 - decay ) * gvalue * gvalue denom_t = rms_t [ gindex ] + epsilon if centered : mg_t [ gindex ] = mg_t [ gindex ] * decay + ( 1 - decay ) * gvalue denom_t -= mg_t [ gindex ] * mg_t [ gindex ] mom_t [ gindex ] = momentum * mom [ gindex ] + lr * gvalue / np.sqrt ( denom_t ) var_t [ gindex ] = var [ gindex ] - mom_t [ gindex ] return var_t , mg_t , rms_t , mom_t
__label__0 expected_config_v1 = `` '' '' feature_map { key : `` x '' value { fixed_len_feature { dtype : dt_float shape { dim { size : 1 } } default_value { dtype : dt_float tensor_shape { dim { size : 1 } } float_val : 33.0 } values_output_tensor_name : `` parseexample/parseexample:3 '' } } } feature_map { key : `` y '' value { var_len_feature { dtype : dt_string values_output_tensor_name : `` parseexample/parseexample:1 '' indices_output_tensor_name : `` parseexample/parseexample:0 '' shapes_output_tensor_name : `` parseexample/parseexample:2 '' } } } `` '' ''
__label__0 return none
__label__0 def testismodule ( self ) : self.asserttrue ( tf_inspect.ismodule ( inspect.getmodule ( inspect.currentframe ( ) ) ) ) self.assertfalse ( tf_inspect.ismodule ( test_decorated_function ) )
__label__0 @ deprecation.deprecated_arg_values ( `` 2016-07-04 '' , `` this is how you update ... '' , warn_once=true , arg0=none ) def _fn ( arg0 ) : # pylint : disable=unused-argument pass
__label__0 requirements : filename : the path to the whl file and new_py_ver : create a nightly tag with current date
__label__0 
__label__0 nest.map_structure ( lambda x , y : none , structure1 , structure1_list , check_types=false )
__label__0 this will only produce the following docs :
__label__0 # handle special fields types ( map key and list index ) . if _is_map ( parent_desc ) and i < len ( fields ) : # next field is the map key . map_key = fields [ i ]
__label__0 output = nest.map_structure ( lambda x1 , x2 : x1 + x2 , inp_a , inp_b )
__label__0 def gather_gpu_devices ( ) : `` '' '' gather gpu device info .
__label__0 def testnoqueuerunners ( self ) : with ops.graph ( ) .as_default ( ) , self.cached_session ( ) as sess : sv = supervisor.supervisor ( logdir=self._test_dir ( `` no_queue_runners '' ) ) self.assertequal ( 0 , len ( sv.start_queue_runners ( sess ) ) ) sv.stop ( )
__label__0 sess.run ( variables.global_variables_initializer ( ) ) queue_runner_impl.start_queue_runners ( sess ) sess.run ( var.assign ( 3.0 ) )
__label__0 the optional ` reshape ` argument , if ` true ` , allows restoring a variable from a save file where the variable had a different shape , but the same number of elements and type . this is useful if you have reshaped a variable and want to reload it from an older checkpoint .
__label__0 def __call__ ( self , path , parent , children ) : `` '' '' visitor interface , see ` traverse ` for details . '' '' ''
__label__0 import_header = `` from tensorflow.compat import v1 as tf , v2 as tf2\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` ` tensorflow.compat.v1 ` was directly imported as ` tf ` `` , report ) self.assertempty ( errors )
__label__0 def get_header_version ( path ) : version = [ _get_header_version ( path , name ) for name in ( `` cudnn_major '' , `` cudnn_minor '' , `` cudnn_patchlevel '' ) ] return `` . `` .join ( version ) if version [ 0 ] else none
__label__0 returns : a local ` tf.distribute.server ` . `` '' '' # specifying port 0 means that the os will choose a free port for the # server . return server ( { `` localhost '' : [ `` localhost:0 '' ] } , protocol= '' grpc '' , config=config , start=start )
__label__0 this is called to signal the hooks that a new session has been created . this has two essential differences with the situation in which ` begin ` is called :
__label__0 note : callables are conceptually very similar to ` tf.operation ` : a ` tf.operation ` is a kind of callable. `` '' ''
__label__0 the optional ` sharded ` argument , if ` true ` , instructs the saver to shard checkpoints per device .
__label__0 returns the input as a unicode string . uses utf-8 encoding for text by default .
__label__0 t = test ( ) self.assertequal ( { 'self ' : t } , tf_inspect.getcallargs ( t.bound ) )
__label__0 if _at_least_version ( cuda_version , `` 11.0 '' ) :
__label__0 def initialize_options ( self ) : self.install_dir = none self.force = 0 self.outfiles = [ ]
__label__0 if not defaults : defaults = none
__label__0 it is an error if the model can not be recovered and no ` init_op ` or ` init_fn ` or ` local_init_op ` are passed .
__label__0 3. for a nested dictionary of dictionaries :
__label__0 t0 = time.time ( ) for _ in range ( test_iter ) : nest.assert_same_structure ( s1 , s2 ) t1 = time.time ( )
__label__0 class tfapiimportanalysisspec ( ast_edits.apianalysisspec ) :
__label__0 `` ` python data_list = [ [ 2 , 4 , 6 , 8 ] , [ [ 1 , 3 , 5 , 7 , 9 ] , [ 3 , 5 , 7 ] ] ] name_list = [ 'evens ' , [ 'odds ' , 'primes ' ] ] out = map_structure_up_to ( name_list , lambda name , sec : `` first_ { } _ { } '' .format ( len ( sec ) , name ) , name_list , data_list )
__label__0 def test_saved_model_loader_load ( self ) : text = `` tf.saved_model.loader.load ( sess , [ 'foo_graph ' ] ) '' expected = `` tf.compat.v1.saved_model.load ( sess , [ 'foo_graph ' ] ) '' _ , report , _ , new_text = self._upgrade ( text ) self.assertequal ( expected , new_text ) expected_info = `` tf.saved_model.load works differently in 2.0 '' self.assertin ( expected_info , report )
__label__0 > > > def compute_loss ( x ) : ... v = tf.variable ( 3.0 ) ... y = x * v ... loss = x * 5 - x * v ... return loss , [ v ]
__label__0 # pylint : disable=invalid-name `` '' '' save and restore variables .
__label__0 # test calling stripped_op_list_for_graph directly op_list = meta_graph.stripped_op_list_for_graph ( meta_graph_def.graph_def ) self.assertequal ( ops , [ o.name for o in op_list.op ] ) for o in op_list.op : self.assertequal ( o.summary , `` '' ) self.assertequal ( o.description , `` '' )
__label__0 this is an example class that is used to test global op dispatchers . the global op dispatcher for tensortracers is defined below. `` '' ''
__label__0 with graph.as_default ( ) , self.session ( ) as sess : saver = saver_module.saver ( var_list=var_list , max_to_keep=1 ) saver.restore ( sess , os.path.join ( test_dir , ckpt_filename ) ) # verify that we have restored weights1 and biases1 . self.evaluate ( [ weights1 , biases1 ] ) # initialize the rest of the variables and run logits . self.evaluate ( init_rest_op ) self.evaluate ( logits )
__label__0 def testsegmentmaxnumsegmentsmore ( self ) : for dtype in self.int_types | self.float_types : minval = dtypes.as_dtype ( dtype ) .min if dtype == np.float64 and self._finddevice ( `` tpu '' ) : minval = -np.inf self.assertallclose ( np.array ( [ 1 , minval , 2 , 5 , minval ] , dtype=dtype ) , self._segmentmaxv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 5 ) )
__label__0 # to avoid graph name collisions between original and loaded code . context._reset_context ( ) # pylint : disable=protected-access
__label__0 # assert calling new fn issues log warning . self.assertequal ( 3 , _fn ( 1 , 2 , deprecated=true ) ) self.assertequal ( 1 , mock_warning.call_count ) ( args , _ ) = mock_warning.call_args self.assertregex ( args [ 0 ] , r '' deprecated and will be removed '' ) self._assert_subset ( set ( [ `` after `` + date , instructions ] ) , set ( args [ 1 : ] ) )
__label__0 args : init_op : ` operation ` to initialize the variables . if set to use_default , create an op that initializes all variables and tables . init_feed_dict : a dictionary that maps ` tensor ` objects to feed values . this feed dictionary will be used when ` init_op ` is evaluated. `` '' '' if init_op is supervisor.use_default : init_op = self._get_first_op_from_collection ( ops.graphkeys.init_op ) if init_op is none : init_op = variables.global_variables_initializer ( ) ops.add_to_collection ( ops.graphkeys.init_op , init_op ) self._init_op = init_op self._init_feed_dict = init_feed_dict
__label__0 __slots__ = [ `` _start_time_secs '' , `` _duration_secs '' ]
__label__0 `` `` '' protocol class for custom tf.nest support . '' '' ''
__label__0 self.assertequal ( argspec , tf_inspect.getfullargspec ( partial_func ) )
__label__0 if max_wait_secs is none : max_wait_secs = float ( `` inf '' ) timer = _countdowntimer ( max_wait_secs )
__label__0 # fetch params to validate initial values self.assertallclose ( [ 1.0 , 2.0 ] , self.evaluate ( var0 ) ) self.assertallclose ( [ 3.0 , 4.0 ] , self.evaluate ( var1 ) ) # step 1 : rms = 1 , mom = 0. so we should see a normal # update : v -= grad * learning_rate self.evaluate ( update ) # check the root mean square accumulators . self.assertallcloseaccordingtotype ( np.array ( [ 0.901 , 0.901 ] ) , self.evaluate ( rms0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ 0.90001 , 0.90001 ] ) , self.evaluate ( rms1 ) ) # check the momentum accumulators self.assertallcloseaccordingtotype ( np.array ( [ ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) , ( 0.1 * 2.0 / math.sqrt ( 0.901 + 1e-5 ) ) ] ) , self.evaluate ( mom0 ) ) self.assertallcloseaccordingtotype ( np.array ( [ ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) , ( 0.01 * 2.0 / math.sqrt ( 0.90001 + 1e-5 ) ) ] ) , self.evaluate ( mom1 ) )
__label__0 usage : ` make_all ( __name__ ) ` or ` make_all ( __name__ , [ sys.modules ( __name__ ) , other_module ] ) ` . the doc string modules must each a docstring , and ` __all__ ` will contain all symbols with ` @ @ ` references , where that symbol currently exists in the module named ` module_name ` .
__label__0 if `` dev '' in old_extension : version_type = nightly_version else : version_type = regular_version
__label__0 # the saver must be called * after * the savedmodel init , because the savedmodel # init will restore the variables from the savedmodel variables directory . # initializing/restoring twice is not ideal but there 's no other way to do it . saver.restore ( sess , path )
__label__0 # check that the parameter nodes have been initialized . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) )
__label__0 returns : a ` tuple ` of ( metadata , components ) , where - metadata is a custom python object that stands for the static config of the current object , which is supposed to be fixed and not affected by data transformation . - components is a ` tuple ` that contains the modifiable fields of the current object .
__label__0 return hipfft_config
__label__0 class iterable ( object ) : `` '' '' interface for distributed objects that admit iteration/reduction . '' '' ''
__label__0 def __init__ ( self , proto , proto_size , * * kwargs ) : `` '' '' initializer . '' '' '' self.proto_size = proto_size super ( ) .__init__ ( proto , * * kwargs )
__label__0 # output is : [ 2 , 4 ] `` `
__label__0 # todo ( sherrym ) : all non-pep8 compliant names will be deprecated shortly . setattr ( supervisor , `` preparesession '' , supervisor.prepare_or_wait_for_session ) setattr ( supervisor , `` startqueuerunners '' , supervisor.start_queue_runners ) setattr ( supervisor , `` startstandardservices '' , supervisor.start_standard_services ) setattr ( supervisor , `` stop '' , supervisor.stop ) setattr ( supervisor , `` requeststop '' , supervisor.request_stop ) setattr ( supervisor , `` loop '' , supervisor.loop ) setattr ( supervisor , `` shouldstop '' , supervisor.should_stop ) setattr ( supervisor , `` stoponexception '' , supervisor.stop_on_exception ) setattr ( supervisor , `` waitforstop '' , supervisor.wait_for_stop ) setattr ( supervisor , `` summarycomputed '' , supervisor.summary_computed )
__label__0 unflattened_custom_mapping = unflattened [ 2 ] [ `` d '' ] self.assertisinstance ( unflattened_custom_mapping , _custommapping ) self.assertequal ( list ( unflattened_custom_mapping.keys ( ) ) , [ 41 ] )
__label__0 def getargspec ( obj ) : `` '' '' tfdecorator-aware replacement for ` inspect.getargspec ` .
__label__0 for path , val in path_to_replace.items ( ) : if path in file : copy_file ( file , os.path.join ( srcs_dir , val ) , path ) break else : copy_file ( file , srcs_dir )
__label__0 self._global_step = global_step train_ops = [ ] aggregated_grad = [ ] var_list = [ ]
__label__0 class typeannotationstest ( test_util.tensorflowtestcase , parameterized.testcase ) :
__label__0 def testgetfullargspecondecoratorsthatdontprovidefullargspec ( self ) : argspec = tf_inspect.getfullargspec ( test_decorated_function_with_defaults ) self.assertequal ( [ ' a ' , ' b ' , ' c ' ] , argspec.args ) self.assertequal ( ( 2 , 'hello ' ) , argspec.defaults )
__label__0 # copyright 2015 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ==============================================================================
__label__0 elif version.parse ( tf.__version__ ) > = version.parse ( `` 2.13 '' ) : # first match takes precedence . # objects are dropped if they have no match . base_dirs = [ # the real keras source files are now in ` site-packages/keras/src/ ... ` pathlib.path ( keras.__file__ ) .parent / `` src '' , # the generated module files in tensorflow are in keras # under ` site-packages/keras/api/_v2/keras/ ... ` . pathlib.path ( tf.keras.__file__ ) .parent , # the generated api-module files are now in ` site-packages/keras/ ... ` pathlib.path ( keras.__file__ ) .parent , pathlib.path ( tensorboard.__file__ ) .parent , pathlib.path ( tensorflow_estimator.__file__ ) .parent , # the tensorflow base dir goes last because ` tf.keras `` base_dir , ]
__label__0 class objectidentityweakset ( objectidentityset ) : `` '' '' like weakref.weakset , but compares objects with `` is '' . '' '' ''
__label__0 note : this is an experimental api and subject to changes .
__label__0 # flatten each input separately , apply the function to corresponding items , # then repack based on the structure of the first input . flat_value_gen = ( _tf_core_flatten_up_to ( # pylint : disable=g-complex-comprehension shallow_tree , input_tree , check_types , expand_composites=expand_composites , ) for input_tree in inputs ) flat_path_gen = ( path for path , _ in _tf_core_yield_flat_up_to ( shallow_tree , inputs [ 0 ] , is_nested_fn ) ) results = [ func ( * args , * * kwargs ) for args in zip ( flat_path_gen , * flat_value_gen ) ] return _tf_core_pack_sequence_as ( structure=shallow_tree , flat_sequence=results , expand_composites=expand_composites , )
__label__0 if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) .splitlines ( ) lines [ 0 ] += ' ' + suffix_str
__label__0 # create new tags new_tag = origin_tag.replace ( old_py_ver , `` cp '' + args.new_py_ver )
__label__0 if ( global_step_tensor.get_shape ( ) .ndims ! = 0 and global_step_tensor.get_shape ( ) .is_fully_defined ( ) ) : raise typeerror ( 'existing `` global_step '' is not scalar : % s ' % global_step_tensor.get_shape ( ) )
__label__0 args : * args : api names in dot delimited format . api_name : api you want to generate currently , only ` tensorflow ` . v1 : names for the tensorflow v1 api . if not set , we will use v2 api names both for tensorflow v1 and v2 apis . allow_multiple_exports : deprecated. `` '' '' self._names = args self._names_v1 = v1 if v1 is not none else args self._api_name = api_name
__label__0 for kwarg in fullargspec.kwonlyargs : parameters.append ( inspect.parameter ( kwarg , inspect.parameter.keyword_only , default=defaults.get ( kwarg , inspect.parameter.empty ) , ) )
__label__0 if args.print_all : print ( report ) print ( detailed_report_header ) print ( report_text ) else : print ( report ) print ( `` \nmake sure to read the detailed log % r\n '' % report_filename )
__label__0 the following code will raise an exception : `` ` python shallow_tree = { `` a '' : `` a '' , `` b '' : `` b '' } input_tree = { `` a '' : 1 , `` c '' : 2 } assert_shallow_structure ( shallow_tree , input_tree ) `` `
__label__0 any other values are considered * * atoms * * . not all collection types are considered nested structures . for example , the following types are considered atoms :
__label__0 import pasta
__label__0 def testunregisterdispatchtargetbadtargeterror ( self ) : fn = lambda x : x + 1 with self.assertraisesregex ( valueerror , `` function . * was not registered '' ) : dispatch.unregister_dispatch_for ( fn )
__label__0 also group this method 's docs with and ` @ abstractmethod ` in the class 's docs .
__label__0 # adding s2 again ( old s2 is removed first , then new s2 appended ) s2 = save.save ( none , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s3 , s2 ] , save.last_checkpoints ) self.assertfalse ( checkpoint_management.checkpoint_exists ( s1 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s3 ) ) self.asserttrue ( checkpoint_management.checkpoint_exists ( s2 ) ) self.assertcheckpointstate ( model_checkpoint_path=s2 , all_model_checkpoint_paths= [ s3 , s2 ] , save_dir=save_dir )
__label__0 # warnings that are emitted only if a specific arg is found . self.function_arg_warnings = { `` tf.nn.conv1d '' : { ( `` use_cudnn_on_gpu '' , 4 ) : ( ast_edits.warning , `` use_cudnn_on_gpu has been removed , behavior is now equivalent '' `` to setting it to true . `` ) , } , `` tf.nn.conv2d '' : { ( `` use_cudnn_on_gpu '' , 4 ) : ( ast_edits.warning , `` use_cudnn_on_gpu has been removed , behavior is now equivalent '' `` to setting it to true . `` ) , } , `` tf.nn.conv2d_backprop_filter '' : { ( `` use_cudnn_on_gpu '' , 5 ) : ( ast_edits.warning , `` use_cudnn_on_gpu has been removed , behavior is now equivalent '' `` to setting it to true . `` ) , } , `` tf.nn.conv2d_backprop_input '' : { ( `` use_cudnn_on_gpu '' , 5 ) : ( ast_edits.warning , `` use_cudnn_on_gpu has been removed , behavior is now equivalent '' `` to setting it to true . `` ) , } , `` tf.gradients '' : { ( `` colocate_gradients_with_ops '' , 4 ) : ( ast_edits.info , `` tf.gradients no longer takes `` `` 'colocate_gradients_with_ops ' argument , it behaves as if it `` `` was set to true . `` ) , } , `` tf.hessians '' : { ( `` colocate_gradients_with_ops '' , 3 ) : ( ast_edits.info , `` tf.hessians no longer takes `` `` 'colocate_gradients_with_ops ' argument , it behaves as if it `` `` was set to true . `` ) , } , `` * .minimize '' : { ( `` colocate_gradients_with_ops '' , 5 ) : ( ast_edits.info , `` optimizer.minimize no longer takes `` `` 'colocate_gradients_with_ops ' argument , it behaves as if it `` `` was set to true . `` ) , } , `` * .compute_gradients '' : { ( `` colocate_gradients_with_ops '' , 4 ) : ( ast_edits.info , `` optimizer.compute_gradients no `` `` longer takes 'colocate_gradients_with_ops ' argument , it `` `` behaves as if it was set to true . `` ) , } , `` tf.cond '' : { ( `` strict '' , 3 ) : ( ast_edits.warning , `` tf.cond no longer takes 'strict ' argument , it behaves as `` `` if was set to true . '' ) } , `` tf.contrib.summary.audio '' : { ( `` family '' , 4 ) : contrib_summary_family_arg_comment , } , `` tf.contrib.summary.create_file_writer '' : { ( `` name '' , 4 ) : ( ast_edits.warning , `` tf.contrib.summary.create_file_writer ( ) no longer supports `` `` implicit writer re-use based on shared logdirs or resource `` `` names ; this call site passed a 'name ' argument that has been `` `` removed . the new tf.compat.v2.summary.create_file_writer ( ) `` `` replacement has a 'name ' parameter but the semantics are `` `` the usual ones to name the op itself and do not control `` `` writer re-use ; writers must be manually re-used if desired . '' ) } , `` tf.contrib.summary.generic '' : { ( `` name '' , 0 ) : ( ast_edits.warning , `` tf.contrib.summary.generic ( ) takes a 'name ' argument for the `` `` op name that also determines the emitted tag ( prefixed by any `` `` active name scopes ) , but tf.compat.v2.summary.write ( ) , which `` `` replaces it , separates these into 'tag ' and 'name ' arguments. `` `` the 'name ' argument here has been converted to 'tag ' to `` `` preserve a meaningful tag , but any name scopes will not be `` `` reflected in the tag without manual editing . `` ) , ( `` family '' , 3 ) : contrib_summary_family_arg_comment , } , `` tf.contrib.summary.histogram '' : { ( `` family '' , 2 ) : contrib_summary_family_arg_comment , } , `` tf.contrib.summary.image '' : { ( `` bad_color '' , 2 ) : ( ast_edits.warning , `` tf.contrib.summary.image no longer takes the 'bad_color ' `` `` argument ; caller must now preprocess if needed . this call `` `` site specifies a bad_color argument so it can not be converted `` `` safely . `` ) , ( `` family '' , 4 ) : contrib_summary_family_arg_comment , } , `` tf.contrib.summary.scalar '' : { ( `` family '' , 2 ) : contrib_summary_family_arg_comment , } , `` tf.image.resize '' : { ( `` align_corners '' , 3 ) : ( ast_edits.warning , `` align_corners is not supported by tf.image.resize , the new `` `` default transformation is close to what v1 provided . if you `` `` require exactly the same transformation as before , use `` `` compat.v1.image.resize . `` ) , } , `` tf.image.resize_bilinear '' : { ( `` align_corners '' , 2 ) : ( ast_edits.warning , `` align_corners is not supported by tf.image.resize , the new `` `` default transformation is close to what v1 provided . if you `` `` require exactly the same transformation as before , use `` `` compat.v1.image.resize_bilinear . `` ) , } , `` tf.image.resize_area '' : { ( `` align_corners '' , 2 ) : ( ast_edits.warning , `` align_corners is not supported by tf.image.resize , the new `` `` default transformation is close to what v1 provided . if you `` `` require exactly the same transformation as before , use `` `` compat.v1.image.resize_area . `` ) , } , `` tf.image.resize_bicubic '' : { ( `` align_corners '' , 2 ) : ( ast_edits.warning , `` align_corners is not supported by tf.image.resize , the new `` `` default transformation is close to what v1 provided . if you `` `` require exactly the same transformation as before , use `` `` compat.v1.image.resize_bicubic . `` ) , } , `` tf.image.resize_nearest_neighbor '' : { ( `` align_corners '' , 2 ) : ( ast_edits.warning , `` align_corners is not supported by tf.image.resize , the new `` `` default transformation is close to what v1 provided . if you `` `` require exactly the same transformation as before , use `` `` compat.v1.image.resize_nearest_neighbor . `` ) , } , } all_renames_v2.add_contrib_direct_import_support ( self.function_arg_warnings )
__label__0 field , field_desc = util.get_field ( proto , [ `` field_one '' , 2 , 1 ] ) self.assertisinstance ( field , test_message_pb2.manyfields ) self.assertequal ( `` repeated_field '' , field_desc.name ) self.assertequal ( 2 , field_desc.number ) self.assertprotoequals ( proto.field_one.repeated_field [ 1 ] , field )
__label__0 results = variable_utils.convert_variables_to_tensors ( data ) expected_results = [ 1 , 2 , 3 , [ 4 ] , 5 , ct ] # only resourcevariables are converted to tensors . self.assertisinstance ( results [ 0 ] , tensor.tensor ) self.assertisinstance ( results [ 1 ] , tensor.tensor ) self.assertisinstance ( results [ 2 ] , tensor.tensor ) self.assertisinstance ( results [ 3 ] , list ) self.assertisinstance ( results [ 4 ] , int ) self.assertis ( results [ 5 ] , ct ) results [ :3 ] = self.evaluate ( results [ :3 ] ) self.assertallequal ( results , expected_results )
__label__0 # lots of unused arguments below , since these are called in a standard manner . # pylint : disable=unused-argument
__label__0 v1.assign ( 0.0 ) v2.assign ( [ 0 , 0 ] ) self.assertnear ( 0.0 , self.evaluate ( v1 ) , 1e-5 ) self.assertallequal ( [ 0 , 0 ] , self.evaluate ( v2 ) )
__label__0 def testinitopfails ( self ) : server = server_lib.server.create_local_server ( ) logdir = self._test_dir ( `` default_init_op_fails '' ) with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) variable_v1.variablev1 ( [ 4.0 , 5.0 , 6.0 ] , name= '' w '' ) # w will not be initialized . sv = supervisor.supervisor ( logdir=logdir , init_op=v.initializer ) with self.assertraisesregex ( runtimeerror , `` variables not initialized : w '' ) : sv.prepare_or_wait_for_session ( server.target )
__label__0 args : input_bytes : input randomized bytes used to create a fuzzeddataprovider. `` '' '' self.fdp = atheris.fuzzeddataprovider ( input_bytes )
__label__0 self._visitor ( path , parent , children )
__label__0 def run_loop ( self ) : # count the steps . current_step = training_util.global_step ( self._sess , self._step_counter ) added_steps = current_step - self._last_step self._last_step = current_step # measure the elapsed time . current_time = time.time ( ) elapsed_time = current_time - self._last_time self._last_time = current_time # reports the number of steps done per second if elapsed_time > 0. : steps_per_sec = added_steps / elapsed_time else : steps_per_sec = float ( `` inf '' ) summary = summary ( value= [ summary.value ( tag=self._summary_tag , simple_value=steps_per_sec ) ] ) if self._sv.summary_writer : self._sv.summary_writer.add_summary ( summary , current_step ) logging.log_first_n ( logging.info , `` % s : % g '' , 10 , self._summary_tag , steps_per_sec )
__label__0 @ test_util.run_v1_only ( `` train.supervisor is for v1 only '' ) def testinitop ( self ) : logdir = self._test_dir ( `` default_init_op '' ) with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] ) sv = supervisor.supervisor ( logdir=logdir ) sess = sv.prepare_or_wait_for_session ( `` '' ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) ) sv.stop ( )
__label__0 if __name__ == '__main__ ' : flags.mark_flags_as_required ( [ 'output_dir ' ] ) app.run ( main )
__label__0 with session.session ( server.target ) as sess : with self.assertraises ( errors_impl.deadlineexceedederror ) : sess.run ( blocking_t , options=config_pb2.runoptions ( timeout_in_ms=1000 ) )
__label__0 raises : valueerror : if ` structure ` contains more atoms than ` flat ` ( assuming indexing starts from ` index ` ) . `` '' '' packed = [ ] sequence_fn = sequence_fn or sequence_like for s in _tf_core_yield_value ( structure ) : if is_nested_fn ( s ) : new_index , child = _tf_core_packed_nest_with_indices ( s , flat , index , is_nested_fn , sequence_fn ) packed.append ( sequence_fn ( s , child ) ) index = new_index else : packed.append ( flat [ index ] ) index += 1 return index , packed
__label__0 please rewrite your checkpoints immediately using the object-based checkpoint apis .
__label__0 with self.cached_session ( ) as sess : # restore the saved values in the parameter nodes . save.restore ( sess , save_path ) # check that the parameter nodes have been restored . self.assertequal ( 10.0 , self.evaluate ( v0 ) ) self.assertequal ( 20.0 , self.evaluate ( v1 ) ) except valueerror as exc : error_msg_template = `` parent directory of { } does n't exist , ca n't save . '' self.assertequal ( error_msg_template.format ( save_path ) , str ( exc ) )
__label__0 1. it seems more natural for lists to be treated ( e.g . in dataset constructors ) as tensors , rather than lists of ( lists of ... ) tensors . 2. this is needed because ` sparsetensorvalue ` is implemented as a ` namedtuple ` that would normally be flattened and we want to be able to create sparse tensor from ` sparsetensorvalue 's similarly to creating tensors from numpy arrays. `` '' ''
__label__0 # tests that mark_used is available in the api . def testmarkused ( self ) : @ tf_should_use.should_use_result ( warn_in_eager=true ) def return_const ( value ) : return constant_op.constant ( value , name='blah3 ' )
__label__0 args : dispatch_target : the function to unregister .
__label__0 # pylint : disable=g-import-not-at-top from tensorflow.python.ops import nn_ops from tensorflow.python.ops import random_ops from tensorflow.python.platform import test
__label__0 def _getattr ( self , name ) : # pylint : disable=g-doc-return-or-yield , g-doc-args `` '' '' imports and caches pre-defined api .
__label__0 args : root : a python package. `` '' '' for _ , name , _ in pkgutil.walk_packages ( root.__path__ , prefix=root.__name__ + ' . ' ) : try : importlib.import_module ( name ) except ( attributeerror , importerror ) : pass
__label__0 > > > s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) > > > s1_list = [ [ [ 1 , 2 ] , 3 ] , 4 , [ 5 , 6 ] ] > > > tf.nest.map_structure ( lambda x , y : none , s1 , s1_list ) traceback ( most recent call last ) : ... typeerror : the two structures do n't have the same nested structure
__label__0 _assert_api_tag = `` assert_api_tag ''
__label__0 # file parameters . tf_src_dir = `` tensorflow '' version_h = `` % s/core/public/version.h '' % tf_src_dir setup_py = `` % s/tools/pip_package/setup.py '' % tf_src_dir readme_md = `` ./readme.md '' tensorflow_bzl = `` % s/tensorflow.bzl '' % tf_src_dir tf_mac_arm64_ci_build = ( `` % s/tools/ci_build/osx/arm64/tensorflow_as_build_release.jenkinsfile '' % tf_src_dir ) tf_mac_arm64_ci_test = ( `` % s/tools/ci_build/osx/arm64/tensorflow_as_test_release.jenkinsfile '' % tf_src_dir ) relevant_files = [ tf_src_dir , version_h , setup_py , readme_md , tf_mac_arm64_ci_build , tf_mac_arm64_ci_test ]
__label__0 doc_controls.set_deprecated ( tf.compat.v1 ) try : doc_controls.set_deprecated ( tf.estimator ) except attributeerror : pass doc_controls.set_deprecated ( tf.feature_column ) doc_controls.set_deprecated ( tf.keras.preprocessing )
__label__0 return getter
__label__0 `` ` class example ( object ) : @ property @ for_subclass_implementers def x ( self ) : return self._x `` `
__label__0 text = `` from foo.a import b '' expected_text = `` from bar.a import b '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 def _init_global_step ( self , global_step=use_default ) : `` '' '' initializes global_step .
__label__0 # visitor that verifies v1 argument names . def arg_test_visitor ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) names_v1 = tf_export.get_v1_names ( attr )
__label__0 > > > tf.nest.map_structure ( lambda x : x + 1 , ( ) ) ( )
__label__0 with ` expand_composites=false ` , we just return the raggedtensor as is .
__label__0 self.assertequal ( `` const/slot '' , slot.op.name ) self.assertequal ( [ 2 ] , slot.get_shape ( ) .as_list ( ) ) self.assertequal ( dtypes.float32 , slot.dtype.base_dtype ) self.assertallequal ( [ 0.0 , 0.0 ] , self.evaluate ( slot ) )
__label__0 if the arg is found , arg_ok_predicate is not none and returns ok , and remove_if_ok is true , the argument is removed from the call .
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' keyword args tests . '' '' ''
__label__0 x = tensortracer ( `` x '' ) y = tensortracer ( `` y '' ) trace = x [ : y ] # pylint : disable=invalid-slice-index self.assertequal ( str ( trace ) , `` __operators__.getitem ( x , slice ( none , y , none ) ) '' )
__label__0 def __call__ ( self , x ) : del x self.assertfalse ( function_utils.has_kwargs ( foohasnokwargs ( ) ) )
__label__0 @ property def ready_for_local_init_op ( self ) : return self._ready_for_local_init_op
__label__0 @ test_util.run_v1_only ( `` exporting/importing meta graphs is only supported in v1 . '' ) def testmultisavercollection ( self ) : test_dir = self._get_test_dir ( `` saver_collection '' ) self._testmultisavercollectionsave ( test_dir ) self._testmultisavercollectionrestore ( test_dir )
__label__0 reordered = function_reorders [ full_name ] new_args = [ ] new_keywords = [ ] idx = 0 for arg in node.args : if sys.version_info [ :2 ] > = ( 3 , 5 ) and isinstance ( arg , ast.starred ) : continue # ca n't move starred to keywords keyword_arg = reordered [ idx ] if keyword_arg : new_keywords.append ( ast.keyword ( arg=keyword_arg , value=arg ) ) else : new_args.append ( arg ) idx += 1
__label__0 def testinitcapturesnonnoneargspec ( self ) : argspec = tf_inspect.fullargspec ( args= [ ' a ' , ' b ' , ' c ' ] , varargs=none , varkw=none , defaults= ( 1 , 'hello ' ) , kwonlyargs= [ ] , kwonlydefaults=none , annotations=none , ) self.assertis ( argspec , tf_decorator.tfdecorator ( `` , test_function , `` , argspec ) .decorator_argspec )
__label__0 def _get_header_version ( path , name ) : `` '' '' returns preprocessor defines in c header file . '' '' '' for line in io.open ( path , `` r '' , encoding= '' utf-8 '' ) .readlines ( ) : match = re.match ( r '' \s * # \s * define % s\s+ ( \d+ ) '' % name , line ) if match : return match.group ( 1 ) return `` ''
__label__0 with self.assertraisesregex ( valueerror , `` same number of elements '' ) : nest.map_structure ( lambda x , y : none , ( 3 , 4 ) , ( 3 , 4 , 5 ) )
__label__0 # these floats should not match according to allclose try : self.assertfalse ( output_checker._allclose ( expected_floats , extracted_floats ) ) except assertionerror as e : msg = ( '\n\nthese matched ! they should not have.\n ' '\n\n expected : { } \n found : { } '.format ( expected_floats , extracted_floats ) ) e.args = ( e.args [ 0 ] + msg , ) raise e
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 def _new_mark_used ( self , * args , * * kwargs ) : object.__getattribute__ ( self , '_tf_should_use_helper ' ) .sate ( ) try : mu = object.__getattribute__ ( object.__getattribute__ ( self , '_tf_should_use_wrapped_value ' ) , 'mark_used ' ) return mu ( * args , * * kwargs ) except attributeerror : pass
__label__0 if __name__ == '__main__ ' : googletest.main ( )
__label__0 class trackablecompatibilitytests ( test.testcase ) :
__label__0 > > > @ tf.function ... def f ( x ) : ... return x > > > f_concrete = f.get_concrete_function ( tf.constant ( 1.0 ) ) > > > f_concrete = f.get_concrete_function ( x=tf.constant ( 1.0 ) )
__label__0 returns : the actual filepath the proto is written to . the filepath will be different depending on whether the proto is split , i.e. , whether it will be a pb or not. `` '' '' if self._parent_splitter is not none : raise valueerror ( `` a child composablesplitter 's ` write ` method should not be called `` `` directly , since it inherits unrelated chunks from a parent object. `` `` please call the parent 's ` write ( ) ` method instead . '' )
__label__0 # save the graph . save.save ( sess , save_path )
__label__0 # configure a server using an explicit serverdefd with an # overridden config . cluster_def = server_lib.clusterspec ( { `` localhost '' : [ `` localhost:0 '' ] } ) .as_cluster_def ( ) server_def = tensorflow_server_pb2.serverdef ( cluster=cluster_def , job_name= '' localhost '' , task_index=0 , protocol= '' grpc '' ) server = server_lib.server ( server_def , config=config , start=false ) self.assertequal ( 0.1 , server.server_def.default_session_config.gpu_options . per_process_gpu_memory_fraction )
__label__0 with ops.control_dependencies ( [ x.op for x in sharded_saves ] ) : # co-locates the merge step with the last device . with ops.device ( saveable_object_util.set_cpu0 ( last_device ) ) : # v2 format write path consists of a metadata merge step . once merged , # attempts to delete the temporary directory , `` < user-fed prefix > _temp '' . merge_step = gen_io_ops.merge_v2_checkpoints ( sharded_prefixes , checkpoint_prefix , delete_old_dirs=true ) with ops.control_dependencies ( [ merge_step ] ) : # returns the prefix `` < user-fed prefix > '' only . does not include the # sharded spec suffix . return array_ops.identity ( checkpoint_prefix )
__label__0 try : all_exports.append ( tf_export.estimator_api_name ) except attributeerror : pass
__label__0 # # # writing ` metagraphdef ` s in tf2
__label__0 args : shallow_tree : a shallow structure , common to all the inputs . func : callable which will be applied to each input individually . * inputs : structures that are compatible with shallow_tree . the function ` func ` is applied to corresponding structures due to partial flattening of each input , so the function must support arity of ` len ( inputs ) ` . * * kwargs : kwargs to feed to func ( ) . special kwarg ` check_types ` is not passed to func , but instead determines whether the types of iterables within the structures have to be same ( e.g . ` map_structure ( func , [ 1 ] , ( 1 , ) ) ` raises a ` typeerror ` exception ) . to allow this set this argument to ` false ` .
__label__0 @ property def coord ( self ) : `` '' '' return the coordinator used by the supervisor .
__label__0 return version ( major , minor , patch , identifier_string , version_type )
__label__0 callable_object = callable ( ) # smoke test : this should not raise an exception , even though # ` callable_object ` does not have a ` __name__ ` attribute . _ = tf_decorator.make_decorator ( callable_object , test_wrapper )
__label__0 self.assertallclose ( np.array ( [ 2 , 3 , maxval , 0 ] , dtype=dtype ) , self._unsortedsegmentmin ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] , dtype=dtype ) , np.array ( [ 3 , -1 , 0 , 1 , 0 , -1 , 3 ] , dtype=np.int32 ) , 4 ) ) self.assertallclose ( np.array ( [ 4 , 3 , minval , 6 ] , dtype=dtype ) , self._unsortedsegmentmax ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] , dtype=dtype ) , np.array ( [ 3 , -1 , 0 , 1 , 0 , -1 , 3 ] , dtype=np.int32 ) , 4 ) )
__label__0 return rocblas_config
__label__0 if fn is not none : return decorated ( fn ) else : return decorated
__label__0 @ parameterized.parameters ( # check examples out of tolerence . [ ' 1.001e-2 ' , [ 0 ] ] , [ ' 0.0 ' , [ 1.001e-3 ] ] , ) def test_fail_tolerences ( self , text , expected_floats ) : extract_floats = tf_doctest_lib._floatextractor ( ) output_checker = tf_doctest_lib.tfdoctestoutputchecker ( )
__label__0 def __init__ ( self , deprecation_message ) :
__label__0 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensor_slices ( [ 5. , 6. , 7. , 8 . ] ) .batch ( 2 ) > > > dataset_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > per_replica_values = strategy.experimental_local_results ( ... distributed_values ) > > > per_replica_values ( < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 5 . ] , dtype=float32 ) > , < tf.tensor : shape= ( 1 , ) , dtype=float32 , numpy=array ( [ 6 . ] , dtype=float32 ) > )
__label__0 dict_mt_nmt = { `` mt '' : mt , `` nmt '' : nmt } dict_mt_nmt_flat_paths = nest.flatten_with_joined_string_paths ( dict_mt_nmt , separator=sep ) self.assertequal ( dict_mt_nmt_flat_paths [ 0 ] [ 0 ] , `` mt/0 '' ) self.assertallequal ( dict_mt_nmt_flat_paths [ 0 ] [ 1 ] , [ 1 ] ) self.assertequal ( dict_mt_nmt_flat_paths [ 1 ] [ 0 ] , `` nmt/0/0 '' ) self.assertallequal ( dict_mt_nmt_flat_paths [ 1 ] [ 1 ] , [ 2 ] )
__label__0 these objects represent the output of an op definition and do not carry a value. `` '' '' pass
__label__0 args : node : current node `` '' '' new_aliases = [ ] import_updated = false import_renames = getattr ( self._api_change_spec , `` import_renames '' , { } ) max_submodule_depth = getattr ( self._api_change_spec , `` max_submodule_depth '' , 1 ) inserts_after_imports = getattr ( self._api_change_spec , `` inserts_after_imports '' , { } )
__label__0 @ test_util.run_all_in_graph_and_eager_modes class variableutilstest ( test.testcase ) :
__label__0 def benchmark_assert_structure ( self ) : s1 = ( ( ( 1 , 2 ) , 3 ) , 4 , ( 5 , 6 ) ) s2 = ( ( ( `` foo1 '' , `` foo2 '' ) , `` foo3 '' ) , `` foo4 '' , ( `` foo5 '' , `` foo6 '' ) ) self.run_and_report ( s1 , s2 , `` assert_same_structure_6_elem '' )
__label__0 args : meta_graph_suffix : suffix for ` metagraphdef ` file . defaults to 'meta'. `` '' '' if self._checkpoints_to_be_deleted : p = self._checkpoints_to_be_deleted.pop ( 0 ) # do not delete the file if we keep_checkpoint_every_n_hours is set and we # have reached n hours of training . should_keep = p [ 1 ] > self._next_checkpoint_time if should_keep : self._next_checkpoint_time += ( self.saver_def.keep_checkpoint_every_n_hours * 3600 ) return
__label__0 return node
__label__0 # new vocab with elements in reverse order and two new elements . new_vocab_path = self._write_vocab ( [ `` orange '' , `` guava '' , `` banana '' , `` apple '' , `` raspberry '' , `` blueberry '' ] , `` new_vocab '' ) # new session and new graph . with ops.graph ( ) .as_default ( ) as g : with self.session ( graph=g ) as sess : fruit_weights = variable_scope.get_variable ( `` fruit_weights '' , shape= [ 6 , 1 ] , initializer= [ [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] ] , partitioner=lambda shape , dtype : [ 2 , 1 ] ) ws_util._warm_start_var_with_vocab ( fruit_weights , new_vocab_path , 6 , self.get_temp_dir ( ) , prev_vocab_path ) self.evaluate ( variables.global_variables_initializer ( ) ) self.asserttrue ( isinstance ( fruit_weights , variables.partitionedvariable ) ) fruit_weights_vars = fruit_weights._get_variable_list ( ) self.assertallclose ( [ [ 2 . ] , [ 1.5 ] , [ 1 . ] ] , fruit_weights_vars [ 0 ] .eval ( sess ) ) self.assertallclose ( [ [ 0.5 ] , [ 0 . ] , [ 0 . ] ] , fruit_weights_vars [ 1 ] .eval ( sess ) )
__label__0 args : sv : a ` supervisor ` . sess : a ` session ` . step_counter : a ` tensor ` holding the step counter . by defaults , it uses sv.global_step. `` '' '' super ( svstepcounterthread , self ) .__init__ ( sv.coord , sv.save_summaries_secs ) self._sv = sv self._sess = sess self._last_time = 0.0 self._last_step = 0 step_counter = sv.global_step if step_counter is none else step_counter self._step_counter = step_counter self._summary_tag = `` % s/sec '' % self._step_counter.op.name
__label__0 # add a prefix to the node names in the current graph and restore using # remapped names . with self.session ( graph=ops_lib.graph ( ) ) as sess : v0 = variable_op ( -1.0 , name= '' restore_prefix/v0 '' ) v1 = variable_op ( -1.0 , name= '' restore_prefix/v1 '' )
__label__0 the parameter name 'varkw ' is changed to 'keywords ' to fit the ` argspec ` struct .
__label__0 def _flatten_with_slice_flattening ( self , x ) : flat = [ ] for val in nest.flatten ( x ) : if isinstance ( val , slice ) : flat.extend ( ( val.start , val.stop , val.step ) ) else : flat.append ( val ) return flat
__label__0 note : a proper structure shall form a tree . the user shall ensure there is no cyclic references within the items in the structure , i.e. , no references in the structure of the input of these functions should be recursive . the behavior is undefined if there is a cycle .
__label__0 this proto implements the ` list [ int64 ] ` portion .
__label__0 contrib_summary_comment = ( ast_edits.warning , `` tf.contrib.summary . * functions have been migrated best-effort to `` `` tf.compat.v2.summary . * equivalents where possible , but the resulting `` `` code is not guaranteed to work , so please check carefully . for more `` `` information about the new summary api , see the effective tf 2.0 `` `` migration document or check the updated tensorboard tutorials . '' )
__label__0 results = test_log_pb2.testresults ( ) results.name = name results.target = test_name results.start_time = start_time results.run_time = run_time results.benchmark_type = test_log_pb2.testresults.benchmarktype.value ( benchmark_type.upper ( ) )
__label__0 for example :
__label__0 tensortracer._overload_all_operators ( ) # pylint : disable=protected-access
__label__0 def _increment_global_step ( increment , graph=none ) : graph = graph or ops.get_default_graph ( ) global_step_tensor = get_global_step ( graph ) if global_step_tensor is none : raise valueerror ( 'global step tensor should be created by ' 'tf.train.get_or_create_global_step before calling increment . ' ) global_step_read_tensor = _get_or_create_global_step_read ( graph ) with graph.as_default ( ) as g , g.name_scope ( none ) : with g.name_scope ( global_step_tensor.op.name + '/ ' ) : with ops.control_dependencies ( [ global_step_read_tensor ] ) : return state_ops.assign_add ( global_step_tensor , increment )
__label__0 def trylock ( fd ) : try : fcntl.flock ( fd , fcntl.lock_ex | fcntl.lock_nb ) return true except exception : # pylint : disable=broad-except return false
__label__0 def testpreparesessionsucceeds ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 1.0 , 2.0 , 3.0 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.report_uninitialized_variables ( ) ) sess = sm.prepare_session ( `` '' , init_op=variables.global_variables_initializer ( ) ) self.assertallclose ( [ 1.0 , 2.0 , 3.0 ] , sess.run ( v ) )
__label__0 to print the last expression , just wrap the last expression in ` _print_if_not_none ( expr ) ` . to detect the last expression use ` ast ` . if the last node is an expression modify the ast to call ` _print_if_not_none ` on it , convert the ast back to source and compile that .
__label__0 flags.define_string ( `` code_url_prefix '' , `` /code/stable/tensorflow '' , `` a url to prepend to code paths when creating links to defining code '' )
__label__0 def clear ( self ) : self._storage.clear ( )
__label__0 finally : dispatch.unregister_dispatch_for ( unary_elementwise_api_handler )
__label__0 the log should be a tuple ` ( severity , lineno , col_offset , msg ) ` , which will be printed and recorded . it is part of the log available in the ` self.log ` property .
__label__0 def __call__ ( self ) : pass
__label__0 requires a local installation of ` tensorflow_docs ` :
__label__0 def testcond ( self ) : text = `` tf.cond ( a , b , c , true , d ) '' expected_text = `` tf.cond ( a , b , c , name=d ) '' _ , unused_report , errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text ) self.assertin ( `` tf.cond '' , errors [ 0 ] ) self.assertin ( `` requires manual check '' , errors [ 0 ] )
__label__0 def testsegmentminnumsegmentsless ( self ) : for dtype in self.int_types | self.float_types : maxval = dtypes.as_dtype ( dtype ) .max if dtype == np.float64 and self._finddevice ( `` tpu '' ) : maxval = np.inf self.assertallclose ( np.array ( [ 0 , maxval , 2 ] , dtype=dtype ) , self._segmentminv2 ( np.array ( [ 0 , 1 , 2 , 3 , 4 , 5 ] , dtype=dtype ) , np.array ( [ 0 , 0 , 2 , 3 , 3 , 3 ] , dtype=np.int32 ) , 3 ) )
__label__0 contrib_layers_layer_norm_comment = ( ast_edits.warning , `` ( manual edit required ) ` tf.contrib.layers.layer_norm ` has been `` `` deprecated , and its implementation has been integrated with `` `` ` tf.keras.layers.layernormalization ` in tensorflow 2.0. `` `` note that , the default value of ` epsilon ` is changed to ` 1e-3 ` in the `` `` new api from ` 1e-12 ` , and this may introduce numerical differences. `` `` please check the new api and use that instead . '' )
__label__0 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # fallback dispatch # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
__label__0 # copyright 2021 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' tests for tensorflow.python.util.fast_module_type . '' '' ''
__label__0 1. an accumulator is created for each variable , and each replica pushes the gradients into the accumulators instead of directly applying them to the variables . 2. each accumulator averages once enough gradients ( replicas_to_aggregate ) have been accumulated . 3. apply the averaged gradients to the variables . 4. only after all variables have been updated , increment the global step . 5. only after step 4 , pushes ` global_step ` in the ` token_queue ` , once for each worker replica . the workers can now fetch the global step , use it to update its local_step variable and start the next batch . please note that some workers can consume multiple minibatches , while some may not consume even one . this is because each worker fetches minibatches as long as a token exists . if one worker is stuck for some reason and does not consume a token , another worker can use it .
__label__0 def update_version_h ( old_version , new_version ) : `` '' '' update tensorflow/core/public/version.h . '' '' '' replace_string_in_line ( `` # define tf_major_version % s '' % old_version.major , `` # define tf_major_version % s '' % new_version.major , version_h ) replace_string_in_line ( `` # define tf_minor_version % s '' % old_version.minor , `` # define tf_minor_version % s '' % new_version.minor , version_h ) replace_string_in_line ( `` # define tf_patch_version % s '' % old_version.patch , `` # define tf_patch_version % s '' % new_version.patch , version_h ) replace_string_in_line ( `` # define tf_version_suffix \ '' % s\ '' '' % old_version.identifier_string , `` # define tf_version_suffix \ '' % s\ '' '' % new_version.identifier_string , version_h )
__label__0 @ compatibility ( eager ) exporting/importing meta graphs is not supported . no graph exists when eager execution is enabled . @ end_compatibility `` '' '' # pylint : disable=g-doc-exception return _import_meta_graph_with_return_elements ( meta_graph_or_file , clear_devices , import_scope , * * kwargs ) [ 0 ]
__label__0 args : value : int byte size `` '' '' global _max_size _max_size = value
__label__0 # the next one has the graph . ev = next ( rr ) ev_graph = graph_pb2.graphdef ( ) ev_graph.parsefromstring ( ev.graph_def ) self.assertprotoequals ( sess.graph.as_graph_def ( add_shapes=true ) , ev_graph )
__label__0 the following example shows how this decorator can be used to update all unary elementwise operations to handle a ` maskedtensor ` type :
__label__0 produce the following api_docs :
__label__0 def kwarg_only ( f : any ) - > any : `` '' '' a wrapper that throws away all non-kwarg arguments . '' '' '' f_argspec = tf_inspect.getfullargspec ( f )
__label__0 `` ` feature_lists : { feature_list : { key : `` movie_ratings '' value : { feature : { float_list : { value : [ 4.5 ] } } feature : { float_list : { value : [ 5.0 ] } } } } } `` `
__label__0 args : module : ( string ) module name . import_rename_spec : importrename instance .
__label__0 def testinvalidkwd ( self ) : with self.assertraisesregex ( typeerror , 'got an unexpected keyword argument ' ) : self._matmul_func.canonicalize ( 2 , 3 , hohoho=true )
__label__0 text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( foo ( bar ) ) ) '' ) expected_text = ( `` tf.nn.softmax_cross_entropy_with_logits ( `` `` labels=tf.stop_gradient ( foo ( bar ) ) ) '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 class callable ( object ) :
__label__0 # ! /usr/bin/python # copyright 2017 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== # # test that checks if we have any issues with case insensitive filesystems .
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_instance_fn_no_doc ( self , mock_warning ) : date = `` 2016-07-04 '' instructions = `` this is how you update ... ''
__label__0 nb : the amount of time this method waits for the session is bounded by max_wait_secs . by default , this function will wait indefinitely .
__label__0 def variables ( self ) : `` '' '' fetches a list of optimizer variables in the default graph .
__label__0 symbol_loc_info = self._tfmw_public_apis [ name ] if symbol_loc_info [ 0 ] : module = importlib.import_module ( symbol_loc_info [ 0 ] ) attr = getattr ( module , symbol_loc_info [ 1 ] ) else : attr = importlib.import_module ( symbol_loc_info [ 1 ] ) setattr ( self._tfmw_wrapped_module , name , attr ) self.__dict__ [ name ] = attr # cache the pair self._fastdict_insert ( name , attr ) return attr
__label__0 def __init__ ( self ) : self.log_level = ast_edits.error self.log_message = ( `` the tf_upgrade_v2 script detected an unaliased `` `` ` import tensorflow ` . the script can only run when `` `` importing with ` import tensorflow as tf ` . '' )
__label__0 report = ( `` tensorflow 2.0 upgrade script\n '' `` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n '' `` converted % d files\n '' % files_processed + `` detected % d issues that require attention '' % num_errors + `` \n '' + `` - '' * 80 + `` \n '' ) + `` '' .join ( report ) detailed_report_header = `` = '' * 80 + `` \n '' detailed_report_header += `` detailed log follows : \n\n '' detailed_report_header += `` = '' * 80 + `` \n ''
__label__0 def _normalize_path ( path ) : `` '' '' returns normalized path , with forward slashes on windows . '' '' '' path = os.path.realpath ( path ) if _is_windows ( ) : path = path.replace ( `` \\ '' , `` / '' ) return path
__label__0 returns : a new structure with the same arity as ` structure [ 0 ] ` , whose atoms correspond to ` func ( x [ 0 ] , x [ 1 ] , ... ) ` where ` x [ i ] ` is the atom in the corresponding location in ` structure [ i ] ` . if there are different structure types and ` check_types ` is ` false ` the structure types of the first structure will be used .
__label__0 # for the module level doc . from tensorflow.python.training import input as _input from tensorflow.python.training.input import * # pylint : disable=redefined-builtin # pylint : enable=wildcard-import
__label__0 trace = x [ : y ] # pylint : disable=invalid-slice-index self.assertequal ( str ( trace ) , `` __operators__.getitem ( % s , slice ( none , y , none ) ) '' % x )
__label__0 returns : a ` str ` object. `` '' '' if isinstance ( value , bytes ) : return as_str ( value , encoding=encoding ) else : return str ( value )
__label__0 expected_config_v2 = expected_config_v1.replace ( 'parseexample/parseexample : ' , 'parseexample/parseexamplev2 : ' )
__label__0 def func ( a=1 , b=2 ) : return ( a , b )
__label__0 for magic in magic_list : if code_line.startswith ( magic ) : return true
__label__0 text = ( `` tf.nn.dropout ( x , # stuff before\n '' `` keep_prob=.4 , # stuff after\n '' `` name=\ '' foo\ '' ) \n '' ) _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.nn.dropout ( x , # stuff before\n '' `` rate=1 - ( .4 ) , # stuff after\n '' `` name=\ '' foo\ '' ) \n '' , )
__label__0 import_header = `` from tensorflow.compat import v1 as tf\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` ` tensorflow.compat.v1 ` was directly imported as ` tf ` `` , report ) self.assertempty ( errors )
__label__0 header_path , header_version = _find_header ( base_paths , `` nccl.h '' , required_version , get_header_version ) nccl_version = header_version.split ( `` . `` ) [ 0 ]
__label__0 fieldtypes = union [ str , int , bool , sequence [ union [ str , int , bool ] ] ]
__label__0 this method runs the ops added by the constructor for saving variables . it requires a session in which the graph was launched . the variables to save must also have been initialized .
__label__0 strict_mode.enable_strict_mode ( ) with self.assertraises ( runtimeerror ) : _fn ( )
__label__0 # create one entry entity for each benchmark entry . the wall-clock timing is # the attribute to be fetched and displayed . the full entry information is # also stored as a non-indexed json blob . for ent in test_result [ `` entries '' ] .get ( `` entry '' , [ ] ) : ent_name = str ( ent [ `` name '' ] ) e_key = client.key ( `` entry '' ) e_val = datastore.entity ( e_key , exclude_from_indexes= [ `` info '' ] ) e_val.update ( { `` test '' : test_name , `` start '' : start_time , `` entry '' : ent_name , `` timing '' : ent [ `` walltime '' ] , `` info '' : str ( json.dumps ( ent ) ) } ) batch.append ( e_val )
__label__0 if __name__ == `` __main__ '' : test.main ( )
__label__0 args : a : value one of the comparison . b : value two of the comparison .
__label__0 args : obj : an object .
__label__0 self.assertallcloseaccordingtotype ( x , self.evaluate ( var ) ) sparse_apply_ftrl = ( gen_training_ops.sparse_apply_ftrl ( var , accum , linear , grad , constant_op.constant ( indices , self._totype ( indices.dtype ) ) , lr , l1 , l2 , lr_power=lr_power , multiply_linear_by_lr=true ) ) out = self.evaluate ( sparse_apply_ftrl ) self.assertshapeequal ( out , sparse_apply_ftrl )
__label__0 if self._global_step is not none and self._summary_writer : # only add the session log if we keep track of global step . # tensorboard can not use start message for purging expired events # if there is no step value . current_step = training_util.global_step ( sess , self._global_step ) self._summary_writer.add_session_log ( sessionlog ( status=sessionlog.start ) , current_step )
__label__0 import_header = `` from tensorflow.compat import v1 as tf1 , v2 as tf\n '' text = import_header + old_symbol expected_text = import_header + old_symbol _ , report , errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text ) self.assertin ( `` ` tensorflow.compat.v2 ` was directly imported as ` tf ` `` , report ) self.assertempty ( errors )
__label__0 class test ( object ) :
__label__0 return filtered_modules
__label__0 from tensorflow.python.platform import googletest from tensorflow.tools.common import test_module1 from tensorflow.tools.common import test_module2 from tensorflow.tools.common import traverse
__label__0 @ test_util.run_v2_only def testresourcesparseapplyadagradv2anddisablecopyonreadrace ( self ) : dtype = np.float32 index_type = np.int32 x_val = [ np.arange ( 10 ) , np.arange ( 10 , 20 ) , np.arange ( 20 , 30 ) ] y_val = [ np.arange ( 1 , 11 ) , np.arange ( 11 , 21 ) , np.arange ( 21 , 31 ) ] x = np.array ( x_val ) .astype ( dtype ) y = np.array ( y_val ) .astype ( dtype ) lr = np.array ( 0.001 , dtype=dtype ) epsilon = np.array ( 1e-8 , dtype=dtype ) grad_val = [ np.arange ( 10 ) , np.arange ( 10 ) ] grad = np.array ( grad_val ) .astype ( dtype ) indices = np.array ( [ 0 , 2 ] ) .astype ( index_type ) var = variables.variable ( x ) accum = variables.variable ( y ) num_iter = 1000 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 class noupdatespec ( apichangespec ) : `` '' '' a specification of an api change which does n't change anything . '' '' ''
__label__0 with self.session ( graph=graph2 ) as sess : saver3 = saver_module.saver ( var_list=new_var_list_1 , max_to_keep=1 ) saver3.restore ( sess , saver0_ckpt ) self.assertallclose ( expected , sess.run ( `` new_hidden1/relu:0 '' ) )
__label__0 def f ( a , b , kw1 , kw3 ) : ...
__label__0 example : create summaries manually every 100 steps in the chief .
__label__0 def apply_gradients ( self , grads_and_vars , global_step=none , name=none ) : `` '' '' apply gradients to variables .
__label__0 def testimportinsidefunction ( self ) : text = `` '' '' def t ( ) : from c import d from foo import baz , a from e import y `` '' '' expected_text = `` '' '' def t ( ) : from c import d from foo import baz from bar import a from e import y `` '' '' _ , new_text = self._upgrade ( renameimports ( ) , text ) self.assertequal ( expected_text , new_text )
__label__0 @ test.mock.patch.object ( logging , `` warning '' , autospec=true ) def test_class_alias ( self , mock_warning ) : class myclass ( object ) : `` '' '' my docstring . '' '' ''
__label__0 fn_has_no_kwargs = lambda x : x self.assertfalse ( function_utils.has_kwargs ( fn_has_no_kwargs ) )
__label__0 raises : valueerror : if this is called before apply_gradients ( ) . `` '' '' if self._gradients_applied is false : raise valueerror ( `` should be called after apply_gradients ( ) . '' )
__label__0 def testmissingpos ( self ) : with self.assertraisesregex ( typeerror , 'missing required positional argument ' ) : self._matmul_func.canonicalize ( 2 )
__label__0 `` ` a `` `
__label__0 raises : runtimeerror : if called with a non-chief supervisor . valueerror : if not ` logdir ` was passed to the constructor as the services need a log directory. `` '' '' if not self._is_chief : raise runtimeerror ( `` only chief supervisor can start standard services. `` `` because only chief supervisors can write events . '' )
__label__0 raises : valueerror : if ` job_name ` does not name a job in this cluster , or no task with index ` task_index ` is defined in that job. `` '' '' try : job = self._cluster_spec [ job_name ] except keyerror : raise valueerror ( `` no such job in cluster : % r '' % job_name ) return list ( sorted ( job.keys ( ) ) )
__label__0 note : the newer ` autotrackable ` api is not supported by ` saver ` . in this case , the ` tf.train.checkpoint ` class should be used .
__label__0 def _map_key_proto ( key_type , key ) : `` '' '' returns mapkey proto for a key of key_type . '' '' '' return _map_key [ key_type ] ( key )
__label__0 @ tf_export ( `` distribute.distributedvalues '' , v1= [ ] ) class distributedvalues ( object ) : `` '' '' base class for representing distributed values .
__label__0 if __name__ == `` __main__ '' : app.run ( ) # pylint : disable=no-value-for-parameter
__label__0 def isgeneratorfunction ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isgeneratorfunction . '' '' '' return _inspect.isgeneratorfunction ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 class newclass ( object ) :
__label__0 def _upgrade ( self , old_file_text ) : in_file = io.stringio ( old_file_text ) out_file = io.stringio ( ) upgrader = ast_edits.astcodeupgrader ( tf_upgrade.tfapichangespec ( ) ) count , report , errors = ( upgrader.process_opened_file ( `` test.py '' , in_file , `` test_out.py '' , out_file ) ) return count , report , errors , out_file.getvalue ( )
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' decorator_utils tests . '' '' ''
__label__0 def testbaditerableparameterserror ( self ) : fn = lambda x : [ t + 1 for t in x ] with self.assertraisesregex ( typeerror , `` iterable_parameters should be a list or tuple of string '' ) : dispatch.add_dispatch_support ( iterable_parameters= '' x '' ) ( fn )
__label__0 # pylint : disable=unused-argument def restore_op ( self , filename_tensor , saveable , preferred_shard ) : `` '' '' create ops to restore 'saveable ' .
__label__0 # forwarding a module is as simple as lazy loading the module from the new path # and then registering it to sys.modules using the old path def _forward_module ( old_name ) : parts = old_name.split ( `` . '' ) parts [ 0 ] = parts [ 0 ] + `` _core '' local_name = parts [ -1 ] existing_name = `` . `` .join ( parts ) _module = _lazyloader ( local_name , globals ( ) , existing_name ) return _sys.modules.setdefault ( old_name , _module )
__label__0 # the steps should also be initialized . self.assertallequal ( 0 , sessions [ 1 ] .run ( global_step ) ) self.assertallequal ( 0 , sessions [ 1 ] .run ( local_step_1 ) )
__label__0 1. python dict ( ordered by key ) :
__label__0 self.assertequal ( 2 , cluster_spec.num_tasks ( `` ps '' ) ) self.assertequal ( 3 , cluster_spec.num_tasks ( `` worker '' ) ) self.assertequal ( 2 , cluster_spec.num_tasks ( `` sparse '' ) ) with self.assertraises ( valueerror ) : cluster_spec.num_tasks ( `` unknown '' )
__label__0 see ` coordinator.stop_on_exception ( ) ` .
__label__0 raises : valueerror : if ready_for_local_init_op is not none but local_init_op is none `` '' '' # sets default values of arguments . if graph is none : graph = ops.get_default_graph ( ) self._local_init_op = local_init_op self._ready_op = ready_op self._ready_for_local_init_op = ready_for_local_init_op self._graph = graph self._recovery_wait_secs = recovery_wait_secs self._target = none self._local_init_run_options = local_init_run_options self._local_init_feed_dict = local_init_feed_dict if ready_for_local_init_op is not none and local_init_op is none : raise valueerror ( `` if you pass a ready_for_local_init_op `` `` you must also pass a local_init_op `` `` , ready_for_local_init_op [ % s ] '' % ready_for_local_init_op )
__label__0 flags = flags.flags
__label__0 def _list_from_env ( env_name , default= [ ] ) : `` '' '' returns comma-separated list from environment variable . '' '' '' if env_name in os.environ : return os.environ [ env_name ] .split ( `` , '' ) return default
__label__0 > > > f_concrete ( tf.constant ( 1.0 ) ) < tf.tensor : shape= ( ) , dtype=float32 , numpy=1.0 > > > > f_concrete ( x=tf.constant ( 1.0 ) ) < tf.tensor : shape= ( ) , dtype=float32 , numpy=1.0 >
__label__0 total_size = 0
__label__0 def testcontribl1 ( self ) : text = `` tf.contrib.layers.l1_regularizer ( scale ) \n '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , `` tf.keras.regularizers.l1 ( scale ) \n '' , ) self.assertnotin ( `` dropping scope '' , unused_report )
__label__0 def _verify_setup ( self ) : `` '' '' check that all is good .
__label__0 it also edits the docstring of the function : ' ( deprecated arguments ) ' is appended to the first line of the docstring and a deprecation notice is prepended to the rest of the docstring .
__label__0 flattened_input_tree = flatten_up_to ( shallow_tree , input_tree ) flattened_shallow_tree = flatten_up_to ( shallow_tree , shallow_tree )
__label__0 # extract the current frame for later use by traceback printing . try : raise valueerror ( ) except valueerror : stack_frame = sys.exc_info ( ) [ 2 ] .tb_frame.f_back
__label__0 def main ( argv ) : del argv build_docs ( output_dir=flags.output_dir , code_url_prefix=flags.code_url_prefix , search_hints=flags.search_hints )
__label__0 tfdecorator captures and exposes the wrapped target , and provides details about the current decorator. `` '' ''
__label__0 upgrade_dir = os.path.join ( self.get_temp_dir ( ) , `` foo '' ) other_dir = os.path.join ( self.get_temp_dir ( ) , `` bar '' ) output_dir = os.path.join ( self.get_temp_dir ( ) , `` baz '' ) os.mkdir ( upgrade_dir ) os.mkdir ( other_dir ) file_a = os.path.join ( other_dir , `` a.py '' ) file_b = os.path.join ( upgrade_dir , `` b.py '' )
__label__0 def wrapper ( * unused_args , * * unused_kwargs ) : pass
__label__0 def isgenerator ( object ) : # pylint : disable=redefined-builtin `` '' '' tfdecorator-aware replacement for inspect.isgenerator . '' '' '' return _inspect.isgenerator ( tf_decorator.unwrap ( object ) [ 1 ] )
__label__0 @ property def unwrapped ( self ) : return self._wrapped
__label__0 from tensorflow.tools.compatibility import all_renames_v2 from tensorflow.tools.compatibility import ast_edits from tensorflow.tools.compatibility import module_deprecations_v2 from tensorflow.tools.compatibility import reorders_v2
__label__0 returns : the expanded string. `` '' '' def replace ( match ) : if match.group ( 1 ) in cmake_vars : return cmake_vars [ match.group ( 1 ) ] return `` '' return _cmake_atvar_regex.sub ( replace , _cmake_var_regex.sub ( replace , input_str ) )
__label__0 args : new_name : new name of argument new_value : value of new argument ( or none if not used ) old_name : old name of argument old_value : value of old argument ( or none if not used )
__label__0 > > > global_batch_size = 16 > > > strategy = tf.distribute.mirroredstrategy ( [ `` gpu:0 '' , `` gpu:1 '' ] ) > > > dataset = tf.data.dataset.from_tensors ( ( [ 1 . ] , [ 2 ] ) ) .repeat ( 100 ) .batch ( global_batch_size ) > > > distributed_iterator = iter ( strategy.experimental_distribute_dataset ( dataset ) ) > > > distributed_iterator.element_spec ( perreplicaspec ( tensorspec ( shape= ( none , 1 ) , dtype=tf.float32 , name=none ) , tensorspec ( shape= ( none , 1 ) , dtype=tf.float32 , name=none ) ) , perreplicaspec ( tensorspec ( shape= ( none , 1 ) , dtype=tf.int32 , name=none ) , tensorspec ( shape= ( none , 1 ) , dtype=tf.int32 , name=none ) ) )
__label__0 raises : valueerror : if the tensors of a saveable are on different devices. `` '' '' per_device = collections.defaultdict ( lambda : [ ] ) for saveable in saveables : canonical_device = set ( pydev.canonical_name ( spec.device ) for spec in saveable.specs ) if len ( canonical_device ) ! = 1 : raise valueerror ( `` all tensors of a saveable object must be `` `` on the same device : % s '' % saveable.name ) per_device [ canonical_device.pop ( ) ] .append ( saveable ) return sorted ( per_device.items ( ) , key=lambda t : t [ 0 ] )
__label__1 def add ( a , b ) : return a + b
__label__0 # remove and recreate the path if os.path.exists ( gen_path ) : if os.path.isdir ( gen_path ) : try : shutil.rmtree ( gen_path ) except oserror : raise runtimeerror ( `` can not delete directory % s due to permission `` `` error , inspect and remove manually '' % gen_path ) else : raise runtimeerror ( `` can not delete non-directory % s , inspect `` , `` and remove manually '' % gen_path ) os.makedirs ( gen_path )
__label__0 args : wrapped_function : the function that decorated function wraps . decorator_name : the name of the decorator .
__label__0 def __init__ ( self , name ) : self.eval_count = 0 def _tensor ( ) : self.eval_count += 1 return constant_op.constant ( [ 1 . ] ) dummy_op = constant_op.constant ( [ 2 . ] ) super ( _countingsaveable , self ) .__init__ ( dummy_op , [ saver_module.basesaverbuilder.savespec ( _tensor , `` '' , name , dtype=dummy_op.dtype , device=dummy_op.device ) ] , name )
__label__1 def generate_fibonacci_sequence ( n ) : sequence = [ 0 , 1 ] while len ( sequence ) < n : sequence.append ( sequence [ -1 ] + sequence [ -2 ] ) return sequence
__label__0 `` ` python code `` `
__label__0 # unused variable names raises valueerror . with ops.graph ( ) .as_default ( ) : with self.cached_session ( ) as sess : x = variable_scope.get_variable ( `` x '' , shape= [ 4 , 1 ] , initializer=ones ( ) , partitioner=lambda shape , dtype : [ 2 , 1 ] ) self._write_checkpoint ( sess )
__label__0 if flags.list : print ( ' * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ' ) for mod in tf_modules : print ( mod.__name__ ) print ( ' * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ' ) return tests
__label__0 def testtarget__get__isproxied ( self ) :
__label__1 def reverse_string ( s ) : return s [ : :-1 ]
__label__0 def __le__ ( self , other : set [ any ] ) - > bool : if not isinstance ( other , set ) : return notimplemented if len ( self ) > len ( other ) : return false for item in self._storage : if item not in other : return false return true
__label__0 # save checkpoint from which to warm-start . self._create_prev_run_var ( `` linear_model/sc_vocab/weights '' , shape= [ 2 , 1 ] , initializer=ones ( ) )
__label__0 def testisolatesessionstate ( self ) : server = self._cached_server
__label__0 class foohaskwargs ( object ) :
__label__0 class removedeprecatedaliaskeyword ( ast_edits.noupdatespec ) : `` '' '' a specification where kw1_alias is removed in g .
__label__0 class exampleparserconfigurationtest ( test.testcase ) :
__label__0 # # # # registered apis
__label__0 class foo ( object ) :
__label__0 args : p : ( filename , time ) pair .
__label__0 def teststartqueuerunnersignoresmonitoredsession ( self ) : zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) init_op = variables.global_variables_initializer ( ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) queue_runner_impl.add_queue_runner ( qr ) with self.cached_session ( ) : init_op.run ( ) threads = queue_runner_impl.start_queue_runners ( monitored_session.monitoredsession ( ) ) self.assertfalse ( threads )
__label__0 def testwarmstart_sparsecolumnintegerized ( self ) : # create feature column . sc_int = fc.categorical_column_with_identity ( `` sc_int '' , num_buckets=10 )
__label__0 else : cls = none constructor_name = none func = func_or_class
__label__0 import numpy as np
__label__0 def test_double_partial ( self ) : expected_test_arg1 = 123 expected_test_arg2 = 456
__label__0 note : this modified program still works fine as a single program . the single program marks itself as the chief .
__label__0 input_tree = [ `` input_tree_0 '' , `` input_tree_1 '' ] shallow_tree = `` shallow_tree '' ( flattened_input_tree_paths , flattened_input_tree ) = get_paths_and_values ( shallow_tree , input_tree ) ( flattened_shallow_tree_paths , flattened_shallow_tree ) = get_paths_and_values ( shallow_tree , shallow_tree ) self.assertequal ( flattened_input_tree_paths , [ ( ) ] ) self.assertequal ( flattened_input_tree , [ input_tree ] ) self.assertequal ( flattened_shallow_tree_paths , [ ( ) ] ) self.assertequal ( flattened_shallow_tree , [ shallow_tree ] )
__label__0 self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 s2 = save2.save ( none , os.path.join ( save_dir , `` s2 '' ) ) self.assertequal ( [ s3 , s2 ] , save2.last_checkpoints ) # created by the first helper . self.asserttrue ( checkpoint_management.checkpoint_exists ( s1 ) ) # deleted by the first helper . self.assertfalse ( checkpoint_management.checkpoint_exists ( s3 ) )
__label__0 returns : a sessionmanager object. `` '' '' return self._session_manager
__label__0 def _find_rocsolver_config ( rocm_install_path ) :
__label__0 def placeholder_value ( self , placeholder_context=none ) : return self.fruit_value
__label__0 # converts all symbols in the v1 namespace to the v2 namespace , raising # an error if the target of the conversion is not in the v1 namespace . def conversion_visitor ( unused_path , unused_parent , children ) : for child in children : _ , attr = tf_decorator.unwrap ( child [ 1 ] ) api_names = tf_export.get_v1_names ( attr ) for name in api_names : if collect : v1_symbols.add ( `` tf . '' + name ) else : _ , _ , _ , text = self._upgrade ( `` tf . '' + name ) if ( text and not text.startswith ( `` tf.compat.v1 '' ) and not text.startswith ( `` tf.compat.v2 '' ) and text not in v1_symbols ) : self.assertfalse ( true , `` symbol % s generated from % s not in v1 api '' % ( text , name ) )
__label__0 args : dtype : data type for the returned tensor name : a name for the operations which create the tensor returns : a tensor. `` '' '' pass
__label__0 text = `` tf.manip.batch_to_space_nd ( input , block_shape , crops , name ) '' expected_text = `` tf.batch_to_space ( input , block_shape , crops , name ) '' _ , unused_report , unused_errors , new_text = self._upgrade ( text ) self.assertequal ( new_text , expected_text )
__label__0 the ` inputs ` , can be thought of as having the same structure layout as ` shallow_tree ` , but with leaf nodes that are themselves tree structures .
__label__0 def _init_saver ( self , saver=use_default ) : `` '' '' initializes saver .
__label__0 with self.assertraisesregex ( valueerror , ( `` do n't have the same nested structure\\.\n\n '' `` first structure : . * ? \n\nsecond structure : `` ) ) : nest.assert_same_structure ( nesttest.named0ab ( 3 , 4 ) , nesttest.named0ab ( [ 3 ] , 4 ) )
__label__0 def _same_value ( a , b ) : `` '' '' a comparison operation that works for multiple object types .
__label__0 returns : a string containing a session target for this server. `` '' '' return c_api.tf_servertarget ( self._server )
__label__0 returns : a ' . '-delimited full-name or none if node was not attribute or name . i.e . ` foo ( ) +b ) .bar ` returns none , while ` a.b.c ` would return `` a.b.c '' . `` '' '' curr = node items = [ ] while not isinstance ( curr , ast.name ) : if not isinstance ( curr , ast.attribute ) : return none items.append ( curr.attr ) curr = curr.value items.append ( curr.id ) return `` . `` .join ( reversed ( items ) )
__label__0 from tensorflow.tools.docs import tf_doctest_lib
__label__0 from tensorflow.tools.pip_package.utils.utils import copy_file from tensorflow.tools.pip_package.utils.utils import create_init_files from tensorflow.tools.pip_package.utils.utils import is_macos from tensorflow.tools.pip_package.utils.utils import is_windows from tensorflow.tools.pip_package.utils.utils import replace_inplace
__label__0 - a ` featurelist l ` may be missing in an example ; it is up to the parser configuration to determine if this is allowed or considered an empty list ( zero length ) . - if a ` featurelist l ` exists , it may be empty ( zero length ) . - if a ` featurelist l ` is non-empty , all features within the ` featurelist ` must have the same data type ` t ` . even across ` sequenceexample ` s , the type ` t ` of the ` featurelist ` identified by the same key must be the same . an entry without any values may serve as an empty feature . - if a ` featurelist l ` is non-empty , it is up to the parser configuration to determine if all features within the ` featurelist ` must have the same size . the same holds for this ` featurelist ` across multiple examples . - for sequence modeling ( [ example ] ( https : //github.com/tensorflow/nmt ) ) , the feature lists represent a sequence of frames . in this scenario , all ` featurelist ` s in a ` sequenceexample ` have the same number of ` feature ` messages , so that the i-th element in each ` featurelist ` is part of the i-th frame ( or time step ) .
__label__0 def rocblas_version_numbers ( path ) : possible_version_files = [ `` include/rocblas/internal/rocblas-version.h '' , # rocm 5.2 `` rocblas/include/internal/rocblas-version.h '' , # rocm 5.1 and prior ] version_file = none for f in possible_version_files : version_file_path = os.path.join ( path , f ) if os.path.exists ( version_file_path ) : version_file = version_file_path break if not version_file : raise configerror ( `` rocblas version file not found in { } '' .format ( possible_version_files ) ) major = _get_header_version ( version_file , `` rocblas_version_major '' ) minor = _get_header_version ( version_file , `` rocblas_version_minor '' ) patch = _get_header_version ( version_file , `` rocblas_version_patch '' ) return major , minor , patch
__label__0 opt = rmsprop.rmspropoptimizer ( learning_rate=2.0 , decay=0.9 , momentum=0.5 , epsilon=1e-5 ) update = opt.apply_gradients ( zip ( [ grads0 , grads1 ] , [ var0 , var1 ] ) ) self.evaluate ( variables.global_variables_initializer ( ) )
__label__0 examples :
__label__0 returns : a saver constructed from ` saver_def ` in ` metagraphdef ` or none .
__label__0 def gather_hostname ( ) : return socket.gethostname ( )
__label__0 def build_wheel ( dir_path : str , cwd : str , project_name : str , collab : str = false ) - > none : `` '' '' build the wheel in the target directory . args : dir_path : directory where the wheel will be stored cwd : path to directory with wheel source files project_name : name to pass to setup.py . collab : defines if this is a collab build `` '' '' env = os.environ.copy ( ) if is_windows ( ) : # homepath is not set by bazel but it 's required by setuptools . env [ `` homepath '' ] = `` c : '' # project_name is needed by setup.py . env [ `` project_name '' ] = project_name
__label__0 class objectidentityset ( collections_abc.mutableset ) : `` '' '' like the built-in set , but compares objects with `` is '' . '' '' ''
__label__0 ops.register_proto_function ( ops.graphkeys.savers , proto_type=saver_pb2.saverdef , to_proto=saver.to_proto , from_proto=saver.from_proto )
__label__0 def testwritenochunks ( self ) : path = os.path.join ( self.create_tempdir ( ) , `` split-none '' ) proto = test_message_pb2.repeatedstring ( strings= [ `` a '' , `` bc '' , `` de '' ] ) returned_path = noopsplitter ( proto ) .write ( path )
__label__0 class validateexporttest ( test.testcase ) : `` '' '' tests for tf_export class . '' '' ''
__label__0 info = `` info '' warning = `` warning '' error = `` error ''
__label__0 @ dispatch.dispatch_for_unary_elementwise_apis ( maskedtensor ) def another_handler ( api_func , x ) : return maskedtensor ( api_func ( x.values ) , ~x.mask )
__label__0 num_files = len ( list ( output_dir.rglob ( `` * '' ) ) ) if num_files < min_num_files_expected : raise valueerror ( f '' the tensorflow api should be more than { min_num_files_expected } files '' f '' ( found { num_files } ) . '' )
__label__0 def __init__ ( self , local_init_op : ops.operation = none , ready_op : ops.operation = none , ready_for_local_init_op : ops.operation = none , graph : ops.graph = none , recovery_wait_secs=30 , local_init_run_options : `` distribute_lib.runoptions '' = none , local_init_feed_dict=none , ) : `` '' '' creates a sessionmanager .
__label__0 def parse_results ( lines ) : `` '' '' parses benchmark results from run_onednn_benchmarks.sh .
__label__0 return { `` cusparse_version '' : header_version , `` cusparse_include_dir '' : os.path.dirname ( header_path ) , `` cusparse_library_dir '' : os.path.dirname ( library_path ) , }
__label__0 def _partitioner ( shape , dtype ) : # pylint : disable=unused-argument # partition each var into 2 equal slices . partitions = [ 1 ] * len ( shape ) partitions [ 0 ] = min ( 2 , shape.dims [ 0 ] .value ) return partitions
__label__0 @ tf_export ( `` distribute.distributeddataset '' , v1= [ ] ) class distributeddatasetinterface ( iterable ) : # pylint : disable=line-too-long `` '' '' represents a dataset distributed among devices and machines .
__label__0 def _new__setattr__ ( self , key , value ) : if key in ( '_tf_should_use_helper ' , '_tf_should_use_wrapped_value ' ) : return object.__setattr__ ( self , key , value ) return setattr ( object.__getattribute__ ( self , '_tf_should_use_wrapped_value ' ) , key , value )
__label__0 note : overriding ` __doc__ ` is only possible after python 3.7 .
__label__0 def testrecoversessionwithreadyforlocalinitopfailstoreadylocal ( self ) : # we use ready_for_local_init_op=report_uninitialized_variables ( ) , # which causes recover_session to not run local_init_op , and to return # initialized=false
__label__0 import astor
__label__0 # copyright 2016 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' library for getting system information during tensorflow tests . '' '' ''
__label__0 with sess.graph.device ( `` /cpu:1 '' ) : ds1 = dataset_ops.dataset.range ( 20 ) it1 = dataset_ops.make_initializable_iterator ( ds1 ) get_next1 = it1.get_next ( ) saveable1 = iterator_ops._iteratorsaveable ( it1._iterator_resource , name= '' saveable_it1 '' ) saver = saver_module.saver ( { `` it0 '' : saveable0 , `` it1 '' : saveable1 } , write_version=self._write_version , sharded=true ) self.evaluate ( it0.initializer ) self.evaluate ( it1.initializer ) saver.restore ( sess , save_path ) self.assertequal ( 2 , self.evaluate ( get_next0 ) ) self.assertequal ( 1 , self.evaluate ( get_next1 ) )
__label__0 _do_not_doc = `` _tf_docs_do_not_document ''
__label__0 from tensorflow.python.platform import test from tensorflow.python.util import lock_util
__label__0 def testpreparesessionsucceedswithinitfn ( self ) : with ops.graph ( ) .as_default ( ) : v = variable_v1.variablev1 ( [ 125 ] , name= '' v '' ) sm = session_manager.sessionmanager ( ready_op=variables.assert_variables_initialized ( ) ) sess = sm.prepare_session ( `` '' , init_fn=lambda sess : sess.run ( v.initializer ) ) self.assertallclose ( [ 125 ] , sess.run ( v ) )
__label__0 splits = fh.get_int_list ( ) values = fh.get_int_or_float_list ( ) weights = fh.get_int_list ( ) try : _ , _ , _ , = tf.raw_ops.raggedcountsparseoutput ( splits=splits , values=values , weights=weights , binary_output=false ) except tf.errors.invalidargumenterror : pass
__label__0 > > > structure = [ ' a ' ] > > > flat_sequence = [ np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ] > > > tf.nest.pack_sequence_as ( structure , flat_sequence ) [ array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ]
__label__0 def testdispatcherrorforunknownparameter ( self ) : with self.assertraisesregex ( valueerror , `` signature includes annotation for unknown parameter ' z ' . `` ) :
__label__0 def func ( m , n , l , k=4 ) : return 2 * m + l + n * k
__label__0 def testaddshouldusewarningwhenusedwithadd ( self ) : def add ( h ) : _ = h + 1 self._testaddshouldusewarningwhenused ( add , name='blah_add ' ) gc.collect ( ) self.assertfalse ( gc.garbage )
__label__0 # attempts to modify the property via an instance will fail . with self.assertraises ( attributeerror ) : myclass ( ) .value = 12 with self.assertraises ( attributeerror ) : del myclass ( ) .value
__label__0 args : * args : arguments for get_slot ( ) . * * kwargs : keyword arguments for get_slot ( ) .
__label__0 def __init__ ( self ) : self.call_log = [ ]
__label__0 def _safe_close ( self , sess : session.session ) : `` '' '' closes a session without raising an exception .
__label__0 def unwrap ( maybe_tf_decorator ) : `` '' '' unwraps an object into a list of tfdecorators and a final target .
__label__0 if self._tfmw_module_name : full_name = 'tf. % s. % s ' % ( self._tfmw_module_name , name ) else : full_name = 'tf. % s ' % name rename = get_rename_v2 ( full_name ) if rename and not has_deprecation_decorator ( attr ) : call_location = _call_location ( ) # skip locations in python source if not call_location.startswith ( ' < ' ) : logging.warning ( 'from % s : the name % s is deprecated . please use % s instead.\n ' , _call_location ( ) , full_name , rename ) self._tfmw_warning_count += 1 return true return false
__label__0 def _is_ast_false ( node ) : if hasattr ( ast , `` nameconstant '' ) : return isinstance ( node , ast.nameconstant ) and node.value is false else : return isinstance ( node , ast.name ) and node.id == `` false ''
__label__0 returns : a python list , the flattened version of the input .
__label__0 self.assertequal ( argspec , tf_inspect.getargspec ( newclass ) )
__label__0 if major_minor_change ( old_version , new_version ) : old_r_major_minor = `` r % s. % s '' % ( old_version.major , old_version.minor ) check_for_lingering_string ( old_r_major_minor )
__label__0 args : modality : enum value of supported modality [ modality.core or modality.data ] structure : the value to test .
__label__0 for name in self._names : _name_to_symbol_mapping [ name ] = func for name_v1 in self._names_v1 : _name_to_symbol_mapping [ 'compat.v1. % s ' % name_v1 ] = func
__label__0 def gather_machine_configuration ( ) : `` '' '' gather machine configuration . this is the top level fn of this library . '' '' '' config = test_log_pb2.machineconfiguration ( )
__label__0 def gather_cpu_info ( ) : `` '' '' gather cpu information . assumes all cpus are the same . '' '' '' cpu_info = test_log_pb2.cpuinfo ( ) cpu_info.num_cores = multiprocessing.cpu_count ( )
__label__0 returns : an ` tf.experimental.optional ` object representing the next value from the ` tf.distribute.distributediterator ` ( if it has one ) or no value. `` '' '' # pylint : enable=line-too-long raise notimplementederror ( `` get_next_as_optional ( ) not implemented in descendants '' )
__label__0 from tensorflow.python.eager import context from tensorflow.python.framework import composite_tensor from tensorflow.python.framework import constant_op from tensorflow.python.framework import ops from tensorflow.python.framework import tensor from tensorflow.python.framework import test_util from tensorflow.python.ops import resource_variable_ops from tensorflow.python.ops import variables from tensorflow.python.platform import test from tensorflow.python.util import nest from tensorflow.python.util import variable_utils
__label__0 # python 2.x __nonzero__ = __bool__
__label__0 this starts services in the background . the services started depend on the parameters to the constructor and may include :
__label__0 try : field_proto = field_proto [ map_key ] if field_proto is not none else none except keyerror : field_proto = none i += 1
__label__0 returns : an object wrapping ` x ` , of type ` type ( x ) ` . `` '' '' type_x = type ( x ) memoized = _wrappers.get ( type_x , none ) if memoized : return memoized ( x , tf_should_use_helper )
__label__0 from tensorflow.python.platform import test from tensorflow.python.platform import tf_logging as logging from tensorflow.python.util import tf_decorator from tensorflow.python.util import tf_inspect
__label__0 def _getfullargspec ( target ) : `` '' '' a python2 version of getfullargspec .
__label__0 def func_without_decorator ( a , b ) : return a + b
__label__0 from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import variable_v1 from tensorflow.python.platform import test from tensorflow.python.training import monitored_session from tensorflow.python.training import training_util
__label__0 def _tf_data_pack_sequence_as ( structure , flat_sequence ) : `` '' '' returns a given flattened sequence packed into a nest .
__label__0 def wrapper ( wrapper_func ) : return tf_decorator.make_decorator ( wrapped_function , wrapper_func , decorator_name )
__label__0 def __call__ ( self , func : t ) - > t : `` '' '' calls this decorator .
__label__0 flat_path_nested_list = nest.flatten_up_to ( shallow_tree=nmt , input_tree=nested_list , check_types=false ) self.assertallequal ( flat_path_nested_list , [ 2 ] )
__label__0 # copyright 2020 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' this is a python api fuzzer for tf.constant . '' '' '' import atheris with atheris.instrument_imports ( ) : import sys import tensorflow as tf
__label__0 returns : a list-like framesummary containing stackframe-like objects , which are namedtuple-like objects with the following fields : filename , lineno , name , line , meant to masquerade as traceback.framesummary objects. `` '' '' thread_key = _get_thread_key ( ) return _tf_stack.extract_stack ( _source_mapper_stacks [ thread_key ] [ -1 ] .internal_map , _source_filter_stacks [ thread_key ] [ -1 ] .internal_set , stacklevel , )
__label__0 # compatibility with python 2 : # python 2 unbound methods have type checks for the first arg , # so we need to extract the underlying function tensor_oper = getattr ( tensor_oper , `` __func__ '' , tensor_oper ) setattr ( cls , operator , tensor_oper )
__label__0 def deprecated ( date , instructions , warn_once=true ) : `` '' '' decorator for marking functions or methods deprecated .
__label__0 text = `` import tensorflow as tf , other_import as y '' expected_text = ( `` import tensorflow.compat.v1 as tf , other_import as y '' ) _ , _ , _ , new_text = self._upgrade ( text ) self.assertequal ( expected_text , new_text )
__label__0 sometimes we may wish to partially flatten a structure , retaining some of the nested structure . we achieve this by specifying a shallow structure , ` shallow_tree ` , we wish to flatten up to .
__label__0 # copyright 2023 the tensorflow authors . all rights reserved . # # licensed under the apache license , version 2.0 ( the `` license '' ) ; # you may not use this file except in compliance with the license . # you may obtain a copy of the license at # # http : //www.apache.org/licenses/license-2.0 # # unless required by applicable law or agreed to in writing , software # distributed under the license is distributed on an `` as is '' basis , # without warranties or conditions of any kind , either express or implied . # see the license for the specific language governing permissions and # limitations under the license . # ============================================================================== `` '' '' version constants . '' '' ''
__label__0 if output_root_directory == root_directory : return self.process_tree_inplace ( root_directory )
__label__0 nmt = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 4 ] ) ) nmt2 = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=true , inner_value=constant_op.constant ( [ 5 ] ) ) nmt_out = nestedmaskedtensor.nested_masked_tensor_with_opposite_masks ( mask=false , inner_value=constant_op.constant ( [ 6 ] ) ) nmt_combined_with_path = nest.map_structure_up_to ( nmt_out , sum_tensors , nmt , nmt2 ) self.assertisinstance ( nmt_combined_with_path , nestedmaskedtensor ) self.assertequal ( nmt_combined_with_path.mask , false ) self.assertequal ( nmt_combined_with_path.value.mask , true ) self.assertallequal ( nmt_combined_with_path.value.value , [ 9 ] )
__label__0 args : filename_tensor : tensor for the path of the file to load . saveables : a list of saveableobject objects . restore_sequentially : true if we want to restore variables sequentially within a shard . reshape : true if we want to reshape loaded tensors to the shape of the corresponding variable . preferred_shard : shard to open first when loading a sharded file . name : name for the returned op .
__label__0 def testboundfuncwithoneparam ( self ) :
__label__0 raises : typeerror : if ` func ` is not callable or if the structures do not match each other by depth tree . typeerror : if ` check_types ` is not ` false ` and the two structures differ in the type of sequence in any of their substructures . valueerror : if no structures are provided. `` '' '' return nest_util.map_structure_up_to ( nest_util.modality.core , structure [ 0 ] , func , * structure , * * kwargs )
__label__0 def save_op ( self , filename_tensor , saveables ) : `` '' '' create an op to save 'saveables ' .
__label__0 args : names_to_saveables : a dictionary mapping name to a variable or saveableobject . each name will be associated with the corresponding variable in the checkpoint . reshape : if true , allow restoring parameters from a checkpoint that where the parameters have a different shape . this is only needed when you try to restore from a dist-belief checkpoint , and only some times . sharded : if true , shard the checkpoints , one per device that has variable nodes . max_to_keep : maximum number of checkpoints to keep . as new checkpoints are created , old ones are deleted . if none or 0 , no checkpoints are deleted from the filesystem but only the last one is kept in the ` checkpoint ` file . presently the number is only roughly enforced . for example in case of restarts more than max_to_keep checkpoints may be kept . keep_checkpoint_every_n_hours : how often checkpoints should be kept . defaults to 10,000 hours . name : string . optional name to use as a prefix when adding operations . restore_sequentially : a bool , which if true , causes restore of different variables to happen sequentially within each device . filename : if known at graph construction time , filename used for variable loading/saving . if none , then the default name `` model '' will be used .
__label__0 def testbasic ( self ) : with self.cached_session ( ) as sess : # countupto will raise out_of_range when it reaches the count . zero64 = constant_op.constant ( 0 , dtype=dtypes.int64 ) var = variable_v1.variablev1 ( zero64 ) count_up_to = var.count_up_to ( 3 ) queue = data_flow_ops.fifoqueue ( 10 , dtypes.float32 ) self.evaluate ( variables.global_variables_initializer ( ) ) qr = queue_runner_impl.queuerunner ( queue , [ count_up_to ] ) threads = qr.create_threads ( sess ) self.assertequal ( sorted ( t.name for t in threads ) , [ `` queuerunnerthread-fifo_queue-countupto:0 '' ] ) for t in threads : t.start ( ) for t in threads : t.join ( ) self.assertequal ( 0 , len ( qr.exceptions_raised ) ) # the variable should be 3. self.assertequal ( 3 , self.evaluate ( var ) )
